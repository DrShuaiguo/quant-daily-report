import os
import json
import datetime
import requests
import smtplib
import arxiv
from email.mime.text import MIMEText
from email.header import Header
from openai import OpenAI
from serpapi import GoogleSearch

# ==========================================
#              1. å…¨å±€é…ç½®åŒºåŸŸ (CONFIG)
# ==========================================

CONFIG = {
    # --- åŸºç¡€è®¾ç½® ---
    "DATA_FILE": "data/reports.json", 
    "MAX_HISTORY": 500,               
    "MIN_SCORE": 5.0,                 # é—¨æ§›åˆ†
    "PUSH_THRESHOLD": 6.0,            # æ¨é€åˆ†
    "FINAL_SAVE_COUNT": 15,           
    
    # --- ArXiv (å›½é™…è®ºæ–‡) è®¾ç½® ---
    "FETCH_COUNT_ARXIV": 30,          
    "ARXIV_KEYWORDS": [
        "quantitative finance",
        "factor model",
        "portfolio optimization",
        "deep learning trading",      
        "reinforcement learning trading", 
        "machine learning trading",   
        "algorithm trading",          
        "market microstructure",
        "risk premia"
    ],
    
    # --- Google Scholar è®¾ç½® ---
    "GOOGLE_QUERIES": [
        'quantitative trading "reinforcement learning" after:2024', 
        'quantitative trading "deep learning" after:2024',          
        '"algorithmic trading" strategy after:2024'                 
    ],
    "FETCH_COUNT_GOOGLE_PER_QUERY": 5, 
}

# ==========================================
#              2. ç¯å¢ƒå˜é‡åŠ è½½
# ==========================================
LLM_API_KEY = os.environ.get("LLM_API_KEY")
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")

# åˆå§‹åŒ– AI (DeepSeek)
client = OpenAI(api_key=LLM_API_KEY, base_url="https://api.deepseek.com")

# ==========================================
#              3. æ ¸å¿ƒæŠ“å–é€»è¾‘
# ==========================================

def fetch_arxiv():
    """æŠ“å– ArXiv (ä¿®å¤ç‰ˆï¼šé€‚é… arxiv åº“æ–°ç‰ˆæœ¬)"""
    print(f"--- æ­£åœ¨æŠ“å– ArXiv ---")
    
    # æ„é€ æŸ¥è¯¢è¯­å¥
    keywords_query = " OR ".join([f'"{k}"' for k in CONFIG['ARXIV_KEYWORDS']])
    query = f'(cat:q-fin.* OR cat:cs.AI) AND ({keywords_query})'
    
    try:
        search = arxiv.Search(
            query=query,
            max_results=CONFIG['FETCH_COUNT_ARXIV'],
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        results = []
        for r in search.results():
            # === ä¿®å¤ç‚¹å¼€å§‹ ===
            # æ–°ç‰ˆ arxiv åº“ä¸­ï¼Œr.categories æœ¬èº«å°±æ˜¯ ['q-fin.CP', 'cs.AI'] è¿™æ ·çš„å­—ç¬¦ä¸²åˆ—è¡¨
            # æ‰€ä»¥ç›´æ¥åˆ¤æ–­å­—ç¬¦ä¸²å³å¯ï¼Œä¸éœ€è¦ .term
            if not any(tag.startswith(('q-fin', 'cs', 'stat')) for tag in r.categories):
                continue
            # === ä¿®å¤ç‚¹ç»“æŸ ===

            results.append({
                "title": r.title,
                "url": r.pdf_url,
                "source": "ArXiv",
                "date": r.published.strftime("%Y-%m-%d"),
                "abstract": r.summary,
                "broker": "Cornell Univ" 
            })
        print(f"ArXiv æŠ“å–åˆ° {len(results)} æ¡")
        return results
    except Exception as e:
        print(f"ArXiv æŠ“å–å¤±è´¥: {e}")
        # ä¸ºäº†è°ƒè¯•ï¼Œæ‰“å°ä¸€ä¸‹é”™è¯¯è¯¦æƒ…
        import traceback
        traceback.print_exc()
        return []

def fetch_google_scholar():
    """æŠ“å– Google Scholar"""
    if not SERPAPI_KEY:
        print("æœªé…ç½® SERPAPI_KEYï¼Œè·³è¿‡")
        return []
        
    print(f"--- æ­£åœ¨æŠ“å– Google Scholar ---")
    all_results = []
    
    for query in CONFIG['GOOGLE_QUERIES']:
        try:
            print(f"æœç´¢: {query} ...")
            params = {
                "engine": "google_scholar",
                "q": query,
                "api_key": SERPAPI_KEY,
                "num": CONFIG['FETCH_COUNT_GOOGLE_PER_QUERY'],
                "hl": "en" # å¼ºåˆ¶è‹±æ–‡ç»“æœ
            }
            search = GoogleSearch(params)
            data = search.get_dict()
            organic_results = data.get("organic_results", [])
            
            for item in organic_results:
                if 'link' not in item: continue
                all_results.append({
                    "title": item.get("title"),
                    "url": item.get("link"),
                    "source": "Scholar",
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "abstract": item.get("snippet", item.get("title")),
                    "broker": "Google Scholar"
                })
        except Exception as e:
            print(f"Scholar æŸ¥è¯¢å‡ºé”™: {e}")
            
    return all_results

# ==========================================
#              4. æ™ºèƒ½åˆ†æä¸ç¿»è¯‘ (æ ¸å¿ƒä¿®æ”¹)
# ==========================================

def analyze_with_llm(item):
    """
    è°ƒç”¨ AIï¼š
    1. è¯„åˆ†
    2. ç¿»è¯‘æ‘˜è¦ä¸ºä¸­æ–‡
    """
    try:
        # --- è¿™é‡Œçš„ Prompt ä¿®æ”¹äº† ---
        prompt = f"""
        ä½ æ˜¯ä¸€åèµ„æ·±çš„é‡åŒ–äº¤æ˜“ç ”ç©¶å‘˜ã€‚è¯·é˜…è¯»ä»¥ä¸‹è‹±æ–‡è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ã€‚
        
        ä»»åŠ¡ï¼š
        1. è¯„åˆ† (0-10åˆ†)ï¼š"å¼ºåŒ–å­¦ä¹ /æ·±åº¦å­¦ä¹ +äº¤æ˜“"ç±»è®ºæ–‡ç»™8åˆ†ä»¥ä¸Šï¼Œçº¯ç†è®ºæ•°å­¦ç»™4åˆ†ä»¥ä¸‹ã€‚
        2. ä¸­æ–‡æ‘˜è¦ï¼šè¯·å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆé€šä¿—æµç•…çš„ä¸­æ–‡ã€‚ç¿»è¯‘æ—¶è¯·ä¿ç•™å…³é”®çš„ç®—æ³•åç§°ï¼ˆå¦‚ Transformer, LSTM, PPO ç­‰ï¼‰ä¸ç¿»è¯‘ï¼Œç¡®ä¿é‡åŒ–åŒè¡Œèƒ½çœ‹æ‡‚ã€‚
        
        è®ºæ–‡æ ‡é¢˜: {item['title']}
        åŸæ–‡æ‘˜è¦: {item['abstract'][:1200]}
        
        è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¿”å›ï¼š
        {{
            "score": <æ•°å­—>,
            "summary": "<è¿™é‡Œå¡«ä¸­æ–‡ç¿»è¯‘å†…å®¹ï¼Œä¸è¦åªå†™ä¸€å¥è¯ï¼Œè¦å®Œæ•´æ¦‚æ‹¬æ ¸å¿ƒé€»è¾‘ï¼Œå­—æ•°æ§åˆ¶åœ¨100-300å­—>"
        }}
        """
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"LLM åˆ†æå¤±è´¥: {e}")
        return {"score": 6.0, "summary": "AI ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹åŸæ–‡ã€‚"}

def send_dingtalk(msg_markdown):
    """å‘é€é’‰é’‰æ¶ˆæ¯ (è°ƒè¯•ç‰ˆ)"""
    if not DINGTALK_WEBHOOK: 
        print(">>> è­¦å‘Š: æœªé…ç½® DINGTALK_WEBHOOKï¼Œè·³è¿‡é’‰é’‰æ¨é€")
        return
    
    try:
        headers = {"Content-Type": "application/json"}
        data = {
            "msgtype": "markdown",
            "markdown": {
                "title": "é‡åŒ–æ—¥æŠ¥æ¨é€", # æ³¨æ„ï¼šå¦‚æœä½ çš„å…³é”®è¯è®¾ä¸º'é‡åŒ–'ï¼Œè¿™ä¸ªæ ‡é¢˜èƒ½å‘½ä¸­
                "text": msg_markdown
            }
        }
        
        # å‘é€è¯·æ±‚
        response = requests.post(DINGTALK_WEBHOOK, json=data)
        
        # === å…³é”®ä¿®æ”¹ï¼šæ‰“å°é’‰é’‰æœåŠ¡å™¨çš„å›å¤ ===
        print(f"é’‰é’‰å‘é€çŠ¶æ€ç : {response.status_code}")
        print(f"é’‰é’‰å“åº”å†…å®¹: {response.text}")
        
    except Exception as e:
        print(f"é’‰é’‰è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {e}")

def send_email(subject, html_content):
    """å‘é€é‚®ä»¶"""
    if not EMAIL_USER or not EMAIL_PASS: return
    try:
        msg = MIMEText(html_content, 'html', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_USER 
        
        smtp = smtplib.SMTP_SSL('smtp.qq.com', 465)
        smtp.login(EMAIL_USER, EMAIL_PASS)
        smtp.send_message(msg)
        smtp.quit()
    except Exception as e:
        print(f"é‚®ä»¶å‘é€å¤±è´¥: {e}")

# ==========================================
#              5. ä¸»ç¨‹åºå…¥å£ (é€»è¾‘ä¿®å¤ç‰ˆ)
# ==========================================

def main():
    print(">>> ä»»åŠ¡å¼€å§‹")
    
    # 1. åŠ è½½å†å² (é¿å…é‡å¤)
    history_ids = []
    if os.path.exists(CONFIG["DATA_FILE"]):
        try:
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                old_data = json.load(f)
                history_ids = [item.get('title') for item in old_data]
        except: pass

    # 2. æŠ“å– (ArXiv + Scholar)
    # å»ºè®®ï¼šä¸ºäº†é˜²æ­¢ ArXiv å¤ªå¤šï¼Œå¯ä»¥æŠŠ Scholar æ”¾åœ¨å‰é¢ï¼Œæˆ–è€…æ‰“ä¹±é¡ºåº
    # ä½†æœ€ç¨³å¦¥çš„è¿˜æ˜¯â€œå…¨éƒ¨åˆ†æï¼Œæœ€åæ’åºâ€
    raw_items = []
    raw_items += fetch_google_scholar() # æŠŠ Scholar æ”¾åˆ°å‰é¢æŠ“
    raw_items += fetch_arxiv()
    
    print(f">>> å…±æŠ“å–åˆ° {len(raw_items)} æ¡ï¼Œå¼€å§‹ AI ç¿»è¯‘ä¸è¯„åˆ†...")
    print(">>> æ³¨æ„ï¼šå°†å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œæ‰“åˆ†ï¼Œè¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ...")

    # 3. AI åˆ†æ (ç§»é™¤ break é™åˆ¶ï¼Œè®©å¤§å®¶å…¬å¹³ç«äº‰)
    processed_items = []
    
    for item in raw_items:
        # å»é‡
        if item['title'] in history_ids: 
            continue
            
        print(f"æ­£åœ¨åˆ†æ: [{item['source']}] {item['title'][:30]}...")
        result = analyze_with_llm(item)
        
        # åªè¦åˆ†æ•°è¾¾æ ‡ï¼Œå…ˆå­˜è¿›ä¸´æ—¶åˆ—è¡¨
        if result['score'] >= CONFIG['MIN_SCORE']:
            item['score'] = result['score']
            item['summary'] = result['summary']
            item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
            # ä¸´æ—¶ ID
            item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_" + str(len(processed_items))
            
            processed_items.append(item)
            
    # 4. æ’åºä¸æˆªæ–­ (å…³é”®ä¿®æ”¹)
    # å…ˆæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
    processed_items.sort(key=lambda x: x['score'], reverse=True)
    
    # ç„¶ååªå–å‰ N å (ç²¾è‹±ç­–ç•¥)
    new_qualified = processed_items[:CONFIG['FINAL_SAVE_COUNT']]
    
    print(f">>> ç» AI ç­›é€‰ï¼Œå…±æœ‰ {len(new_qualified)} æ¡å…¥é€‰ä»Šæ—¥æ—¥æŠ¥")

    # 5. æ¨é€é€»è¾‘
    if new_qualified:
        # A. ä¿å­˜åˆ° JSON
        if os.path.exists(CONFIG["DATA_FILE"]):
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                current = json.load(f)
        else: current = []
        
        # æŠŠæœ€æ–°çš„æ’åˆ°æœ€å‰é¢
        final_data = new_qualified + current
        with open(CONFIG["DATA_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_data[:CONFIG['MAX_HISTORY']], f, ensure_ascii=False, indent=2)

        # B. é’‰é’‰æ¨é€ (æ¨é€å‰ 5 æ¡)
        top_picks = [r for r in new_qualified if r['score'] >= CONFIG['PUSH_THRESHOLD']]
        # å¦‚æœè™½ç„¶å…¥é€‰äº†ï¼Œä½†åˆ†æ•°éƒ½æ²¡è¾¾åˆ°æ¨é€é—¨æ§›ï¼Œå°±ä¸æ¨é’‰é’‰
        if top_picks:
            # å–å‰ 5 ä¸ªï¼Œæˆ–è€…å…¨éƒ¨ top_picks ä¸­è¾ƒå°‘çš„é‚£ä¸ª
            push_list = top_picks[:5]
            
            ding_md = "# ğŸ“… ä»Šæ—¥é‡åŒ–è®ºæ–‡æ‘˜è¦\n\n"
            for r in push_list:
                ding_md += f"### {r['title']}\n"
                ding_md += f"**{r['score']}åˆ†** | {r['source']}\n\n"
                ding_md += f"> **ä¸­æ–‡æ‘˜è¦**ï¼š\n> {r['summary']}\n\n"
                ding_md += f"[ğŸ“„ åŸæ–‡é“¾æ¥]({r['url']})\n\n---\n"
            send_dingtalk(ding_md)
        else:
            print(">>> è™½ç„¶æœ‰å…¥é€‰æ–‡ç« ï¼Œä½†åˆ†æ•°å‡æœªè¾¾åˆ°æ¨é€é—¨æ§› (PUSH_THRESHOLD)ï¼Œè·³è¿‡é’‰é’‰ã€‚")

        # C. é‚®ä»¶æ¨é€ (æ¨é€æ‰€æœ‰å…¥é€‰çš„ï¼Œæœ€å¤š 15 æ¡)
        email_html = "<h2>ğŸ“… ä»Šæ—¥é‡åŒ–äº¤æ˜“å­¦æœ¯ç²¾é€‰</h2><hr>"
        for r in new_qualified:
            color = "red" if r['score'] >= 8 else "black"
            email_html += f"""
            <div style='margin-bottom:20px; padding:15px; border:1px solid #ddd; border-radius:5px;'>
                <h3 style='margin-top:0'><a href='{r['url']}'>{r['title']}</a> <span style='color:{color}'>({r['score']}åˆ†)</span></h3>
                <p style='color:#666; font-size:12px'>{r['source']} | {r['date']}</p>
                <div style='background:#f9f9f9; padding:10px; border-left:4px solid #1890ff;'>
                    <p style='margin:0; font-weight:bold;'>ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ‘˜è¦ï¼š</p>
                    <p style='margin-top:5px; line-height:1.6;'>{r['summary']}</p>
                </div>
            </div>
            """
        send_email(f"é‡åŒ–æ—¥æŠ¥ ({datetime.date.today()}) - {len(new_qualified)}ç¯‡ AI ç²¾è¯»", email_html)
        
    else:
        print(">>> æ— æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
