import os
import json
import datetime
import requests
import smtplib
import akshare as ak
import arxiv
from email.mime.text import MIMEText
from email.header import Header
from openai import OpenAI
from serpapi import GoogleSearch

# ==========================================
#              1. å…¨å±€é…ç½®åŒºåŸŸ (CONFIG)
#           æ‰€æœ‰ä½ æƒ³æ”¹çš„å‚æ•°éƒ½åœ¨è¿™é‡Œï¼
# ==========================================

CONFIG = {
    # --- åŸºç¡€è®¾ç½® ---
    "DATA_FILE": "data/reports.json", # æ•°æ®å­˜å‚¨è·¯å¾„
    "MAX_HISTORY": 500,               # å†å²è®°å½•ä¿ç•™å¤šå°‘æ¡
    "MIN_SCORE": 5.0,                 # ä½äºè¿™ä¸ªåˆ†çš„æ–‡ç« ä¸æ”¶å½•
    "PUSH_THRESHOLD": 6,            # é«˜äºè¿™ä¸ªåˆ†æ‰æ¨é€åˆ°é’‰é’‰/æ‰‹æœº
    
    # --- æŠ“å–æ•°é‡ (æ¯ä¸ªæºå°è¯•è·å–çš„åŸå§‹æ¡æ•°) ---
    "FETCH_COUNT_ARXIV": 20,          # ArXiv æŠ“å¤šç‚¹ï¼Œå› ä¸ºè¦è¿‡æ»¤
    "FETCH_COUNT_GOOGLE": 10,
    "FETCH_COUNT_AKSHARE": 50,        # Aè‚¡ç ”æŠ¥æ‚éŸ³å¤šï¼ŒæŠ“50æ¡å›æ¥ç­›é€‰
    "FINAL_SAVE_COUNT": 15,           # æœ€ç»ˆæ¯å¤©ä¿å­˜å¹¶å±•ç¤ºçš„æœ€å¤§æ¡æ•°
    
    # --- ArXiv (å›½é™…è®ºæ–‡) è®¾ç½® ---
    # æœç´¢å…³é”®è¯ (é€»è¾‘æ˜¯ OR)
    "ARXIV_KEYWORDS": [
        "quantitative finance",
        "factor model",
        "portfolio optimization",
        "deep learning trading",
        "market microstructure",
        "risk premia",
        "machine learning trading",
        "reinforcement learning trading",
        "algorithm trading"
    ],
    
    # --- Google Scholar (è°·æ­Œå­¦æœ¯) è®¾ç½® ---
    "GOOGLE_QUERY": 'quantitative finance "machine learning" trading after:2024',
    
    # --- Akshare (å›½å†…ç ”æŠ¥) è®¾ç½® ---
    # 1. å¿…è¯»åˆ¸å•†ç™½åå• (åªçœ‹è¿™äº›é‡‘å·¥å¼ºé˜Ÿçš„æŠ¥å‘Š)
    "TARGET_BROKERS": [
        "ä¸­ä¿¡å»ºæŠ•", "åæ³°è¯åˆ¸", "å¤©é£è¯åˆ¸", "å…´ä¸šè¯åˆ¸", 
        "å›½æ³°å›å®‰", "æ‹›å•†è¯åˆ¸", "ä¸­é‡‘å…¬å¸", "ç”³ä¸‡å®æº",
        "æµ·é€šè¯åˆ¸", "å¹¿å‘è¯åˆ¸"
    ],
    # 2. æ ‡é¢˜å…³é”®è¯ (å¿…é¡»åŒ…å«å…¶ä¸­ä¹‹ä¸€)
    "AK_KEYWORDS": [
        "é‡‘å·¥", "é‡åŒ–", "å› å­", "é€‰è‚¡", "æ‹©æ—¶", 
        "èµ„äº§é…ç½®", "æ·±åº¦ç ”ç©¶", "åŸºæœ¬é¢é‡åŒ–", "å¤šå› å­",
        "æœºå™¨å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "é«˜é¢‘"
    ]
}

# ==========================================
#              2. ç¯å¢ƒå˜é‡åŠ è½½
# ==========================================
LLM_API_KEY = os.environ.get("LLM_API_KEY")
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")

# åˆå§‹åŒ– AI å®¢æˆ·ç«¯
client = OpenAI(api_key=LLM_API_KEY, base_url="https://api.deepseek.com")

# ==========================================
#              3. æ ¸å¿ƒæŠ“å–é€»è¾‘
# ==========================================

def fetch_arxiv():
    """æŠ“å– ArXiv"""
    print(f"--- æ­£åœ¨æŠ“å– ArXiv (å…³é”®è¯: {CONFIG['ARXIV_KEYWORDS'][:3]}...) ---")
    
    # æ„é€ æŸ¥è¯¢è¯­å¥: cat:q-fin.* AND ("keyword1" OR "keyword2" ...)
    keywords_query = " OR ".join([f'"{k}"' for k in CONFIG['ARXIV_KEYWORDS']])
    query = f'cat:q-fin.* AND ({keywords_query})'
    
    search = arxiv.Search(
        query=query,
        max_results=CONFIG['FETCH_COUNT_ARXIV'],
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    results = []
    for r in search.results():
        results.append({
            "title": r.title,
            "url": r.pdf_url,
            "source": "ArXiv",
            "date": r.published.strftime("%Y-%m-%d"),
            "abstract": r.summary,
            "broker": "Cornell Univ" 
        })
    return results

def fetch_google_scholar():
    """æŠ“å– Google Scholar"""
    if not SERPAPI_KEY:
        print("æœªé…ç½® SERPAPI_KEYï¼Œè·³è¿‡ Google Scholar")
        return []
        
    print(f"--- æ­£åœ¨æŠ“å– Google Scholar ---")
    params = {
        "engine": "google_scholar",
        "q": CONFIG['GOOGLE_QUERY'],
        "api_key": SERPAPI_KEY,
        "num": CONFIG['FETCH_COUNT_GOOGLE']
    }
    search = GoogleSearch(params)
    data = search.get_dict()
    organic_results = data.get("organic_results", [])
    
    results = []
    for item in organic_results:
        results.append({
            "title": item.get("title"),
            "url": item.get("link"),
            "source": "Scholar",
            "date": datetime.datetime.now().strftime("%Y-%m-%d"), # è°·æ­Œå­¦æœ¯å¾ˆéš¾è·å–ç²¾ç¡®æ—¥æœŸï¼Œç”¨å½“å¤©ä»£æ›¿
            "abstract": item.get("snippet", "No abstract available"),
            "broker": "Google"
        })
    return results

def fetch_akshare_reports():
    """æŠ“å– Aè‚¡é‡‘å·¥ç ”æŠ¥ (æ ¸å¿ƒé€»è¾‘å¢å¼ºç‰ˆ)"""
    print(f"--- æ­£åœ¨æŠ“å– Aè‚¡ç ”æŠ¥ (ç›®æ ‡: {len(CONFIG['TARGET_BROKERS'])}å®¶åˆ¸å•†) ---")
    results = []
    try:
        # è·å–æœ€è¿‘çš„ç ”æŠ¥æ•°æ® (é»˜è®¤å–æœ€æ–°çš„ 100 æ¡ raw data æ¥ç­›é€‰)
        # æ³¨æ„ï¼šakshare æ¥å£è¿”å›çš„æ˜¯å…¨å¸‚åœºçš„ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å†…å­˜é‡Œåšç­›é€‰
        target_date = datetime.datetime.now().strftime("%Y%m%d")
        
        # ä¸ºäº†é˜²æ­¢å‘¨æœ«æ²¡æœ‰ç ”æŠ¥ï¼Œå¦‚æœä»Šå¤©æ²¡æœ‰ï¼Œå¯ä»¥å°è¯•å¾€å‰æ¨ï¼ˆè¿™é‡Œç®€åŒ–ï¼ŒåªæŠ“å½“å¤©çš„æ¥å£æ•°æ®ï¼‰
        # stock_em_yjbg æ¥å£è¿”å›çš„æ˜¯æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥æ›´æ–°çš„åˆ—è¡¨
        df = ak.stock_em_yjbg(date=target_date)
        
        if df.empty:
            print("ä»Šæ—¥ Akshare æ¥å£æš‚æ— æ•°æ®")
            return []

        # === ç­›é€‰é€»è¾‘ 1: å¿…é¡»æ˜¯ç™½åå•åˆ¸å•† ===
        # å‡è®¾ df é‡Œçš„åˆ—åæ˜¯ 'æœºæ„åç§°'
        df = df[df['æœºæ„åç§°'].isin(CONFIG['TARGET_BROKERS'])]
        
        # === ç­›é€‰é€»è¾‘ 2: æ ‡é¢˜å¿…é¡»åŒ…å«å…³é”®è¯ ===
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ„å»º "A|B|C"
        keywords_pattern = "|".join(CONFIG['AK_KEYWORDS'])
        df = df[df['æ–‡ç« æ ‡é¢˜'].str.contains(keywords_pattern, na=False)]
        
        # å–å‰ N æ¡
        df = df.head(CONFIG['FETCH_COUNT_AKSHARE'])
        
        for _, row in df.iterrows():
            results.append({
                "title": row['æ–‡ç« æ ‡é¢˜'],
                "url": row['pdfé“¾æ¥'], 
                "source": "ç ”æŠ¥",
                "date": row['å‘å¸ƒæ—¥æœŸ'],
                "abstract": f"æ¥è‡ª {row['æœºæ„åç§°']} çš„æ·±åº¦æŠ¥å‘Šï¼š{row['æ–‡ç« æ ‡é¢˜']}", # ç ”æŠ¥æ— æ‘˜è¦ï¼Œç”¨è¿™ä¸ªä»£æ›¿
                "broker": row['æœºæ„åç§°']
            })
            
    except Exception as e:
        print(f"Akshare æŠ“å–å¼‚å¸¸ (å¯èƒ½æ˜¯æ¥å£å˜åŠ¨æˆ–ç½‘ç»œé—®é¢˜): {e}")
        
    return results

# ==========================================
#              4. æ™ºèƒ½åˆ†æä¸åˆ†å‘
# ==========================================

def analyze_with_llm(item):
    """è°ƒç”¨ AI è¿›è¡Œè¯„åˆ†å’Œæ€»ç»“"""
    try:
        # é’ˆå¯¹ç ”æŠ¥å’Œè®ºæ–‡ä½¿ç”¨ä¸åŒçš„ Prompt ç­–ç•¥
        content_type = "å­¦æœ¯è®ºæ–‡" if item['source'] in ['ArXiv', 'Scholar'] else "Aè‚¡é‡‘å·¥ç ”æŠ¥"
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–åŸºé‡‘ç»ç†ã€‚è¯·è¯„ä¼°ä»¥ä¸‹{content_type}çš„ä»·å€¼ã€‚
        
        æ ‡é¢˜: {item['title']}
        æ¥æº: {item['broker']}
        æ‘˜è¦/å†…å®¹: {item['abstract'][:800]}
        
        è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¿”å›ï¼š
        {{
            "score": <0-10åˆ†, 7åˆ†ä»£è¡¨æœ‰å®æˆ˜å‚è€ƒä»·å€¼, 9åˆ†ä»£è¡¨å¿…è¯»>,
            "summary": "<ç”¨ä¸­æ–‡ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒç­–ç•¥æˆ–åˆ›æ–°ç‚¹,ä¸è¶…è¿‡50å­—>"
        }}
        """
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"LLM åˆ†æå¤±è´¥: {e}")
        return {"score": 5.0, "summary": "AI åˆ†ææš‚æ—¶ä¸å¯ç”¨"}

def send_dingtalk(msg_markdown):
    """å‘é€é’‰é’‰æ¶ˆæ¯"""
    if not DINGTALK_WEBHOOK: return
    try:
        headers = {"Content-Type": "application/json"}
        data = {
            "msgtype": "markdown",
            "markdown": {
                "title": "é‡åŒ–æ—¥æŠ¥æ¨é€",
                "text": msg_markdown
            }
        }
        requests.post(DINGTALK_WEBHOOK, json=data)
    except Exception as e:
        print(f"é’‰é’‰å‘é€å¤±è´¥: {e}")

def send_email(subject, html_content):
    """å‘é€é‚®ä»¶"""
    if not EMAIL_USER or not EMAIL_PASS: return
    try:
        msg = MIMEText(html_content, 'html', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_USER 
        
        smtp = smtplib.SMTP_SSL('smtp.qq.com', 465)
        smtp.login(EMAIL_USER, EMAIL_PASS)
        smtp.send_message(msg)
        smtp.quit()
    except Exception as e:
        print(f"é‚®ä»¶å‘é€å¤±è´¥: {e}")

# ==========================================
#              5. ä¸»ç¨‹åºå…¥å£
# ==========================================

def main():
    print(">>> ä»»åŠ¡å¼€å§‹")
    
    # 1. åŠ è½½å†å²æ•°æ® (å»é‡ç”¨)
    history_ids = []
    if os.path.exists(CONFIG["DATA_FILE"]):
        with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
            old_data = json.load(f)
            history_ids = [item.get('title') for item in old_data] # ç”¨æ ‡é¢˜åšå”¯ä¸€ID

    # 2. æŠ“å–æ‰€æœ‰æº
    raw_items = []
    raw_items += fetch_arxiv()
    raw_items += fetch_google_scholar()
    raw_items += fetch_akshare_reports()
    
    print(f">>> å…±æŠ“å–åˆ° {len(raw_items)} æ¡åŸå§‹æ•°æ®ï¼Œå¼€å§‹ AI ç­›é€‰...")

    # 3. AI åˆ†æä¸ç­›é€‰
    new_qualified_reports = []
    
    for item in raw_items:
        # å»é‡
        if item['title'] in history_ids:
            continue
            
        print(f"æ­£åœ¨åˆ†æ: [{item['source']}] {item['title'][:20]}...")
        result = analyze_with_llm(item)
        
        # åªæœ‰é«˜äºæœ€ä½åˆ†çš„æ‰æ”¶å½•
        if result['score'] >= CONFIG['MIN_SCORE']:
            item['score'] = result['score']
            item['summary'] = result['summary']
            item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
            # ç”Ÿæˆä¸€ä¸ªç®€çŸ­IDä¾›å‰ç«¯ä½¿ç”¨
            item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_" + str(len(new_qualified_reports))
            
            new_qualified_reports.append(item)
            
            # å¦‚æœå‡‘å¤Ÿäº†å½“å¤©çš„æœ€å¤§æ•°é‡ï¼Œå°±åœæ­¢ï¼ˆçœé’±çœæ—¶é—´ï¼‰
            if len(new_qualified_reports) >= CONFIG['FINAL_SAVE_COUNT']:
                break
    
    # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
    new_qualified_reports.sort(key=lambda x: x['score'], reverse=True)

    # 4. å¦‚æœæœ‰æ–°å†…å®¹ï¼Œæ‰§è¡Œä¿å­˜å’Œæ¨é€
    if new_qualified_reports:
        print(f">>> å‘ç° {len(new_qualified_reports)} æ¡ä¼˜è´¨å†…å®¹ï¼Œæ­£åœ¨ä¿å­˜å’Œæ¨é€...")
        
        # A. ä¿å­˜åˆ° JSON (ä¾› Geeker Admin å‰ç«¯ä½¿ç”¨)
        if os.path.exists(CONFIG["DATA_FILE"]):
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                current_data = json.load(f)
        else:
            current_data = []
            
        # åˆå¹¶å¹¶ä¿ç•™æœ€æ–°çš„ N æ¡
        final_data = new_qualified_reports + current_data
        final_data = final_data[:CONFIG['MAX_HISTORY']]
        
        with open(CONFIG["DATA_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        # B. é’‰é’‰/é£ä¹¦ æ¨é€ (åªæ¨åˆ†æ•°æœ€é«˜çš„ Top 5)
        top_picks = [r for r in new_qualified_reports if r['score'] >= CONFIG['PUSH_THRESHOLD']]
        if top_picks:
            ding_md = "# ğŸ“… ä»Šæ—¥é‡åŒ–æƒ…æŠ¥\n\n"
            for r in top_picks[:5]: # æœ€å¤šæ¨5æ¡
                ding_md += f"### {r['title']}\n"
                ding_md += f"**{r['score']}åˆ†** | {r['source']} | {r['broker']}\n"
                ding_md += f"> {r['summary']}\n"
                ding_md += f"[ğŸ“„ ç‚¹å‡»é˜…è¯» PDF]({r['url']})\n\n---\n"
            send_dingtalk(ding_md)

        # C. é‚®ä»¶æ¨é€ (æ¨æ‰€æœ‰ç¬¦åˆè¦æ±‚çš„)
        email_html = "<h2>ğŸ“… ä»Šæ—¥é‡åŒ–æƒ…æŠ¥æ±‡æ€»</h2><hr>"
        for r in new_qualified_reports:
            color = "red" if r['score'] >= 8 else "black"
            email_html += f"""
            <div style='margin-bottom:15px; padding:10px; border-left:4px solid #1890ff; background:#f5f5f5'>
                <h3 style='margin:0'><a href='{r['url']}'>{r['title']}</a> <span style='color:{color}'>({r['score']}åˆ†)</span></h3>
                <p style='margin:5px 0; font-size:12px; color:#666'>{r['source']} | {r['broker']} | {r['fetch_date']}</p>
                <p style='margin:5px 0'><strong>AIç‚¹è¯„:</strong> {r['summary']}</p>
            </div>
            """
        send_email(f"é‡åŒ–æ—¥æŠ¥ ({datetime.date.today()}) - {len(new_qualified_reports)}ç¯‡æ›´æ–°", email_html)
        
    else:
        print(">>> ä»Šæ—¥æ— æ»¡è¶³æ¡ä»¶çš„é«˜åˆ†å†…å®¹æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
