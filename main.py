import os
import json
import datetime
import requests
import smtplib
import arxiv
from email.mime.text import MIMEText
from email.header import Header
from openai import OpenAI
from serpapi import GoogleSearch

# ==========================================
#              1. å…¨å±€é…ç½®åŒºåŸŸ (CONFIG)
# ==========================================

CONFIG = {
    # --- åŸºç¡€è®¾ç½® ---
    "DATA_FILE": "data/reports.json", 
    "MAX_HISTORY": 500,               
    "MIN_SCORE": 5.0,                 # åªè¦ AI æ²¡æŒ‚ï¼ŒåŸºæœ¬éƒ½èƒ½è¿‡
    "PUSH_THRESHOLD": 6.0,            # é—¨æ§›è®¾ä½ç‚¹ï¼Œä¿è¯ä½ èƒ½æ”¶åˆ°æ¨é€
    "FINAL_SAVE_COUNT": 15,           
    
    # --- ArXiv (å›½é™…è®ºæ–‡) è®¾ç½® ---
    "FETCH_COUNT_ARXIV": 30,          # æŠ“å¤šç‚¹ç”¨äºè¿‡æ»¤
    "ARXIV_KEYWORDS": [
        "quantitative finance",
        "factor model",
        "portfolio optimization",
        "deep learning trading",      # æ·±åº¦å­¦ä¹ 
        "reinforcement learning trading", # å¼ºåŒ–å­¦ä¹ 
        "machine learning trading",   # æœºå™¨å­¦ä¹ 
        "algorithm trading",          # ç®—æ³•äº¤æ˜“
        "market microstructure",
        "risk premia"
    ],
    
    # --- Google Scholar (è°·æ­Œå­¦æœ¯) è®¾ç½® ---
    # è¿™é‡Œçš„é€»è¾‘æ”¹äº†ï¼šæˆ‘ä»¬ä¼šè½®è¯¢ä¸‹é¢è¿™å‡ ä¸ªæŸ¥è¯¢è¯ï¼Œç¡®ä¿è¦†ç›–é¢
    "GOOGLE_QUERIES": [
        'quantitative trading "reinforcement learning" after:2024', # å¼ºåŒ–å­¦ä¹ +é‡åŒ–
        'quantitative trading "deep learning" after:2024',          # æ·±åº¦å­¦ä¹ +é‡åŒ–
        '"algorithmic trading" strategy after:2024'                 # ç®—æ³•äº¤æ˜“
    ],
    "FETCH_COUNT_GOOGLE_PER_QUERY": 5, # æ¯ä¸ªè¯æŠ“ 5 æ¡ï¼Œæ€»å…±æŠ“ 15 æ¡
}

# ==========================================
#              2. ç¯å¢ƒå˜é‡åŠ è½½
# ==========================================
LLM_API_KEY = os.environ.get("LLM_API_KEY")
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")

# åˆå§‹åŒ– AI å®¢æˆ·ç«¯
# å¦‚æœè¦æ¢ Kimiï¼Œbase_url æ”¹ä¸º: "https://api.moonshot.cn/v1"
client = OpenAI(api_key=LLM_API_KEY, base_url="https://api.deepseek.com")

# ==========================================
#              3. æ ¸å¿ƒæŠ“å–é€»è¾‘
# ==========================================

def fetch_arxiv():
    """æŠ“å– ArXiv"""
    print(f"--- æ­£åœ¨æŠ“å– ArXiv (å…³é”®è¯: {CONFIG['ARXIV_KEYWORDS'][:3]}...) ---")
    
    # æ„é€  OR æŸ¥è¯¢
    keywords_query = " OR ".join([f'"{k}"' for k in CONFIG['ARXIV_KEYWORDS']])
    # é™åˆ¶åˆ†ç±»ä¸º q-fin (é‡åŒ–é‡‘è) æˆ– cs.AI (äººå·¥æ™ºèƒ½)
    query = f'(cat:q-fin.* OR cat:cs.AI) AND ({keywords_query})'
    
    try:
        search = arxiv.Search(
            query=query,
            max_results=CONFIG['FETCH_COUNT_ARXIV'],
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        results = []
        for r in search.results():
            # ç®€å•å»é‡ï¼šå¦‚æœåˆ†ç±»å®Œå…¨ä¸æ²¾è¾¹ï¼Œè·³è¿‡
            # (ArXiv æœç´¢æœ‰æ—¶å€™å¾ˆå®½æ³›)
            if not any(tag.startswith(('q-fin', 'cs', 'stat')) for tag in [t.term for t in r.categories]):
                continue

            results.append({
                "title": r.title,
                "url": r.pdf_url,
                "source": "ArXiv",
                "date": r.published.strftime("%Y-%m-%d"),
                "abstract": r.summary,
                "broker": "Cornell Univ" 
            })
        print(f"ArXiv æŠ“å–åˆ° {len(results)} æ¡")
        return results
    except Exception as e:
        print(f"ArXiv æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_google_scholar():
    """æŠ“å– Google Scholar (å¤šå…³é”®è¯è½®è¯¢ç‰ˆ)"""
    if not SERPAPI_KEY:
        print("æœªé…ç½® SERPAPI_KEYï¼Œè·³è¿‡ Google Scholar")
        return []
        
    print(f"--- æ­£åœ¨æŠ“å– Google Scholar (å¤šè½®æœç´¢) ---")
    all_results = []
    
    # éå†é…ç½®é‡Œçš„æ¯ä¸€ä¸ªæŸ¥è¯¢è¯­å¥
    for query in CONFIG['GOOGLE_QUERIES']:
        try:
            print(f"æ­£åœ¨æœ Scholar: {query} ...")
            params = {
                "engine": "google_scholar",
                "q": query,
                "api_key": SERPAPI_KEY,
                "num": CONFIG['FETCH_COUNT_GOOGLE_PER_QUERY'],
                "hl": "en" # å¼ºåˆ¶è‹±æ–‡ç»“æœï¼Œç›¸å…³æ€§æ›´é«˜
            }
            search = GoogleSearch(params)
            data = search.get_dict()
            organic_results = data.get("organic_results", [])
            
            if not organic_results:
                print(f"è­¦å‘Š: æŸ¥è¯¢ '{query}' æœªè¿”å›ä»»ä½•ç»“æœ")
                continue

            for item in organic_results:
                # å¿…é¡»è¦æœ‰é“¾æ¥æ‰æ”¶å½•
                if 'link' not in item:
                    continue
                    
                all_results.append({
                    "title": item.get("title"),
                    "url": item.get("link"),
                    "source": "Scholar",
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "abstract": item.get("snippet", item.get("title")), # æ‘˜è¦å¯èƒ½ä¸ºç©º
                    "broker": "Google Scholar"
                })
        except Exception as e:
            print(f"Scholar å•æ¬¡æŸ¥è¯¢å‡ºé”™: {e}")
            continue
            
    print(f"Google Scholar å…±æŠ“å–åˆ° {len(all_results)} æ¡")
    return all_results

# ==========================================
#              4. æ™ºèƒ½åˆ†æä¸åˆ†å‘
# ==========================================

def analyze_with_llm(item):
    """è°ƒç”¨ AI è¿›è¡Œè¯„åˆ†å’Œæ€»ç»“"""
    try:
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–åŸºé‡‘ç»ç†ã€‚è¯·è¯„ä¼°ä»¥ä¸‹å­¦æœ¯è®ºæ–‡å¯¹â€œå®æˆ˜é‡åŒ–äº¤æ˜“â€çš„ä»·å€¼ã€‚
        
        æ ‡é¢˜: {item['title']}
        æ‘˜è¦: {item['abstract'][:800]}
        
        è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¿”å›ï¼š
        {{
            "score": <0-10åˆ†, å‡¡æ˜¯æ¶‰åŠ'å¼ºåŒ–å­¦ä¹ /æ·±åº¦å­¦ä¹ +äº¤æ˜“'çš„ç›´æ¥ç»™8åˆ†ä»¥ä¸Š, çº¯ç†è®ºæ•°å­¦ç»™4åˆ†>,
            "summary": "<ç”¨ä¸­æ–‡ä¸€å¥è¯æ¦‚æ‹¬å…¶æ ¸å¿ƒç®—æ³•æˆ–ç­–ç•¥æ¨¡å‹>"
        }}
        """
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"LLM åˆ†æå¤±è´¥: {e}")
        # å¤±è´¥æ—¶ç»™ä¸ªé»˜è®¤åˆ†ï¼Œé˜²æ­¢å› ä¸ºAIæ³¢åŠ¨æ¼æ‰é‡è¦è®ºæ–‡
        return {"score": 6.0, "summary": "AI æš‚æ—¶ç½¢å·¥ï¼Œè¯·äººå·¥é˜…è¯»"}

def send_dingtalk(msg_markdown):
    """å‘é€é’‰é’‰æ¶ˆæ¯"""
    if not DINGTALK_WEBHOOK: return
    try:
        headers = {"Content-Type": "application/json"}
        data = {
            "msgtype": "markdown",
            "markdown": {
                "title": "é‡åŒ–æ—¥æŠ¥æ¨é€",
                "text": msg_markdown
            }
        }
        requests.post(DINGTALK_WEBHOOK, json=data)
    except Exception as e:
        print(f"é’‰é’‰å‘é€å¤±è´¥: {e}")

def send_email(subject, html_content):
    """å‘é€é‚®ä»¶"""
    if not EMAIL_USER or not EMAIL_PASS: return
    try:
        msg = MIMEText(html_content, 'html', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_USER 
        
        smtp = smtplib.SMTP_SSL('smtp.qq.com', 465)
        smtp.login(EMAIL_USER, EMAIL_PASS)
        smtp.send_message(msg)
        smtp.quit()
    except Exception as e:
        print(f"é‚®ä»¶å‘é€å¤±è´¥: {e}")

# ==========================================
#              5. ä¸»ç¨‹åºå…¥å£
# ==========================================

def main():
    print(">>> ä»»åŠ¡å¼€å§‹")
    
    # 1. åŠ è½½å†å²æ•°æ®
    history_ids = []
    if os.path.exists(CONFIG["DATA_FILE"]):
        try:
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                old_data = json.load(f)
                history_ids = [item.get('title') for item in old_data]
        except:
            history_ids = []

    # 2. æŠ“å– (åªæŠ“ ArXiv å’Œ Google Scholar)
    raw_items = []
    raw_items += fetch_arxiv()
    raw_items += fetch_google_scholar()
    
    print(f">>> å…±æŠ“å–åˆ° {len(raw_items)} æ¡åŸå§‹æ•°æ®ï¼Œå¼€å§‹ AI ç­›é€‰...")

    # 3. AI åˆ†æä¸ç­›é€‰
    new_qualified_reports = []
    
    for item in raw_items:
        # å»é‡
        if item['title'] in history_ids:
            continue
            
        print(f"æ­£åœ¨åˆ†æ: {item['title'][:40]}...")
        result = analyze_with_llm(item)
        
        if result['score'] >= CONFIG['MIN_SCORE']:
            item['score'] = result['score']
            item['summary'] = result['summary']
            item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
            item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_" + str(len(new_qualified_reports))
            
            new_qualified_reports.append(item)
            
            if len(new_qualified_reports) >= CONFIG['FINAL_SAVE_COUNT']:
                break
    
    # æŒ‰åˆ†æ•°æ’åº
    new_qualified_reports.sort(key=lambda x: x['score'], reverse=True)

    # 4. ä¿å­˜å’Œæ¨é€
    if new_qualified_reports:
        print(f">>> å‘ç° {len(new_qualified_reports)} æ¡ä¼˜è´¨å†…å®¹ï¼Œæ­£åœ¨æ¨é€...")
        
        # A. ä¿å­˜åˆ° JSON
        if os.path.exists(CONFIG["DATA_FILE"]):
            try:
                with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                    current_data = json.load(f)
            except:
                current_data = []
        else:
            current_data = []
            
        final_data = new_qualified_reports + current_data
        with open(CONFIG["DATA_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_data[:CONFIG['MAX_HISTORY']], f, ensure_ascii=False, indent=2)

        # B. é’‰é’‰/é£ä¹¦ æ¨é€ (Top 5)
        top_picks = [r for r in new_qualified_reports if r['score'] >= CONFIG['PUSH_THRESHOLD']]
        if top_picks:
            ding_md = "# ğŸ“… ä»Šæ—¥é‡åŒ–å­¦æœ¯æ—¥æŠ¥\n\n"
            for r in top_picks[:5]:
                ding_md += f"### {r['title']}\n"
                ding_md += f"**{r['score']}åˆ†** | {r['source']}\n"
                ding_md += f"> {r['summary']}\n"
                ding_md += f"[ğŸ“„ é˜…è¯»å…¨æ–‡]({r['url']})\n\n---\n"
            send_dingtalk(ding_md)

        # C. é‚®ä»¶æ¨é€
        email_html = "<h2>ğŸ“… ä»Šæ—¥é‡åŒ–äº¤æ˜“å­¦æœ¯ç²¾é€‰</h2><hr>"
        for r in new_qualified_reports:
            color = "red" if r['score'] >= 8 else "black"
            email_html += f"""
            <div style='margin-bottom:15px; padding:10px; border-left:4px solid #52c41a; background:#f6ffed'>
                <h3 style='margin:0'><a href='{r['url']}'>{r['title']}</a> <span style='color:{color}'>({r['score']}åˆ†)</span></h3>
                <p style='margin:5px 0; font-size:12px; color:#666'>{r['source']} | {r['date']}</p>
                <p style='margin:5px 0'><strong>AI è§£è¯»:</strong> {r['summary']}</p>
            </div>
            """
        send_email(f"é‡åŒ–æ—¥æŠ¥ ({datetime.date.today()}) - {len(new_qualified_reports)}ç¯‡æ›´æ–°", email_html)
        
    else:
        print(">>> ä»Šæ—¥æ— æ»¡è¶³æ¡ä»¶çš„é«˜åˆ†å†…å®¹æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
