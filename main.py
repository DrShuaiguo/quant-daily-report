import os
import json
import datetime
import requests
import smtplib
import arxiv
from email.mime.text import MIMEText
from email.header import Header
from openai import OpenAI
from serpapi import GoogleSearch

# ==========================================
#              1. å…¨å±€é…ç½®åŒºåŸŸ (CONFIG)
#         âš™ï¸ æ‰€æœ‰çš„æ•°å­—éƒ½åœ¨è¿™é‡Œï¼
# ==========================================

CONFIG = {
    # --- æ–‡ä»¶è·¯å¾„ ---
    "DATA_FILE": "data/reports.json",      # ã€ç²¾ååº“ã€‘ç»™å‰ç«¯/é‚®ä»¶çœ‹
    "HISTORY_FILE": "data/history.json",   # ã€é»‘åå•ã€‘ç»™çˆ¬è™«å»é‡ç”¨
    
    # --- æ ¸å¿ƒå®¹é‡æ§åˆ¶ (æ— é­”æ³•æ•°å­—!) ---
    # 1. é»‘åå•å®¹é‡: å¿…é¡»è¿œå¤§äº MAX_SEARCH_DEPTHï¼Œå¦åˆ™ä¼šé‡å¤æŠ“å–æ—§æ–‡
    "MAX_HISTORY_SIZE": 3000,         
    
    # 2. ç²¾ååº“å®¹é‡: reports.json ä¿ç•™å¤šå°‘æ¡é«˜åˆ†æ–‡ç« ä¾›å›çœ‹
    "MAX_REPORT_SIZE": 500,           
    
    # 3. é‚®ä»¶å‘é€ä¸Šé™: å³ä½¿æœ‰100ç¯‡æ–°æ–‡ç« ï¼Œé‚®ä»¶é‡Œä¹Ÿåªå±•ç¤ºå‰ N ç¯‡ï¼Œé˜²æ­¢é‚®ä»¶è¿‡å¤§å‘ä¸å‡ºå»
    "MAX_EMAIL_ITEM_LIMIT": 50,       
    
    # --- é˜ˆå€¼è®¾ç½® ---
    "MIN_SCORE": 5.0,                 # 5åˆ†ä»¥ä¸Šæ‰æœ‰èµ„æ ¼è¿› reports.json
    "PUSH_THRESHOLD": 6.0,            # 6åˆ†ä»¥ä¸Šæ‰æ¨é’‰é’‰
    
    "FINAL_SAVE_COUNT": 15,           # æ¯å¤©æœ€å¤šæ–°å¢ 15 ç¯‡ç²¾å (æ§åˆ¶é¢„ç®—å’Œé˜…è¯»é‡)
    "DINGTALK_PUSH_LIMIT": 5,         # é’‰é’‰åªæ¨ Top 5
    
    # --- æŠ“å–è®¾ç½® ---
    "CANDIDATE_POOL_SIZE": 20,        # æ¯æ¬¡å¿…é¡»å‡‘é½ N ç¯‡ã€æœªè¯»ã€‘æ–‡ç« å–‚ç»™ AI
    "MAX_SEARCH_DEPTH": 1000,         # ArXiv æœ€å¤§ç¿»é¡µæ·±åº¦ (é’»å¤´é•¿åº¦)
    
    "FETCH_COUNT_GOOGLE_PER_QUERY": 10, # Google æ¯ä¸ªå…³é”®è¯æŠ“ N æ¡
    
    "MAX_TEXT_LENGTH_FOR_AI": 1200,   # æ‘˜è¦æˆªæ–­é•¿åº¦
    "SEARCH_YEAR": "2024",            # æœç´¢å¹´ä»½
    
    # --- å…³é”®è¯ ---
    "ARXIV_KEYWORDS": [
        "quantitative finance", "factor model", "portfolio optimization",
        "deep learning trading", "reinforcement learning trading", 
        "machine learning trading", "algorithm trading",          
        "market microstructure", "risk premia", "quantitative trading",
        "deep reinforcement learning", "transformer finance",
        "large language model trading"
    ],
    "GOOGLE_QUERIES": [
        'quantitative trading "reinforcement learning"', 
        'quantitative trading "deep learning"',          
        '"algorithmic trading" strategy',
        'transformers for "stock prediction"', 
        '"LLM" agents for "quantitative trading"'
    ]
}

# ==========================================
#              2. ç¯å¢ƒä¸å®¢æˆ·ç«¯
# ==========================================
LLM_API_KEY = os.environ.get("LLM_API_KEY")
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")

client = OpenAI(api_key=LLM_API_KEY, base_url="https://api.deepseek.com")

# ==========================================
#              3. æŠ“å–å‡½æ•°
# ==========================================

def fetch_arxiv_smart(history_titles):
    """
    æ™ºèƒ½æŠ“å– ArXiv: 
    æ ¹æ® history.json é‡Œçš„æ ‡é¢˜è¿›è¡Œå»é‡ã€‚
    """
    target_count = CONFIG['CANDIDATE_POOL_SIZE']
    print(f"--- ArXiv æ™ºèƒ½æ·±æŒ– (ç›®æ ‡: æ‰¾åˆ° {target_count} ç¯‡æœªè¯») ---")
    
    keywords_query = " OR ".join([f'"{k}"' for k in CONFIG['ARXIV_KEYWORDS']])
    query = f'(cat:q-fin.* OR cat:cs.AI) AND ({keywords_query})'
    
    candidates = []
    try:
        search = arxiv.Search(
            query=query,
            # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§æ·±åº¦
            max_results=CONFIG['MAX_SEARCH_DEPTH'], 
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        scanned = 0
        for r in search.results():
            scanned += 1
            if not any(tag.startswith(('q-fin', 'cs', 'stat')) for tag in r.categories): continue
            
            # === æ ¸å¿ƒå»é‡ ===
            if r.title in history_titles:
                continue 
                
            candidates.append({
                "title": r.title, "url": r.pdf_url, "source": "ArXiv",
                "date": r.published.strftime("%Y-%m-%d"), "abstract": r.summary,
                "broker": "Cornell Univ" 
            })
            
            if len(candidates) >= target_count:
                print(f"--> å·²å‡‘é½ {len(candidates)} ç¯‡æœªè¯»ï¼Œåœæ­¢æ‰«æã€‚")
                break
                
        print(f"æ‰«æç»“æŸ: å…±æ‰«æ {scanned} ç¯‡ï¼Œç­›é€‰å‡º {len(candidates)} ç¯‡æ–°æ–‡ç« ã€‚")
        return candidates
    except Exception as e:
        print(f"ArXiv Error: {e}")
        return []

def fetch_google_scholar():
    if not SERPAPI_KEY: return []
    print(f"--- æ­£åœ¨æŠ“å– Google Scholar ---")
    all_results = []
    for base_query in CONFIG['GOOGLE_QUERIES']:
        try:
            params = {
                "engine": "google_scholar", "q": f'{base_query} after:{CONFIG["SEARCH_YEAR"]}',
                "api_key": SERPAPI_KEY, 
                # ä½¿ç”¨é…ç½®ä¸­çš„æ•°é‡
                "num": CONFIG['FETCH_COUNT_GOOGLE_PER_QUERY'], 
                "hl": "en"
            }
            search = GoogleSearch(params)
            for item in search.get_dict().get("organic_results", []):
                if 'link' not in item: continue
                all_results.append({
                    "title": item.get("title"), "url": item.get("link"),
                    "source": "Scholar", "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "abstract": item.get("snippet", item.get("title")), "broker": "Google Scholar"
                })
        except: pass
    return all_results

# ==========================================
#              4. AI åˆ†æ
# ==========================================

def analyze_with_llm(item):
    try:
        # ä½¿ç”¨é…ç½®ä¸­çš„æˆªæ–­é•¿åº¦
        prompt = f"""
        ä½ æ˜¯ä¸€åé‡åŒ–äº¤æ˜“å‘˜ã€‚è¯„ä¼°ä»¥ä¸‹è®ºæ–‡å¯¹â€œå®æˆ˜äº¤æ˜“â€çš„ä»·å€¼ã€‚
        æ ‡é¢˜: {item['title']}
        æ‘˜è¦: {item['abstract'][:CONFIG['MAX_TEXT_LENGTH_FOR_AI']]}
        
        1. è¯„åˆ†(0-10): å®æˆ˜å¼º(RL/DeepLearning/Alpha)ç»™8-10åˆ†ï¼Œçº¯ç†è®ºç»™3-5åˆ†ï¼Œæ— å…³ç»™0åˆ†ã€‚
        2. ä¸­æ–‡æ‘˜è¦: ç¿»è¯‘æ ¸å¿ƒï¼Œä¿ç•™æœ¯è¯­ã€‚
        è¿”å›JSON: {{"score": 0, "summary": "..."}}
        """
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except: return {"score": 0, "summary": "Error"}

def send_dingtalk(msg):
    if not DINGTALK_WEBHOOK: return
    try: requests.post(DINGTALK_WEBHOOK, json={"msgtype": "markdown", "markdown": {"title": "é‡åŒ–æ—¥æŠ¥", "text": msg}})
    except: pass

def send_email(subject, html):
    if not EMAIL_USER or not EMAIL_PASS: return
    try:
        msg = MIMEText(html, 'html', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_USER
        smtp = smtplib.SMTP_SSL('smtp.qq.com', 465)
        smtp.login(EMAIL_USER, EMAIL_PASS)
        smtp.send_message(msg)
        smtp.quit()
    except: pass

# ==========================================
#              5. ä¸»ç¨‹åº (åŒæ–‡ä»¶ + æ— é­”æ³•æ•°å­—)
# ==========================================

def main():
    print(">>> ä»»åŠ¡å¼€å§‹")
    
    # 1. åŠ è½½ã€é»‘åå•ã€‘
    history_titles = []
    if os.path.exists(CONFIG["HISTORY_FILE"]):
        try:
            with open(CONFIG["HISTORY_FILE"], 'r', encoding='utf-8') as f:
                history_titles = json.load(f)
        except: pass
        
    print(f"è½½å…¥å†å²è®°å½•: {len(history_titles)} æ¡")

    # ä¸´æ—¶åˆ—è¡¨ï¼šç”¨äºå­˜æ”¾æœ¬æ¬¡æ–°åˆ†æçš„æ ‡é¢˜ (è¿½åŠ åˆ° history.json)
    new_analyzed_titles = []
    
    # ä¸´æ—¶åˆ—è¡¨ï¼šç”¨äºå­˜æ”¾æœ¬æ¬¡å…¥é€‰çš„é«˜åˆ†æ–‡ç«  (è¿½åŠ åˆ° reports.json)
    qualified_items = []

    # === é˜¶æ®µä¸€ï¼šArXiv ===
    candidates = fetch_arxiv_smart(history_titles)
    
    for item in candidates:
        if len(qualified_items) >= CONFIG['FINAL_SAVE_COUNT']:
            print(">>> ä»Šæ—¥é«˜åˆ†åé¢å·²æ»¡ï¼Œåœæ­¢åˆ†æã€‚")
            break
            
        print(f"åˆ†æ: {item['title'][:30]}...")
        result = analyze_with_llm(item)
        
        # åªè¦åˆ†æè¿‡ï¼Œå°±åŠ å…¥å·²è¯» (é˜²æ­¢é‡å¤åˆ†æ)
        new_analyzed_titles.append(item['title'])
        
        if result['score'] >= CONFIG['MIN_SCORE']:
            item.update(result)
            item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
            # ç»™ä¸ªID
            item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_" + str(len(qualified_items))
            qualified_items.append(item)

    # === é˜¶æ®µäºŒï¼šScholar è¡¥è´§ ===
    if len(qualified_items) < CONFIG['FINAL_SAVE_COUNT']:
        needed = CONFIG['FINAL_SAVE_COUNT'] - len(qualified_items)
        print(f">>> Scholar è¡¥è´§ (ç¼º {needed} æ¡)...")
        
        scholar_candidates = fetch_google_scholar()
        for item in scholar_candidates:
            if len(qualified_items) >= CONFIG['FINAL_SAVE_COUNT']: break
            if item['title'] in history_titles: continue 
            if item['title'] in new_analyzed_titles: continue 
            
            print(f"åˆ†æ: {item['title'][:30]}...")
            result = analyze_with_llm(item)
            
            new_analyzed_titles.append(item['title'])
            
            if result['score'] >= CONFIG['MIN_SCORE']:
                item.update(result)
                item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
                item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_s_" + str(len(qualified_items))
                qualified_items.append(item)

    # === ä¿å­˜é€»è¾‘ (æˆªæ–­ä¿æŠ¤) ===
    
    # A. ä¿å­˜ history.json
    if new_analyzed_titles:
        final_history = new_analyzed_titles + history_titles
        # ã€å…³é”®ã€‘ä½¿ç”¨ CONFIG ä¸­çš„å‚æ•°è¿›è¡Œæˆªæ–­
        final_history = final_history[:CONFIG['MAX_HISTORY_SIZE']]
        
        os.makedirs(os.path.dirname(CONFIG["HISTORY_FILE"]), exist_ok=True)
        with open(CONFIG["HISTORY_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_history, f, ensure_ascii=False, indent=2)
            
    # B. ä¿å­˜ reports.json
    if qualified_items:
        qualified_items.sort(key=lambda x: x['score'], reverse=True)
        
        if os.path.exists(CONFIG["DATA_FILE"]):
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                old_reports = json.load(f)
        else: old_reports = []
        
        final_reports = qualified_items + old_reports
        # ã€å…³é”®ã€‘ä½¿ç”¨ CONFIG ä¸­çš„å‚æ•°è¿›è¡Œæˆªæ–­
        final_reports = final_reports[:CONFIG['MAX_REPORT_SIZE']]
        
        with open(CONFIG["DATA_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_reports, f, ensure_ascii=False, indent=2)

        # === æ¨é€é€»è¾‘ ===
        
        # 1. é’‰é’‰ (Top N)
        top_picks = [r for r in qualified_items if r['score'] >= CONFIG['PUSH_THRESHOLD']]
        if top_picks:
            push_limit = CONFIG['DINGTALK_PUSH_LIMIT']
            ding_md = "# ğŸ“… é‡åŒ–æ—¥æŠ¥\n\n"
            for r in top_picks[:push_limit]:
                ding_md += f"### {r['title']}\n**{r['score']}åˆ†** | {r['source']}\n> {r['summary']}\n[ğŸ“„ é“¾æ¥]({r['url']})\n\n---\n"
            if len(qualified_items) > push_limit:
                ding_md += f"\n> ğŸ’¡ è¿˜æœ‰ {len(qualified_items)-push_limit} ç¯‡å·²å‘é‚®ç®±ã€‚"
            send_dingtalk(ding_md)

        # 2. é‚®ä»¶ (é™åˆ¶é‚®ä»¶é•¿åº¦ï¼Œé˜²æ­¢å‘é€å¤±è´¥)
        # ã€å…³é”®ã€‘ä½¿ç”¨ CONFIG ä¸­çš„å‚æ•°æ§åˆ¶é‚®ä»¶åˆ—è¡¨é•¿åº¦
        email_items = qualified_items[:CONFIG['MAX_EMAIL_ITEM_LIMIT']]
        
        html = f"<h2>é‡åŒ–æ—¥æŠ¥ ({len(email_items)}ç¯‡)</h2><hr>"
        for r in email_items:
            color = "red" if r['score']>=8 else "black"
            html += f"<div><h3><a href='{r['url']}'>{r['title']}</a> <span style='color:{color}'>({r['score']}åˆ†)</span></h3><p>{r['source']} | {r['date']}</p><div style='background:#f9f9f9;padding:10px'>{r['summary']}</div></div><br>"
            
        if len(qualified_items) > len(email_items):
            html += f"<p>... (è¿˜æœ‰ {len(qualified_items) - len(email_items)} ç¯‡æœªæ˜¾ç¤º)</p>"
            
        send_email(f"é‡åŒ–æ—¥æŠ¥ - {len(qualified_items)}ç¯‡", html)
        
        print(f">>> æˆåŠŸæ›´æ–°: æ–°å¢å†å² {len(new_analyzed_titles)} æ¡, æ–°å¢ç²¾å {len(qualified_items)} æ¡")
    else:
        print(">>> æ— ç¬¦åˆæ¡ä»¶çš„æ–°æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
