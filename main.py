import os
import json
import datetime
import requests
import smtplib
import arxiv
from email.mime.text import MIMEText
from email.header import Header
from openai import OpenAI
from serpapi import GoogleSearch

# ==========================================
#              1. å…¨å±€é…ç½®åŒºåŸŸ (CONFIG)
#         âš™ï¸ çº¯å‡€ç‰ˆï¼šæ— é­”æ³•æ•°å­—
# ==========================================

CONFIG = {
    # --- æ–‡ä»¶è·¯å¾„ ---
    "DATA_FILE": "data/reports.json",      # ã€ç²¾ååº“ã€‘ç»™å‰ç«¯/é‚®ä»¶çœ‹ (åªå­˜é«˜åˆ†)
    "HISTORY_FILE": "data/history.json",   # ã€é»‘åå•ã€‘ç»™çˆ¬è™«å»é‡ç”¨ (å­˜æ‰€æœ‰è¯»è¿‡çš„)
    
    # --- æ ¸å¿ƒå®¹é‡æ§åˆ¶ ---
    "MAX_HISTORY_SIZE": 3000,         # é»‘åå•å®¹é‡ (å¿…é¡» > æŒ–æ˜æ·±åº¦)
    "MAX_REPORT_SIZE": 500,           # ç²¾ååº“å®¹é‡ (ä¿ç•™æœ€è¿‘500ç¯‡é«˜åˆ†)
    "MAX_EMAIL_ITEM_LIMIT": 50,       # é‚®ä»¶ä¿æŠ¤é˜€ (é˜²æ­¢é‚®ä»¶è¿‡å¤§å‘ä¸å‡ºå»)
    
    # --- é˜ˆå€¼è®¾ç½® ---
    "MIN_SCORE": 4.0,                 # 4åˆ†ä»¥ä¸Šæ‰æœ‰èµ„æ ¼è¿› reports.json
    "PUSH_THRESHOLD": 6.0,            # 6åˆ†ä»¥ä¸Šæ‰æ¨é’‰é’‰
    
    "FINAL_SAVE_COUNT": 15,           # æ¯å¤©æœ€å¤šæ”¶å½• 15 ç¯‡
    "DINGTALK_PUSH_LIMIT": 5,         # é’‰é’‰åªæ¨ Top 5
    
    # --- æŠ“å–è®¾ç½® ---
    "CANDIDATE_POOL_SIZE": 20,        # æ¯æ¬¡å¿…é¡»å‡‘é½ N ç¯‡ã€æœªè¯»ã€‘æ–‡ç« å–‚ç»™ AI
    "MAX_SEARCH_DEPTH": 1000,         # ArXiv æœ€å¤§ç¿»é¡µæ·±åº¦
    
    "FETCH_COUNT_GOOGLE_PER_QUERY": 10, # Google æ¯ä¸ªå…³é”®è¯æŠ“ N æ¡
    
    "MAX_TEXT_LENGTH_FOR_AI": 1200,   # æ‘˜è¦æˆªæ–­é•¿åº¦
    "SEARCH_YEAR": "2024",            # æœç´¢å¹´ä»½
    
    # --- å…³é”®è¯ ---
    "ARXIV_KEYWORDS": [
        "quantitative finance", "factor model", "portfolio optimization",
        "deep learning trading", "reinforcement learning trading", 
        "machine learning trading", "algorithm trading",          
        "market microstructure", "risk premia", "quantitative trading",
        "deep reinforcement learning", "transformer finance",
        "large language model trading"
    ],
    "GOOGLE_QUERIES": [
        'quantitative trading "reinforcement learning"', 
        'quantitative trading "deep learning"',          
        '"algorithmic trading" strategy',
        'transformers for "stock prediction"', 
        '"LLM" agents for "quantitative trading"'
    ]
}

# ==========================================
#              2. ç¯å¢ƒä¸å®¢æˆ·ç«¯
# ==========================================
LLM_API_KEY = os.environ.get("LLM_API_KEY")
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASS = os.environ.get("EMAIL_PASS")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")

client = OpenAI(api_key=LLM_API_KEY, base_url="https://api.deepseek.com")

# ==========================================
#              3. æŠ“å–å‡½æ•°
# ==========================================

def fetch_arxiv_smart(history_titles):
    """
    æ™ºèƒ½æŠ“å– ArXiv: 
    æ ¹æ® history.json é‡Œçš„æ ‡é¢˜è¿›è¡Œå»é‡ã€‚
    """
    target_count = CONFIG['CANDIDATE_POOL_SIZE']
    print(f"--- ArXiv æ™ºèƒ½æ·±æŒ– (ç›®æ ‡: æ‰¾åˆ° {target_count} ç¯‡æœªè¯») ---")
    
    keywords_query = " OR ".join([f'"{k}"' for k in CONFIG['ARXIV_KEYWORDS']])
    query = f'(cat:q-fin.* OR cat:cs.AI) AND ({keywords_query})'
    
    candidates = []
    try:
        search = arxiv.Search(
            query=query,
            max_results=CONFIG['MAX_SEARCH_DEPTH'], 
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        scanned = 0
        for r in search.results():
            scanned += 1
            if not any(tag.startswith(('q-fin', 'cs', 'stat')) for tag in r.categories): continue
            
            # === ç®€å•å»é‡ ===
            # åªå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œä¸åšå¤æ‚çš„å¤§å°å†™è½¬æ¢
            if r.title.strip() in history_titles:
                continue 
                
            candidates.append({
                "title": r.title.strip(), # å­˜çš„æ—¶å€™ä¹Ÿå»ä¸€ä¸‹ç©ºæ ¼
                "url": r.pdf_url, 
                "source": "ArXiv",
                "date": r.published.strftime("%Y-%m-%d"), 
                "abstract": r.summary,
                "broker": "Cornell Univ" 
            })
            
            if len(candidates) >= target_count:
                print(f"--> å·²å‡‘é½ {len(candidates)} ç¯‡æœªè¯»ï¼Œåœæ­¢æ‰«æã€‚")
                break
                
        print(f"æ‰«æç»“æŸ: å…±æ‰«æ {scanned} ç¯‡ï¼Œç­›é€‰å‡º {len(candidates)} ç¯‡æ–°æ–‡ç« ã€‚")
        return candidates
    except Exception as e:
        print(f"ArXiv Error: {e}")
        return []

def fetch_google_scholar():
    if not SERPAPI_KEY: return []
    print(f"--- æ­£åœ¨æŠ“å– Google Scholar ---")
    all_results = []
    for base_query in CONFIG['GOOGLE_QUERIES']:
        try:
            params = {
                "engine": "google_scholar", 
                "q": f'{base_query} after:{CONFIG["SEARCH_YEAR"]}',
                "api_key": SERPAPI_KEY, 
                "num": CONFIG['FETCH_COUNT_GOOGLE_PER_QUERY'], 
                "hl": "en"
            }
            search = GoogleSearch(params)
            for item in search.get_dict().get("organic_results", []):
                if 'link' not in item: continue
                all_results.append({
                    "title": item.get("title").strip(), # å»ç©ºæ ¼
                    "url": item.get("link"),
                    "source": "Scholar", 
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "abstract": item.get("snippet", item.get("title")), 
                    "broker": "Google Scholar"
                })
        except: pass
    return all_results

# ==========================================
#              4. AI åˆ†æ
# ==========================================

def analyze_with_llm(item):
    try:
        prompt = f"""
        ä½ æ˜¯ä¸€åé‡åŒ–äº¤æ˜“å‘˜ã€‚è¯„ä¼°ä»¥ä¸‹è®ºæ–‡å¯¹â€œå®æˆ˜äº¤æ˜“â€çš„ä»·å€¼ã€‚
        æ ‡é¢˜: {item['title']}
        æ‘˜è¦: {item['abstract'][:CONFIG['MAX_TEXT_LENGTH_FOR_AI']]}
        
        1. è¯„åˆ†(0-10): å®æˆ˜å¼º(RL/DeepLearning/Alpha)ç»™8-10åˆ†ï¼Œçº¯ç†è®ºç»™3-5åˆ†ï¼Œæ— å…³ç»™0åˆ†ã€‚
        2. ä¸­æ–‡æ‘˜è¦: ç¿»è¯‘æ ¸å¿ƒï¼Œä¿ç•™æœ¯è¯­ã€‚
        è¿”å›JSON: {{"score": 0, "summary": "..."}}
        """
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except: return {"score": 0, "summary": "Error"}

def send_dingtalk(msg):
    if not DINGTALK_WEBHOOK: return
    try: requests.post(DINGTALK_WEBHOOK, json={"msgtype": "markdown", "markdown": {"title": "é‡åŒ–æ—¥æŠ¥", "text": msg}})
    except: pass

def send_email(subject, html):
    if not EMAIL_USER or not EMAIL_PASS: return
    try:
        msg = MIMEText(html, 'html', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_USER
        smtp = smtplib.SMTP_SSL('smtp.qq.com', 465)
        smtp.login(EMAIL_USER, EMAIL_PASS)
        smtp.send_message(msg)
        smtp.quit()
    except: pass

# ==========================================
#              5. ä¸»ç¨‹åº (Simple & Clean)
# ==========================================

def main():
    print(">>> ä»»åŠ¡å¼€å§‹")
    
    # 1. åŠ è½½ã€é»‘åå•ã€‘
    history_titles = []
    if os.path.exists(CONFIG["HISTORY_FILE"]):
        try:
            with open(CONFIG["HISTORY_FILE"], 'r', encoding='utf-8') as f:
                history_titles = json.load(f)
        except: pass
        
    print(f"è½½å…¥å†å²è®°å½•: {len(history_titles)} æ¡")

    # æœ¬æ¬¡è¿è¡Œæ–°å¢çš„å·²è¯»æ ‡é¢˜ (æ— è®ºåˆ†é«˜ä½ï¼Œéƒ½è¿½åŠ åˆ°è¿™é‡Œ)
    new_analyzed_titles = []
    
    # æœ¬æ¬¡è¿è¡Œå…¥é€‰çš„é«˜åˆ†æ–‡ç«  (è¿½åŠ åˆ° reports.json)
    qualified_items = []

    # === é˜¶æ®µä¸€ï¼šArXiv ===
    candidates = fetch_arxiv_smart(history_titles)
    
    for item in candidates:
        if len(qualified_items) >= CONFIG['FINAL_SAVE_COUNT']:
            print(">>> [ArXiv] ä»Šæ—¥é«˜åˆ†åé¢å·²æ»¡ï¼Œåœæ­¢åˆ†æã€‚")
            break
            
        print(f"åˆ†æ: {item['title'][:30]}...")
        result = analyze_with_llm(item)
        
        # åªè¦åˆ†æè¿‡ï¼Œå°±è®°å½• (ç”¨äºå»é‡)
        new_analyzed_titles.append(item['title'])
        
        if result['score'] >= CONFIG['MIN_SCORE']:
            item.update(result)
            item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
            item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_" + str(len(qualified_items))
            qualified_items.append(item)

    # === é˜¶æ®µäºŒï¼šScholar è¡¥è´§ (ç®€å•ç‰ˆå»é‡) ===
    if len(qualified_items) < CONFIG['FINAL_SAVE_COUNT']:
        needed = CONFIG['FINAL_SAVE_COUNT'] - len(qualified_items)
        print(f">>> Scholar è¡¥è´§ (ç¼º {needed} æ¡)...")
        
        scholar_candidates = fetch_google_scholar()
        for item in scholar_candidates:
            if len(qualified_items) >= CONFIG['FINAL_SAVE_COUNT']: break
            
            # --- ç®€å•å»é‡é€»è¾‘ ---
            # 1. æŸ¥å†å²æ€»è´¦
            if item['title'] in history_titles: continue 
            # 2. æŸ¥åˆšæ‰ ArXiv çš„è´¦ (é˜²æ­¢æœ¬æ¬¡è¿è¡Œæ’è½¦)
            if item['title'] in new_analyzed_titles: continue 
            
            print(f"åˆ†æ: {item['title'][:30]}...")
            result = analyze_with_llm(item)
            
            new_analyzed_titles.append(item['title'])
            
            if result['score'] >= CONFIG['MIN_SCORE']:
                item.update(result)
                item['fetch_date'] = datetime.datetime.now().strftime("%Y-%m-%d")
                item['id'] = datetime.datetime.now().strftime("%Y%m%d") + "_s_" + str(len(qualified_items))
                qualified_items.append(item)

    # === ä¿å­˜é€»è¾‘ ===
    
    # A. ä¿å­˜ history.json (æ‰€æœ‰æ ‡é¢˜ï¼Œç”¨äºå»é‡)
    if new_analyzed_titles:
        final_history = new_analyzed_titles + history_titles
        final_history = final_history[:CONFIG['MAX_HISTORY_SIZE']]
        
        os.makedirs(os.path.dirname(CONFIG["HISTORY_FILE"]), exist_ok=True)
        with open(CONFIG["HISTORY_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_history, f, ensure_ascii=False, indent=2)
            
    # B. ä¿å­˜ reports.json (ä»…é«˜åˆ†æ–‡ç« ï¼Œç”¨äºå±•ç¤º)
    if qualified_items:
        qualified_items.sort(key=lambda x: x['score'], reverse=True)
        
        if os.path.exists(CONFIG["DATA_FILE"]):
            with open(CONFIG["DATA_FILE"], 'r', encoding='utf-8') as f:
                old_reports = json.load(f)
        else: old_reports = []
        
        final_reports = qualified_items + old_reports
        final_reports = final_reports[:CONFIG['MAX_REPORT_SIZE']]
        
        with open(CONFIG["DATA_FILE"], 'w', encoding='utf-8') as f:
            json.dump(final_reports, f, ensure_ascii=False, indent=2)

        # === æ¨é€é€»è¾‘ ===
        
        # 1. é’‰é’‰ (Top 5)
        top_picks = [r for r in qualified_items if r['score'] >= CONFIG['PUSH_THRESHOLD']]
        if top_picks:
            push_limit = CONFIG['DINGTALK_PUSH_LIMIT']
            ding_md = "# ğŸ“… é‡åŒ–æ—¥æŠ¥\n\n"
            for r in top_picks[:push_limit]:
                ding_md += f"### {r['title']}\n**{r['score']}åˆ†** | {r['source']}\n> {r['summary']}\n[ğŸ“„ é“¾æ¥]({r['url']})\n\n---\n"
            if len(qualified_items) > push_limit:
                ding_md += f"\n> ğŸ’¡ è¿˜æœ‰ {len(qualified_items)-push_limit} ç¯‡å·²å‘é‚®ç®±ã€‚"
            send_dingtalk(ding_md)

        # 2. é‚®ä»¶ (é™åˆ¶æœ€å¤§æ¡æ•°)
        email_items = qualified_items[:CONFIG['MAX_EMAIL_ITEM_LIMIT']]
        html = f"<h2>é‡åŒ–æ—¥æŠ¥ ({len(qualified_items)}ç¯‡)</h2><hr>"
        for r in email_items:
            color = "red" if r['score']>=8 else "black"
            html += f"<div><h3><a href='{r['url']}'>{r['title']}</a> <span style='color:{color}'>({r['score']}åˆ†)</span></h3><p>{r['source']} | {r['date']}</p><div style='background:#f9f9f9;padding:10px'>{r['summary']}</div></div><br>"
        
        if len(qualified_items) > len(email_items):
             html += f"<p>... (è¿˜æœ‰ {len(qualified_items) - len(email_items)} ç¯‡æœªæ˜¾ç¤º)</p>"

        send_email(f"é‡åŒ–æ—¥æŠ¥ - {len(qualified_items)}ç¯‡", html)
        
        print(f">>> æˆåŠŸæ›´æ–°: æ–°å¢å†å² {len(new_analyzed_titles)} æ¡, æ–°å¢ç²¾å {len(qualified_items)} æ¡")
    else:
        print(">>> æ— ç¬¦åˆæ¡ä»¶çš„æ–°æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
