[
  {
    "title": "A3T-GCN for FTSE100 Components Price Forecasting",
    "url": "https://arxiv.org/pdf/2511.21873v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We examine the predictive power of a novel hybrid A3T-GCN architecture for forecasting closing stock prices of FTSE100 constituents. The dataset comprises 79 companies and 375,329 daily observations from 2007 to 2024, with node features including technical indicators (RSI, MACD), normalized and log returns, and annualized log returns over multiple windows (ALR1W, ALR2W, ALR1M, ALR2M). Graphs are constructed based on sector classifications and correlations of returns or financial ratios. Our results show that the A3T-GCN model using annualized log-returns and shorter sequence lengths improves prediction accuracy while reducing computational requirements. Additionally, longer historical sequences yield only modest improvements, highlighting their importance for longer-term forecasts.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了一种新型混合A3T-GCN架构在预测富时100指数成分股收盘价方面的预测能力。数据集包含2007年至2024年79家公司的375,329个日度观测值，节点特征包括技术指标（RSI、MACD）、归一化和对数收益率，以及多个时间窗口的年化对数收益率（ALR1W、ALR2W、ALR1M、ALR2M）。图结构基于行业分类和收益率或财务比率的相关系数构建。结果表明，使用年化对数收益率和较短序列长度的A3T-GCN模型提高了预测精度，同时降低了计算需求。此外，较长的历史序列仅带来有限的改进，突显了其对长期预测的重要性。",
    "fetch_date": "2026-01-03",
    "id": "20260103_2b03ff62"
  },
  {
    "title": "Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba",
    "url": "https://arxiv.org/pdf/2511.22101v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "The report goes through the main steps of replicating and improving the article \"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.\" The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过复制并改进《在Uniswap V3中使用深度强化学习的自适应流动性提供》一文，提出了一种结合Mamba与DDQN（Dueling Double Deep Q-networks）及新奖励函数的新结构。论文涵盖了从Uniswap Subgraph获取数据、实现细节、结果分析，并引入了两个新基准进行比较。尽管尚未在所有数据集上应用，但新模型在部分测试中表现更优，且具有更强的理论支持。",
    "fetch_date": "2026-01-03",
    "id": "20260103_cb6973a7"
  },
  {
    "title": "LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline",
    "url": "https://arxiv.org/pdf/2512.07867v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文开发了一种基于LLM的透明、可审计的宏观金融压力测试管道，结合结构化提示与可选的国家基本面及新闻检索。该系统为G7国家生成机器可读的宏观经济情景（涵盖GDP增长、通胀及政策利率），并通过基于因子的映射将其转化为投资组合损失，从而支持相对于传统计量经济学基线的在险价值（VaR）和预期短缺（ES）评估。研究表明，LLM能生成连贯且针对特定国家的压力叙事，产生稳定的尾部风险放大效应，且对检索选择的敏感性有限。全面的合理性检查、情景诊断和基于ANOVA的方差分解表明，风险变异主要由投资组合构成和提示设计驱动，而非检索机制。该管道包含快照、确定性模式和哈希验证工件，以确保可重复性和可审计性。总体而言，结果表明，当LLM生成的宏观情景与透明结构和严格验证相结合时，可为实战交易提供有价值的风险模拟工具。",
    "fetch_date": "2026-01-03",
    "id": "20260103_0b660ac8"
  },
  {
    "title": "Black-Litterman and ESG Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.21850v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We introduce a simple portfolio optimization strategy using ESG data with the Black-Litterman allocation framework. ESG scores are used as a bias for Stein shrinkage estimation of equilibrium risk premiums used in assigning Black-Litterman asset weights. Assets are modeled as multivariate affine normal-inverse Gaussian variables using CVaR as a risk measure. This strategy, though very simple, when employed with a soft turnover constraint is exceptionally successful. Portfolios are reallocated daily over a 4.7 year period, each with a different set of hyperparameters used for optimization. The most successful strategies have returns of approximately 40-45% annually.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种结合ESG数据的简单投资组合优化策略，采用Black-Litterman资产配置框架。ESG评分被用作Stein收缩估计的偏差，用于计算Black-Litterman资产权重中的均衡风险溢价。资产被建模为多元仿射正态逆高斯变量，并使用CVaR作为风险度量。该策略虽然简单，但在采用软换手率约束时表现异常出色。投资组合在4.7年期间每日重新配置，每次使用不同的超参数进行优化。最成功的策略年化回报率约为40-45%。",
    "fetch_date": "2026-01-03",
    "id": "20260103_44a40e03"
  },
  {
    "title": "Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting",
    "url": "https://arxiv.org/pdf/2512.07860v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "This paper investigates an optimal integration of deep learning with financial models for robust asset price forecasting. Specifically, we developed a hybrid framework combining a Long Short-Term Memory (LSTM) network with the Merton-Lévy jump-diffusion model. To optimise this framework, we employed the Grey Wolf Optimizer (GWO) for the LSTM hyperparameter tuning, and we explored three calibration methods for the Merton-Levy model parameters: Artificial Neural Networks (ANNs), the Marine Predators Algorithm (MPA), and the PyTorch-based TorchSDE library. To evaluate the predictive performance of our hybrid model, we compared it against several benchmark models, including a standard LSTM and an LSTM combined with the Fractional Heston model. This evaluation used three real-world financial datasets: Brent oil prices, the STOXX 600 index, and the IT40 index. Performance was assessed using standard metrics, including Mean Squared Error (MSE), Mean Absolute Error(MAE), Mean Squared Percentage Error (MSPE), and the coefficient of determination (R2). Our experimental results demonstrate that the hybrid model, combining a GWO-optimized LSTM network with the Levy-Merton Jump-Diffusion model calibrated using an ANN, outperformed the base LSTM model and all other models developed in this study.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文研究了一种将深度学习与金融模型相结合进行资产价格稳健预测的混合框架。具体而言，作者开发了一个结合长短期记忆网络与Merton-Lévy跳跃扩散模型的混合模型。为优化该框架，采用灰狼优化器进行LSTM超参数调优，并探索了三种Merton-Levy模型参数校准方法：人工神经网络、海洋捕食者算法和基于PyTorch的TorchSDE库。通过在布伦特原油价格、STOXX 600指数和IT40指数三个真实金融数据集上的实验评估，使用均方误差、平均绝对误差、均方百分比误差和决定系数等标准指标，与标准LSTM及LSTM结合分数Heston模型等基准模型进行比较。实验结果表明，结合GWO优化LSTM与Levy-Merton跳跃扩散模型的混合框架在预测性能上表现优异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_b3fb4ca0"
  },
  {
    "title": "Portfolio Optimization via Transfer Learning",
    "url": "https://arxiv.org/pdf/2511.21221v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "认识到资产市场通常表现出共享的信息特征，我们开发了一种基于迁移学习的投资组合策略，该策略利用跨市场信息，通过前向验证来提升目标市场的投资表现。我们的策略渐近地识别并利用信息丰富的数据集，选择性地纳入有效信息，同时摒弃误导性信息。这使得我们的策略能够渐近地实现最大夏普比率。通过数值研究以及两个投资组合的案例研究（一个由A股和H股双重上市的股票组成，另一个包含美国各行业的股票），展示了该策略的优异表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_d33d2a08"
  },
  {
    "title": "Constrained deep learning for pricing and hedging european options in incomplete markets",
    "url": "https://arxiv.org/pdf/2511.20837v1",
    "source": "ArXiv",
    "date": "2025-11-25",
    "abstract": "In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在不完全金融市场中，欧式期权的定价和对冲因不可对冲风险而缺乏唯一无套利解。本文提出一种约束深度学习方法来最小化损益分布，使用单一神经网络表示期权价格函数，其梯度作为对冲策略，通过强制自融资组合条件的损失函数进行优化。针对期权收益非光滑特性（如香草看涨期权在平价点不可微，数字期权不连续）与神经网络固有平滑性的冲突，比较了无约束网络与显式嵌入终端收益条件的约束架构，借鉴偏微分方程求解技术。框架假设两种可交易资产：标的资产和捕捉波动率动态的流动性看涨期权。数值实验评估了该方法在具有不同非光滑性的简单期权、奇异Equinox期权及市场跳跃场景下的表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_061536e2"
  },
  {
    "title": "Evolutionary Discovery of Heuristic Policies for Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2511.23122v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\\textbf{\\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \\method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《Evolutionary Discovery of Heuristic Policies for Traffic Signal Control》提出了一种名为Temporal Policy Evolution for Traffic的方法，旨在解决交通信号控制（TSC）中经典启发式方法过于简化、深度强化学习（DRL）泛化能力差且策略不透明，以及在线大型语言模型（LLMs）延迟高且缺乏环境特定优化的问题。该方法利用LLMs作为进化引擎，通过结构化状态抽象（SSA）将高维交通数据转换为时序逻辑事实，并结合信用分配反馈（CAF）追踪微观决策错误与宏观结果的关系，从而在无需训练的情况下生成轻量级、鲁棒的启发式策略，针对特定交通环境进行优化。",
    "fetch_date": "2026-01-03",
    "id": "20260103_4318e585"
  },
  {
    "title": "Factors Influencing Cryptocurrency Prices: Evidence from Bitcoin, Ethereum, Dash, Litecoin, and Monero",
    "url": "https://arxiv.org/pdf/2511.22782v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "This paper examines factors that influence prices of most common five cryptocurrencies such as Bitcoin, Ethereum, Dash, Litecoin, and Monero over 2010-2018 using weekly data. The study employs ARDL technique and documents several findings. First, cryptomarket-related factors such as market beta, trading volume, and volatility appear to be significant determinant for all five cryptocurrencies both in short- and long-run. Second, attractiveness of cryptocurrencies also matters in terms of their price determination, but only in long-run. This indicates that formation (recognition) of the attractiveness of cryptocurrencies are subjected to time factor. In other words, it travels slowly within the market. Third, SP500 index seems to have weak positive long-run impact on Bitcoin, Ethereum, and Litcoin, while its sign turns to negative losing significance in short-run, except Bitcoin that generates an estimate of -0.20 at 10% significance level. Lastly, error-correction models for Bitcoin, Etherem, Dash, Litcoin, and Monero show that cointegrated series cannot drift too far apart, and converge to a long-run equilibrium at a speed of 23.68%, 12.76%, 10.20%, 22.91%, and 14.27% respectively.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文使用ARDL技术分析了2010-2018年比特币、以太坊、达世币、莱特币和门罗币这五种主要加密货币的周度数据，探讨了影响其价格的因素。研究发现：1）加密货币市场相关因素（如市场贝塔、交易量、波动性）在短期和长期对所有五种货币均有显著影响；2）加密货币的吸引力（如认可度）仅在长期影响价格，表明市场认知形成缓慢；3）标普500指数对比特币、以太坊和莱特币有微弱的长期正向影响，但短期影响不显著（除比特币在10%显著性水平下为负向）；4）误差修正模型显示各币种均存在长期均衡关系，收敛速度在10.20%-23.68%之间。",
    "fetch_date": "2026-01-03",
    "id": "20260103_72eb24b5"
  },
  {
    "title": "Beta-Dependent Gamma Feedback and Endogenous Volatility Amplification in Option Markets",
    "url": "https://arxiv.org/pdf/2511.22766v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "We develop a theoretical framework that aims to link micro-level option hedging and stock-specific factor exposure with macro-level market turbulence and explain endogenous volatility amplification during gamma-squeeze events. By explicitly modeling market-maker delta-neutral hedging and incorporating beta-dependent volatility normalization, we derive a stability condition that characterizes the onset of a gamma-squeeze event. The model captures a nonlinear recursive feedback loop between market-maker hedging and price movements and the resulting self-reinforcing dynamics. From a complex-systems perspective, the dynamics represent a bounded nonlinear response in which effective gain depends jointly on beta-normalized shock perception and gamma-scaled sensitivity. Our analysis highlights that low-beta stocks exhibit disproportionately strong feedback even for modest absolute price movements.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个理论框架，旨在将微观层面的期权对冲和股票特定因子暴露与宏观层面的市场动荡联系起来，解释伽玛挤压事件中的内生波动率放大。通过显式建模做市商的Delta中性对冲并纳入Beta依赖的波动率归一化，推导出表征伽玛挤压事件发生的稳定性条件。模型捕捉了做市商对冲与价格变动之间的非线性递归反馈循环及其产生的自我强化动态。从复杂系统视角看，该动态代表了一种有界非线性响应，其中有效增益同时取决于Beta归一化的冲击感知和伽玛缩放敏感性。分析强调，低Beta股票即使面对温和的绝对价格变动，也会表现出不成比例的强烈反馈效应。",
    "fetch_date": "2026-01-03",
    "id": "20260103_40682348"
  },
  {
    "title": "The Risk-Adjusted Intelligence Dividend: A Quantitative Framework for Measuring AI Return on Investment Integrating ISO 42001 and Regulatory Exposure",
    "url": "https://arxiv.org/pdf/2511.21975v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Organizations investing in artificial intelligence face a fundamental challenge: traditional return on investment calculations fail to capture the dual nature of AI implementations, which simultaneously reduce certain operational risks while introducing novel exposures related to algorithmic malfunction, adversarial attacks, and regulatory liability. This research presents a comprehensive financial framework for quantifying AI project returns that explicitly integrates changes in organizational risk profiles. The methodology addresses a critical gap in current practice where investment decisions rely on optimistic benefit projections without accounting for the probabilistic costs of AI-specific threats including model drift, bias-related litigation, and compliance failures under emerging regulations such as the European Union Artificial Intelligence Act and ISO/IEC 42001. Drawing on established risk quantification methods, including annual loss expectancy calculations and Monte Carlo simulation techniques, this framework enables practitioners to compute net benefits that incorporate both productivity gains and the delta between pre-implementation and post-implementation risk exposures. The analysis demonstrates that accurate AI investment evaluation requires explicit modeling of control effectiveness, reserve requirements for algorithmic failures, and the ongoing operational costs of maintaining model performance. Practical implications include specific guidance for establishing governance structures, conducting phased validations, and integrating risk-adjusted metrics into capital allocation decisions, ultimately enabling evidence-based AI portfolio management that satisfies both fiduciary responsibilities and regulatory mandates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "组织投资人工智能面临一个根本性挑战：传统的投资回报率计算未能捕捉AI实施的双重性——在降低某些运营风险的同时，引入了与算法故障、对抗性攻击和监管责任相关的新型风险敞口。本研究提出了一个全面的财务框架，用于量化AI项目回报，明确整合组织风险状况的变化。该方法解决了当前实践中的一个关键空白，即投资决策依赖于乐观的效益预测，而未考虑AI特定威胁（包括模型漂移、偏见相关诉讼以及欧盟人工智能法案和ISO/IEC 42001等新兴法规下的合规失败）的概率成本。借鉴已建立的风险量化方法，包括年度损失预期计算和蒙特卡洛模拟技术，该框架使从业者能够计算净效益，既包含生产力收益，也包含实施前和实施后风险敞口之间的差异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_dbd49bab"
  },
  {
    "title": "Extended Convolution Bounds on the Fréchet Problem: Robust Risk Aggregation and Risk Sharing",
    "url": "https://arxiv.org/pdf/2511.21929v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "In this paper, we provide extended convolution bounds for the Fréchet problem and discuss related implications in quantitative risk management. First, we establish a new form of inequality for the Range-Value-at-Risk (RVaR). Based on this inequality, we obtain bounds for robust risk aggregation with dependence uncertainty for (i) RVaR, (ii) inter-RVaR difference and (iii) inter-quantile difference, and provide sharpness conditions. These bounds are called extended convolution bounds, which not only complement the results in the literature (convolution bounds in Blanchet et al. (2025)) but also offer results for some variability measures. Next, applying the above inequality, we study the risk sharing for the averaged quantiles (corresponding to risk sharing for distortion risk measures with special inverse S-shaped distortion functions), which is a non-convex optimization problem. We obtain the expression of the minimal value of the risk sharing and the explicit expression for the corresponding optimal allocation, which is comonotonic risk sharing for large losses and counter-comonotonic risk sharing for small losses or large gains. Finally, we explore the dependence structure for the optimal allocations, showing that the optimal allocation does not exist if the risk is not bounded from above.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对Fréchet问题提出了扩展卷积界，并探讨了其在量化风险管理中的应用。首先，建立了Range-Value-at-Risk（RVaR）的新不等式，并基于此获得了在依赖不确定性下关于（i）RVaR、（ii）RVaR间差值及（iii）分位数间差值的稳健风险聚合界，同时给出了锐度条件。这些扩展卷积界不仅补充了现有文献结果，还为一些变异性度量提供了结论。其次，应用上述不等式研究了平均分位数的风险分担问题（对应于具有特殊逆S形扭曲函数的扭曲风险度量的风险分担），这是一个非凸优化问题。我们获得了风险分担的最小值表达式及相应最优分配的显式表达式，即对大损失采用共单调风险分担，对小损失或大收益采用反共单调风险分担。最后，探讨了相关依赖结构。",
    "fetch_date": "2026-01-03",
    "id": "20260103_a2552928"
  },
  {
    "title": "Informative Risk Measures in the Banking Industry: A Proposal based on the Magnitude-Propensity Approach",
    "url": "https://arxiv.org/pdf/2511.21556v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Despite decades of research in risk management, most of the literature has focused on scalar risk measures (like e.g. Value-at-Risk and Expected Shortfall). While such scalar measures provide compact and tractable summaries, they provide a poor informative value as they miss the intrinsic multivariate nature of risk.To contribute to a paradigmatic enhancement, and building on recent theoretical work by Faugeras and Pagés (2024), we propose a novel multivariate representation of risk that better reflects the structure of potential portfolio losses, while maintaining desirable properties of interpretability and analytical coherence. The proposed framework extends the classical frequency-severity approach and provides a more comprehensive characterization of extreme events. Several empirical applications based on real-world data demonstrate the feasibility, robustness and practical relevance of the methodology, suggesting its potential for both regulatory and managerial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "尽管风险管理研究已有数十年，但多数文献聚焦于标量风险度量（如风险价值和预期缺口）。虽然这些标量度量提供了简洁易处理的总结，但其信息价值有限，因为它们忽略了风险固有的多元性。为促进范式改进，并基于Faugeras和Pagés（2024）的最新理论工作，本文提出了一种新颖的多元风险表示方法，能更好地反映潜在投资组合损失的结构，同时保持可解释性和分析一致性的理想特性。该框架扩展了经典的频率-严重性方法，提供了对极端事件的更全面刻画。基于真实数据的多个实证应用证明了该方法的可行性、稳健性和实际相关性，表明其在监管和管理应用中的潜力。",
    "fetch_date": "2026-01-03",
    "id": "20260103_200d1274"
  },
  {
    "title": "The Quantum Network of Assets: A Non-Classical Framework for Market Correlation and Structural Risk",
    "url": "https://arxiv.org/pdf/2511.21515v2",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Classical correlation matrices capture only linear and pairwise co-movements, leaving higher-order, nonlinear, and state-dependent interactions of financial markets unrepresented. This paper introduces the Quantum Network of Assets (QNA), a density-matrix based framework that embeds cross-asset dependencies into a quantum-information representation. The approach does not assume physical quantum effects but uses the mathematical structure of density operators, entropy, and mutual information to describe market organisation at a structural level.\n  Within this framework we define two structural measures: the Entanglement Risk Index (ERI), which summarises global non-separability and the compression of effective market degrees of freedom, and the Quantum Early-Warning Signal (QEWS), which tracks changes in entropy to detect latent information build-up. These measures reveal dependency geometry that classical covariance-based tools cannot capture.\n  Using NASDAQ-100 data from 2024-2025, we show that quantum entropy displays smoother evolution and clearer regime distinctions than classical entropy, and that ERI rises during periods of structural tightening even when volatility remains low. Around the 2025 US tariff announcement, QEWS shows a marked pre-event increase in structural tension followed by a sharp collapse after the announcement, indicating that structural transitions can precede price movements without implying predictive modelling.\n  QNA therefore provides a structural diagnostic of market fragility, regime shifts, and latent information flow. The framework suggests new directions for systemic risk research by linking empirical asset networks with tools from quantum information theory.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "经典相关性矩阵仅捕捉线性和成对的共动，未能体现金融市场的高阶、非线性和状态依赖的相互作用。本文引入资产量子网络（QNA），一种基于密度矩阵的框架，将跨资产依赖性嵌入量子信息表示中。该方法不假设物理量子效应，而是利用密度算子、熵和互信息的数学结构来描述市场在结构层面的组织。在此框架内，我们定义了两个结构性度量：纠缠风险指数（ERI），用于总结全局不可分性和有效市场自由度的压缩；以及量子早期预警信号（QEWS），通过追踪熵的变化来检测潜在信息的积累。这些度量揭示了基于经典协方差的工具无法捕捉的依赖性几何结构。使用2024-2025年的纳斯达克100指数数据，我们表明量子熵比经典熵展现出更平滑的演化和更清晰的制度区分，并且ERI在结构紧缩期间上升，即使波动率保持不变。",
    "fetch_date": "2026-01-03",
    "id": "20260103_ee106c42"
  },
  {
    "title": "Algorithmic trading and ai: A review of strategies and market impact",
    "url": "https://www.researchgate.net/profile/Titilola-Falaiye/publication/378548435_Algorithmic_Trading_and_AI_A_Review_of_Strategies_and_Market_Impact/links/65e60893e7670d36abfd1738/Algorithmic-Trading-and-AI-A-Review-of-Strategies-and-Market-Impact.pdf",
    "source": "Scholar",
    "date": "2026-01-03",
    "abstract": "… This review seeks to delve into the intricate strategies employed in algorithmic trading, … redefined these strategies. Furthermore, it aims to unravel the impact of algorithmic trading on …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "该论文综述了算法交易中采用的复杂策略，并探讨了人工智能（特别是机器学习）如何重新定义这些策略。同时，文章旨在揭示算法交易对市场的影响，包括市场效率、流动性和波动性等方面。",
    "fetch_date": "2026-01-03",
    "id": "20260103_fc1d08db"
  },
  {
    "title": "Retail Investor Horizon and Earnings Announcements",
    "url": "https://arxiv.org/pdf/2512.00280v2",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "This paper moves beyond aggregate measures of retail intensity to explore investment horizon as a distinguishing feature of earnings-related return patterns. Using self-reported holding periods from StockTwits (2010-2021), we observe that separating retail activity into \"long-horizon\" and \"short-horizon\" cohorts reveals divergent price anomalies. Long-horizon composition is associated with underreaction, characterized by larger initial reactions and pronounced Post-Earnings Announcement Drift (PEAD), suggesting a slow but persistent convergence toward fundamental value. In contrast, short-horizon activity parallels sentiment-driven overreaction, where elevated pre-event sentiment precedes weaker subsequent performance and price reversals. A zero-cost strategy exploiting this heterogeneity, going long on long-horizon stocks and short on short-horizon stocks, yields risk-adjusted alphas of 0.43% per month. These findings suggest that accounting for investment horizon helps disentangles the fundamental signal in retail flow from speculative noise.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文超越零售投资者整体参与度的衡量，通过投资期限（基于StockTwits 2010-2021年自报持有期）区分“长期”与“短期”零售投资者活动，揭示了财报公告前后不同的价格异常模式。长期投资者活动与反应不足相关，表现为较大的初始反应和显著的财报公告后漂移（PEAD），暗示向基本面价值的缓慢持续收敛；而短期投资者活动则与情绪驱动的过度反应相关，表现为事件前情绪高涨、后续表现疲软及价格反转。利用这种异质性构建的零成本策略（做多长期股票、做空短期股票）每月可获得0.43%的风险调整后阿尔法收益。这表明，考虑投资期限有助于从零售资金流中分离基本面信号与投机噪音。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b70a95e0"
  },
  {
    "title": "Forward-Oriented Causal Observables for Non-Stationary Financial Markets",
    "url": "https://arxiv.org/pdf/2512.24621v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We study short-horizon forecasting in financial time series under strict causal constraints, treating the market as a non-stationary stochastic system in which any predictive observable must be computable online from information available up to the decision time. Rather than proposing a machine-learning predictor or a direct price-forecast model, we focus on \\emph{constructing} an interpretable causal signal from heterogeneous micro-features that encode complementary aspects of the dynamics (momentum, volume pressure, trend acceleration, and volatility-normalized price location). The construction combines (i) causal centering, (ii) linear aggregation into a composite observable, (iii) causal stabilization via a one-dimensional Kalman filter, and (iv) an adaptive ``forward-like'' operator that mixes the composite signal with a smoothed causal derivative term. The resulting observable is mapped into a transparent decision functional and evaluated through realized cumulative returns and turnover. An application to high-frequency EURUSDT (1-minute) illustrates that causally constructed observables can exhibit substantial economic relevance in specific regimes, while degrading under subsequent regime shifts, highlighting both the potential and the limitations of causal signal design in non-stationary markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们研究在严格因果约束下的金融时间序列短期预测，将市场视为一个非平稳随机系统，其中任何预测性可观测值必须能够在线从决策时间前可获得的信息中计算得出。我们并非提出一个机器学习预测器或直接的价格预测模型，而是专注于从编码动态互补方面（动量、成交量压力、趋势加速和波动率归一化价格位置）的异构微观特征中构建一个可解释的因果信号。该构建结合了（i）因果中心化，（ii）线性聚合为复合可观测值，（iii）通过一维卡尔曼滤波器进行因果稳定化，以及（iv）一个将复合信号与平滑因果导数项混合的自适应“前向”算子。所得可观测值被映射到一个透明的决策函数中，并通过实现的累积收益和换手率进行评估。对高频EURUSDT（1分钟）的应用表明，因果构建的可观测值在特定制度下可展现出显著的经济相关性，而在次优条件下则会退化。",
    "fetch_date": "2026-01-02",
    "id": "20260102_fb4c17f1"
  },
  {
    "title": "Generative AI-enhanced Sector-based Investment Portfolio Construction",
    "url": "https://arxiv.org/pdf/2512.24526v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025).\n  Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency.\n  This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了如何将领先提供商（OpenAI、Google、Anthropic、DeepSeek和xAI）的大型语言模型（LLMs）应用于量化行业投资组合构建。研究使用LLMs识别标普500行业指数中的可投资股票范围，并评估其选股结果与经典投资组合优化方法结合后的表现。每个模型被提示为每个行业选择和加权20只股票，结果投资组合在两个不同的样本外时期（稳定市场阶段：2025年1月至3月，波动阶段：2025年4月至6月）与其相应的行业指数进行比较。结果显示，LLM投资组合表现具有强烈的时间依赖性：在稳定市场条件下，LLM加权投资组合在累计回报和风险调整（夏普比率）指标上经常优于行业指数；但在波动时期，许多LLM投资组合表现不佳，表明当前模型可能难以适应其训练数据中代表性不足的制度转换或高波动环境。重要的是，当基于LLM的选股与传统优化方法结合时，表现有所改善。",
    "fetch_date": "2026-01-02",
    "id": "20260102_388c3be1"
  },
  {
    "title": "Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem",
    "url": "https://arxiv.org/pdf/2512.24251v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于深度强化学习（DRL）的方法，用于解决车队规模与混合车辆路径问题（FSMVRP）。该方法将问题建模为马尔可夫决策过程（MDP），并开发了一种名为FRIPN的新型策略网络，能够同时处理车队组合和路径规划决策。论文通过随机生成实例和基准数据集进行了全面实验，结果表明该方法能在几秒内生成接近最优的解决方案，适用于短期车辆租赁和按需物流等实际场景。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6fbbf264"
  },
  {
    "title": "Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2512.00243v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Our work investigates the economic efficiency of the prevailing \"ladder-step\" investment strategy in oil and gas exploration, which advocates for the incremental acquisition of geological information throughout the project lifecycle. By employing a multi-agent Deep Reinforcement Learning (DRL) framework, we model an alternative strategy that prioritizes the early acquisition of high-quality information assets. We simulate the entire upstream value chain-comprising competitive bidding, exploration, and development phases-to evaluate the economic impact of this approach relative to traditional methods. Our results demonstrate that front-loading information investment significantly reduces the costs associated with redundant data acquisition and enhances the precision of reserve valuation. Specifically, we find that the alternative strategy outperforms traditional methods in highly competitive environments by mitigating the \"winner's curse\" through more accurate bidding. Furthermore, the economic benefits are most pronounced during the development phase, where superior data quality minimizes capital misallocation. These findings suggest that optimal investment timing is structurally dependent on market competition rather than solely on price volatility, offering a new paradigm for capital allocation in extractive industries.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究通过多智能体深度强化学习（DRL）框架，评估了石油天然气勘探领域传统“阶梯式”信息资产投资策略的经济效率，并提出了一种优先早期获取高质量信息资产的替代策略。模拟上游价值链（包括竞争性投标、勘探和开发阶段）显示，该替代策略能显著降低冗余数据采集成本，提高储量评估精度，在高度竞争环境中通过更精准的投标缓解“赢家诅咒”，并在开发阶段因数据质量优化而减少资本错配，表明信息投资时机优化具有结构性经济价值。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b7002262"
  },
  {
    "title": "Stochastic factors can matter: improving robust growth under ergodicity",
    "url": "https://arxiv.org/pdf/2512.24906v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strategies obtained from portfolio optimization are very sensitive to them. To mitigate this well-known phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price process $X$, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of admissible models allows $X$ to depend on a multivariate stochastic factor $Y$ and fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor process $Y$. A principal motivation of this framework comes from pairs trading, where $X$ is the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certain partial differential equation (PDE). We demonstrate that utilizing the stochastic factor leads to improvement in robust growth complementing the conclusions of the previous study by Itkin et. al. (arXiv:2211.15628 [q-fin.MF], forthcoming in $\\textit{Finance and Stochastics}$), which additionally robustified the dynamics of the stochastic factor leading to $Y$-independent optimal strategies. Our analysis leads to new financial insights, quantifying the improvement in growth the investor can achieve by optimally incorporating stochastic factors into their trading decisions. We illustrate our theoretical results on several numerical examples including an application to pairs trading.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文针对资产收益率漂移难以准确建模且投资组合优化策略对其高度敏感的问题，研究了在高维不完全市场中、资产价格过程X存在漂移不确定性下的稳健增长优化，并引入了遍历性假设。该框架允许X依赖于多元随机因子Y，并固定了(a)联合波动结构、(b)长期联合遍历密度和(c)随机因子过程Y的动力学。主要动机来自配对交易，其中X为价差过程。研究确定了稳健最优增长率，构建了最坏情况下的可接受模型，并通过求解特定偏微分方程(PDE)来表征稳健增长最优策略。结果表明，利用随机因子可提升稳健增长，补充了先前研究的结论。",
    "fetch_date": "2026-01-02",
    "id": "20260102_7acb0abd"
  },
  {
    "title": "Signature approach for pricing and hedging path-dependent options with frictions",
    "url": "https://arxiv.org/pdf/2511.23295v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "We introduce a novel signature approach for pricing and hedging path-dependent options with instantaneous and permanent market impact under a mean-quadratic variation criterion. Leveraging the expressive power of signatures, we recast an inherently nonlinear and non-Markovian stochastic control problem into a tractable form, yielding hedging strategies in (possibly infinite) linear feedback form in the time-augmented signature of the control variables, with coefficients characterized by non-standard infinite-dimensional Riccati equations on the extended tensor algebra. Numerical experiments demonstrate the effectiveness of these signature-based strategies for pricing and hedging general path-dependent payoffs in the presence of frictions. In particular, market impact naturally smooths optimal trading strategies, making low-truncated signature approximations highly accurate and robust in frictional markets, contrary to the frictionless case.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新颖的签名方法，用于在均值-二次变分准则下，对具有瞬时和永久市场摩擦影响的路径依赖期权进行定价和对冲。利用签名的表达能力，我们将一个本质非线性且非马尔可夫性的随机控制问题转化为可处理的形式，得到了以控制变量的时间增强签名的（可能无限）线性反馈形式表示的对冲策略，其系数由扩展张量代数上的非标准无限维Riccati方程表征。数值实验证明了这些基于签名的策略在存在摩擦的情况下，对一般路径依赖收益进行定价和对冲的有效性。特别是，市场摩擦自然地平滑了最优交易策略，使得低阶截断签名近似在高摩擦市场中具有高精度和鲁棒性，这与无摩擦情况相反。",
    "fetch_date": "2026-01-02",
    "id": "20260102_647600a8"
  },
  {
    "title": "Boundary error control for numerical solution of BSDEs by the convolution-FFT method",
    "url": "https://arxiv.org/pdf/2512.24714v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文首先回顾了基于卷积快速傅里叶变换（CFFT）的倒向随机微分方程（BSDE）数值解法，随后提出了一种改进期权定价中边界误差的方法。通过优化原方法中的阻尼与平移方案，将目标函数转化为有界周期函数以适用傅里叶变换，其中时变平移方案显著降低了边界误差。数值实验与误差分析验证了改进后方法在精度和收敛性上的提升。",
    "fetch_date": "2026-01-02",
    "id": "20260102_1ef00a6e"
  },
  {
    "title": "Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.24580v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于风险敏感强化学习（RSRL）的新框架，该框架结合了对状态转移不确定性的鲁棒性。作者定义了两个不同但耦合的风险度量：一个内部风险度量处理状态和成本的随机性，一个外部风险度量捕捉状态转移动态的不确定性。该框架通过允许对内部和外部风险度量使用一般的相干风险度量，统一并推广了大多数现有的强化学习框架。在此框架内，作者构建了一个风险敏感鲁棒马尔可夫决策过程（RSRMDP），推导了其贝尔曼方程，并在给定后验分布下提供了误差分析。作者进一步开发了一种贝叶斯动态规划（Bayesian DP）算法，该算法在后验更新和价值迭代之间交替进行。该方法采用了一个基于风险的贝尔曼算子估计器，该估计器结合了蒙特卡洛采样和凸优化，作者为此证明了强一致性保证。此外，作者证明了该算法在训练环境中收敛到接近最优的策略，并在狄利克雷后验和条件风险价值（CVaR）下分析了样本复杂度和计算复杂度。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9e6c8c0d"
  },
  {
    "title": "Minimal Solutions to the Skorokhod Reflection Problem Driven by Jump Processes and an Application to Reinsurance",
    "url": "https://arxiv.org/pdf/2512.24491v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider a reflected process in the positive orthant driven by an exogenous jump process. For a given input process, we show that there exists a unique minimal strong solution to the given particle system up until a certain maximal stopping time, which is stated explicitly in terms of the dual formulation of a linear programming problem associated with the state of the system. We apply this model to study the ruin time of interconnected insurance firms, where the stopping time can be interpreted as the failure time of a reinsurance agreement between the firms. Our work extends the analysis of the particle system in Baker, Hambly, and Jettkant (2025) to the case of jump driving processes, and the existence result of Reiman (1984) beyond the case of sub-stochastic reflection matrices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了由外生跳跃过程驱动的正象限反射过程。对于给定的输入过程，我们证明了在某个最大停时之前，该粒子系统存在唯一的最小强解，该停时通过系统状态相关的线性规划问题的对偶形式明确给出。我们将该模型应用于研究互联保险公司的破产时间，其中停时可解释为公司间再保险协议的失效时间。我们的工作将Baker、Hambly和Jettkant（2025）对粒子系统的分析扩展到跳跃驱动过程的情况，并将Reiman（1984）的存在性结果推广到非次随机反射矩阵的情形。",
    "fetch_date": "2026-01-02",
    "id": "20260102_230af948"
  },
  {
    "title": "Utility Maximisation with Model-independent Constraints",
    "url": "https://arxiv.org/pdf/2512.24371v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider an agent who has access to a financial market, including derivative contracts, who looks to maximise her utility. Whilst the agent looks to maximise utility over one probability measure, or class of probability measures, she must also ensure that the mark-to-market value of her portfolio remains above a given threshold. When the mark-to-market value is based on a more pessimistic valuation method, such as model-independent bounds, we recover a novel optimisation problem for the agent where the agents investment problem must satisfy a pathwise constraint.\n  For complete markets, the expression of the optimal terminal wealth is given, using the max-plus decomposition for supermartingales. Moreover, for the Black-Scholes-Merton model the explicit form of the process involved in such decomposition is obtained, and we are able to investigate numerically optimal portfolios in the presence of options which are mispriced according to the agent's beliefs.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了一个在金融市场（包括衍生品合约）中寻求效用最大化的代理人问题。代理人虽然在一个概率测度或一类概率测度下最大化效用，但必须同时确保其投资组合的盯市价值维持在给定阈值之上。当盯市价值基于更悲观的估值方法（如模型无关边界）时，我们为代理人推导出一个新颖的优化问题，其中投资问题必须满足路径约束。对于完全市场，利用超鞅的最大加分解给出了最优终端财富的表达式。此外，对于Black-Scholes-Merton模型，我们获得了该分解中涉及过程的显式形式，并能够数值研究在代理人认为期权被错误定价情况下的最优投资组合。",
    "fetch_date": "2026-01-02",
    "id": "20260102_de031d92"
  },
  {
    "title": "A Test of Lookahead Bias in LLM Forecasts",
    "url": "https://arxiv.org/pdf/2512.23847v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state-of-the-art pre-training data detection techniques, we estimate the likelihood that a given prompt appeared in an LLM's training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们开发了一种统计检验方法，用于检测大型语言模型（LLMs）生成的经济预测中的前瞻性偏差。利用最先进的预训练数据检测技术，我们估计给定提示出现在LLM训练语料库中的可能性，这一统计量我们称为前瞻倾向（LAP）。我们正式证明，LAP与预测准确性之间的正相关表明前瞻性偏差的存在及其程度，并将该检验应用于两个预测任务：新闻标题预测股票收益和财报电话会议记录预测资本支出。我们的检验为评估LLM生成预测的有效性和可靠性提供了一种成本效益高的诊断工具。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9a4a2330"
  },
  {
    "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations",
    "url": "https://arxiv.org/pdf/2512.00249v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在支持兵棋推演的作战模拟领域，智能体开发长期以基于规则的脚本方法为主，深度强化学习（RL）方法近期才被引入。脚本智能体在受控环境中提供可预测性和一致性，但在动态复杂场景中因其固有僵化性而表现不足。相反，RL智能体在适应性和学习方面表现优异，在处理意外情况方面具有改进潜力，但面临重大挑战，如黑盒决策过程和在大型模拟环境中的可扩展性问题。本文提出了一种新颖的分层混合人工智能（AI）方法，将脚本智能体的可靠性和可预测性与RL的动态自适应学习能力相结合。通过分层构建AI系统，该方法旨在利用脚本智能体处理常规战术级决策，利用RL智能体处理更高级别的战略决策，从而在利用各自优势的同时解决每种方法的局限性。",
    "fetch_date": "2026-01-02",
    "id": "20260102_f8dabb31"
  },
  {
    "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions",
    "url": "https://arxiv.org/pdf/2512.00142v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究介绍了DeFi TrustBoost框架，该框架结合区块链技术和可解释AI，旨在解决贷款机构在审批来自低收入家庭的小企业贷款申请时面临的挑战。该框架强调满足区块链和AI系统的四个关键要求：机密性、数据保护法规合规性、对抗攻击抵抗力和监管审计合规性。它提出了一种用于自动化AI决策防篡改审计的技术，以及一种链上（区块链内）和链下数据存储策略，以促进金融组织内部及跨组织的协作。",
    "fetch_date": "2026-01-02",
    "id": "20260102_82b264a1"
  },
  {
    "title": "Responsible LLM Deployment for High-Stake Decisions by Decentralized Technologies and Human-AI Interactions",
    "url": "https://arxiv.org/pdf/2512.04108v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "High-stakes decision domains are increasingly exploring the potential of Large Language Models (LLMs) for complex decision-making tasks. However, LLM deployment in real-world settings presents challenges in data security, evaluation of its capabilities outside controlled environments, and accountability attribution in the event of adversarial decisions. This paper proposes a framework for responsible deployment of LLM-based decision-support systems through active human involvement. It integrates interactive collaboration between human experts and developers through multiple iterations at the pre-deployment stage to assess the uncertain samples and judge the stability of the explanation provided by post-hoc XAI techniques. Local LLM deployment within organizations and decentralized technologies, such as Blockchain and IPFS, are proposed to create immutable records of LLM activities for automated auditing to enhance security and trace back accountability. It was tested on Bert-large-uncased, Mistral, and LLaMA 2 and 3 models to assess the capability to support responsible financial decisions on business lending.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《通过去中心化技术和人机交互实现高风险决策的负责任大语言模型部署》探讨了在金融等高风险决策领域部署大语言模型（LLMs）的挑战，包括数据安全、模型能力评估和决策问责。提出一个负责任部署框架，强调人类专家在部署前阶段的主动参与，通过多轮迭代评估不确定样本和事后可解释人工智能（XAI）技术的稳定性。建议在组织内部署本地LLMs，并利用区块链和IPFS等去中心化技术创建不可篡改的记录，以增强安全性和可追溯性。该框架在Bert-large-uncased、Mistral和LLaMA 2/3模型上进行了测试，用于评估其在商业贷款等金融决策中的支持能力。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6751ad03"
  },
  {
    "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
    "url": "https://arxiv.org/pdf/2512.01565v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为Deep FlexQP的加速非线性规划方法，通过深度展开技术改进QP优化器。该方法基于对QP约束的精确松弛，无论原始约束是否可行，都能找到最优解或最小化约束违反的稀疏解。Deep FlexQP具有良好的可扩展性、鲁棒性和热启动能力，通过数据驱动技术学习维度无关的反馈策略，能够泛化到更大维度的问题，并在投资组合优化等基准测试中优于现有方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2e864486"
  },
  {
    "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets",
    "url": "https://arxiv.org/pdf/2512.10971v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AI-Trader：实时金融市场中自主智能体的基准测试》提出首个完全自动化、实时且数据无污染的LLM智能体金融决策评估基准。该基准覆盖美股、A股和加密货币三大市场，采用多粒度交易模拟实时金融环境，并引入革命性的全自主最小信息范式——智能体仅接收基本上下文，需独立搜索、验证和整合实时市场信息。研究评估了六种主流LLM，旨在解决动态实时环境中决策制定的挑战，对量化交易实战具有直接应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23a72d9d"
  },
  {
    "title": "Autodeleveraging: Impossibilities and Optimization",
    "url": "https://arxiv.org/pdf/2512.01112v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by $\\approx 28\\times$ relative to our optimal policy, imposing roughly \\$653 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "自动去杠杆化（ADL）是永续期货交易所的最后损失社会化机制，在维持偿付能力的清算失败时触发。本文首次对ADL进行严格建模，证明了ADL机制面临一个根本性的“三难困境”：没有任何政策能同时满足交易所的偿付能力、收入和交易者的公平性。这一不可能定理意味着，随着参与规模扩大，一种新型的“道德风险”会渐近增长，使得“零损失”社会化变得不可能。建设性地，本文展示了三类ADL机制可以最优地应对这一三难困境，提供公平性、对价格冲击的鲁棒性以及交易所收入最大化。通过分析2025年10月10日Hyperliquid数据集（当时ADL在12分钟内反复用于平仓21亿美元头寸），本文实证比较了这些ADL机制与实践中使用的标准方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_dd83332c"
  },
  {
    "title": "Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model",
    "url": "https://arxiv.org/pdf/2512.00630v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文评估了基于Qwen3-8B模型通过rLoRA微调进行金融文本分类的性能。该方法采用噪声嵌入指令微调增强鲁棒性，结合秩稳定低秩优化与FlashAttention提升训练效率，适用于金融情感分析和新闻分类等量化交易相关任务。研究对比了传统Transformer模型及多个大模型，展示了该技术在实战交易系统中的潜在应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_f74585e2"
  },
  {
    "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts",
    "url": "https://arxiv.org/pdf/2512.00916v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "评估罕见事件预测具有挑战性，因为标准指标在事件发生率下降时会失效。F1分数、AUPRC、MCC和准确率等指标会导致退化阈值（收敛于0或1），其值被类别不平衡而非尾部判别能力主导。我们开发了一族罕见事件稳定（RES）指标，其最优阈值在事件概率趋近于零时仍保持严格内部性，确保在极端罕见情况下的连贯决策规则。模拟显示RES指标保持稳定阈值、一致的模型排名和近乎完全的流行度不变性，而传统指标则表现出统计显著的阈值漂移和结构崩溃。信用违约应用证实了这些结果：RES指标产生可解释的违约概率阈值（4-9%），并在子采样下保持稳健，而经典指标在操作上失效。RES框架为评估极端风险预测提供了原则性的、流行度不变的基础。",
    "fetch_date": "2026-01-01",
    "id": "20260101_70ccf6da"
  },
  {
    "title": "Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election",
    "url": "https://arxiv.org/pdf/2512.00893v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "We study how the 2024 U.S. presidential election, viewed as a major political risk event, affected cryptocurrency markets by distinguishing human-driven peer-to-peer stablecoin transactions from automated algorithmic activity. Using structural break analysis, we find that human-driven Ethereum Request for Comment 20 (ERC-20) transactions shifted on November 3, two days before the election, while exchange trading volumes reacted only on Election Day. Automated smart-contract activity adjusted much later, with structural breaks appearing in January 2025. We validate these shifts using surrogate-based robustness tests. Complementary energy-spectrum analysis of Bitcoin and Ethereum identifies pronounced post-election turbulence, and a structural vector autoregression confirms a regime shift in stablecoin dynamics. Overall, human-driven stablecoin flows act as early-warning indicators of political stress, preceding both exchange behavior and algorithmic responses.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究探讨了2024年美国总统大选这一重大政治风险事件对加密货币市场的影响，通过区分人类驱动的点对点稳定币交易与自动化算法活动。利用结构断点分析发现，人类驱动的ERC-20交易在选举前两天（11月3日）已出现变化，而交易所交易量仅在选举日才反应。自动化智能合约活动调整更晚，结构断点出现在2025年1月。通过替代性稳健性检验验证了这些变化。对比特币和以太坊的互补能量谱分析识别出显著的选举后市场波动，结构向量自回归模型确认了稳定币动态中的制度转变。总体而言，人类驱动的稳定币流动可作为政治压力的早期预警指标，领先于交易所行为和算法响应。",
    "fetch_date": "2026-01-01",
    "id": "20260101_1eadb97f"
  },
  {
    "title": "Efficient Calibration in the rough Bergomi model by Wasserstein distance",
    "url": "https://arxiv.org/pdf/2512.00448v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Despite the empirical success in modeling volatility of the rough Bergomi (rBergomi) model, it suffers from pricing and calibration difficulties stemming from its non-Markovian structure. To address this, we propose a comprehensive computational framework that enhances both simulation and calibration. First, we develop a modified Sum-of-Exponentials (mSOE) Monte Carlo scheme which hybridizes an exact simulation of the singular kernel near the origin with a multi-factor approximation for the remainder. This method achieves high accuracy, particularly for out-of-the-money options, with an $\\mathcal{O}(n)$ computational cost. Second, based on this efficient pricing engine, we then propose a distribution-matching calibration scheme by using Wasserstein distance as the optimization objective. This leverages a minimax formulation against Lipschitz payoffs, which effectively distributes pricing errors and improving robustness. Our numerical results confirm the mSOE scheme's convergence and demonstrate that the calibration algorithm reliably identifies model parameters and generalizes well to path-dependent options, which offers a powerful and generic tool for practical model fitting.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "针对粗糙Bergomi模型在定价和校准上的困难，本文提出了一个综合计算框架。首先，开发了一种改进的指数和蒙特卡洛方案，通过混合原点附近奇异核的精确模拟与剩余部分的多因子近似，实现了高精度（尤其对价外期权）和O(n)计算成本。其次，基于此高效定价引擎，提出了一种使用Wasserstein距离作为优化目标的分布匹配校准方案，通过极小极大公式对抗Lipschitz收益，有效分配定价误差并提高鲁棒性。数值结果验证了方案的收敛性，并表明校准算法能可靠识别模型参数并良好推广至路径依赖期权，为实际模型拟合提供了强大通用工具。",
    "fetch_date": "2026-01-01",
    "id": "20260101_5a90d3c8"
  },
  {
    "title": "Does it take two to tango: Interaction between Credit Default Swaps and National Stock Indices",
    "url": "https://arxiv.org/pdf/2512.07887v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper investigates both short and long-run interaction between BIST-100 index and CDS prices over January 2008 to May 2015 using ARDL technique. The paper documents several findings. First, ARDL analysis shows that 1 TL increase in CDS shrinks BIST-100 index by 22.5 TL in short-run and 85.5 TL in long-run. Second, 1000 TL increase in BIST index price causes 25 TL and 44 TL reducation in Turkey's CDS prices in short- and long-run respectively. Third, a percentage increase in interest rate shrinks BIST index by 359 TL and a percentage increase in inflation rate scales CDS prices up to 13.34 TL both in long-run. In case of short-run, these impacts are limited with 231 TL and 5.73 TL respectively. Fourth, a kurush increase in TL/USD exchange rate leads 24.5 TL (short-run) and 78 TL (long-run) reductions in BIST, while it augments CDS prices by 2.5 TL (short-run) and 3 TL (long-run) respectively. Fifth, each negative political events decreases BIST by 237 TL in short-run and 538 TL in long-run, while it increases CDS prices by 33 TL in short-run and 89 TL in long-run. These findings imply the highly dollar indebted capital structure of Turkish firms, and overly sensitivity of financial markets to the uncertainties in political sphere. Finally, the paper provides evidence for that BIST and CDS with control variables drift too far apart, and converge to a long-run equilibrium at a moderate monthly speed.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文使用ARDL技术研究了2008年1月至2015年5月期间土耳其BIST-100指数与CDS价格之间的短期和长期互动关系。主要发现包括：1）CDS价格每上涨1土耳其里拉，BIST-100指数在短期和长期分别下跌22.5里拉和85.5里拉；2）BIST指数每上涨1000里拉，土耳其CDS价格在短期和长期分别下降25里拉和44里拉；3）利率每上升1个百分点，BIST指数在长期下跌359里拉；通胀率每上升1个百分点，CDS价格在长期上涨13.34里拉；4）土耳其里拉兑美元汇率每上涨1库鲁什，BIST指数在短期和长期分别下跌24.5里拉和78里拉，同时CDS价格分别上涨2.5里拉和3里拉；5）每个负面政治事件使BIST指数在短期和长期分别下跌237里拉和538里拉，同时使CDS价格分别上涨33里拉和89里拉。这些发现揭示了土耳其企业高度美元化的资本结构及其对金融市场的过度敏感性。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2689c421"
  },
  {
    "title": "A Hybrid Architecture for Options Wheel Strategy Decisions: LLM-Generated Bayesian Networks for Transparent Trading",
    "url": "https://arxiv.org/pdf/2512.01123v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Large Language Models (LLMs) excel at understanding context and qualitative nuances but struggle with the rigorous and transparent reasoning required in high-stakes quantitative domains such as financial trading. We propose a model-first hybrid architecture for the options \"wheel\" strategy that combines the strengths of LLMs with the robustness of a Bayesian Network. Rather than using the LLM as a black-box decision-maker, we employ it as an intelligent model builder. For each trade decision, the LLM constructs a context-specific Bayesian network by interpreting current market conditions, including prices, volatility, trends, and news, and hypothesizing relationships among key variables. The LLM also selects relevant historical data from an 18.75-year, 8,919-trade dataset to populate the network's conditional probability tables. This selection focuses on scenarios analogous to the present context. The instantiated Bayesian network then performs transparent probabilistic inference, producing explicit probability distributions and risk metrics to support decision-making. A feedback loop enables the LLM to analyze trade outcomes and iteratively refine subsequent network structures and data selection, learning from both successes and failures. Empirically, our hybrid system demonstrates effective performance on the wheel strategy. Over nearly 19 years of out-of-sample testing, it achieves a 15.3% annualized return with significantly superior risk-adjusted performance (Sharpe ratio 1.08 versus 0.62 for market benchmarks) and dramatically lower drawdown (-8.2% versus -60%) while maintaining a 0% assignment rate through strategic option rolling. Crucially, each trade decision is fully explainable, involving on average 27 recorded decision factors (e.g., volatility level, option premium, risk indicators, market context).",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于期权“轮动”策略决策的混合架构，结合了大型语言模型（LLM）与贝叶斯网络的优势。LLM不直接作为黑盒决策器，而是作为智能模型构建器，根据当前市场条件（如价格、波动率、趋势、新闻）构建情境特定的贝叶斯网络，并假设关键变量间的关系。LLM还从18.75年、8,919笔交易的历史数据集中选取类似情境的数据，填充网络的条件概率表。实例化的贝叶斯网络执行透明的概率推理，生成明确的概率分布和风险指标以支持决策。反馈循环使LLM能分析交易结果并迭代优化后续网络结构。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2daebce1"
  },
  {
    "title": "Arbitrage-Free Option Price Surfaces via Chebyshev Tensor Bases and a Hamiltonian Fog Post-Fit",
    "url": "https://arxiv.org/pdf/2512.01967v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We study the construction of arbitrage-free option price surfaces from noisy bid-ask quotes across strike and maturity. Our starting point is a Chebyshev representation of the call price surface on a warped log-moneyness/maturity rectangle, together with linear sampling and no-arbitrage operators acting on a collocation grid. Static no-arbitrage requirements are enforced as linear inequalities, while the surface is fitted directly to prices via a coverage-seeking quadratic objective that trades off squared band misfit against spectral and transport-inspired regularisation of the Chebyshev coefficients. This yields a strictly convex quadratic program in the modal coefficients, solvable at practical scales with off-the-shelf solvers (OSQP).\n  On top of the global backbone, we introduce a local post-fit layer based on a discrete fog of risk-neutral densities on a three-dimensional lattice (m,t,u) and an associated Hamiltonian-type energy. On each patch of the (m,t) plane, the fog variables are coupled to a nodal price field obtained from the baseline surface, yielding a joint convex optimisation problem that reweights noisy quotes and applies noise-aware local corrections while preserving global static no-arbitrage and locality.\n  The method is designed such that for equity options panels, the combined procedure achieves high inside-spread coverage in stable regimes (in calm years, 98-99% of quotes are priced inside the bid-ask intervals) and low rates of static no-arbitrage violations (below 1%). In stressed periods, the fog layer provides a mechanism for controlled leakage outside the band: when local quotes are mutually inconsistent or unusually noisy, the optimiser allocates fog mass outside the bid-ask tube and justifies small out-of-band deviations of the post-fit surface, while preserving a globally arbitrage-free and well-regularised description of the option surface.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "该论文提出了一种构建无套利期权价格曲面的方法，通过切比雪夫张量基和哈密顿雾后拟合技术。核心包括：使用切比雪夫基在扭曲的对数货币性/期限矩形上表示看涨期权价格曲面，结合线性采样和无套利算子；通过线性不等式强制静态无套利约束，采用覆盖性二次目标函数直接拟合价格，平衡带状误差平方与谱正则化；最终转化为严格凸二次规划问题。在全局基础上，引入基于三维格点风险中性密度雾和哈密顿型能量的局部后拟合层，通过联合凸优化对噪声报价进行重加权和局部修正。该方法理论性强，但未涉及实际交易策略或市场动态，对实战交易的价值主要体现在模型构建层面。",
    "fetch_date": "2026-01-01",
    "id": "20260101_9c7bbf8b"
  },
  {
    "title": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections",
    "url": "https://arxiv.org/pdf/2512.01408v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao \\cite{KZ98}-type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文通过贝叶斯分布鲁棒视角重新审视默顿连续时间投资组合选择问题。核心创新在于：在贝叶斯默顿模型中，将单一模糊集置于先验漂移项上（而非传统的时间矩形分布鲁棒控制），从而保留学习能力与可处理性。通过极小极大交换将鲁棒控制简化为优化先验的非线性泛函，并利用Wasserstein不确定性下的显式渐近最优推前映射来表征小半径最坏情况先验。该方法通过针对默顿泛函定制的非线性Wasserstein投影校准模糊半径，实验表明相较于分布鲁棒控制（DRC）减少了悲观性并提升了性能。",
    "fetch_date": "2026-01-01",
    "id": "20260101_582de585"
  },
  {
    "title": "The Endogenous Constraint: Hysteresis, Stagflation, and the Structural Inhibition of Monetary Velocity in the Bitcoin Network (2016-2025)",
    "url": "https://arxiv.org/pdf/2512.07886v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Bitcoin operates as a macroeconomic paradox: it combines a strictly predetermined, inelastic monetary issuance schedule with a stochastic, highly elastic demand for scarce block space. This paper empirically validates the Endogenous Constraint Hypothesis, positing that protocol-level throughput limits generate a non-linear negative feedback loop between network friction and base-layer monetary velocity. Using a verified Transaction Cost Index (TCI) derived from Blockchain.com on-chain data and Hansen's (2000) threshold regression, we identify a definitive structural break at the 90th percentile of friction (TCI ~ 1.63). The analysis reveals a bifurcation in network utility: while the network exhibits robust velocity growth of +15.44% during normal regimes, this collapses to +6.06% during shock regimes, yielding a statistically significant Net Utility Contraction of -9.39% (p = 0.012). Crucially, Instrumental Variable (IV) tests utilizing Hashrate Variation as a supply-side instrument fail to detect a significant relationship in a linear specification (p=0.196), confirming that the velocity constraint is strictly a regime-switching phenomenon rather than a continuous linear function. Furthermore, we document a \"Crypto Multiplier\" inversion: high friction correlates with a +8.03% increase in capital concentration per entity, suggesting that congestion forces a substitution from active velocity to speculative hoarding.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "比特币网络的内生约束：滞后效应、滞胀与货币流通速度的结构性抑制（2016-2025）。本文实证验证了内生约束假说，认为协议层面的吞吐量限制在网络摩擦与基础层货币流通速度之间形成了非线性负反馈循环。通过使用基于Blockchain.com链上数据验证的交易成本指数（TCI）和Hansen（2000）的门槛回归，我们在摩擦的90百分位数（TCI约1.63）处识别出明确的结构性断点。分析揭示了网络效用的分叉：在正常状态下，网络表现出+15.44%的强劲流通速度增长，而在冲击状态下则崩溃至+6.06%，产生了统计显著的净效用收缩-9.39%（p=0.012）。关键的是，利用哈希率变化作为供给侧工具变量的工具变量（IV）测试在线性规范中未检测到显著关系（p=0.196），证实了流通速度约束严格是一种状态转换现象，而非连续线性函数。",
    "fetch_date": "2026-01-01",
    "id": "20260101_95084b35"
  },
  {
    "title": "Equilibrium Investment with Random Risk Aversion: (Non-)uniqueness, Optimality, and Comparative Statics",
    "url": "https://arxiv.org/pdf/2512.00830v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "This paper investigates infinite-dimensional portfolio selection problem under a general distribution of the risk aversion parameter. We provide a complete characterization of all deterministic equilibrium investment strategies. Our results reveal that the solution structure depends critically on the distribution of risk aversion: the equilibrium is unique whenever it exists in the case of finite expected risk aversion, whereas an infinite expectation can lead to infinitely many equilibria or to a unique trivial one (pi equals 0). To address this multiplicity, we introduce three optimality criteria-optimal, uniformly optimal, and uniformly strictly optimal-and explicitly characterize the existence and uniqueness of the corresponding equilibria. Under the same necessary and sufficient condition, the optimal and uniformly optimal equilibria exist uniquely and coincide. Furthermore, by additionally assuming that the market price of risk is non-zero near the terminal time, we show that the optimal (and hence uniformly optimal) equilibrium is also uniformly strictly optimal. Finally, we perform comparative statics to demonstrate that a risk aversion distribution dominating another in the reverse hazard rate order leads to a less aggressive equilibrium strategy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在风险厌恶参数服从一般分布下的无限维投资组合选择问题。我们完整刻画了所有确定性均衡投资策略。结果表明，解的结构关键取决于风险厌恶的分布：当期望风险厌恶有限时，均衡存在则唯一；而无限期望可能导致无穷多均衡或唯一平凡均衡（π=0）。针对多重性，我们引入了三种最优性准则——最优、一致最优和一致严格最优——并明确刻画了相应均衡的存在性与唯一性。在相同充要条件下，最优与一致最优均衡存在唯一且重合。进一步假设市场风险价格在终端时刻附近非零，我们证明最优（从而一致最优）均衡也是一致严格最优的。最后，通过比较静态分析表明，若一个风险厌恶分布随机占优于另一个，则其均衡投资策略在随机占优意义下更大。",
    "fetch_date": "2026-01-01",
    "id": "20260101_33a484bf"
  },
  {
    "title": "Convergence Rates of Turnpike Theorems for Portfolio Choice in Stochastic Factor Models",
    "url": "https://arxiv.org/pdf/2512.00346v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Turnpike theorems state that if an investor's utility is asymptotically equivalent to a power utility, then the optimal investment strategy converges to the CRRA strategy as the investment horizon tends to infinity. This paper aims to derive the convergence rates of the turnpike theorem for optimal feedback functions in stochastic factor models. In these models, optimal feedback functions can be decomposed into two terms: myopic portfolios and excess hedging demands. We obtain convergence rates for myopic portfolios in nonlinear stochastic factor models and for excess hedging demands in quadratic term structure models, where the interest rate is a quadratic function of a multivariate Ornstein-Uhlenbeck process. We show that the convergence rates are determined by (i) the decay speed of the price of a zero-coupon bond and (ii) how quickly the investor's utility becomes power-like at high levels of wealth. As an application, we consider optimal collective investment problems and show that sharing rules for terminal wealth affect convergence rates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了随机因子模型中投资组合选择的“大道定理”收敛速率。大道定理指出，若投资者效用渐近等价于幂效用，则最优投资策略在投资期限趋于无穷时收敛于CRRA策略。论文旨在推导随机因子模型中最优反馈函数收敛速率的理论结果，将最优反馈分解为短视组合与超额对冲需求两部分，并在非线性随机因子模型和二次期限结构模型中分别获得其收敛速率。收敛速率取决于（i）零息债券价格的衰减速度，以及（ii）投资者效用在高财富水平下趋近幂效用的速度。作为应用，论文探讨了最优集体投资问题，表明终端财富的分享规则会影响收敛速率。",
    "fetch_date": "2026-01-01",
    "id": "20260101_fd006166"
  },
  {
    "title": "Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network",
    "url": "https://arxiv.org/pdf/2512.00299v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sion's maxmin theorem. For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在随机占优约束下的S型非凹效用最大化静态投资组合选择问题。主要贡献包括：1）用一阶随机占优约束替代清算边界要求，为风险管理提供替代方案；2）提出数值算法检测二阶随机占优约束下的表现不佳区域，通过修改该区域分布获得（次）优解。核心金融洞见是：决策者应在表现不佳情景中遵循随机占优约束，在表现良好情景中追求效用最大化。该研究属于理论优化方法，未涉及强化学习/深度学习/Alpha生成等实战交易技术。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23e46ee2"
  },
  {
    "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.23515v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "Alpha-R1：通过强化学习实现基于LLM推理的Alpha因子筛选。信号衰减和市场机制转换是数据驱动投资策略在非平稳市场中面临的持续挑战。传统时间序列和机器学习方法主要依赖历史相关性，当经济环境变化时往往难以泛化。虽然大语言模型在处理非结构化信息方面具有强大能力，但其通过明确的经济推理支持量化因子筛选的潜力尚未充分开发。现有基于因子的方法通常将Alpha简化为数值时间序列，忽略了决定因子何时具有经济相关性的语义逻辑。我们提出Alpha-R1，一个通过强化学习训练的80亿参数推理模型，用于上下文感知的Alpha筛选。Alpha-R1基于因子逻辑和实时新闻进行推理，评估不断变化的市场条件下的Alpha相关性，根据上下文一致性选择性地激活或停用因子。在多个资产池中的实证结果表明，Alpha-R1持续优于基准策略，并表现出对Alpha衰减的改进鲁棒性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_7845ee87"
  },
  {
    "title": "Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading",
    "url": "https://arxiv.org/pdf/2512.02227v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, while the S&P 500 index yielded a return of $15.97\\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, whereas the BTC price increased by $3.80\\%$. Our code is available on \\href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场因其时间动态性和低信噪比特性，成为AI智能体的关键应用领域。传统算法交易系统通常需要专业团队多年开发和测试。本文提出一个金融智能体编排框架，旨在将金融智能民主化给公众。该框架将传统算法交易系统的各个组件映射为智能体，包括规划器、编排器、阿尔法智能体、风险智能体、投资组合智能体、回测智能体、执行智能体、审计智能体和记忆智能体。论文展示了两个内部交易实例：在股票交易任务（2024年4月至12月每小时数据）中，该方法实现了20.42%的收益率、2.63的夏普比率和-3.59%的最大回撤，而同期标普500指数收益率为15.97%；在BTC交易任务（2025年7月27日至8月13日每分钟数据）中，实现了8.39%的收益率、0.38的夏普比率和-2.80%的最大回撤，而同期BTC价格上涨3.80%。代码已在GitHub开源。",
    "fetch_date": "2025-12-31",
    "id": "20251231_c175bb6a"
  },
  {
    "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction",
    "url": "https://arxiv.org/pdf/2512.23596v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究非平稳环境下股票收益预测的机器学习模型，揭示了一个根本性的非平稳性-复杂度权衡：复杂模型能减少设定误差，但需要更长的训练窗口，这会引入更强的非平稳性。作者通过一种新颖的模型选择方法解决了这一矛盾，该方法使用锦标赛程序联合优化模型类别和训练窗口大小，在非平稳验证数据上自适应评估候选模型。理论分析表明，该方法能平衡设定误差、估计方差和非平稳性，表现接近事后最佳模型。将该方法应用于17个行业投资组合收益，持续优于标准的滚动窗口基准，样本外R²平均提高14-23%。在NBER指定的衰退期间，改进尤为显著：例如，在海湾战争衰退期间，该方法实现了正的R²，而基准为负；在2001年衰退期间，R²绝对值至少提高80个基点；在2008年金融危机期间也表现优异。从经济角度看，基于该方法的交易策略具有实际应用价值。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbb43c5"
  },
  {
    "title": "Squeezed Covariance Matrix Estimation: Analytic Eigenvalue Control",
    "url": "https://arxiv.org/pdf/2512.23021v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "We revisit Gerber's Informational Quality (IQ) framework, a data-driven approach for constructing correlation matrices from co-movement evidence, and address two obstacles that limit its use in portfolio optimization: guaranteeing positive semidefinite ness (PSD) and controlling spectral conditioning. We introduce a squeezing identity that represents IQ estimators as a convex-like combination of structured channel matrices, and propose an atomic-IQ parameterization in which each channel-class matrix is built from PSD atoms with a single class-level normalization. This yields constructive PSD guarantees over an explicit feasibility region, avoiding reliance on ex-post projection. To regulate conditioning, we develop an analytic eigen floor that targets either a minimum eigenvalue or a desired condition number and, when necessary, repairs PSD violations in closed form while remaining compatible with the squeezing identity. In long-only tangency back tests with transaction costs, atomic-IQ improves out-of-sample Sharpe ratios and delivers a more stable risk profile relative to a broad set of standard covariance estimators.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视Gerber的信息质量（IQ）框架——一种基于数据驱动方法从共动证据构建相关矩阵的框架，并解决了限制其在投资组合优化中应用的两个障碍：保证正半定性（PSD）和控制谱条件数。作者引入了一种压缩恒等式，将IQ估计量表示为结构化通道矩阵的类凸组合，并提出了一种原子IQ参数化方法，其中每个通道类矩阵由具有单一类级别归一化的PSD原子构建。这在一个明确的可行区域内提供了构造性的PSD保证，避免依赖事后投影。为了调节条件数，作者开发了一种解析特征值下限方法，旨在实现最小特征值或期望的条件数，并在必要时以闭式形式修复PSD违规，同时保持与压缩恒等式的兼容性。在考虑交易成本的仅多头切线投资组合回测中，原子IQ方法相对于一系列标准协方差估计器，提高了样本外夏普比率，并提供了更稳定的风险特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_cefa3cff"
  },
  {
    "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.22895v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\\% higher Return, 5\\% higher Sharpe ratio, 5\\% higher Sortino ratio, and 2\\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "SAMP-HDRL：通过分层深度强化学习实现多智能体投资组合管理的分段分配与动量调整效用。该框架针对非平稳市场中的投资组合优化挑战（如制度转换、动态相关性和DRL策略可解释性有限），提出动态资产分组方法，将市场划分为高质量和普通子集。上层智能体提取全局市场信号，下层智能体在掩码约束下执行组内分配。基于效用的资本分配机制整合风险资产与无风险资产，确保全局与局部决策的协调一致。在三种市场制度（2019-2021）的回测中，SAMP-HDRL在波动和震荡条件下持续优于9个传统基准和9个DRL基准，相比最强基准至少实现5%的更高回报、夏普比率、索提诺比率和2%的更高欧米茄比率，在动荡市场中收益尤为显著。消融研究证实了各模块的有效性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_b5de9d1a"
  },
  {
    "title": "AutoQuant: An Auditable Expert-System Framework for Execution-Constrained Auto-Tuning in Cryptocurrency Perpetual Futures",
    "url": "https://arxiv.org/pdf/2512.22476v1",
    "source": "ArXiv",
    "date": "2025-12-27",
    "abstract": "Backtests of cryptocurrency perpetual futures are fragile when they ignore microstructure frictions and reuse evaluation windows during parameter search. We study four liquid perpetuals (BTC/USDT, ETH/USDT, SOL/USDT, AVAX/USDT) and quantify how execution delay, funding, fees, and slippage can inflate reported performance. We introduce AutoQuant, an execution-centric, alpha-agnostic framework for auditable strategy configuration selection. AutoQuant encodes strict T+1 execution semantics and no-look-ahead funding alignment, runs Bayesian optimization under realistic costs, and applies a two-stage double-screening protocol across held-out rolling windows and a cost-sensitivity grid. We show that fee-only and zero-cost backtests can materially overestimate annualized returns relative to a fully costed configuration, and that double screening tends to reduce drawdowns under the same strict semantics even when returns are not higher. A CSCV/PBO diagnostic indicates substantial residual overfitting risk, motivating AutoQuant as validation and governance infrastructure rather than a claim of persistent alpha. Returns are reported for small-account simulations with linear trading costs and without market impact or capacity modeling.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AutoQuant：一种用于加密货币永续期货执行约束自动调参的可审计专家系统框架》针对加密货币永续期货回测中忽略微观结构摩擦（如执行延迟、资金费、手续费、滑点）导致性能虚高的问题，提出以执行为中心、与阿尔法无关的可审计策略配置选择框架。该框架采用严格的T+1执行语义和无前瞻性资金费对齐，在真实成本下进行贝叶斯优化，并通过保留滚动窗口和成本敏感性网格的双阶段双重筛选协议。研究表明，仅考虑手续费和零成本的回测会显著高估年化收益，而双重筛选在相同严格语义下即使收益未提高也能降低回撤。CSCV/PBO诊断显示存在显著的残余过拟合风险，因此AutoQuant更适合作为验证和治理基础设施，而非声称持续阿尔法的工具。收益报告基于小账户模拟和线性交易成本。",
    "fetch_date": "2025-12-31",
    "id": "20251231_1842d9c9"
  },
  {
    "title": "Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling",
    "url": "https://arxiv.org/pdf/2512.21798v2",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance. This paper investigates the use of deep generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) to generate realistic synthetic financial return series for portfolio construction and risk modeling applications. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean--variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据为解决量化金融实证研究中的隐私、可访问性和可复现性挑战提供了实用方案。本文研究了深度生成模型，特别是时间序列生成对抗网络（TimeGAN）和变分自编码器（VAE），在生成用于投资组合构建和风险建模应用的现实合成金融收益序列方面的应用。以标普500的历史日收益率为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。研究表明，TimeGAN生成的合成数据在分布形状、波动模式和自相关行为方面接近真实收益。当应用于均值-方差投资组合优化时，所得合成数据集产生的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE提供更稳定的训练，但倾向于平滑极端市场波动，从而影响风险估计。",
    "fetch_date": "2025-12-31",
    "id": "20251231_3fd18e9d"
  },
  {
    "title": "Beyond Binary Screens: A Continuous Shariah Compliance Index for Asset Pricing and Portfolio Design",
    "url": "https://arxiv.org/pdf/2512.22858v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Binary Shariah screens vary across standards and apply hard thresholds that create discontinuous classifications. We construct a Continuous Shariah Compliance Index (CSCI) in $[0,1]$ by mapping standard screening ratios to smooth scores between conservative ``comfort'' bounds and permissive outer bounds, and aggregating them conservatively with a sectoral activity factor. Using CRSP/Compustat U.S. equities (1999-2024) with lagged accounting inputs and monthly rebalancing, we find that CSCI-based long-only portfolios have historical risk-adjusted performance similar to an emulated binary Islamic benchmark. Tightening the minimum compliance threshold reduces the investable universe and diversification and is associated with lower Sharpe ratios. The framework yields a practical compliance gradient that supports portfolio construction, constraint design, and cross-standard comparisons without reliance on pass/fail screening.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种连续伊斯兰教法合规指数（CSCI），用于替代传统的二元筛选方法。该指数在[0,1]区间内对标准筛选比率进行平滑评分，并通过行业活动因子进行保守聚合。基于美国股票数据（1999-2024年）的回测显示，CSCI构建的纯多头投资组合在历史风险调整后表现与模拟的二元伊斯兰基准相似。提高最低合规阈值会缩小可投资范围、降低分散化程度，并与较低的夏普比率相关。该框架提供了一种实用的合规梯度，支持投资组合构建、约束设计及跨标准比较，无需依赖通过/失败筛选。",
    "fetch_date": "2025-12-31",
    "id": "20251231_a943828e"
  },
  {
    "title": "Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%",
    "url": "https://arxiv.org/pdf/2512.03107v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出ECLIPSE框架，将AI幻觉视为模型语义熵与可用证据容量之间的不匹配。该框架结合多样本聚类的熵估计和一种新颖的困惑度分解方法，以衡量模型如何使用检索到的证据。在受控的金融问答数据集上，ECLIPSE实现了0.89的ROC AUC和0.90的平均精度，显著优于仅基于语义熵的基线（AUC 0.50）。然而，该方法的有效性依赖于校准的令牌级不确定性，在缺乏令牌级对数概率的模型上性能会显著下降。",
    "fetch_date": "2025-12-31",
    "id": "20251231_36769bcf"
  },
  {
    "title": "Broken Symmetry of Stock Returns -- a Modified Jones-Faddy Skew t-Distribution",
    "url": "https://arxiv.org/pdf/2512.23640v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We argue that negative skew and positive mean of the distribution of stock returns are largely due to the broken symmetry of stochastic volatility governing gains and losses. Starting with stochastic differential equations for stock returns and for stochastic volatility we argue that the distribution of stock returns can be effectively split in two -- for gains and losses -- assuming difference in parameters of their respective stochastic volatilities. A modified Jones-Faddy skew t-distribution utilized here allows to reflect this in a single organic distribution which tends to meaningfully capture this asymmetry. We illustrate its application on distribution of daily S&P500 returns, including analysis of its tails.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出股票收益率分布的负偏和正均值主要源于控制收益与损失的随机波动率对称性破缺。通过建立股票收益率和随机波动率的随机微分方程，作者认为可将收益率分布有效拆分为收益和损失两部分，并假设其各自随机波动率参数存在差异。采用的修正Jones-Faddy偏斜t分布能够以单一有机分布反映这种不对称性，并有效捕捉该特征。论文以标普500日收益率分布为例进行了应用展示，包括尾部分析。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbf4bea"
  },
  {
    "title": "Impact of Volatility on Time-Based Transaction Ordering Policies",
    "url": "https://arxiv.org/pdf/2512.23386v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了Arbitrum的快速通道拍卖（ELA），这是一种提前进行的次高价拍卖，获胜者将获得一分钟的独家延迟优势。基于风险厌恶投标人的单轮模型，我们提出一个假设：由于预测短期波动性的困难以及投标人的风险厌恶，优先访问的价值相对于风险中性估值有所折价。我们使用与高频ETH价格匹配的ELA投标记录来测试这些预测，发现结果与模型一致。",
    "fetch_date": "2025-12-31",
    "id": "20251231_688d9f03"
  },
  {
    "title": "Lambda Expected Shortfall",
    "url": "https://arxiv.org/pdf/2512.23139v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "The Lambda Value-at-Risk (Lambda$-VaR) is a generalization of the Value-at-Risk (VaR), which has been actively studied in quantitative finance. Over the past two decades, the Expected Shortfall (ES) has become one of the most important risk measures alongside VaR because of its various desirable properties in the practice of optimization, risk management, and financial regulation. Analogously to the intimate relation between ES and VaR, we introduce the Lambda Expected Shortfall (Lambda-ES), as a generalization of ES and a counterpart to Lambda-VaR. Our definition of Lambda-ES has an explicit formula and many convenient properties, and we show that it is the smallest quasi-convex and law-invariant risk measure dominating Lambda-VaR under mild assumptions. We examine further properties of Lambda-ES, its dual representation, and related optimization problems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "Lambda期望损失（Lambda-ES）是期望损失（ES）的推广，作为Lambda风险价值（Lambda-VaR）的对应概念。该定义具有显式公式和多种便利性质，在温和假设下是最小化Lambda-VaR的拟凸且律不变风险度量。论文探讨了Lambda-ES的进一步性质、对偶表示及相关优化问题。",
    "fetch_date": "2025-12-31",
    "id": "20251231_6138efd6"
  },
  {
    "title": "A Note on the Conditions for COS Convergence",
    "url": "https://arxiv.org/pdf/2512.02745v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We study the truncation error of the COS method and give simple, verifiable conditions that guarantee convergence. In one dimension, COS is admissible when the density belongs to both L1 and L2 and has a finite weighted L2 moment of order strictly greater than one. We extend the result to multiple dimensions by requiring the moment order to exceed the dimension. These conditions enlarge the class of densities covered by previous analyses and include heavy-tailed distributions such as Student t with small degrees of freedom.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了COS方法的截断误差，并给出了保证收敛的简单可验证条件。在一维情况下，当密度函数同时属于L1和L2空间且具有严格大于1阶的有限加权L2矩时，COS方法是可接受的。通过要求矩的阶数超过维度，我们将结果扩展到多维情况。这些条件扩大了先前分析所涵盖的密度函数类别，包括具有小自由度的Student t等重尾分布。",
    "fetch_date": "2025-12-31",
    "id": "20251231_8fd2e7eb"
  },
  {
    "title": "Visibility-Graph Asymmetry as a Structural Indicator of Volatility Clustering",
    "url": "https://arxiv.org/pdf/2512.02352v2",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Volatility clustering is one of the most robust stylized facts of financial markets, yet it is typically detected using moment-based diagnostics or parametric models such as GARCH. This paper shows that clustered volatility also leaves a clear imprint on the time-reversal symmetry of horizontal visibility graphs (HVGs) constructed on absolute returns in physical time. For each time point, we compute the maximal forward and backward visibility distances, $L^{+}(t)$ and $L^{-}(t)$, and use their empirical distributions to build a visibility-asymmetry fingerprint comprising the Kolmogorov--Smirnov distance, variance difference, entropy difference, and a ratio of extreme visibility spans. In a Monte Carlo study, these HVG asymmetry features sharply separate volatility-clustered GARCH(1,1) dynamics from i.i.d.\\ Gaussian noise and from randomly shuffled GARCH series that preserve the marginal distribution but destroy temporal dependence; a simple linear classifier based on the fingerprint achieves about 90\\% in-sample accuracy. Applying the method to daily S\\&P500 data reveals a pronounced forward--backward imbalance, including a variance difference $Δ\\mathrm{Var}$ that exceeds the simulated GARCH values by two orders of magnitude and vanishes after shuffling. Overall, the visibility-graph asymmetry fingerprint emerges as a simple, model-free, and geometrically interpretable indicator of volatility clustering and time irreversibility in financial time series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种基于水平可见性图（HVG）非对称性的新方法来检测金融市场中的波动率聚集现象。通过计算绝对收益序列在物理时间上的前向和后向最大可见距离，并构建包含Kolmogorov-Smirnov距离、方差差、熵差和极端可见跨度比率的非对称性指纹，该方法能够有效区分具有波动率聚集的GARCH(1,1)动态与独立同分布的高斯噪声。在蒙特卡洛研究中，基于该指纹的简单线性分类器实现了约90%的样本内准确率。应用于标普500日度数据时，该方法揭示了显著的前后向不平衡特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_0d518006"
  },
  {
    "title": "The Three-Dimensional Decomposition of Volatility Memory",
    "url": "https://arxiv.org/pdf/2512.02166v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper develops a three-dimensional decomposition of volatility memory into orthogonal components of level, shape, and tempo. The framework unifies regime-switching, fractional-integration, and business-time approaches within a single canonical representation that identifies how each dimension governs persistence strength, long-memory form, and temporal speed. We establish conditions for existence, uniqueness, and ergodicity of this decomposition and show that all GARCH-type processes arise as special cases. Empirically, applications to SPY and EURUSD (2005--2024) reveal that volatility memory is state-dependent: regime and tempo gates dominate in equities, while fractional-memory gates prevail in foreign exchange. The unified tri-gate model jointly captures these effects. By formalizing volatility dynamics through a level--shape--tempo structure, the paper provides a coherent link between information flow, market activity, and the evolving memory of financial volatility.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种将波动率记忆分解为水平、形态和节奏三个正交维度的三维分解框架。该框架在单一规范表示中统一了机制转换、分数积分和业务时间方法，识别了每个维度如何控制持续性强度、长记忆形式和时间速度。我们建立了该分解的存在性、唯一性和遍历性条件，并证明所有GARCH类过程都是其特例。对SPY和EURUSD（2005-2024）的实证应用表明，波动率记忆具有状态依赖性：机制和节奏维度在股票市场中占主导，而分数记忆维度在外汇市场中更为显著。统一的“三闸门”模型共同捕捉了这些效应。通过将波动率动态形式化为水平-形态-节奏结构，本文为信息流、市场活动和金融波动率演化记忆之间提供了连贯的联系。",
    "fetch_date": "2025-12-31",
    "id": "20251231_587625c2"
  },
  {
    "title": "Hidden Order in Trades Predicts the Size of Price Moves",
    "url": "https://arxiv.org/pdf/2512.15720v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p < 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场呈现一个明显悖论：虽然方向性价格变动基本不可预测（符合弱式有效市场假说），但价格变动幅度却显示出系统性结构。本文证明，基于15状态马尔可夫转移矩阵以秒级分辨率计算的实时订单流熵，能够预测日内收益的幅度而不提供方向性信息。对36个交易日内3850万笔SPY交易的分析显示，在熵低于第5百分位的条件下，后续5分钟绝对收益增加2.89倍（t=12.41，p<0.0001），而方向性准确率保持在45.0%——与随机水平无统计学差异（p=0.12）。这种解耦源于基本对称性：熵在符号置换下保持不变，可检测知情交易的存在而不揭示其方向。五个非重叠测试期的前向验证确认了样本外可预测性，标签置换安慰剂检验得出z=14.4。这些发现表明，信息论度量可作为市场微观结构中的波动率状态变量。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a096c37e"
  },
  {
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
    "url": "https://arxiv.org/pdf/2512.21878v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "MASFIN是一个模块化多智能体框架，将大型语言模型（LLMs）与结构化金融指标和非结构化新闻相结合，同时嵌入明确的偏差缓解协议。该系统利用GPT-4.1-nano实现可重复性和成本效益推理，生成包含15-30只股票的周度投资组合，其配置权重针对短期表现进行了优化。在为期八周的评估中，MASFIN实现了7.33%的累计回报，在八周中的六周内表现优于标普500、纳斯达克100和道琼斯基准，尽管波动性较高。这些发现展示了具有偏差意识的生成式AI框架在金融预测中的潜力，并凸显了模块化多智能体设计在推进实用、透明和可重复方法方面的机遇。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0237ad76"
  },
  {
    "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling",
    "url": "https://arxiv.org/pdf/2512.21791v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种统一的合成金融数据多标准评估框架，并应用于三种代表性生成范式：统计ARIMA-GARCH基线、变分自编码器（VAEs）和时间序列生成对抗网络（TimeGAN）。基于标普500历史日数据，评估了保真度（最大均值差异MMD）、时间结构（自相关和波动率聚集）以及在下游任务中的实际效用，特别是均值-方差投资组合优化和波动率预测。实证结果表明：ARIMA-GARCH能捕捉线性趋势和条件波动率但无法复现非线性动态；VAEs生成平滑轨迹但低估极端事件；TimeGAN在真实性和时间连贯性之间取得了最佳平衡（如TimeGAN获得最低MMD：1.84e-3，5次种子平均）。最后，论文根据应用需求和计算约束提出了生成模型选择的实用指南。统一的评估协议和可复现性为实战交易中解决数据稀缺和保密性问题提供了直接价值，特别是对需要大量数据进行模型训练和压力测试的量化策略开发。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a7fa7b3b"
  },
  {
    "title": "Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification",
    "url": "https://arxiv.org/pdf/2512.22109v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We study the construction and rebalancing of sparse index-tracking portfolios from an operational research perspective, with explicit emphasis on uncertainty quantification and implementability. The decision variables are portfolio weights constrained to sum to one; the aims are to track a reference index closely while controlling the number of names and the turnover induced by rebalancing. We cast index tracking as a high-dimensional linear regression of index returns on constituent returns, and employ a sparsity-inducing Laplace prior on the weights. A single global shrinkage parameter controls the trade-off between tracking error and sparsity, and is calibrated by an empirical-Bayes stochastic approximation scheme. Conditional on this calibration, we approximate the posterior distribution of the portfolio weights using proximal Langevin-type Markov chain Monte Carlo algorithms tailored to the budget constraint. This yields posterior uncertainty on tracking error, portfolio composition and prospective rebalancing moves. Building on these posterior samples, we propose rules for rebalancing that gate trades through magnitude-based thresholds and posterior activation probabilities, thereby trading off expected tracking error against turnover and portfolio size. A case study on tracking the S&P~500 index is carried out to showcase how our tools shape the decision process from portfolio construction to rebalancing.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文从运筹学视角研究稀疏指数跟踪投资组合的构建与再平衡，重点强调不确定性量化与可实施性。决策变量为总和为1的投资组合权重；目标是在控制持仓数量与再平衡引起的换手率的同时，紧密跟踪基准指数。作者将指数跟踪建模为指数收益对成分股收益的高维线性回归，并对权重施加诱导稀疏性的拉普拉斯先验。单一全局收缩参数控制跟踪误差与稀疏性之间的权衡，并通过经验贝叶斯随机逼近方案进行校准。基于此校准，作者采用针对预算约束设计的近似近端朗之万型马尔可夫链蒙特卡洛算法，近似投资组合权重的后验分布，从而获得关于跟踪误差、投资组合构成及预期再平衡操作的后验不确定性。基于这些后验样本，作者提出了通过基于幅度的阈值和后验激活概率来门控交易的再平衡规则。",
    "fetch_date": "2025-12-30",
    "id": "20251230_7977417e"
  },
  {
    "title": "Applications of synthetic financial data in portfolio and risk modeling",
    "url": "https://arxiv.org/pdf/2512.21798v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data offers a practical way to address the privacy and accessibility challenges that limit research in quantitative finance. This paper examines the use of generative models, in particular TimeGAN and Variational Autoencoders (VAEs), for creating synthetic return series that support portfolio construction, trading analysis, and risk modeling. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean-variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据在投资组合和风险建模中的应用。该论文探讨了使用生成模型（特别是TimeGAN和变分自编码器VAE）创建合成收益率序列，以支持投资组合构建、交易分析和风险建模。研究以标普500历史日收益率作为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。结果表明，TimeGAN生成的合成数据在分布形态、波动率模式和自相关行为方面接近真实收益率。应用于均值-方差投资组合优化时，合成数据集得出的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE训练更稳定但倾向于平滑极端市场波动，从而影响风险估计。分析支持将合成数据集作为真实金融数据的替代品，用于解决隐私和可访问性限制。",
    "fetch_date": "2025-12-30",
    "id": "20251230_c2ad24df"
  },
  {
    "title": "FX Market Making with Internal Liquidity",
    "url": "https://arxiv.org/pdf/2512.04603v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "As the FX markets continue to evolve, many institutions have started offering passive access to their internal liquidity pools. Market makers act as principal and have the opportunity to fill those orders as part of their risk management, or they may choose to adjust pricing to their external OTC franchise to facilitate the matching flow. It is, a priori, unclear how the strategies managing internal liquidity should depend on market condions, the market maker's risk appetite, and the placement algorithms deployed by participating clients. The market maker's actions in the presence of passive orders are relevant not only for their own objectives, but also for those liquidity providers who have certain expectations of the execution speed. In this work, we investigate the optimal multi-objective strategy of a market maker with an option to take liquidity on an internal exchange, and draw important qualitative insights for real-world trading.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "随着外汇市场不断发展，许多机构开始提供对其内部流动性池的被动访问。做市商作为主体，有机会将这些订单作为其风险管理的一部分进行成交，或者他们可以选择调整其外部场外交易特许经营的价格以促进匹配流动。从先验角度看，管理内部流动性的策略应如何依赖市场条件、做市商的风险偏好以及参与客户部署的配置算法尚不明确。做市商在被动订单存在下的行为不仅与其自身目标相关，也与对执行速度有特定预期的流动性提供者相关。在本研究中，我们探讨了做市商在可选择从内部交易所获取流动性时的最优多目标策略，并为实际交易提供了重要的定性见解。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4668657e"
  },
  {
    "title": "Investigating Conditional Restricted Boltzmann Machines in Regime Detection",
    "url": "https://arxiv.org/pdf/2512.21823v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "This study investigates the efficacy of Conditional Restricted Boltzmann Machines (CRBMs) for modeling high-dimensional financial time series and detecting systemic risk regimes. We extend the classical application of static Restricted Boltzmann Machines (RBMs) by incorporating autoregressive conditioning and utilizing Persistent Contrastive Divergence (PCD) to incorporate complex temporal dependency structures. Comparing a discrete Bernoulli-Bernoulli architecture against a continuous Gaussian-Bernoulli variant across a multi-asset dataset spanning 2013-2025, we observe a dichotomy between generative fidelity and regime detection. While the Gaussian CRBM successfully preserves static asset correlations, it exhibits limitations in generating long-range volatility clustering. Thus, we analyze the free energy as a relative negative log-likelihood (surprisal) under a fixed, trained model. We demonstrate that the model's free energy serves as a robust, regime stability metric. By decomposing the free energy into quadratic (magnitude) and structural (correlation) components, we show that the model can distinguish between pure magnitude shocks and market regimes. Our findings suggest that the CRBM offers a valuable, interpretable diagnostic tool for monitoring systemic risk, providing a supplemental metric to implied volatility metrics like the VIX.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了条件受限玻尔兹曼机（CRBMs）在建模高维金融时间序列和检测系统性风险状态方面的有效性。通过引入自回归条件和使用持续对比散度（PCD），扩展了静态受限玻尔兹曼机（RBMs）的经典应用，以纳入复杂的时间依赖结构。在2013-2025年跨资产数据集中，比较离散伯努利-伯努利架构与连续高斯-伯努利变体，观察到生成保真度与状态检测之间的二分法。虽然高斯CRBM成功保留了静态资产相关性，但在生成长程波动率聚类方面存在局限性。因此，我们在固定训练模型下分析自由能作为相对负对数似然（惊奇度）。研究表明，模型的自由能可作为稳健的状态稳定性度量。通过将自由能分解为二次（幅度）和结构（相关性）分量，模型能够区分纯幅度冲击和市场状态。",
    "fetch_date": "2025-12-30",
    "id": "20251230_137a0b37"
  },
  {
    "title": "S&P 500 Stock's Movement Prediction using CNN",
    "url": "https://arxiv.org/pdf/2512.21804v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].\n  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文使用卷积神经网络（CNN）预测标普500指数成分股的股价走势，创新之处在于直接利用包含股票拆分/股息事件的多维原始市场数据，而非传统工程化金融数据。虽然CNN在图像分类领域表现优异，但论文未深入探讨金融数据的复杂性，且主要基于单维度数据，对实战交易的价值有限，更多是为未来研究奠定基础。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4893f447"
  },
  {
    "title": "A High-Level Framework for Practically Model-Independent Pricing",
    "url": "https://arxiv.org/pdf/2512.15718v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We present a high-level framework that explains why, in practice, different pricing models calibrated to the same vanilla surface tend to produce similar valuations for exotic derivatives. Our approach acts as an overlay on the Monte Carlo infrastructure already used in banks, combining path reweighting with a conic optimisation layer without requiring any changes to existing code. This construction delivers narrow, practically model-independent price bands for exotics, reconciling front-office practice with the robust, model-independent ideas developed in the academic literature.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种高层框架，解释了为何在实践中，校准至相同普通期权曲面的不同定价模型往往对奇异衍生品给出相近的估值。该方法作为银行现有蒙特卡洛基础设施的覆盖层，结合路径重加权与锥优化层，无需修改现有代码。该框架为奇异品生成狭窄、近乎模型独立的价格区间，将前台实践与学术文献中发展的稳健、模型独立理念相统一。",
    "fetch_date": "2025-12-30",
    "id": "20251230_877a57bf"
  },
  {
    "title": "Enhancing trading strategies: a multi-indicator analysis for profitable algorithmic trading",
    "url": "https://link.springer.com/article/10.1007/s10614-024-10669-3",
    "source": "Scholar",
    "date": "2025-12-30",
    "abstract": "… backtesting to compare the strategy's historical performance to benchmarks. The … algorithmic trading models. This research contributes to understanding of algorithmic trading strategies …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该论文通过多指标分析增强交易策略，采用回测方法比较策略历史表现与基准，为理解算法交易模型提供了理论贡献。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0b3c59aa"
  },
  {
    "title": "Variational Quantum Eigensolver for Real-World Finance: Scalable Solutions for Dynamic Portfolio Optimization Problems",
    "url": "https://arxiv.org/pdf/2512.22001v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We present a scalable, hardware-aware methodology for extending the Variational Quantum Eigensolver (VQE) to large, realistic Dynamic Portfolio Optimization (DPO) problems. Building on the scaling strategy from our previous work, where we tailored a VQE workflow to both the DPO formulation and the target QPU, we now put forward two significant advances. The first is the implementation of the Ising Sample-based Quantum Configuration Recovery (ISQR) routine, which improves solution quality in Quadratic Unconstrained Binary Optimization problems. The second is the use of the VQE Constrained method to decompose the optimization task, enabling us to handle DPO instances with more variables than the available qubits on current hardware. These advances, which are broadly applicable to other optimization problems, allow us to address a portfolio with a size relevant to the financial industry, consisting of up to 38 assets and covering the full Spanish stock index (IBEX 35). Our results, obtained on a real Quantum Processing Unit (IBM Fez), show that this tailored workflow achieves financial performance on par with classical methods while delivering a broader set of high-quality investment strategies, demonstrating a viable path towards obtaining practical advantage from quantum optimization in real financial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种可扩展、硬件感知的方法，将变分量子本征求解器（VQE）应用于大规模、现实的动态投资组合优化（DPO）问题。通过引入伊辛样本量子配置恢复（ISQR）例程和改进的VQE约束方法，该研究成功处理了包含多达38种资产（覆盖西班牙IBEX 35指数）的投资组合，并在真实量子处理单元（IBM Fez）上实现了与经典方法相当的财务绩效。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4c468636"
  },
  {
    "title": "Mean-Field Price Formation on Trees with a Network of Relative Performance Concerns",
    "url": "https://arxiv.org/pdf/2512.21621v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Financial firms and institutional investors are routinely evaluated based on their performance relative to their peers. These relative performance concerns significantly influence risk-taking behavior and market dynamics. While the literature studying Nash equilibrium under such relative performance competitions is extensive, its effect on asset price formation remains largely unexplored. This paper investigates mean-field equilibrium price formation of a single risky stock in a discrete-time market where agents exhibit exponential utility and relative performance concerns. Unlike existing literature that typically treats asset prices as exogenous, we impose a market-clearing condition to determine the price dynamics endogenously within a relative performance equilibrium. Using a binomial tree framework, we establish the existence and uniqueness of the market-clearing mean-field equilibrium in both single- and multi-population settings. Finally, we provide illustrative numerical examples demonstrating the equilibrium price distributions and agents' optimal position sizes.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "金融机构和机构投资者通常根据其相对于同行的表现进行评估。这些相对绩效关注显著影响风险承担行为和市场动态。虽然研究此类相对绩效竞争下纳什均衡的文献广泛，但其对资产价格形成的影响在很大程度上仍未得到探索。本文研究了在离散时间市场中，具有指数效用和相对绩效关注的单一风险股票的平均场均衡价格形成。与通常将资产价格视为外生变量的现有文献不同，我们施加了市场出清条件，以在相对绩效均衡内内生地确定价格动态。使用二叉树框架，我们建立了单群体和多群体设置下市场出清平均场均衡的存在性和唯一性。最后，我们提供了说明性的数值示例，展示了均衡价格分布和代理人的最优头寸规模。",
    "fetch_date": "2025-12-30",
    "id": "20251230_f73cd03a"
  },
  {
    "title": "A Co-evolutionary Approach for Heston Calibration",
    "url": "https://arxiv.org/pdf/2512.03922v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "We evaluate a co-evolutionary calibration framework for the Heston model in which a genetic algorithm (GA) over parameters is coupled to an evolving neural inverse map from option surfaces to parameters. While GA-history sampling can reduce training loss quickly and yields strong in-sample fits to the target surface, learning-curve diagnostics show a widening train--validation gap across generations, indicating substantial overfitting induced by the concentrated and less diverse dataset. In contrast, a broad, space-filling dataset generated via Latin hypercube sampling (LHS) achieves nearly comparable calibration accuracy while delivering markedly better out-of-sample stability across held-out surfaces. These results suggest that apparent improvements from co-evolutionary data generation largely reflect target-specific specialization rather than a more reliable global inverse mapping, and that maintaining dataset diversity is critical for robust amortized calibration.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文评估了一种用于Heston模型校准的协同进化框架，其中参数遗传算法（GA）与从期权曲面到参数的进化神经逆映射相结合。研究发现，虽然GA历史采样能快速降低训练损失并在样本内对目标曲面实现强拟合，但学习曲线诊断显示代际间训练-验证差距扩大，表明由集中且多样性不足的数据集引发了显著过拟合。相比之下，通过拉丁超立方采样（LHS）生成的广泛、空间填充数据集实现了几乎相当的校准精度，同时在保留曲面上表现出明显更好的样本外稳定性。这些结果表明，协同进化数据生成带来的明显改进主要反映了针对特定目标的专门化，而非更可靠的全局逆映射，且保持数据集多样性对于稳健的摊销校准至关重要。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8a431d9a"
  },
  {
    "title": "The Effect of High-Speed Rail Connectivity on Capital Market Earnings Forecast Error: Evidence from the Chinese Stock Market",
    "url": "https://arxiv.org/pdf/2512.03709v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "This study examines how China's high-speed rail (HSR) expansion affects analyst earnings forecast errors from an economic information friction perspective. Using firm-year panel data from 2008-2019, a period that covers HSR's early introduction and rapid nationwide rollout, the findings show that analysts' relative earnings forecast errors (RFE) decline significantly only after firms' cities become connected by high-speed rail. The placebo test, which artificially shifts HSR connectivity 3 years earlier than the actual opening year, yields an insignificant DID coefficient, rejecting the possibility that forecast errors were improving before the infrastructure shock. This supports the conclusion that forecast error reduction is linked to real geographic accessibility improvements rather than coincidence, pre-existing trends, or analyst anticipation. Economically, the study highlights that HSR reduces analysts' costs of gathering private, incremental information, particularly soft information obtained via plant or management visits. The rail network does not directly alter firms' internal capital allocation or earnings generation paths, but it lowers spatial barriers to information collection, enabling analysts to update EPS expectations under reduced travel friction. This work provides intuitive evidence that geography and mobility improvements contribute to forecasting accuracy in China's emerging, decentralized capital market corridors, and it encourages future research to consider transport accessibility as an exogenous information cost shock rather than an internal firm-capital shock.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究从经济信息摩擦视角，探讨中国高速铁路（HSR）扩张如何影响分析师盈利预测误差。利用2008-2019年企业年度面板数据（覆盖HSR早期引入和全国快速推广期），研究发现仅在企业所在城市通过高铁连接后，分析师的相对盈利预测误差（RFE）显著下降。安慰剂测试（人为将HSR连通时间提前3年）显示DID系数不显著，排除了预测误差在基础设施冲击前已改善的可能性，支持预测误差减少与真实地理可达性改善相关，而非巧合、既有趋势或分析师预期。经济意义上，研究强调HSR降低了分析师收集私有增量信息（特别是通过工厂或管理层访问获取的软信息）的成本。铁路网络并未直接改变企业内部资本配置或盈利生成路径，但降低了信息获取的空间壁垒。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8749e3da"
  },
  {
    "title": "A Stochastic Thermodynamics Approach to Price Impact and Round-Trip Arbitrage: Theory and Empirical Implications",
    "url": "https://arxiv.org/pdf/2512.03123v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "This paper develops a comprehensive theoretical framework that imports concepts from stochastic thermodynamics to model price impact and characterize the feasibility of round-trip arbitrage in financial markets. A trading cycle is treated as a non-equilibrium thermodynamic process, where price impact represents dissipative work and market noise plays the role of thermal fluctuations. The paper proves a Financial Second Law: under general convex impact functionals, any round-trip trading strategy yields non-positive expected profit. This structural constraint is complemented by a fluctuation theorem that bounds the probability of profitable cycles in terms of dissipated work and market volatility. The framework introduces a statistical ensemble of trading strategies governed by a Gibbs measure, leading to a free energy decomposition that connects expected cost, strategy entropy, and a market temperature parameter. The framework provides rigorous, testable inequalities linking microstructural impact to macroscopic no-arbitrage conditions, offering a novel physics-inspired perspective on market efficiency. The paper derives explicit analytical results for prototypical trading strategies and discusses empirical validation protocols.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个将随机热力学概念引入金融市场的理论框架，用于建模价格影响并分析往返套利的可行性。交易周期被视为非平衡热力学过程，其中价格影响代表耗散功，市场噪声类比热涨落。论文证明了“金融第二定律”：在一般凸影响泛函下，任何往返交易策略的期望利润均为非正。该结构约束辅以一个涨落定理，将盈利周期的概率与耗散功和市场波动率联系起来。框架引入了由吉布斯测度支配的交易策略统计系综，导出了连接期望成本、策略熵和市场温度参数的自由能分解。该框架提供了连接微观结构影响与宏观无套利条件的严格可检验不等式，为市场效率提供了新颖的物理学视角。论文针对典型交易策略推导了显式解析结果。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a03b3233"
  },
  {
    "title": "The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading",
    "url": "https://arxiv.org/pdf/2512.15732v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the \"Holy Grail\" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of \"Galaxy Empire,\" a hybrid framework coupling LSTM/Transformer-based perception with a genetic \"Time-is-Life\" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\\%$) and live performance (Capital Decay $>70\\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \\textit{Aleatoric Uncertainty} in low-entropy time-series, the \\textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《红皇后的陷阱：高频交易中深度进化的局限》对实战交易具有重要警示价值。该论文对名为“银河帝国”的混合框架（结合LSTM/Transformer感知与遗传“时间即生命”生存机制）进行了严谨的事后分析。在加密货币高频环境中部署500个自主代理后，观察到训练指标（验证年化收益率>300%）与实际表现（资本衰减>70%）之间的灾难性背离。研究通过多学科视角解构了失败原因，识别出三个关键失效模式：低熵时间序列中偶然不确定性的过拟合、高方差下进化选择固有的幸存者偏差，以及缺乏订单流数据时无法克服微观结构摩擦的数学不可能性。研究结果为模型复杂性增加在缺乏信息不对称时加剧系统脆弱性提供了实证证据。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f1a0b406"
  },
  {
    "title": "Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks",
    "url": "https://arxiv.org/pdf/2512.05868v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文《利用脉冲神经网络预测高频金融数据中的价格变动》针对高频交易环境中传统模型难以捕捉的毫秒级价格尖峰问题，提出了一种生物启发的解决方案。研究将高频股票数据转换为脉冲序列，并评估了三种架构：基于无监督STDP训练的经典脉冲神经网络、具有显式抑制竞争的新型脉冲神经网络，以及监督式反向传播网络。通过贝叶斯优化结合新颖的惩罚性脉冲准确率目标函数进行超参数调优，确保模型预测的价格尖峰率与实证事件率一致。模拟交易显示，经PSA优化的模型在性能上持续优于仅使用脉冲准确率调优的对比模型及基线方法。",
    "fetch_date": "2025-12-29",
    "id": "20251229_8824f09c"
  },
  {
    "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.05753v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "针对现代战争中认知雷达快速部署以对抗干扰的关键挑战，本文提出了一种基于深度强化学习的快速抗干扰雷达部署算法（FARDA）。现有方法主要基于进化算法，耗时且易陷入局部最优。FARDA通过神经网络高效推理，将雷达部署建模为端到端任务，设计了集成神经模块感知热图信息和新奖励格式的深度强化学习算法。实证结果表明，该方法在达到与进化算法相当覆盖范围的同时，部署速度提升约7000倍。消融实验验证了FARDA各组件的必要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5a33edc9"
  },
  {
    "title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming",
    "url": "https://arxiv.org/pdf/2512.15735v3",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种将强化学习（RL）控制器与抗扰扩展状态观测器（ESO）相结合的统一控制架构，并辅以事件触发机制（ETM）以减少不必要的计算。ESO用于实时估计系统状态和集总扰动，为有效扰动补偿奠定基础。为在缺乏精确系统描述的情况下获得近似最优行为，采用基于值迭代的自适应动态规划（ADP）方法进行策略逼近。ETM的引入确保学习模块的参数更新仅在状态偏差超过预设界限时执行，从而避免过度学习活动并显著降低计算负荷。通过李雅普诺夫导向分析表征了闭环系统的稳定性。数值实验进一步证实，与标准时间触发ADP方案相比，该方法在保持强大控制性能和扰动容忍度的同时，显著减少了采样和处理工作量。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f5507cc4"
  },
  {
    "title": "A Unified AI System For Data Quality Control and DataOps Management in Regulated Environments",
    "url": "https://arxiv.org/pdf/2512.05559v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "In regulated domains such as finance, the integrity and governance of data pipelines are critical - yet existing systems treat data quality control (QC) as an isolated preprocessing step rather than a first-class system component. We present a unified AI-driven Data QC and DataOps Management framework that embeds rule-based, statistical, and AI-based QC methods into a continuous, governed layer spanning ingestion, model pipelines, and downstream applications. Our architecture integrates open-source tools with custom modules for profiling, audit logging, breach handling, configuration-driven policies, and dynamic remediation. We demonstrate deployment in a production-grade financial setup: handling streaming and tabular data across multiple asset classes and transaction streams, with configurable thresholds, cloud-native storage interfaces, and automated alerts. We show empirical gains in anomaly detection recall, reduction of manual remediation effort, and improved auditability and traceability in high-throughput data workflows. By treating QC as a system concern rather than an afterthought, our framework provides a foundation for trustworthy, scalable, and compliant AI pipelines in regulated environments.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在金融等受监管领域，数据管道的完整性和治理至关重要。本文提出了一种统一的AI驱动数据质量控制和DataOps管理框架，将基于规则、统计和AI的质量控制方法嵌入到一个跨越数据摄取、模型管道和下游应用的连续治理层中。该架构整合了开源工具与定制模块，用于数据剖析、审计日志记录、违规处理、配置驱动策略和动态修复。在金融生产环境中部署的演示表明，该框架能够处理多资产类别和交易流中的流数据和表格数据，具有可配置阈值、云原生存储接口和自动警报功能。实证结果显示，在高吞吐量数据工作流中，异常检测召回率得到提升，手动修复工作量减少，审计性和可追溯性得到改善。通过将质量控制视为系统核心而非事后补救，该框架为可信、可扩展且合规的AI管道提供了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_144f113c"
  },
  {
    "title": "Differential ML with a Difference",
    "url": "https://arxiv.org/pdf/2512.05301v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Differential ML (Huge and Savine 2020) is a technique for training neural networks to provide fast approximations to complex simulation-based models for derivatives pricing and risk management. It uses price sensitivities calculated through pathwise adjoint differentiation to reduce pricing and hedging errors. However, for options with discontinuous payoffs, such as digital or barrier options, the pathwise sensitivities are biased, and incorporating them into the loss function can magnify errors. We consider alternative methods for estimating sensitivities and find that they can substantially reduce test errors in prices and in their sensitivities. Using differential labels calculated through the likelihood ratio method expands the scope of Differential ML to discontinuous payoffs. A hybrid method incorporates gamma estimates as well as delta estimates, providing further regularization.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "差分机器学习（Differential ML）是一种训练神经网络快速近似复杂模拟模型的技术，用于衍生品定价和风险管理。该方法利用通过路径伴随微分计算的价格敏感性来减少定价和对冲误差。然而，对于具有不连续收益的期权（如数字或障碍期权），路径敏感性存在偏差，将其纳入损失函数可能放大误差。研究探讨了替代敏感性估计方法，发现这些方法能显著降低价格及其敏感性的测试误差。使用似然比法计算的差分标签将差分机器学习扩展到不连续收益领域。混合方法结合了伽马估计和德尔塔估计，提供了进一步的规范化。",
    "fetch_date": "2025-12-29",
    "id": "20251229_c043a012"
  },
  {
    "title": "Continuous-time reinforcement learning for optimal switching over multiple regimes",
    "url": "https://arxiv.org/pdf/2512.04697v2",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了多体制下最优切换问题的连续时间强化学习。在熵正则化的探索性框架下，代理通过关联连续时间有限状态马尔可夫链的生成矩阵，随机化切换时机和体制选择。我们建立了相关哈密顿-雅可比-贝尔曼方程组的适定性，并给出了最优策略的表征。通过分析方程组，严格证明了策略改进和策略迭代的收敛性。当温度参数趋近于零时，我们还展示了探索性框架下的价值函数向经典框架价值函数的收敛。最后，基于鞅表征的策略评估，设计并实现了一种强化学习算法。借助神经网络的数值算例说明了所提RL算法的有效性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_126a24ad"
  },
  {
    "title": "Convolution-FFT for option pricing in the Heston model",
    "url": "https://arxiv.org/pdf/2512.05326v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "We propose a convolution-FFT method for pricing European options under the Heston model that leverages a continuously differentiable representation of the joint characteristic function. Unlike existing Fourier-based methods that rely on branch-cut adjustments or empirically tuned damping parameters, our approach yields a stable integrand even under large frequency oscillations. Crucially, we derive fully analytical error bounds that quantify both truncation error and discretization error in terms of model parameters and grid settings. To the best of our knowledge, this is the first work to provide such explicit, closed-form error estimates for an FFT-based convolution method specialized to the Heston model. Numerical experiments confirm the theoretical rates and illustrate robust, high-accuracy option pricing at modest computational cost.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于Heston模型下欧式期权定价的卷积-FFT方法，该方法利用联合特征函数的连续可微表示。与现有依赖分支切割调整或经验调优阻尼参数的傅里叶方法不同，本方法即使在大频率振荡下也能产生稳定的被积函数。关键贡献在于推导了完全解析的误差界，以模型参数和网格设置量化截断误差和离散化误差。据我们所知，这是首个为专门针对Heston模型的基于FFT的卷积方法提供此类显式闭式误差估计的工作。数值实验验证了理论收敛速度，并展示了以适中计算成本实现稳健、高精度的期权定价。",
    "fetch_date": "2025-12-29",
    "id": "20251229_111d7440"
  },
  {
    "title": "Market Reactions to Material Cybersecurity Incident Disclosures",
    "url": "https://arxiv.org/pdf/2512.06144v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study examines short-term market responses to material cybersecurity incidents disclosed under Item 1.05 of Form 8-K. Drawing on a sample of disclosures made between 2023 and 2025, daily stock price movements were evaluated over a standardized event window surrounding each filing. On average, companies experienced negative price reactions following the disclosure of a material cybersecurity incident. Comparisons across company characteristics indicate that smaller companies tended to incur more pronounced declines, while differences by sector and beta were not evident. These findings offer empirical insight into how public markets interpret cybersecurity risks when they are formally reported and suggest that firm size may influence the degree of sensitivity to such events.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本研究考察了根据8-K表格第1.05项披露的重大网络安全事件引发的短期市场反应。基于2023年至2025年间的披露样本，评估了每份申报文件周围标准化事件窗口内的每日股价变动。平均而言，公司在披露重大网络安全事件后经历了负面价格反应。跨公司特征的比较表明，规模较小的公司往往遭受更明显的股价下跌，而按行业和贝塔值的差异并不明显。这些发现为公开市场如何解释正式报告的网络安全风险提供了实证见解，并表明公司规模可能影响对此类事件的敏感程度。",
    "fetch_date": "2025-12-29",
    "id": "20251229_da2b1b43"
  },
  {
    "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem",
    "url": "https://arxiv.org/pdf/2512.05946v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "资源分配问题因其组合复杂性而保持NP难。虽然深度强化学习方法（如Rainbow DQN）通过优先回放和分布头提高了可扩展性，但经典函数逼近器限制了其表示能力。本文提出变分量子Rainbow DQN（VQR-DQN），将环形拓扑变分量子电路与Rainbow DQN集成，以利用量子叠加和纠缠。我们将人力资源分配问题（HRAP）建模为基于官员能力、事件时间表和转移时间的组合动作空间的马尔可夫决策过程（MDP）。在四个HRAP基准测试中，VQR-DQN相比随机基线实现了26.8%的归一化完工时间减少，并优于Double DQN和经典Rainbow DQN 4.9-13.4%。这些收益与电路表达能力、纠缠和策略质量之间的理论联系一致，展示了量子增强DRL在大规模资源分配中的潜力。",
    "fetch_date": "2025-12-29",
    "id": "20251229_4b5ad711"
  },
  {
    "title": "FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction",
    "url": "https://arxiv.org/pdf/2512.15728v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \\emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\\% and stability of 93.33\\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文介绍了FedSight AI，这是一个利用大型语言模型（LLMs）模拟联邦公开市场委员会（FOMC）审议过程以预测联邦基金目标利率的多智能体系统架构。该系统通过智能体分析结构化指标（如经济数据）和非结构化输入（如褐皮书），进行辩论和投票，模拟委员会决策逻辑。其Chain-of-Draft（CoD）扩展通过强制简洁的多阶段推理，进一步提升了效率和准确性。在2023-2024年会议评估中，FedSight CoD实现了93.75%的准确率和93.33%的稳定性，优于包括MiniFed和Ordinal Random Forest（RF）在内的基线模型，同时提供了与真实FOMC沟通一致的透明推理。",
    "fetch_date": "2025-12-29",
    "id": "20251229_dc3f93f2"
  },
  {
    "title": "Standard and stressed value at risk forecasting using dynamic Bayesian networks",
    "url": "https://arxiv.org/pdf/2512.05661v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究引入动态贝叶斯网络（DBN）框架来预测风险价值（VaR）和压力风险价值（SVaR），并与几种常用模型进行性能比较。使用1991年至2020年标普500指数的日收益率数据，通过滚动周期和历史收益率生成10天99%的VaR和SVaR预测，而三个DBN模型同时使用历史收益率和预测收益率。通过标准回测和预测误差指标评估模型预测准确性。结果显示，自回归模型提供最准确的VaR预测，而DBN模型尽管包含前瞻性收益率预测，其表现与历史模拟模型相当。对于SVaR，所有模型均产生高度保守的预测，违约次数极少且准确性差异有限。虽然DBN未超越传统模型，但其作为前瞻性方法的可行性为未来将因果推断整合到金融风险预测的研究奠定了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f41d7e01"
  },
  {
    "title": "Risk aversion of insider and dynamic asymmetric information",
    "url": "https://arxiv.org/pdf/2512.05011v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies a Kyle-Back model with a risk-averse insider possessing exponential utility and a dynamic stochastic signal about the asset's terminal fundamental value. While the existing literature considers either risk-neutral insiders with dynamic signals or risk-averse insiders with static signals, we establish equilibrium when both features are present. Our approach imposes no restrictions on the magnitude of the risk aversion parameter, extending beyond previous work that requires sufficiently small risk aversion. We employ a weak conditioning methodology to construct a Schrödinger bridge between the insider's signal and the asset price process, an approach that naturally accommodates stochastic signal evolution and removes risk aversion constraints.\n  We derive necessary conditions for equilibrium, showing that the optimal insider strategy must be continuous with bounded variation. Under these conditions, we characterize the market-maker pricing rule and insider strategy that achieve equilibrium. We obtain explicit closed-form solutions for important cases including deterministic and quadratic signal volatilities, demonstrating the tractability of our framework.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个Kyle-Back模型，其中内幕交易者具有风险厌恶特征（采用指数效用函数）并拥有关于资产最终基础价值的动态随机信号。现有文献通常只考虑风险中性内幕交易者配动态信号，或风险厌恶内幕交易者配静态信号，而本文在两者同时存在的情况下建立了均衡。我们的方法不对风险厌恶参数的大小施加限制，超越了先前要求风险厌恶足够小的工作。我们采用弱条件方法在内幕交易者信号与资产价格过程之间构建了一个薛定谔桥，这种方法自然地适应了随机信号的演化并消除了风险厌恶约束。我们推导了均衡的必要条件，表明最优内幕交易策略必须是连续且有界变差的。在这些条件下，我们描述了实现均衡的做市商定价规则和内幕交易策略。我们针对包括确定性和二次信号波动率在内的重要案例获得了显式闭式解，证明了我们框架的可处理性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5800e438"
  },
  {
    "title": "Coordinated Mean-Field Control for Systemic Risk",
    "url": "https://arxiv.org/pdf/2512.04704v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "We develop a robust linear-quadratic mean-field control framework for systemic risk under model uncertainty, in which a central bank jointly optimizes interest rate policy and supervisory monitoring intensity against adversarial distortions. Our model features multiple policy instruments with interactive dynamics, implemented via a variance weight that depends on the policy rate, generating coupling effects absent in single-instrument models. We establish viscosity solutions for the associated HJB--Isaacs equation, prove uniqueness via comparison principles, and provide verification theorems. The linear-quadratic structure yields explicit feedback controls derived from a coupled Riccati system, preserving analytical tractability despite adversarial uncertainty. Simulations reveal distinct loss-of-control regimes driven by robustness-breakdown and control saturation, alongside a pronounced asymmetry in sensitivity between the mean and variance channels. These findings demonstrate the importance of instrument complementarity in systemic risk modeling and control.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文开发了一个稳健的线性二次平均场控制框架，用于在模型不确定性下处理系统性风险。中央银行通过联合优化利率政策和监管监控强度来对抗对抗性扭曲。模型具有多个政策工具的动态交互，通过依赖于政策利率的方差权重实现，产生了单工具模型中不存在的耦合效应。作者建立了相关HJB-Isaacs方程的粘性解，通过比较原理证明了唯一性，并提供了验证定理。线性二次结构产生了从耦合Riccati系统导出的显式反馈控制，尽管存在对抗性不确定性，但仍保持了分析的可处理性。模拟揭示了由稳健性崩溃和控制饱和驱动的不同失控机制，以及均值通道和方差通道之间显著的敏感性不对称。这些发现证明了在系统性风险建模和控制中工具互补性的重要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_05510e0b"
  },
  {
    "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2512.04653v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种用于多交叉口自适应交通信号控制的半集中训练去中心化执行架构，通过区域划分、参数共享和复合状态奖励设计来解决完全集中式方法的维度灾难和完全分布式方法的局部可观测性问题。虽然架构具有可迁移性并实现了两个具体模型，但其针对交通信号控制这一特定领域，与量化交易的实战应用关联较弱。",
    "fetch_date": "2025-12-29",
    "id": "20251229_6d6beb51"
  },
  {
    "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks",
    "url": "https://arxiv.org/pdf/2512.07162v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "DeepSVM：一种基于物理信息深度算子网络学习随机波动率模型的方法。实时校准随机波动率模型（SVMs）的计算瓶颈在于需要反复求解耦合偏微分方程（PDEs）。本文提出的DeepSVM无需标记训练数据，通过硬约束假设强制满足终端收益和静态无套利条件，并采用残差自适应细化（RAR）稳定高梯度区域的训练。该模型在典型市场动态范围内实现了高度准确的期权定价，但发现其衍生品（Greeks）在平价（ATM）区域存在噪声，凸显了物理信息算子学习中高阶正则化的需求。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b22309f0"
  },
  {
    "title": "Learning to Hedge Swaptions",
    "url": "https://arxiv.org/pdf/2512.06639v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了基于强化学习的深度对冲框架在掉期期权动态对冲中的应用，并与传统的基于敏感性的Rho对冲方法进行了性能对比。研究设计了三种不同目标函数（均方误差、下行风险、条件风险价值）的智能体以捕捉不同的风险偏好，并评估这些目标如何塑造对冲风格。基于三因子无套利动态Nelson-Siegel模型进行模拟实验，结果表明使用两种互换作为对冲工具时能实现接近最优的对冲效果。深度对冲策略能根据市场状态动态调整对冲组合对风险因子的敞口。实验显示，即使在模型存在一定误设的情况下，其表现仍持续优于Rho对冲策略。这些结果突显了强化学习在提供更高效、更具韧性的掉期期权对冲策略方面的潜力。",
    "fetch_date": "2025-12-28",
    "id": "20251228_80dea847"
  },
  {
    "title": "Unveiling Hedge Funds: Topic Modeling and Sentiment Correlation with Fund Performance",
    "url": "https://arxiv.org/pdf/2512.06620v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "The hedge fund industry presents significant challenges for investors due to its opacity and limited disclosure requirements. This pioneering study introduces two major innovations in financial text analysis. First, we apply topic modeling to hedge fund documents-an unexplored domain for automated text analysis-using a unique dataset of over 35,000 documents from 1,125 hedge fund managers. We compared three state-of-the-art methods: Latent Dirichlet Allocation (LDA), Top2Vec, and BERTopic. Our findings reveal that LDA with 20 topics produces the most interpretable results for human users and demonstrates higher robustness in topic assignments when the number of topics varies, while Top2Vec shows superior classification performance. Second, we establish a novel quantitative framework linking document sentiment to fund performance, transforming qualitative information traditionally requiring expert interpretation into systematic investment signals. In sentiment analysis, contrary to expectations, the general-purpose DistilBERT outperforms the finance-specific FinBERT in generating sentiment scores, demonstrating superior adaptability to diverse linguistic patterns found in hedge fund documents that extend beyond specialized financial news text. Furthermore, sentiment scores derived using DistilBERT in combination with Top2Vec show stronger correlations with subsequent fund performance compared to other model combinations. These results demonstrate that automated topic modeling and sentiment analysis can effectively process hedge fund documents, providing investors with new data-driven decision support tools.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究在金融文本分析领域引入两大创新：首先，将主题建模应用于对冲基金文档这一自动化文本分析未涉足领域，使用来自1,125家对冲基金管理人的超过35,000份文档数据集，比较了潜在狄利克雷分配（LDA）、Top2Vec和BERTopic三种前沿方法。研究发现，20个主题的LDA对人类用户最具可解释性，且在主题数量变化时表现出更高鲁棒性，而Top2Vec则展现更优的分类性能。其次，建立了一个将文档情感与基金表现相关联的量化框架，将传统需要专家解读的定性信息转化为系统性投资信号。在情感分析中，通用模型DistilBERT意外地优于金融专用模型FinBERT，显示出对对冲基金多样化语言模式的更强适应性。",
    "fetch_date": "2025-12-28",
    "id": "20251228_3d108839"
  },
  {
    "title": "Hybrid Quantum-Classical Ensemble Learning for S\\&P 500 Directional Prediction",
    "url": "https://arxiv.org/pdf/2512.15738v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\\% directional accuracy on S\\&P 500 prediction, a 3.10\\% improvement over individual models.\n  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\\% vs.\\ 52.80\\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\\% to +1.5\\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\\%$), improving ensemble performance (Top-7 models: 60.14\\% vs.\\ all 35 models: 51.2\\%).\n  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种混合量子-经典集成学习框架，用于标普500指数的方向性预测。核心创新包括：1）架构多样性优于数据集多样性——在相同数据上结合不同算法（LSTM、决策Transformer、XGBoost等）比单一架构多数据集训练效果更佳（60.14% vs. 52.80%）；2）4量子比特变分量子电路增强情感分析，为各模型带来0.8%-1.5%的准确率提升；3）智能筛选排除弱预测器（准确率<52%），优化集成性能。最终实现60.14%的方向预测准确率，较单一模型提升3.10%，对量化交易实战具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_fab16cfe"
  },
  {
    "title": "Deep learning approaches in Finance: Forecasting volatility and enhancing Quantitative trading strategies",
    "url": "https://etheses.whiterose.ac.uk/id/eprint/27202/",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… services industry using Deep Learning architectures. The main focus is on advancing current approaches in the areas of Risk Management and Quantitative Trading. The former is …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文聚焦于深度学习在金融领域的应用，主要致力于推动风险管理与量化交易领域的现有方法。核心内容包括利用深度学习架构预测波动率，并以此增强量化交易策略。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6403d992"
  },
  {
    "title": "Intraday and Post-Market Investor Sentiment for Stock Price Prediction: A Deep Learning Framework with Explainability and Quantitative Trading Strategy",
    "url": "https://www.mdpi.com/2079-8954/13/5/390",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… In contrast to traditional deep learning models, which are often … Quantitative trading backtesting under the T+1 trading … Most academic studies neglect real-world trading constraints…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种用于股价预测的深度学习框架，结合了盘中与盘后投资者情绪分析，并具备模型可解释性。研究特别设计了考虑T+1交易制度的量化交易策略进行回测，强调了传统学术研究常忽略的实际交易约束，对实战交易具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ce0d31fc"
  },
  {
    "title": "Research on quantitative investment strategies based on deep learning",
    "url": "https://www.mdpi.com/1999-4893/12/2/35",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… with four trading strategies (Long Call, Short Call, Long Put, Short Put) where deep learning … mirror the impact of its accuracy on quantitative trading strategies in a straightforward way. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于深度学习的量化投资策略研究，涉及四种交易策略（买入看涨期权、卖出看涨期权、买入看跌期权、卖出看跌期权），通过深度学习模型直接反映其预测准确性对量化交易策略的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d4332f98"
  },
  {
    "title": "Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing",
    "url": "https://arxiv.org/pdf/2512.17923v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们引入了一种新颖的混淆测试方法，用于验证大型语言模型是否通过因果推理而非时间关联来检测结构性市场模式。在标普500期权数据的242个交易日（覆盖95.6%）上测试三种做市商对冲约束模式（伽马持仓、股票钉住、0DTE对冲），我们发现LLM使用无偏提示（仅提供原始伽马暴露值，无制度标签或时间背景）实现了71.5%的检测率。WHO-WHOM-WHAT因果框架迫使模型识别观察到的市场动态背后的经济参与者（做市商）、受影响方（方向性交易者）和结构性机制（强制对冲）。关键的是，即使季度经济盈利能力变化，检测准确率（91.2%）保持稳定，表明模型识别的是结构性约束而非盈利模式。当提供制度标签时，检测率提升至100%，但71.5%的无偏率验证了真正的模式识别。我们的发现表明，LLM通过纯结构性推理检测复杂金融机制具有新兴能力，对系统交易有潜在影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_19f9e1af"
  },
  {
    "title": "Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance",
    "url": "https://arxiv.org/pdf/2512.15739v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种贝叶斯分析框架，用于金融风险预测与合规中的不确定性管理。该方法在波动率预测、欺诈检测和合规监控方面提升了风险处理能力。通过评估标普500指数日收益率的单日95%风险价值预测（训练期2000-2019，测试期2020-2024），发现LSTM基线接近名义校准，而带学生t分布的GARCH(1,1)模型低估尾部风险。提出的折扣因子DLM模型产生略宽松的VaR估计，存在聚集性违规证据。贝叶斯逻辑回归提高了欺诈检测的召回率和AUC-ROC，分层Beta状态空间模型提供了透明自适应的合规风险评估。该框架以精确的不确定性量化、可解释性和GPU加速分析为特点。",
    "fetch_date": "2025-12-28",
    "id": "20251228_29b631c7"
  },
  {
    "title": "Analysis of Contagion in China's Stock Market: A Hawkes Process Perspective",
    "url": "https://arxiv.org/pdf/2512.08000v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "This study explores contagion in the Chinese stock market using Hawkes processes to analyze autocorrelation and cross-correlation in multivariate time series data. We examine whether market indices exhibit trending behavior and whether sector indices influence one another. By fitting self-exciting and inhibitory Hawkes processes to daily returns of indices like the Shanghai Composite, Shenzhen Component, and ChiNext, as well as sector indices (CSI Consumer, Healthcare, and Financial), we identify long-term dependencies and trending patterns, including upward, downward, and oversold rebound trends. Results show that during high trading activity, sector indices tend to sustain their trends, while low activity periods exhibit strong sector rotation. This research models stock price movements using spatiotemporal Hawkes processes, leveraging conditional intensity functions to explain sector rotation, advancing the understanding of financial contagion.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究使用Hawkes过程分析中国股市的传染效应，通过拟合自激励和抑制型Hawkes过程到上证综指、深证成指、创业板指及消费、医疗、金融等行业指数的日收益率数据，识别长期依赖性和趋势模式（包括上升、下降和超跌反弹趋势）。研究发现高交易活跃度期间行业指数倾向于维持趋势，低活跃度期间则呈现显著的行业轮动现象。该研究通过时空Hawkes过程建模股价变动，利用条件强度函数解释行业轮动，深化了对金融传染机制的理解。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d0ecab01"
  },
  {
    "title": "Asian option valuation under price impact",
    "url": "https://arxiv.org/pdf/2512.07154v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We study the valuation of Asian options in a binomial market with permanent price impact, extending the Cox-Ross-Rubinstein framework under a modified risk-neutral probability. We obtain an exact pathwise representation for geometric Asian options and derive two-sided bounds for arithmetic Asian options. Our analysis identifies the no-arbitrage region in terms of hedging volumes and shows that permanent price impact systematically raises Asian option prices. Numerical examples illustrate the effect of the impact parameter and hedging volumes on the resulting prices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在带有永久价格影响的二项式市场中研究亚式期权的定价，扩展了修正风险中性概率下的Cox-Ross-Rubinstein框架。研究获得了几何亚式期权的精确路径表示，并推导了算术亚式期权的双边边界。分析从对冲头寸角度识别了无套利区间，表明永久价格影响会系统性提高亚式期权价格。数值示例展示了影响参数和对冲头寸对最终价格的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_a5b016a2"
  },
  {
    "title": "Market Reactions and Information Spillovers in Bank Mergers: A Multi-Method Analysis of the Japanese Banking Sector",
    "url": "https://arxiv.org/pdf/2512.06550v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Major bank mergers and acquisitions (M&A) transform the financial market structure, but their valuation and spillover effects remain open to question. This study examines the market reaction to two M&A events: the 2005 creation of Mitsubishi UFJ Financial Group following the Financial Big Bang in Japan, and the 2018 merger involving Resona Holdings after the global financial crisis. The multi-method analysis in this research combines several distinct methods to explore these M&A events. An event study using the market model, the capital asset pricing model (CAPM), and the Fama-French three-factor model is implemented to estimate cumulative abnormal returns (CAR) for valuation purposes. Vector autoregression (VAR) models are used to test for Granger causality and map dynamic effects using impulse response functions (IRFs) to investigate spillovers. Propensity score matching (PSM) helps provide a causal estimate of the average treatment effect on the treated (ATT). The analysis detected a significant positive market reaction to the mergers. The findings also suggest the presence of prolonged positive spillovers to other banks, which may indicate a synergistic effect among Japanese banks. Combining these methods provides a unique perspective on M&A events in the Japanese banking sector, offering valuable insights for investors, managers, and regulators concerned with market efficiency and systemic stability",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究通过多方法分析评估日本银行业两次重大并购事件（2005年三菱UFJ金融集团成立与2018年Resona控股合并）的市场反应与信息溢出效应。采用事件研究法（市场模型、CAPM、Fama-French三因子模型）计算累积异常收益（CAR），运用向量自回归（VAR）模型检验格兰杰因果关系并通过脉冲响应函数（IRFs）分析动态溢出效应，辅以倾向得分匹配（PSM）估计平均处理效应（ATT）。研究发现并购产生显著正向市场反应，并存在对其他银行的持续正向溢出效应，暗示日本银行业可能存在协同效应。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b46875b4"
  },
  {
    "title": "Amortizing Perpetual Options",
    "url": "https://arxiv.org/pdf/2512.06505v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "In this work, we introduce amortizing perpetual options (AmPOs), a fungible variant of continuous-installment options suitable for exchange-based trading. Traditional installment options lapse when holders cease their payments, destroying fungibility across units of notional. AmPOs replace explicit installment payments and the need for lapsing logic with an implicit payment scheme via a deterministic decay in the claimable notional. This amortization ensures all units evolve identically, preserving fungibility. Under the Black-Scholes framework, AmPO valuation can be reduced to an equivalent vanilla perpetual American option on a dividend-paying asset. In this way, analytical expressions are possible for the exercise boundaries and risk-neutral valuations for calls and puts. These formulas and relations allow us to derive the Greeks and study comparative statics with respect to the amortization rate. Illustrative numerical case studies demonstrate how the amortization rate shapes option behavior and reveal the resulting tradeoffs in the effective volatility sensitivity.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了摊销型永续期权（AmPOs），这是一种适用于交易所交易的可替代连续分期期权变体。传统分期期权在持有人停止支付时会失效，破坏了名义金额单位间的可替代性。AmPOs通过可索赔名义金额的确定性衰减隐含支付方案，取代了显性分期付款和失效逻辑。这种摊销确保所有单位以相同方式演变，保持了可替代性。在Black-Scholes框架下，AmPO估值可简化为等价于带股息资产上的普通永续美式期权。通过这种方式，可以得出看涨和看跌期权的行权边界和风险中性估值的解析表达式。这些公式和关系使我们能够推导希腊字母，并研究相对于摊销率的比较静态。示例数值案例研究展示了摊销率如何塑造期权行为，并揭示了有效波动率敏感性中的权衡取舍。",
    "fetch_date": "2025-12-28",
    "id": "20251228_7b969bb3"
  },
  {
    "title": "Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market",
    "url": "https://arxiv.org/pdf/2512.06473v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $ρ_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "复杂系统中的相关性常被非平稳性、长程记忆和重尾波动所掩盖，限制了传统协方差分析的有效性。为应对这些挑战，本文使用多分形去趋势互相关系数ρ_r构建了尺度与波动依赖的相关矩阵，该系数能选择性地强调不同幅度的波动。作者检验了这些去趋势相关矩阵的谱特性，并与从合成高斯及q高斯信号以相同方式计算得到的矩阵谱特性进行比较。结果表明，即使在时间序列不存在互相关的情况下，去趋势处理、重尾分布和波动阶参数r共同产生的谱也与随机情况显著偏离。将该框架应用于2021-2024年间140种主要加密货币的一分钟收益率数据，揭示了稳健的集体模式，包括一个主导的市场因子和几个行业成分，其强度取决于分析的尺度和波动阶。在滤除市场模式后，经验特征值的主体部分与随机矩阵理论预测的极限分布紧密对齐。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ae9ae498"
  },
  {
    "title": "Wealth or Stealth? The Camouflage Effect in Insider Trading",
    "url": "https://arxiv.org/pdf/2512.06309v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "We consider a Kyle-type model where insider trading takes place among a potentially large population of liquidity traders and is subject to legal penalties. Insiders exploit the liquidity provided by the trading masses to \"camouflage\" their actions and balance expected wealth with the necessary stealth to avoid detection. Under a diverse spectrum of prosecution schemes, we establish the existence of equilibria for arbitrary population sizes and a unique limiting equilibrium. A convergence analysis determines the scale of insider trading by a stealth index $γ$, revealing that the equilibrium can be closely approximated by a simple limit due to diminished price informativeness. Empirical aspects are derived from two calibration experiments using non-overlapping data sets spanning from 1980 to 2018, which underline the indispensable role of a large population in insider trading models with legal risk, along with important implications for the incidence of stealth trading and the deterrent effect of legal enforcement.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个Kyle型模型，研究内幕交易者在面临法律处罚风险时，如何利用大量流动性交易者提供的流动性来“伪装”其交易行为，在预期财富与避免被发现的必要隐蔽性之间取得平衡。通过分析多种起诉方案，证明了任意市场规模下均衡的存在性及唯一的极限均衡。收敛分析通过隐蔽指数γ确定内幕交易规模，并揭示由于价格信息性减弱，均衡可被一个简单极限近似逼近。实证部分基于1980年至2018年的两个非重叠数据集进行校准实验，强调了在考虑法律风险的内幕交易模型中大规模交易群体的不可或缺作用，并对隐蔽交易的发生频率及法律执行的威慑效应提出了重要启示。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6e05cc4e"
  },
  {
    "title": "Not All Factors Crowd Equally: Modeling, Measuring, and Trading on Alpha Decay",
    "url": "https://arxiv.org/pdf/2512.11913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We derive a specific functional form for factor alpha decay -- hyperbolic decay alpha(t) = K/(1+lambda*t) -- from a game-theoretic equilibrium model, and test it against linear and exponential alternatives. Using eight Fama-French factors (1963--2024), we find: (1) Hyperbolic decay fits mechanical factors. Momentum exhibits clear hyperbolic decay (R^2 = 0.65), outperforming linear (0.51) and exponential (0.61) baselines -- validating the equilibrium foundation. (2) Not all factors crowd equally. Mechanical factors (momentum, reversal) fit the model; judgment-based factors (value, quality) do not -- consistent with a signal-ambiguity taxonomy paralleling Hua and Sun's \"barriers to entry.\" (3) Crowding accelerated post-2015. Out-of-sample, the model over-predicts remaining alpha (0.30 vs. 0.15), correlating with factor ETF growth (rho = -0.63). (4) Average returns are efficiently priced. Crowding-based factor selection fails to generate alpha (Sharpe: 0.22 vs. 0.39 factor momentum benchmark). (5) Crowding predicts tail risk. Out-of-sample (2001--2024), crowded reversal factors show 1.7--1.8x higher crash probability (bottom decile returns), while crowded momentum shows lower crash risk (0.38x, p = 0.006). Our findings extend equilibrium crowding models (DeMiguel et al.) to temporal dynamics and show that crowding predicts crashes, not means -- useful for risk management, not alpha generation.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文从博弈论均衡模型推导出因子阿尔法衰减的特定函数形式——双曲衰减alpha(t)=K/(1+lambda*t)，并在线性和指数衰减基准上进行检验。基于八个Fama-French因子（1963-2024）的研究发现：（1）机械因子（如动量）呈现清晰的双曲衰减（R²=0.65），验证了均衡基础；（2）因子拥挤存在异质性——机械因子（动量、反转）符合模型，而基于判断的因子（价值、质量）则不符合，这与Hua和Sun的“进入壁垒”信号模糊度分类一致；（3）2015年后拥挤加速，样本外模型高估剩余阿尔法（0.30 vs. 0.15），且与因子ETF增长负相关（ρ=-0.63）；（4）平均收益已被有效定价，基于拥挤的因子选择未能产生阿尔法（夏普比率：0.22 vs. 因子动量基准0.39）；（5）拥挤可预测尾部风险——样本外（2001-2024）拥挤的反转因子崩盘概率高1.7-1.8倍，而拥挤的动量因子崩盘风险较低。",
    "fetch_date": "2025-12-27",
    "id": "20251227_0101e588"
  },
  {
    "title": "Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making",
    "url": "https://arxiv.org/pdf/2512.17936v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in Türkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "面对日益增长的金融不确定性和市场复杂性，本研究提出了一种新颖的风险感知金融预测框架，该框架将先进的机器学习技术与直觉模糊多准则决策（MCDM）相结合。该框架专为BIST 100指数设计，并通过土耳其一家主要国防公司的案例研究进行验证，融合了结构化金融数据、非结构化文本数据和宏观经济指标，以提高预测准确性和鲁棒性。它包含一套混合模型，包括极端梯度提升（XGBoost）、长短期记忆（LSTM）网络和图神经网络（GNN），以提供具有量化不确定性的概率预测。实证结果显示高预测准确性，净利润平均绝对百分比误差（MAPE）为3.03%，关键财务指标的95%置信区间较窄。风险感知分析显示有利的风险回报特征，夏普比率为1.25，索提诺比率更高为1.80，表明在市场波动下相对较低的下行波动性和稳健表现。敏感性分析表明关键财务指标具有稳定性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_96d3f766"
  },
  {
    "title": "Exploratory Mean-Variance with Jumps: An Equilibrium Approach",
    "url": "https://arxiv.org/pdf/2512.09224v1",
    "source": "ArXiv",
    "date": "2025-12-10",
    "abstract": "Revisiting the continuous-time Mean-Variance (MV) Portfolio Optimization problem, we model the market dynamics with a jump-diffusion process and apply Reinforcement Learning (RL) techniques to facilitate informed exploration within the control space. We recognize the time-inconsistency of the MV problem and adopt the time-inconsistent control (TIC) approach to analytically solve for an exploratory equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem. Our approach accounts for time-inconsistent preferences and actions, and our equilibrium policy is the best option an investor can take at any given time during the investment period. Moreover, we leverage the martingale properties of the equilibrium policy, design a RL model, and propose an Actor-Critic RL algorithm. All of our RL model parameters converge to the corresponding true values in a simulation study. Our numerical study on 24 years of real market data shows that the proposed RL model is profitable in 13 out of 14 tests, demonstrating its practical applicability in real world investment.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视连续时间均值-方差（MV）投资组合优化问题，采用跳跃扩散过程建模市场动态，并应用强化学习（RL）技术在控制空间内进行知情探索。针对MV问题的时间不一致性，采用时间不一致控制（TIC）方法解析求解探索性均衡投资策略——一个以经典MV问题均衡控制为中心的高斯分布。该策略考虑了时间不一致的偏好与行动，是投资期内任一时刻的最佳选择。此外，利用均衡策略的鞅性质设计RL模型，提出Actor-Critic RL算法。模拟研究中所有RL模型参数均收敛至真实值，基于24年真实市场数据的数值研究表明，所提RL模型在14次测试中13次盈利，证明了其在实际投资中的适用性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_b4ce1604"
  },
  {
    "title": "Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market",
    "url": "https://arxiv.org/abs/2506.06356",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… -day turnover quantitative trading algorithm that integrates advanced deep learning techniques … Index Terms—quantitative trading, deep learning, crosssectional prediction, multi-day …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度学习增强的多日换手率量化交易算法应用于中国A股市场。该算法整合了先进的深度学习技术，专注于横截面预测和多日交易策略。",
    "fetch_date": "2025-12-27",
    "id": "20251227_c3728bb7"
  },
  {
    "title": "Intelligent optimization based multi-factor deep learning stock selection model and quantitative trading strategy",
    "url": "https://www.mdpi.com/2227-7390/10/4/566",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… Thirdly, this paper designs and implements a quantitative trading strategy. Based on the CS-GRU stock selection model, this paper designs and implements a quantitative trading …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了一种基于智能优化的多因子深度学习选股模型与量化交易策略。首先，构建了CS-GRU选股模型（结合了Cuckoo Search优化算法与门控循环单元网络），用于预测股票收益。其次，基于该模型设计并实施了量化交易策略，包括信号生成、仓位管理和风险控制等实战环节。最后，通过回测验证了策略的有效性，表明该模型在实战交易中具有潜在应用价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_887f4f01"
  },
  {
    "title": "Research on Deep Learning-Based Quantitative Trading Models",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-99477-7_12",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… This paper examines the role of quantitative trading in finance and the potential applications of deep learning. Quantitative trading automates investment strategies using mathematical …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文探讨了量化交易在金融领域的作用及深度学习的潜在应用。量化交易利用数学模型自动化投资策略...",
    "fetch_date": "2025-12-27",
    "id": "20251227_ed097bf0"
  },
  {
    "title": "Sustainability, accuracy, fairness, and explainability (safe) machine learning in quantitative trading",
    "url": "https://www.mdpi.com/2227-7390/13/3/442",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… based signal strategies: those based on deep learning models and those grounded in classical machine learning techniques. The deep learning models employed in this research were …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了可持续性、准确性、公平性和可解释性（SAFE）机器学习在量化交易中的应用，比较了基于深度学习模型和经典机器学习技术的信号策略。研究采用的深度学习模型包括...，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_020ded1a"
  },
  {
    "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
    "url": "https://arxiv.org/pdf/2512.10913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "强化学习（RL）作为一种金融决策的创新方法，为传统方法难以解决的复杂投资问题提供了专门解决方案。该综述分析了2017-2025年的167篇文章，重点关注做市、投资组合优化和算法交易。研究发现RL在金融应用中存在关键性能问题和挑战，但在做市等领域相比传统方法具有优势。研究提出了一个统一框架来解决可解释性、鲁棒性和部署可行性等常见问题。基于合成数据的实证证据表明，实施质量和领域知识通常比算法复杂性更重要。研究强调需要可解释的RL架构以满足监管要求、增强非平稳环境下的鲁棒性，并建立标准化基准测试协议。建议机构应减少对算法复杂性的关注，更多关注市场微观结构、监管约束和风险管理。",
    "fetch_date": "2025-12-27",
    "id": "20251227_801c29bb"
  },
  {
    "title": "Local and Global Balance in Financial Correlation Networks: an Application to Investment Decisions",
    "url": "https://arxiv.org/pdf/2512.10606v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "The global balance is a well-known indicator of the behavior of a signed network. Recent literature has introduced the concept of local balance as a measure of the contribution of a single node to the overall balance of the network. In the present research, we investigate the potential of using deviations of local balance from global balance as a criterion for selecting outperforming assets. The underlying idea is that, during financial crises, most assets in the investment universe behave similarly: losses are severe and widespread, and the global balance of the correlation-based signed network reaches its maximum value. Under such circumstances, standard diversification (mainly related to portfolio size) is unable to reduce risk or limit losses. Therefore, it may be useful to concentrate portfolio exposures on the few assets - if such assets exist-that behave differently from the rest of the market. We argue that these assets are those for which the local balance strongly departs from the global balance of the underlying signed network. The paper supports this hypothesis through an application using real financial data. The results, in both descriptive and predictive contexts, confirm the proposed intuition.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于金融相关网络局部平衡与全局平衡偏离度的资产选择方法。核心观点是：在金融危机期间，大多数资产表现趋同，传统分散化策略失效；此时，应集中投资于那些局部平衡与网络全局平衡显著偏离的资产，因为这些资产的市场行为与整体市场不同。研究通过真实金融数据验证了这一假设，表明该方法在描述性和预测性场景下均能识别出表现优异的资产。",
    "fetch_date": "2025-12-27",
    "id": "20251227_9fed57fe"
  },
  {
    "title": "A New Application of Hoeffding's Inequality Can Give Traders Early Warning of Financial Regime Change",
    "url": "https://arxiv.org/pdf/2512.08851v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "Hoeffding's Inequality provides the maximum probability that a series of n draws from a bounded random variable differ from the variable's true expectation u by more than given tolerance t. The random variable is typically the error rate of a classifier in machine learning applications. Here, a trading strategy is premised on the assumption of an underlying distribution of causal factors, in other words, a market regime, and the random variable is the performance of that trading strategy. A larger deviation of observed performance from the trader's expectation u can be characterized as a lower probability that the financial regime supporting that strategy remains in force, and a higher probability of financial regime change. The changing Hoeffding probabilities can be used as an early warning indicator of this change.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种将霍夫丁不等式应用于量化交易的新方法。该方法将交易策略的绩效视为有界随机变量，通过计算观察到的绩效与预期绩效之间的偏差概率，来评估当前市场状态（即金融制度）是否发生变化。当偏差增大时，霍夫丁概率降低，表明支持该策略的市场制度可能不再有效，从而为金融制度变更提供早期预警。该方法为基于市场制度假设的交易策略提供了一种理论上的风险监控工具。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d6fbced2"
  },
  {
    "title": "Pareto-optimal reinsurance under dependence uncertainty",
    "url": "https://arxiv.org/pdf/2512.11430v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper studies Pareto-optimal reinsurance design in a monopolistic market with multiple primary insurers and a single reinsurer, all with heterogeneous risk preferences. The risk preferences are characterized by a family of risk measures, called Range Value-at-Risk (RVaR), which includes both Value-at-Risk (VaR) and Expected Shortfall (ES) as special cases. Recognizing the practical difficulty of accurately estimating the dependence structure among the insurers' losses, we adopt a robust optimization approach that assumes the marginal distributions are known while leaving the dependence structure unspecified. We provide a complete characterization of optimal indemnity schedules under the worst-case scenario, showing that the infinite-dimensional optimization problem can be reduced to a tractable finite-dimensional problem involving only two or three parameters for each indemnity function. Additionally, for independent and identically distributed risks, we exploit the argument of asymptotic normality to derive optimal two-parameter layer contracts. Finally, numerical applications are considered in a two-insurer setting to illustrate the influence of the dependence structures and heterogeneous risk tolerances on optimal strategies and the corresponding risk evaluation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究垄断市场中多个原保险公司与单一再保险公司之间的帕累托最优再保险设计，各方具有异质性风险偏好。风险偏好由一系列风险度量（称为范围风险价值，RVaR）表征，该度量包含风险价值（VaR）和预期损失（ES）作为特例。考虑到准确估计保险公司损失间依赖结构的实际困难，作者采用鲁棒优化方法，假设边际分布已知而依赖结构未指定。在最坏情况下，作者完整刻画了最优赔偿方案，表明无限维优化问题可简化为仅涉及每个赔偿函数两到三个参数的可处理有限维问题。此外，对于独立同分布风险，作者利用渐近正态性论证推导出最优两参数分层合约。最后，通过双保险公司设置的数值应用说明了依赖结构的影响。",
    "fetch_date": "2025-12-27",
    "id": "20251227_80cceaa5"
  },
  {
    "title": "Generative AI for Analysts",
    "url": "https://arxiv.org/pdf/2512.19705v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨生成式人工智能（AI）如何改变金融分析师的工作。以2023年FactSet AI平台上线为自然实验，发现采用AI后报告内容显著丰富全面——信息源增加40%，主题覆盖扩大34%，高级分析方法使用提升25%，同时时效性改善。然而，预测误差上升59%，因为AI辅助报告呈现更平衡的正负面信息，尤其对认知负荷较重的分析师更难整合。使用其他数据供应商的安慰剂测试证实这些效应是FactSet AI集成独有的。总体而言，研究揭示了生成式AI在金融信息生产中既带来生产力提升，也存在认知局限。",
    "fetch_date": "2025-12-27",
    "id": "20251227_778da003"
  },
  {
    "title": "Option-Implied Zero-Coupon Yields: Unifying Bond and Equity Markets",
    "url": "https://arxiv.org/pdf/2512.10823v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "This paper addresses a critical inconsistency in models of the term structure of interest rates (TSIR), where zero-coupon bonds are priced under risk-neutral measures distinct from those used in equity markets. We propose a unified TSIR framework that treats zero-coupon bonds as European options with deterministic payoffs ensuring that they are priced under the same risk-neutral measure that governs equity derivatives. Using put-call parity, we extract zero-coupon bond implied yield curves from S&P 500 index options and compare them with the US daily treasury par yield curves. As the implied yield curves contain maturity time T and strike price K as independent variables, we investigate the K-dependence of the implied yield curve. Our findings, that at-the-money, option-implied yield curves provide the closest match to treasury par yield curves, support the view that the equity options market contains information that is highly relevant for the TSIR. By insisting that the risk-neutral measure used for bond valuation is the same as that revealed by equity derivatives, we offer a new organizing principle for future TSIR research.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对利率期限结构模型中的关键不一致性提出解决方案，即零息债券与权益衍生品使用不同的风险中性测度定价。作者提出统一框架，将零息债券视为具有确定性收益的欧式期权，确保其与权益衍生品在同一风险中性测度下定价。利用看跌-看涨平价关系，从标普500指数期权中提取零息债券隐含收益率曲线，并与美国国债平价收益率曲线比较。研究发现，平值期权隐含收益率曲线与国债曲线最接近，表明权益期权市场包含对利率期限结构高度相关的信息。该框架为未来TSIR研究提供了新的组织原则。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d45f5bf0"
  },
  {
    "title": "Volatility time series modeling by single-qubit quantum circuit learning",
    "url": "https://arxiv.org/pdf/2512.10584v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We employ single-qubit quantum circuit learning (QCL) to model the dynamics of volatility time series. To assess its effectiveness, we generate synthetic data using the Rational GARCH model, which is specifically designed to capture volatility asymmetry. Our results show that QCL-based volatility predictions preserve the negative return-volatility correlation, a hallmark of asymmetric volatility dynamics. Moreover, analysis of the Hurst exponent and multifractal characteristics indicates that the predicted series, like the original synthetic data, exhibits anti-persistent behavior and retains its multifractal structure.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究采用单量子比特量子电路学习（QCL）对波动率时间序列进行建模。通过使用专门捕捉波动率不对称性的Rational GARCH模型生成合成数据进行评估，结果表明基于QCL的波动率预测保留了负收益-波动率相关性（这是不对称波动率动态的标志特征）。此外，对赫斯特指数和多重分形特征的分析表明，预测序列与原始合成数据类似，表现出反持续性行为并保留了其多重分形结构。",
    "fetch_date": "2025-12-27",
    "id": "20251227_568b9ce8"
  },
  {
    "title": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
    "url": "https://arxiv.org/pdf/2512.17929v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了在宏观经济关系不确定且随时间变化的情况下，中央银行应如何动态设定短期名义利率以稳定通胀和失业率。我们将货币政策建模为一个序贯决策问题，中央银行每季度观察宏观经济状况并选择利率调整。使用公开可用的历史联邦储备经济数据（FRED），我们构建了一个线性高斯转移模型，并实现了一个具有二次损失奖励函数的离散动作马尔可夫决策过程。我们选择了九种不同的强化学习方法与泰勒规则和朴素基线进行比较，包括表格Q学习变体、SARSA、Actor-Critic、深度Q网络、具有不确定性量化的贝叶斯Q学习以及具有部分可观测性的POMDP公式。令人惊讶的是，标准的表格Q学习取得了最佳性能（-615.13 ± 309.58 平均回报），优于增强的RL方法和传统政策规则。我们的结果表明，虽然复杂的RL技术在货币政策应用中显示出前景，但更简单的方法在应对不确定性时可能更稳健。",
    "fetch_date": "2025-12-27",
    "id": "20251227_ebb24070"
  },
  {
    "title": "EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction",
    "url": "https://arxiv.org/pdf/2512.12727v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "EXFormer：一种用于外汇收益率预测的多尺度趋势感知Transformer，具有动态变量选择功能。该论文提出了一种新颖的基于Transformer的架构，专门用于预测每日汇率收益率。它引入了多尺度趋势感知自注意力机制，采用具有不同感受野的并行卷积分支，以基于局部斜率对齐观测值，在保持长程依赖性的同时对制度转换保持敏感。动态变量选择器为28个与汇率收益率相关的外生协变量分配时变重要性权重，提供先验可解释性。嵌入的挤压-激励块重新校准通道响应，以强调信息特征并抑制预测中的噪声。使用欧元/美元、美元/日元和英镑/美元的每日数据，在五个不同的滑动窗口上进行了样本外评估。EXFormer始终优于随机游走和其他基线模型，提高了方向准确性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_95743fd6"
  },
  {
    "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2512.12250v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该研究提出了一种混合建模框架，将随机波动率模型与长短期记忆神经网络相结合，用于标普500指数的波动率预测。SV模型提升了统计精度并捕捉了潜在的波动动态，特别是在应对突发事件时；LSTM网络则增强了模型检测金融时间序列中复杂非线性模式的能力。研究采用滚动窗口方法训练模型并生成一步超前波动率预测，通过统计测试和投资模拟评估了混合SV-LSTM模型的性能。结果表明，该混合方法优于单独的SV和LSTM模型，为改进风险评估和战略投资规划提供了基础。",
    "fetch_date": "2025-12-26",
    "id": "20251226_36f39515"
  },
  {
    "title": "Deep Hedging with Reinforcement Learning: A Practical Framework for Option Risk Management",
    "url": "https://arxiv.org/pdf/2512.12420v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "We present a reinforcement-learning (RL) framework for dynamic hedging of equity index option exposures under realistic transaction costs and position limits. We hedge a normalized option-implied equity exposure (one unit of underlying delta, offset via SPY) by trading the underlying index ETF, using the option surface and macro variables only as state information and not as a direct pricing engine. Building on the \"deep hedging\" paradigm of Buehler et al. (2019), we design a leak-free environment, a cost-aware reward function, and a lightweight stochastic actor-critic agent trained on daily end-of-day panel data constructed from SPX/SPY implied volatility term structure, skew, realized volatility, and macro rate context. On a fixed train/validation/test split, the learned policy improves risk-adjusted performance versus no-hedge, momentum, and volatility-targeting baselines (higher point-estimate Sharpe); only the GAE policy's test-sample Sharpe is statistically distinguishable from zero, although confidence intervals overlap with a long-SPY benchmark so we stop short of claiming formal dominance. Turnover remains controlled and the policy is robust to doubled transaction costs. The modular codebase, comprising a data pipeline, simulator, and training scripts, is engineered for extensibility to multi-asset overlays, alternative objectives (e.g., drawdown or CVaR), and intraday data. From a portfolio management perspective, the learned overlay is designed to sit on top of an existing SPX or SPY allocation, improving the portfolio's mean-variance trade-off with controlled turnover and drawdowns. We discuss practical implications for portfolio overlays and outline avenues for future work.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于强化学习的深度对冲框架，用于在现实交易成本和头寸限制下动态管理股指期权风险敞口。该方法采用无泄漏环境设计、成本感知的奖励函数和轻量级随机演员-评论家智能体，使用SPX/SPY隐含波动率期限结构、偏度、已实现波动率和宏观利率等日频面板数据进行训练。在固定训练/验证/测试集划分下，学习到的策略相比无对冲、动量策略和波动率目标基准展现出更高的风险调整后绩效（夏普比率点估计值更高），且交易周转率受控，策略对加倍交易成本具有稳健性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_dfaa45fb"
  },
  {
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "url": "https://arxiv.org/pdf/2512.14744v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《VERAFI：通过神经符号策略生成实现可验证的代理金融智能》针对金融AI系统在推理中产生计算错误和违反监管规定（即使检索完美）的盲点，提出了一个结合神经符号策略生成的代理框架。VERAFI融合了最先进的密集检索与交叉编码器重排序、支持金融工具的代理，以及覆盖GAAP合规性、SEC要求和数学验证的自动化推理策略。在FinanceBench上的综合评估显示显著改进：传统密集检索加重排序仅实现52.4%的事实正确率，而VERAFI的综合方法达到94.7%，相对提升81%。神经符号策略层本身比纯代理处理贡献了4.3个百分点的增益，专门针对持续的数学和逻辑错误。通过将金融领域专业知识直接整合到推理过程中，VERAFI为在实战交易中实现可信赖的金融智能提供了一条实用路径。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b47b6951"
  },
  {
    "title": "Transfer Learning (Il)liquidity",
    "url": "https://arxiv.org/pdf/2512.11731v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "The estimation of the Risk Neutral Density (RND) implicit in option prices is challenging, especially in illiquid markets. We introduce the Deep Log-Sum-Exp Neural Network, an architecture that leverages Deep and Transfer learning to address RND estimation in the presence of irregular and illiquid strikes. We prove key statistical properties of the model and the consistency of the estimator. We illustrate the benefits of transfer learning to improve the estimation of the RND in severe illiquidity conditions through Monte Carlo simulations, and we test it empirically on SPX data, comparing it with popular estimation methods. Overall, our framework shows recovery of the RND in conditions of extreme illiquidity with as few as three option quotes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《迁移学习（非）流动性》提出了一种用于估计期权价格中隐含风险中性密度（RND）的深度对数求和指数神经网络架构，特别针对非流动性和不规则行权价市场。该模型利用深度学习和迁移学习技术，在极端流动性不足条件下（仅需三个期权报价）仍能有效恢复RND，并通过蒙特卡洛模拟和SPX数据实证验证了其优于传统方法的性能。",
    "fetch_date": "2025-12-26",
    "id": "20251226_f3dbe600"
  },
  {
    "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
    "url": "https://arxiv.org/pdf/2512.17945v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融机构在部署机器学习模型进行信用风险评估时，面临预测准确性与可解释性之间的权衡。单调性约束使模型行为与领域知识保持一致，但其性能成本——即“单调性的代价”——尚未得到充分量化。本文在五个公共数据集和三个库上，对信用违约概率的单调约束与无约束梯度提升模型进行了基准测试。我们将单调性代价定义为从无约束模型转向约束模型时标准性能指标的相对变化，通过配对比较和自助法不确定性进行估计。实验结果表明，AUC的单调性代价范围从基本为零到约2.9%：在大型数据集上约束几乎无成本（通常低于0.2%，常与零无异），而在约束覆盖广泛的小型数据集上成本最高（约2-3%）。因此，适当指定的单调性约束通常能以较小的准确性损失提供可解释性，特别是在大规模信用组合中。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b992ef93"
  },
  {
    "title": "The Impact of Bitcoin ETF Approval on Bitcoin's Hedging Properties Against Traditional Assets",
    "url": "https://arxiv.org/pdf/2512.12815v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "The approval of the Bitcoin Spot ETF in January 2024 marked a transformative event in cryptocurrency markets, signaling increased institutional adoption and integration into traditional finance. This study examines Bitcoin's changing relationships with traditional assets, including equities, gold, and fiat currencies, following this milestone. Using rolling correlation analysis, Chow tests, and DCC-GARCH models, we found that Bitcoin's correlation with the S\\&P 500 increased significantly post-ETF approval, indicating stronger alignment with equities. Its relationship with gold stabilized near zero, while its correlation with the U.S. Dollar Index remained consistently negative, reflecting its continued independence from fiat currencies. These findings offer insights into Bitcoin's evolving role in portfolios, implications for market stability, and future research opportunities on cryptocurrency integration into traditional financial systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "2024年1月比特币现货ETF获批标志着加密货币市场进入机构化与主流金融整合的新阶段。本研究通过滚动相关性分析、邹检验和DCC-GARCH模型，实证分析了该事件后比特币与传统资产（股票、黄金、法币）的动态关系。核心发现：比特币与标普500指数的相关性显著增强，表明其与股票市场联动性提升；与黄金的相关性稳定在零值附近；与美元指数的负相关性持续，保持对法币的独立性。这些结果为投资组合配置、市场稳定性评估及加密货币与传统金融体系融合研究提供了实证依据。",
    "fetch_date": "2025-12-26",
    "id": "20251226_aa031b92"
  },
  {
    "title": "Institutionalizing risk curation in decentralized credit",
    "url": "https://arxiv.org/pdf/2512.11976v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper maps the emerging market for decentralized credit in which ERC 4626 vaults and third-party curators, rather than monolithic lending protocols alone, increasingly determine underwriting and leverage decisions. We show that modular vaults differ in capital utilization, cross-chain and cross asset concentration, and liquidity risk structure. Further, we show that a small set of curators intermediates a disproportionate share of system TVL, exhibits clustered tail co movement, and captures markedly different fee margins despite broadly similar collateral composition. These findings indicate that the main locus of risk in DeFi lending has migrated upward from base protocols, where underwriting is effectively centralized in a single DAO governed parameter set, to a permissionless curator layer in which competing vault managers decide which assets and loans are originated. We argue that this shift requires a corresponding upgrade in transparency standards and outline a simple set of onchain disclosures that would allow users and DAOs to evaluate curator strategies on a comparable, money market style basis.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了去中心化信贷市场中ERC 4626金库和第三方策展人（而非单一借贷协议）日益主导承销和杠杆决策的新兴格局。研究发现模块化金库在资本利用率、跨链跨资产集中度及流动性风险结构方面存在差异，少数策展人中介了不成比例的系统总锁定价值（TVL），表现出聚集性尾部联动，且尽管抵押品构成相似却获得显著不同的费用边际。这表明DeFi借贷的主要风险点已从基础协议（承销权集中于单一DAO治理参数集）上移至无需许可的策展层，由竞争性金库管理者决定资产和贷款的发起。作者主张需相应提升透明度标准，并提出一套简单的链上披露方案，使用户和DAO能以可比的货币市场风格评估策展策略。",
    "fetch_date": "2025-12-26",
    "id": "20251226_470ff790"
  },
  {
    "title": "Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling",
    "url": "https://arxiv.org/pdf/2512.12526v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究对MSCI世界指数应用经验模态分解（EMD），将得到的本征模态函数（IMFs）转化为图表示，以便用图神经网络（GNNs）建模。使用CEEMDAN提取了九个IMF，涵盖高频波动到长期趋势。每个IMF通过四种时间序列到图的方法（自然可见性、水平可见性、递归图和转移图）转化为图。拓扑分析显示明显的尺度依赖结构：高频IMF产生密集、高度连接的小世界图，而低频IMF产生更稀疏、特征路径长度更长的网络。基于可见性的方法对振幅变化更敏感，通常产生更高的聚类，而递归图更好地保留了时间依赖性。这些结果为设计针对分解成分结构特性的GNN架构提供了指导，支持更有效的金融时间序列预测建模。",
    "fetch_date": "2025-12-26",
    "id": "20251226_17fa87b6"
  },
  {
    "title": "Explainable Prediction of Economic Time Series Using IMFs and Neural Networks",
    "url": "https://arxiv.org/pdf/2512.12499v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "This study investigates the contribution of Intrinsic Mode Functions (IMFs) derived from economic time series to the predictive performance of neural network models, specifically Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. To enhance interpretability, DeepSHAP is applied, which estimates the marginal contribution of each IMF while keeping the rest of the series intact. Results show that the last IMFs, representing long-term trends, are generally the most influential according to DeepSHAP, whereas high-frequency IMFs contribute less and may even introduce noise, as evidenced by improved metrics upon their removal. Differences between MLP and LSTM highlight the effect of model architecture on feature relevance distribution, with LSTM allocating importance more evenly across IMFs.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了从经济时间序列中提取的本征模态函数（IMFs）对神经网络模型（特别是多层感知机MLP和长短期记忆网络LSTM）预测性能的贡献。为增强可解释性，应用DeepSHAP方法评估每个IMF的边际贡献，同时保持序列其余部分不变。结果表明，根据DeepSHAP分析，代表长期趋势的最后几个IMF通常最具影响力，而高频IMF贡献较小甚至可能引入噪声——移除这些高频分量后模型指标得到改善。MLP与LSTM之间的差异凸显了模型架构对特征相关性分布的影响，其中LSTM在IMF间的权重分配更为均匀。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b8bacb63"
  },
  {
    "title": "Unified Approach to Portfolio Optimization using the `Gain Probability Density Function' and Applications",
    "url": "https://arxiv.org/pdf/2512.11649v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This article proposes a unified framework for portfolio optimization (PO), recognizing an object called the `gain probability density function (PDF)' as the fundamental object of the problem from which any objective function could be derived. The gain PDF has the advantage of being 1-dimensional for any given portfolio and thus is easy to visualize and interpret. The framework allows us to naturally incorporate all existing approaches (Markowitz, CVaR-deviation, higher moments...) and represents an interesting basis to develop new approaches. It leads us to propose a method to directly match a target PDF defined by the portfolio manager, giving them maximal control on the PO problem and moving beyond approaches that focus only on expected return and risk. As an example, we develop an application involving a new objective function to control high profits, to be applied after a conventional PO (including expected return and risk criteria) and thus leading to sub-optimality w.r.t. the conventional objective function. We then propose a methodology to quantify a cost associated with this optimality deviation in a common budget unit, providing a meaningful information to portfolio managers. Numerical experiments considering portfolios with energy-producing assets illustrate our approach. The framework is flexible and can be applied to other sectors (financial assets, etc).",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种基于“收益概率密度函数（PDF）”的统一投资组合优化框架，该框架将收益PDF视为问题的基本对象，任何目标函数均可从中推导。该PDF具有一维特性，易于可视化和解释，能够自然整合现有方法（如马科维茨、CVaR-偏差、高阶矩等），并为开发新方法提供了基础。文章提出了一种直接匹配投资组合经理定义的目标PDF的方法，使其能超越仅关注预期收益和风险的传统方法，对优化问题实现最大控制。作为示例，文章开发了一种在传统优化后控制高利润的新目标函数应用，这会导致相对于传统目标函数的次优性，并提出了一种以通用预算单位量化这种最优性偏差成本的方法，为投资组合经理提供有意义的信息。",
    "fetch_date": "2025-12-26",
    "id": "20251226_32dbca85"
  },
  {
    "title": "Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall",
    "url": "https://arxiv.org/pdf/2512.12334v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文扩展了动态贝叶斯网络（DBN）在计算市场风险中的应用，用于估计10天97.5%的预期损失（ES）和压力预期损失（SES）。研究以标普500指数为代理，比较了三种DBN结构学习算法与多种传统市场风险模型（使用正态或偏斜t分布）的表现。回测显示，所有模型在2.5%水平下均未能产生统计上准确的ES和SES预测，反映了建模极端尾部行为的困难。对于ES，EGARCH(1,1)模型（正态）预测最准确；对于SES，GARCH(1,1)模型（正态）表现最佳。所有依赖分布的模型在使用偏斜t分布时性能显著下降。DBN的表现与历史模拟法相当。",
    "fetch_date": "2025-12-26",
    "id": "20251226_5b6423f9"
  },
  {
    "title": "Universal Dynamics of Financial Bubbles in Isolated Markets: Evidence from the Iranian Stock Market",
    "url": "https://arxiv.org/pdf/2512.12054v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Speculative bubbles exhibit common statistical signatures across many financial markets, suggesting the presence of universal underlying mechanisms. We test this hypothesis in the Iranian stock market, an economy that is highly isolated, subject to capital controls, and largely inaccessible to foreign investors. Using the Log-Periodic Power Law Singularity (LPPLS) model, we analyze two major bubble episodes in 2020 and 2023. The estimated critical exponents beta around 0.46 and 0.20 fall within the empirical ranges documented for canonical historical bubbles such as the 1929 DJIA crash and the 2000 Nasdaq episode. The Tehran Stock Exchange displays clear LPPLS hallmarks, including faster-than-exponential price acceleration, log-periodic corrections, and stable estimates of the critical time horizon. These results indicate that endogenous herding, imitation, and positive-feedback dynamics, rather than exogenous shocks, play a dominant role even in politically and economically isolated markets. By showing that an emerging and semi-closed financial system conforms to the same dynamical patterns observed in global markets, this paper provides new empirical support for the universality of bubble dynamics. To the best of our knowledge, it also presents the first systematic LPPLS analysis of bubbles in the Tehran Stock Exchange. The findings highlight the usefulness of LPPLS-based diagnostic tools for monitoring systemic risk in emerging or restricted economies.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文通过应用对数周期幂律奇异性（LPPLS）模型，分析了伊朗股市（一个高度孤立、受资本管制且外国投资者难以进入的市场）在2020年和2023年的两次主要泡沫事件。研究发现，其估算的关键指数β值（约0.46和0.20）与历史上经典泡沫（如1929年道琼斯工业平均指数崩盘和2000年纳斯达克泡沫）的经验范围一致。德黑兰证券交易所显示出清晰的LPPLS特征，包括快于指数的价格加速、对数周期性修正以及关键时间范围的稳定估计。结果表明，即使是在政治和经济上孤立的市场中，内生的羊群效应、模仿行为和正反馈动态，而非外生冲击，也起着主导作用。通过展示一个新兴的半封闭金融体系遵循与全球市场相同的动态模式，该论文为泡沫动力学的普适性提供了新的实证支持。",
    "fetch_date": "2025-12-26",
    "id": "20251226_16dc055e"
  },
  {
    "title": "High-Frequency Analysis of a Trading Game with Transient Price Impact",
    "url": "https://arxiv.org/pdf/2512.11765v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study the high-frequency limit of an $n$-trader optimal execution game in discrete time. Traders face transient price impact of Obizhaeva--Wang type in addition to quadratic instantaneous trading costs $θ(ΔX_t)^2$ on each transaction $ΔX_t$. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval $[0,T]$, the discrete equilibrium inventories converge at rate $1/N$ to the continuous-time equilibrium of an Obizhaeva--Wang model with additional quadratic costs $\\vartheta_0(ΔX_0)^2$ and $\\vartheta_T(ΔX_T)^2$ on initial and terminal block trades, where $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case $n=2$ where $\\vartheta_0=\\vartheta_T=1/2$. In particular, we show how the coefficients $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$ arise endogenously in the high-frequency limit: the initial and terminal block costs of the continuous-time model are identified as the limits of the cumulative discrete instantaneous costs incurred over small neighborhoods of $0$ and $T$, respectively, and these limits are independent of $θ>0$. By contrast, when $θ=0$ the discrete-time equilibrium strategies and costs exhibit persistent oscillations and admit no high-frequency limit, mirroring the non-existence of continuous-time equilibria without boundary block costs. Our results show that two different types of trading frictions -- a fine time discretization and small instantaneous costs in continuous time -- have similar regularizing effects and select a canonical model in the limit.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了具有瞬态价格影响的n交易者最优执行博弈的高频极限。交易者面临Obizhaeva-Wang型瞬态价格影响以及每笔交易ΔX_t的二次瞬时交易成本θ(ΔX_t)^2。存在唯一的纳什均衡，交易者选择最小化预期执行成本的清算策略。在高频极限下，离散均衡库存以1/N的速率收敛于具有额外二次成本ϑ_0(ΔX_0)^2和ϑ_T(ΔX_T)^2的Obizhaeva-Wang模型的连续时间均衡，其中ϑ_0=(n-1)/2，ϑ_T=1/2。该结果扩展并改进了先前针对n=2情况的研究。",
    "fetch_date": "2025-12-26",
    "id": "20251226_54f3bc58"
  },
  {
    "title": "Risk Limited Asset Allocation with a Budget Threshold Utility Function and Leptokurtotic Distributions of Returns",
    "url": "https://arxiv.org/pdf/2512.11666v2",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "An analytical solution to single-horizon asset allocation for an investor with a piecewise-linear utility function, called herein the \"budget threshold utility,\" and exogenous position limits is presented. The resulting functional form has a surprisingly simple structure and can be readily interpreted as representing the addition of a simple \"risk cost\" to otherwise frictionless trading.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对具有预算阈值效用函数和外部头寸限制的投资者，提出了单期资产配置的解析解。该解具有简洁的结构，可解释为在无摩擦交易基础上增加了简单的“风险成本”。",
    "fetch_date": "2025-12-26",
    "id": "20251226_2daddc47"
  },
  {
    "title": "Sources and Nonlinearity of High Volume Return Premium: An Empirical Study on the Differential Effects of Investor Identity versus Trading Intensity (2020-2024)",
    "url": "https://arxiv.org/pdf/2512.14134v2",
    "source": "ArXiv",
    "date": "2025-12-16",
    "abstract": "Chae and Kang (2019, \\textit{Pacific-Basin Finance Journal}) documented a puzzling Low Volume Return Premium (LVRP) in Korea -- contradicting global High Volume Return Premium (HVRP) evidence. We resolve this puzzle. Using Korean market data (2020-2024), we demonstrate that HVRP exists in Korea but is masked by (1) pooling heterogeneous investor types and (2) using inappropriate intensity normalization. When institutional buying intensity is normalized by market capitalization rather than trading value, a perfect monotonic relationship emerges: highest-conviction institutional buying (Q4) generates +\\institutionLedQFourDayPlusFiftyCAR\\ cumulative abnormal returns over 50 days, while lowest-intensity trades (Q1) yield modest returns (+\\institutionLedQOneDayPlusFiftyCAR). Retail investors exhibit a flat pattern -- their trading generates near-zero returns regardless of conviction level -- confirming the pure noise trader hypothesis. During the Donghak Ant Movement (2020-2021), however, coordinated retail investors temporarily transformed from noise traders to liquidity providers, generating returns comparable to institutional trading. Our findings reconcile conflicting international evidence and demonstrate that detecting informed trading signals requires investor-type decomposition, nonlinear quartile analysis, and conviction-based (market cap) rather than participation-based (trading value) measurement.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该研究（2020-2024）通过分析韩国市场数据，解决了韩国市场低成交量回报溢价（LVRP）与全球高成交量回报溢价（HVRP）证据相矛盾的谜题。研究发现，当按市值（而非交易额）对机构买入强度进行标准化时，HVRP在韩国市场显现：机构最高确信度买入（Q4）在50天内产生显著累积异常收益（+\\institutionLedQFourDayPlusFiftyCAR），而最低强度交易（Q1）收益较低（+\\institutionLedQOneDayPlusFiftyCAR）。散户交易则呈现平坦模式，收益接近零，符合纯噪声交易者假说；但在东学蚂蚁运动（2020-2021）期间，协调行动的散户暂时转变为流动性提供者，产生与机构相当的收益。研究通过区分投资者身份（机构vs散户）和交易强度标准化方法，调和了国际上的矛盾发现，对实战交易中识别机构驱动信号、优化因子构建具有直接价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_63fdd5c4"
  },
  {
    "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
    "url": "https://arxiv.org/pdf/2512.12924v1",
    "source": "ArXiv",
    "date": "2025-12-15",
    "abstract": "We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种可解释的假设驱动交易框架，采用严格的向前滚动验证方法，旨在减少过拟合和前瞻性偏差。该框架结合了可解释的假设驱动信号生成与强化学习，并执行严格的样本外测试。方法包括：严格执行信息集纪律、在34个独立测试期进行滚动窗口验证、通过自然语言假设解释保持完全可解释性、纳入实际交易成本和头寸约束。在2015-2024年间对100只美国股票验证五种市场微观结构模式，系统产生适中的年化收益（0.55%，夏普比率0.33），具有出色的下行保护（最大回撤-2.76%）和市场中性特征（beta=0.058）。表现呈现强烈的制度依赖性，在高波动期（2020-2024年，季度收益0.60%）产生正收益，而在稳定市场（2015-2019年，-0.16%）表现不佳。报告统计上不显著的总体结果（p值0.34），以展示一个可重复、诚实的验证协议，优先考虑可解释性。",
    "fetch_date": "2025-12-25",
    "id": "20251225_8db06af9"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading: Challenges and opportunities",
    "url": "https://ieeexplore.ieee.org/abstract/document/9779600/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… rise of quantitative trading (QT), which refers to automatically generating trading signals with … for more than 70% and 40% of the trading volume in developed markets (eg, USA) and …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了深度强化学习在量化交易中的应用，量化交易指通过算法自动生成交易信号，在发达市场（如美国）已占据超过70%和40%的交易量。论文分析了深度强化学习在量化交易领域面临的挑战与机遇，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_e3153d4f"
  },
  {
    "title": "Trademaster: A holistic quantitative trading platform empowered by reinforcement learning",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b8f6f7f2ba4137124ac976286eacb611-Abstract-Datasets_and_Benchmarks.html",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… As shown in Figure 2, we formulate quantitative trading tasks as a Markov Decision Process (MDP) following a standard RL scenario, where an agent (investor) interacts with an …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "Trademaster是一个基于强化学习的综合性量化交易平台。该研究将量化交易任务建模为马尔可夫决策过程，遵循标准强化学习框架，其中智能体（投资者）与环境交互进行决策。",
    "fetch_date": "2025-12-25",
    "id": "20251225_fe85737d"
  },
  {
    "title": "FinRL: Deep reinforcement learning framework to automate trading in quantitative finance",
    "url": "https://dl.acm.org/doi/abs/10.1145/3490354.3494366",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this paper, we present the first open-source framework FinRL as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了首个开源框架FinRL，作为一个完整的流程，旨在帮助量化交易员克服陡峭的学习曲线。FinRL以简洁性为特点，利用深度强化学习自动化量化金融中的交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_f95bf03c"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading",
    "url": "https://ieeexplore.ieee.org/abstract/document/10626209/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this section, we explore the distinctive attributes of Quantitative Trading (QT) and elaborate on the rationale behind framing the entire QT process as a Partially Observable Markov …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了将深度强化学习应用于量化交易，将整个量化交易过程建模为部分可观测马尔可夫决策过程，具有实战价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_d085e88f"
  },
  {
    "title": "Reinforcement Learning Framework for Quantitative Trading",
    "url": "https://arxiv.org/abs/2411.07585",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… We investigate how a reinforcement learning agent can utilize financial indicators in specific market conditions and trends to enhance overall trading accuracy. By understanding the …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文研究强化学习智能体如何利用特定市场条件和趋势下的金融指标来提升整体交易准确性。通过理解市场动态，该框架旨在优化量化交易策略。",
    "fetch_date": "2025-12-25",
    "id": "20251225_c5574310"
  },
  {
    "title": "Deep reinforcement learning in quantitative algorithmic trading: A review",
    "url": "https://arxiv.org/abs/2106.00123",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… Deep Reinforcement Learning (DRL) agents proved to be to … reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度强化学习在量化算法交易中的应用综述：该论文聚焦于深度强化学习（DRL）在金融人工智能子领域——特别是自动化低频量化股票交易中的实际应用。研究表明，DRL智能体在该领域展现出潜力，通过结合强化学习与深度学习技术，探索在实战交易中生成阿尔法（Alpha）的策略，具有较高的实践参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_307e048c"
  },
  {
    "title": "Quantitative trading on stock market based on deep reinforcement learning",
    "url": "https://ieeexplore.ieee.org/abstract/document/8851831/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… intelligence, quantitative trading attracts … reinforcement learning in quantitative trading. A LSTM-based agent is proposed to learn the temporal pattern in data and automatically trades …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度强化学习的股票市场量化交易方法，采用LSTM智能体学习数据中的时序模式，实现自动化交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_92f14b99"
  },
  {
    "title": "Deep Learning in Quantitative Trading",
    "url": "https://www.cambridge.org/core/elements/deep-learning-in-quantitative-trading/C39DE06D255470F6232BC97E2E5474E7",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… for developing deep learning algorithms for quantitative trading. This … deep learning algorithms to various financial problems. One of the most fundamental tasks in quantitative trading …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨深度学习在量化交易中的应用，重点研究开发深度学习算法以解决金融问题，特别是量化交易中的核心任务。",
    "fetch_date": "2025-12-25",
    "id": "20251225_410d6f5e"
  },
  {
    "title": "Portfolio Optimization for Index Tracking with Constraints on Downside Risk and Carbon Footprint",
    "url": "https://arxiv.org/pdf/2512.21092v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "Historically, financial risk management has mostly addressed risk factors that arise from the financial environment. Climate risks present a novel and significant challenge for companies and financial markets. Investors aiming for avoidance of firms with high carbon footprints require suitable risk measures and portfolio management strategies. This paper presents the construction of decarbonized indices for tracking the S \\& P-500 index of the U.S. stock market, as well as the Indian index NIFTY-50, employing two distinct methodologies and study their performances. These decarbonized indices optimize the portfolio weights by minimizing the mean-VaR and mean-ES and seek to reduce the risk of significant financial losses while still pursuing decarbonization goals. Investors can thereby find a balance between financial performance and environmental responsibilities. Ensuring transparency in the development of these indices will encourage the excluded and under-weighted asset companies to lower their carbon footprints through appropriate action plans. For long-term passive investors, these indices may present a more favourable option than green stocks.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了两种构建去碳化指数的方法，用于跟踪美国标普500指数和印度NIFTY-50指数。通过最小化均值-风险价值（mean-VaR）和均值-期望损失（mean-ES）来优化投资组合权重，旨在降低重大财务损失风险的同时实现去碳化目标。投资者可借此在财务绩效与环境责任间取得平衡，这些指数为长期被动投资者提供了比绿色股票更优的选择，并可能激励高碳排企业降低碳足迹。",
    "fetch_date": "2025-12-25",
    "id": "20251225_9ee0c108"
  },
  {
    "title": "Reinforcement learning in quantitative trading: A survey",
    "url": "https://www.techrxiv.org/doi/full/10.36227/techrxiv.19303853",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… two concepts on quantitative trading evolved with time along with the emergence of RL. To this end, we devote one section to discuss the literature of quantitative trading with the tools of …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该论文是一篇关于强化学习在量化交易中应用的综述性研究，主要梳理了随着强化学习发展而演变的两个量化交易概念，并专门用一节讨论了使用相关工具的量化交易文献。作为综述，它提供了理论框架和文献梳理，但缺乏具体的实战策略、代码实现或可验证的Alpha生成方法，因此对直接实战交易的价值有限，更适合作为理论参考。",
    "fetch_date": "2025-12-25",
    "id": "20251225_c48f4542"
  },
  {
    "title": "Discrete-time asset price bubbles with short sales prohibitions under model uncertainty",
    "url": "https://arxiv.org/pdf/2512.21115v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "In this study, we investigate asset price bubbles in a discrete-time, discrete-state market under model uncertainty and short sales prohibitions. Building on a new fundamental theorem of asset pricing and a superhedging duality in this setting, we introduce a notion of bubble based on a novel definition of the fundamental price, and analyze their types and characterization. We show that two distinct types of bubbles arise, depending on the maturity structure of the asset. For assets with bounded maturity and no dividend payments, the $G$-supermartingale property of prices provides a necessary and sufficient condition for the existence of bubbles. In contrast, when maturity is unbounded, the infi-supermartingale property yields a necessary condition, while the $G$-supermartingale property remains sufficient. Moreover, there is no bubble under a strengthened no dominance condition. As applications, we examine price bubbles for several standard contingent claims. We show that put-call parity generally fails for fundamental prices, whereas it holds for market prices under no dominance assumption. Furthermore, we establish bounds for the fundamental and market prices of American call options in terms of the corresponding European call prices, adjusted by the associated bubble components.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究在离散时间、离散状态市场下，结合模型不确定性和卖空禁令，探讨资产价格泡沫。基于该设定下的新基本资产定价定理和超对冲对偶性，我们引入了一种基于新定义的基本价格的泡沫概念，并分析了其类型和特征。研究表明，根据资产的期限结构，会出现两种不同类型的泡沫。对于有界期限且无股息支付的资产，价格的G-上鞅性质为泡沫存在提供了必要且充分条件。相比之下，当期限无界时，infi-上鞅性质产生必要条件，而G-上鞅性质仍为充分条件。此外，在强化的无支配条件下不存在泡沫。作为应用，我们检验了几种标准或有债权的价格泡沫。研究表明，基本价格下的看跌-看涨平价通常不成立，而在无支配假设下市场价格则成立。此外，我们建立了美式看涨期权的基本价格和市场价格的界限。",
    "fetch_date": "2025-12-25",
    "id": "20251225_d9f8f8a4"
  },
  {
    "title": "Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal",
    "url": "https://arxiv.org/pdf/2512.20850v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对限价订单簿中的做市商，研究了结合随机控制和脉冲控制的最优做市问题。该问题被表述为Hamilton-Jacobi-Bellman拟变分不等式（HJBQVI）。作者提出了一种隐式时间离散化方案，并结合策略迭代算法。该方法消除了显式方法典型的时间步长限制，确保了无条件稳定性。通过验证单调性、稳定性和一致性条件，并应用比较原理，建立了收敛于唯一黏性解的理论基础。",
    "fetch_date": "2025-12-25",
    "id": "20251225_aea7403d"
  },
  {
    "title": "Fixed-Income Pricing and the Replication of Liabilities",
    "url": "https://arxiv.org/pdf/2512.14662v2",
    "source": "ArXiv",
    "date": "2025-12-16",
    "abstract": "This paper develops a model-free framework for static fixed-income pricing and the replication of liability cash flows. We show that the absence of static arbitrage across a universe of fixed-income instruments is equivalent to the existence of a strictly positive discount curve that reproduces all observed market prices. We then study the replication and super-replication of liabilities and establish conditions ensuring the existence of least-cost super-replicating portfolios, including a rigorous interpretation of swap--repo replication within this static framework. The results provide a unified foundation for discount-curve construction and liability-driven investment, with direct relevance for economic capital assessment and regulatory practice.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种无模型的静态固定收益定价框架及负债现金流复制方法。研究表明，固定收益工具间不存在静态套利等价于存在严格为正的贴现曲线能重现所有市场价格。进一步探讨了负债的复制与超复制，建立了确保最低成本超复制组合存在的条件，并对互换-回购复制进行了严谨解释。该结果为贴现曲线构建和负债驱动投资提供了统一理论基础，对经济资本评估和监管实践具有直接参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_63fda1dc"
  },
  {
    "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
    "url": "https://arxiv.org/pdf/2512.16251v2",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们提出了共识瓶颈资产定价模型（CB-APM），这是一种部分可解释的神经网络，通过捕捉分散的投资者信念如何通过共识形成过程压缩为资产价格，来复制卖方分析师推理过程。该模型通过建模这一“瓶颈”来汇总公司和宏观层面的信息，不仅预测美国股票的未来风险溢价，还在结构上以可解释的方式将信念聚合与预期回报联系起来。CB-APM改进了长期回报预测，在预测准确性和解释力方面均优于标准深度学习方法。全面的投资组合分析表明，CB-APM的样本外预测转化为具有经济意义的回报，具有单调的回报差异和跨正则化设置的稳定多空表现。实证上，CB-APM利用共识作为正则化器来增强长期可预测性，并产生基于共识的可解释组件，阐明信息如何在回报中定价。此外，回归和基于Gibbons-Ross-Shanken（GRS）的定价诊断揭示了所学内容。",
    "fetch_date": "2025-12-24",
    "id": "20251224_da05e7f2"
  },
  {
    "title": "Reinforcement learning for quantitative trading",
    "url": "https://dl.acm.org/doi/abs/10.1145/3582560",
    "source": "Scholar",
    "date": "2025-12-24",
    "abstract": "… we used are reinforcement learning, quantitative finance, algorithmic trading, portfolio … a brief overview of financial markets and quantitative trading. Then, we introduce the preliminaries …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了强化学习在量化交易中的应用，涵盖量化金融、算法交易和投资组合管理等领域。文章首先概述了金融市场和量化交易的基本概念，随后介绍了强化学习的理论基础及其在交易策略优化中的实际应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_3a476d40"
  },
  {
    "title": "Adaptive quantitative trading: An imitative deep reinforcement learning approach",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5587",
    "source": "Scholar",
    "date": "2025-12-24",
    "abstract": "… mathematical symbols then formally introduce the quantitative trading problem in detail. … always the goal of quantitative traders. This further proves that our trading agent can benefit from …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "《自适应量化交易：一种模仿深度强化学习方法》提出了一种基于深度强化学习的交易代理，能够从市场数据中学习并自适应调整交易策略。该方法通过模仿学习结合强化学习框架，旨在实现持续盈利这一量化交易的核心目标。论文表明该交易代理能够从历史数据中受益并优化交易决策，具有较高的实战应用潜力。",
    "fetch_date": "2025-12-24",
    "id": "20251224_766e09bf"
  },
  {
    "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
    "url": "https://arxiv.org/pdf/2512.18317v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\\,\\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种可信赖的强化学习方法，用于工业压缩空气系统的控制。该框架能够在实际边界条件下实现安全、节能的运行，并引入了结合输入扰动测试、基于梯度的敏感性分析和SHAP（Shapley Additive exPlanations）特征归因的多层次可解释性管道。对多种压缩机配置的实证评估表明，学习到的策略在物理上是合理的，能够预测未来需求，并始终尊重系统边界。与已安装的工业控制器相比，所提出的方法减少了不必要的过压，在不依赖显式物理模型的情况下实现了约4%的节能。结果进一步表明，系统压力和预测信息主导了策略决策，而压缩机级别的输入则起次要作用。总体而言，效率提升、预测行为和透明验证的结合支持了强化学习在工业能源系统中的可信赖部署。",
    "fetch_date": "2025-12-24",
    "id": "20251224_4eec8973"
  },
  {
    "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection",
    "url": "https://arxiv.org/pdf/2512.16411v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了经验相对熵的分布，推导了有限样本的集中不等式、渐近分布以及前渐近状态下的Berry-Esseen界，并提出了一种基于相对熵的变点检测方法。通过模拟和真实数据（包括股票指数波动率）验证了该方法相对于基于矩或信息准则的传统方法的实用性。",
    "fetch_date": "2025-12-24",
    "id": "20251224_c7720f56"
  },
  {
    "title": "Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities",
    "url": "https://arxiv.org/pdf/2512.16080v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在去中心化金融（DeFi）中，由于时间相关的复杂性，设计固定收益借贷自动做市商（AMM）极具挑战性。现有协议仅支持单一期限借贷。本文基于BondMM协议，论证其数学不变量足够优雅，可推广至任意期限。因此，本文提出改进设计BondMM-A，支持任何期限的借贷活动。通过将不同期限的固定收益工具整合到单一智能合约中，BondMM-A为用户和流动性提供者（LPs）提供更大的操作自由和资本效率。实验结果表明，BondMM-A在利率稳定性和金融稳健性方面表现优异。",
    "fetch_date": "2025-12-24",
    "id": "20251224_c04304c9"
  },
  {
    "title": "Multi-Objective Bayesian Optimization of Deep Reinforcement Learning for Environmental, Social, and Governance (ESG) Financial Portfolio Management",
    "url": "https://arxiv.org/pdf/2512.14992v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "DRL agents circumvent the issue of classic models in the sense that they do not make assumptions like the financial returns being normally distributed and are able to deal with any information like the ESG score if they are configured to gain a reward that makes an objective better. However, the performance of DRL agents has high variability and it is very sensible to the value of their hyperparameters. Bayesian optimization is a class of methods that are suited to the optimization of black-box functions, that is, functions whose analytical expression is unknown, are noisy and expensive to evaluate. The hyperparameter tuning problem of DRL algorithms perfectly suits this scenario. As training an agent just for one objective is a very expensive period, requiring millions of timesteps, instead of optimizing an objective being a mixture of a risk-performance metric and an ESG metric, we choose to separate the objective and solve the multi-objective scenario to obtain an optimal Pareto set of portfolios representing the best tradeoff between the Sharpe ratio and the ESG mean score of the portfolio and leaving to the investor the choice of the final portfolio. We conducted our experiments using environments encoded within the OpenAI Gym, adapted from the FinRL platform. The experiments are carried out in the Dow Jones Industrial Average (DJIA) and the NASDAQ markets in terms of the Sharpe ratio achieved by the agent and the mean ESG score of the portfolio. We compare the performance of the obtained Pareto sets in hypervolume terms illustrating how portfolios are the best trade-off between the Sharpe ratio and mean ESG score. Also, we show the usefulness of our proposed methodology by comparing the obtained hypervolume with one achieved by a Random Search methodology on the DRL hyperparameter space.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种用于环境、社会和治理（ESG）金融投资组合管理的多目标贝叶斯优化深度强化学习方法。深度强化学习（DRL）代理无需假设金融收益服从正态分布，并能处理ESG评分等信息，但其性能对超参数高度敏感。贝叶斯优化适用于优化黑盒函数，DRL的超参数调优正符合此场景。由于单目标训练成本高昂（需数百万时间步），作者将目标分离，通过多目标优化获得代表夏普比率与投资组合ESG平均评分之间最佳权衡的帕累托最优投资组合集，最终由投资者选择具体组合。",
    "fetch_date": "2025-12-24",
    "id": "20251224_e237fd82"
  },
  {
    "title": "Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning",
    "url": "https://arxiv.org/pdf/2512.17185v1",
    "source": "ArXiv",
    "date": "2025-12-19",
    "abstract": "Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.\n  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.\n  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "《系统性风险雷达：用于早期市场崩盘预警的多层图框架》提出了一种将金融市场建模为多层图以检测系统性脆弱性和崩盘状态转换早期迹象的框架。该研究评估了SRR在互联网泡沫、全球金融危机和COVID-19冲击三个重大危机中的表现，实验表明结构网络信息相比纯特征模型能提供更有用的早期预警信号。虽然该框架展示了图衍生特征在压力事件期间捕捉市场结构有意义变化的能力，但当前实现主要基于相关性分析，属于理论验证阶段。论文建议通过添加更多图层（行业/因子暴露、情绪）和更强大的时序架构（LSTM/GRU或Transformer编码器）来扩展SRR，这为实战交易系统开发提供了有价值的理论框架和方向指引。",
    "fetch_date": "2025-12-24",
    "id": "20251224_7b111b14"
  },
  {
    "title": "An Efficient Machine Learning Framework for Option Pricing via Fourier Transform",
    "url": "https://arxiv.org/pdf/2512.16115v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "The increasing need for rapid recalibration of option pricing models in dynamic markets places stringent computational demands on data generation and valuation algorithms. In this work, we propose a hybrid algorithmic framework that integrates the smooth offset algorithm (SOA) with supervised machine learning models for the fast pricing of multiple path-independent options under exponential Lévy dynamics. Building upon the SOA-generated dataset, we train neural networks, random forests, and gradient boosted decision trees to construct surrogate pricing operators. Extensive numerical experiments demonstrate that, once trained, these surrogates achieve order-of-magnitude acceleration over direct SOA evaluation. Importantly, the proposed framework overcomes key numerical limitations inherent to fast Fourier transform-based methods, including the consistency of input data and the instability in deep out-of-the-money option pricing.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "动态市场中期权定价模型快速重新校准的需求日益增长，这对数据生成和估值算法提出了严格的计算要求。本文提出了一种混合算法框架，将平滑偏移算法（SOA）与监督机器学习模型相结合，用于在指数Lévy动态下快速定价多种路径无关期权。基于SOA生成的数据集，我们训练神经网络、随机森林和梯度提升决策树来构建替代定价算子。大量数值实验表明，一旦训练完成，这些替代模型相比直接SOA评估实现了数量级的加速。重要的是，所提出的框架克服了基于快速傅里叶变换方法固有的关键数值限制，包括输入数据的一致性和深度虚值期权定价的不稳定性。",
    "fetch_date": "2025-12-24",
    "id": "20251224_21c43b5a"
  },
  {
    "title": "SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs",
    "url": "https://arxiv.org/pdf/2512.15088v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出SigMA（Signature Multi-head Attention）神经网络架构，将路径签名与多头自注意力机制结合，用于学习分数布朗运动驱动的随机微分方程参数。该方法针对具有粗糙动态和长程依赖的系统建模，如量化金融中的分数布朗运动、分数Ornstein-Uhlenbeck和粗糙Heston模型。研究聚焦于通过合成生成路径进行参数估计，探讨路径签名在深度学习架构中如何平衡估计精度与模型复杂度。",
    "fetch_date": "2025-12-24",
    "id": "20251224_a46aba2c"
  },
  {
    "title": "Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes",
    "url": "https://arxiv.org/pdf/2512.14991v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究针对具有无界连续状态空间、有界连续动作和多项式增长奖励的受控扩散过程的强化学习——这些设定自然出现在金融、经济和运筹学中。为克服连续高维领域的挑战，我们提出一种基于模型的算法，自适应地划分联合状态-动作空间。该算法在每个分区内维护漂移、波动率和奖励的估计器，并在估计偏差超过统计置信度时细化离散化。这种自适应方案平衡了探索与近似，使得在无界域中高效学习成为可能。我们的分析建立了遗憾界，其依赖于问题时域、状态维度、奖励增长阶数以及为无界扩散过程定制的新定义的“缩放维度”概念。这些界限将现有有界设定结果作为特例恢复，同时将理论保证扩展到更广泛的扩散类问题。最后，我们通过数值实验验证了方法的有效性，包括在高维问题中的应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_140ee511"
  },
  {
    "title": "Modelling financial time series with $φ^{4}$ quantum field theory",
    "url": "https://arxiv.org/pdf/2512.17225v1",
    "source": "ArXiv",
    "date": "2025-12-19",
    "abstract": "We use a $φ^{4}$ quantum field theory with inhomogeneous couplings and explicit symmetry-breaking to model an ensemble of financial time series from the S$\\&$P 500 index. The continuum nature of the $φ^4$ theory avoids the inaccuracies that occur in Ising-based models which require a discretization of the time series. We demonstrate this using the example of the 2008 global financial crisis. The $φ^{4}$ quantum field theory is expressive enough to reproduce the higher-order statistics such as the market kurtosis, which can serve as an indicator of possible market shocks. Accurate reproduction of high kurtosis is absent in binarized models. Therefore Ising models, despite being widely employed in econophysics, are incapable of fully representing empirical financial data, a limitation not present in the generalization of the $φ^{4}$ scalar field theory. We then investigate the scaling properties of the $φ^{4}$ machine learning algorithm and extract exponents which govern the behavior of the learned couplings (or weights and biases in ML language) in relation to the number of stocks in the model. Finally, we use our model to forecast the price changes of the AAPL, MSFT, and NVDA stocks. We conclude by discussing how the $φ^{4}$ scalar field theory could be used to build investment strategies and the possible intuitions that the QFT operations of dimensional compactification and renormalization can provide for financial modelling.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文使用具有非均匀耦合和显式对称性破缺的φ⁴量子场论来建模标普500指数中的金融时间序列集合。与需要离散化时间序列的伊辛模型相比，φ⁴理论的连续性质避免了不准确性，并能重现高阶统计量（如市场峰度），可作为市场冲击的指标。论文还研究了φ⁴机器学习算法的标度特性，并提取了控制学习耦合（或ML中的权重和偏置）与模型中股票数量关系的指数。最后，使用该模型预测了AAPL、MSFT和NVDA股票的价格变化。",
    "fetch_date": "2025-12-24",
    "id": "20251224_1cb49c57"
  },
  {
    "title": "Global universal approximation with Brownian signatures",
    "url": "https://arxiv.org/pdf/2512.16396v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在加权粗糙路径空间上建立了L^p型全局通用逼近定理，证明了对时间扩展粗糙路径的签名进行线性泛函作用在L^p距离意义下是稠密的。特别地，这些定理适用于布朗运动，因此布朗运动时间扩展签名的线性泛函可以逼近任何适应于布朗滤波的p可积随机过程，包括随机微分方程的解。",
    "fetch_date": "2025-12-24",
    "id": "20251224_94b3ca35"
  },
  {
    "title": "Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions",
    "url": "https://arxiv.org/pdf/2512.15113v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Long-term price forecasting remains a formidable challenge due to the inherent uncertainty over the long term, despite some success in short-term predictions. Nonetheless, accurate long-term forecasts are essential for high-net-worth individuals, institutional investors, and traders. The proposed improved genetic algorithm-optimized support vector regression (IGA-SVR) model is specifically designed for long-term price prediction of global indices. The performance of the IGA-SVR model is rigorously evaluated and compared against the state-of-the-art baseline models, the Long Short-Term Memory (LSTM), and the forward-validating genetic algorithm optimized support vector regression (OGA-SVR). Extensive testing was conducted on the five global indices, namely Nifty, Dow Jones Industrial Average (DJI), DAX Performance Index (DAX), Nikkei 225 (N225), and Shanghai Stock Exchange Composite Index (SSE) from 2021 to 2024 of daily price prediction up to a year. Overall, the proposed IGA-SVR model achieved a reduction in MAPE by 19.87% compared to LSTM and 50.03% compared to OGA-SVR, demonstrating its superior performance in long-term daily price forecasting of global indices. Further, the execution time for LSTM was approximately 20 times higher than that of IGA-SVR, highlighting the high accuracy and computational efficiency of the proposed model. The genetic algorithm selects the optimal hyperparameters of SVR by minimizing the arithmetic mean of the Mean Absolute Percentage Error (MAPE) calculated over the full training dataset and the most recent five years of training data. This purposefully designed training methodology adjusts for recent trends while retaining long-term trend information, thereby offering enhanced generalization compared to the LSTM and rolling-forward validation approach employed by OGA-SVR, which forgets long-term trends and suffers from recency bias.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种改进的遗传算法优化支持向量回归（IGA-SVR）模型，专门用于全球股票指数的长期价格预测。在2021年至2024年期间，对Nifty、道琼斯工业平均指数（DJI）、DAX绩效指数（DAX）、日经225指数（N225）和上证综合指数（SSE）等五个全球指数进行了长达一年的每日价格预测测试。结果表明，与LSTM和OGA-SVR相比，IGA-SVR模型在平均绝对百分比误差（MAPE）上分别降低了19.87%和50.03%，显示出其在长期预测中的优越性能。",
    "fetch_date": "2025-12-24",
    "id": "20251224_b3187dce"
  },
  {
    "title": "Arbitrage-Free Pricing with Diffusion-Dependent Jumps",
    "url": "https://arxiv.org/pdf/2512.15071v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Standard jump-diffusion models assume independence between jumps and diffusion components. We develop a multi-type jump-diffusion model where jump occurrence and magnitude depend on contemporaneous diffusion movements. Unlike previous one-sided models that create arbitrage opportunities, our framework includes upward and downward jumps triggered by both large upward and large downward diffusion increments. We derive the explicit no-arbitrage condition linking the physical drift to model parameters and market risk premia by constructing an Equivalent Martingale Measure using Girsanov's theorem and a normalized Esscher transform. This condition provides a rigorous foundation for arbitrage-free pricing in models with diffusion-dependent jumps.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "标准跳跃扩散模型假设跳跃与扩散成分相互独立。本文开发了一种多类型跳跃扩散模型，其中跳跃的发生和幅度取决于同期的扩散运动。与以往可能产生套利机会的单边模型不同，我们的框架包含由大幅向上和向下扩散增量触发的向上和向下跳跃。通过使用Girsanov定理和归一化Esscher变换构建等价鞅测度，我们推导出将物理漂移与模型参数及市场风险溢价联系起来的明确无套利条件。该条件为具有扩散依赖跳跃的模型中的无套利定价提供了严格基础。",
    "fetch_date": "2025-12-24",
    "id": "20251224_33dbfdbc"
  },
  {
    "title": "Optimal Signal Extraction from Order Flow: A Matched Filter Perspective on Normalization and Market Microstructure",
    "url": "https://arxiv.org/pdf/2512.18648v1",
    "source": "ArXiv",
    "date": "2025-12-21",
    "abstract": "We demonstrate that the choice of normalization for order flow intensity is fundamental to signal extraction in finance, not merely a technical detail. Through theoretical modeling, Monte Carlo simulation, and empirical validation using Korean market data, we prove that market capitalization normalization acts as a ``matched filter'' for informed trading signals, achieving 1.32--1.97$\\times$ higher correlation with future returns compared to traditional trading value normalization. The key insight is that informed traders scale positions by firm value (market capitalization), while noise traders respond to daily liquidity (trading volume), creating heteroskedastic corruption when normalizing by trading volume. By reframing the normalization problem using signal processing theory, we show that dividing order flow by market capitalization preserves the information signal while traditional volume normalization multiplies the signal by inverse turnover -- a highly volatile quantity. Our theoretical predictions are robust across parameter specifications and validated by empirical evidence showing 482\\% improvement in explanatory power. These findings have immediate implications for high-frequency trading algorithms, risk factor construction, and information-based trading strategies.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文论证了订单流强度归一化方法的选择对信号提取至关重要，而不仅仅是技术细节。通过理论建模、蒙特卡洛模拟和韩国市场数据的实证验证，证明市值归一化可作为知情交易信号的“匹配滤波器”，相比传统的交易额归一化，与未来收益的相关性提高1.32-1.97倍。核心观点是：知情交易者按公司价值（市值）调整头寸，而噪声交易者则响应日流动性（交易量），使用交易量归一化会导致异方差干扰。通过信号处理理论重新构建归一化问题，发现除以市值能保留信息信号，而传统的交易量归一化会将信号乘以逆换手率——一个高度波动的变量。理论预测在不同参数设定下均稳健，实证证据显示解释力提升482%。这些发现对高频交易、算法交易和风险模型具有直接应用价值。",
    "fetch_date": "2025-12-24",
    "id": "20251224_eb1783a9"
  },
  {
    "title": "Switching between states and the COVID-19 turbulence",
    "url": "https://arxiv.org/pdf/2512.20477v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "In Aarab (2020), I examine U.S. stock return predictability across economic regimes and document evidence of time-varying expected returns across market states in the long run. The analysis introduces a state-switching specification in which the market state is proxied by the slope of the yield curve, and proposes an Aligned Economic Index built from the popular predictors of Welch and Goyal (2008) (augmented with bond and equity premium measures). The Aligned Economic Index under the state-switching model exhibits statistically and economically meaningful in-sample ($R^2 = 5.9\\%$) and out-of-sample ($R^2_{\\text{oos}} = 4.12\\%$) predictive power across both recessions and expansions, while outperforming a range of widely used predictors. In this work, I examine the added value for professional practitioners by computing the economic gains for a mean-variance investor and find substantial added benefit of using the new index under the state switching model across all market states. The Aligned Economic Index can thus be implemented on a consistent real-time basis. These findings are crucial for both academics and practitioners as expansions are much longer-lived than recessions. Finally, I extend the empirical exercises by incorporating data through September 2020 and document sizable gains from using the Aligned Economic Index, relative to more traditional approaches, during the COVID-19 market turbulence.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过引入状态切换模型（以收益率曲线斜率作为市场状态代理变量）和构建对齐经济指数（基于Welch和Goyal（2008）的经典预测因子，并加入债券和股权溢价指标），实证显示该指数在样本内（R²=5.9%）和样本外（R²oos=4.12%）均对美股收益具有统计及经济意义的预测能力，且优于多种常用预测因子。研究进一步通过均值-方差投资者模型验证了该模型在所有市场状态下均能带来显著经济收益，并强调该指数可实时应用于实践。这些发现对从业者尤为重要，因为经济扩张期远长于衰退期。",
    "fetch_date": "2025-12-24",
    "id": "20251224_a39154aa"
  },
  {
    "title": "The Aligned Economic Index & The State Switching Model",
    "url": "https://arxiv.org/pdf/2512.20460v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \\parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \\textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文研究了美国股票收益在不同经济状态下的可预测性，提出了两个对实战交易有价值的贡献：一是引入基于收益率曲线斜率实时定义市场状态的状态切换预测回归模型，相比传统单状态模型显著提高了样本内和样本外预测性能；二是通过偏最小二乘法构建了新的聚合预测指标——对齐经济指数，在状态切换模型下展现出统计和经济意义上显著的样本内外预测能力。论文聚焦于经济状态依赖的股票溢价预测，其状态切换模型和对齐经济指数可直接应用于实战中的择时和风险管理策略。",
    "fetch_date": "2025-12-24",
    "id": "20251224_23ae3263"
  },
  {
    "title": "GIFfluence: A Visual Approach to Investor Sentiment and the Stock Market",
    "url": "https://arxiv.org/pdf/2512.20027v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "We study dynamic visual representations as a proxy for investor sentiment about the stock market. Our sentiment index, GIFsentiment, is constructed from millions of posts in the Graphics Interchange Format (GIF) on a leading investment social media platform. GIFsentiment correlates with seasonal mood variations and the severity of COVID lockdowns. It is positively associated with contemporaneous market returns and negatively predicts returns for up to four weeks, even after controlling for other sentiment and attention measures. These effects are stronger among portfolios that are more susceptible to mispricing. GIFsentiment positively predicts trading volume, market volatility, and flows toward equity funds and away from debt funds. Our evidence suggests that GIFsentiment is a proxy for misperceptions that are later corrected.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于投资社交媒体平台GIF帖子的投资者情绪指标GIFsentiment。研究发现：1）该指标与季节性情绪波动和疫情封锁严重程度相关；2）与同期市场收益正相关，并能负向预测未来四周收益（控制其他情绪指标后仍显著）；3）对易受错误定价影响的投资组合预测效果更强；4）能正向预测交易量、市场波动率以及资金从债券基金流向股票基金。证据表明GIFsentiment可作为市场错误认知的代理指标，这些认知后续会被修正。",
    "fetch_date": "2025-12-24",
    "id": "20251224_78339dde"
  },
  {
    "title": "Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2512.19986v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种用于基数约束投资组合优化的协方差感知单纯形投影方法。针对元启发式算法中修复算子将不可行候选解映射到可行区域时忽略协方差结构的问题，该方法采用两阶段修复：首先基于波动率归一化评分选择目标资产数量，然后使用与跟踪误差风险对齐的协方差感知几何进行权重投影。在S&P 500数据上的实证表明，该方法在不依赖收益预测的情况下显著降低了投资组合方差，且改进具有统计显著性。消融实验显示波动率归一化选择贡献了主要方差降低，而协方差感知投影提供了额外的稳定改进。",
    "fetch_date": "2025-12-24",
    "id": "20251224_432db6dc"
  },
  {
    "title": "Needles in a haystack: using forensic network science to uncover insider trading",
    "url": "https://arxiv.org/pdf/2512.18918v1",
    "source": "ArXiv",
    "date": "2025-12-21",
    "abstract": "Although the automation and digitisation of anti-financial crime investigation has made significant progress in recent years, detecting insider trading remains a unique challenge, partly due to the limited availability of labelled data. To address this challenge, we propose using a data-driven networks approach that flags groups of corporate insiders who report coordinated transactions that are indicative of insider trading. Specifically, we leverage data on 2.9 million trades reported to the U.S. Securities and Exchange Commission (SEC) by company insiders (C-suite executives, board members and major shareholders) between 2014 and 2024. Our proposed algorithm constructs weighted edges between insiders based on the temporal similarity of their trades over the 10-year timeframe. Within this network we then uncover trends that indicate insider trading by focusing on central nodes and anomalous subgraphs. To highlight the validity of our approach we evaluate our findings with reference to two null models, generated by running our algorithm on synthetic empirically calibrated and shuffled datasets. The results indicate that our approach can be used to detect pairs or clusters of insiders whose behaviour suggests insider trading and/or market manipulation.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种利用法证网络科学检测内幕交易的数据驱动网络方法。通过分析2014-2024年间美国证券交易委员会（SEC）收到的290万笔公司内部人（高管、董事会成员和大股东）交易报告，该算法基于交易时间相似性构建内部人之间的加权边网络，并通过识别中心节点和异常子图来揭示内幕交易模式。研究使用合成经验校准和随机数据集生成的零模型验证了方法的有效性，表明该方法能有效检测协同交易的内幕交易对或集群。",
    "fetch_date": "2025-12-24",
    "id": "20251224_944d270d"
  },
  {
    "title": "Quantitative Financial Modeling for Sri Lankan Markets: Approach Combining NLP, Clustering and Time-Series Forecasting",
    "url": "https://arxiv.org/pdf/2512.20216v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "This research introduces a novel quantitative methodology tailored for quantitative finance applications, enabling banks, stockbrokers, and investors to predict economic regimes and market signals in emerging markets, specifically Sri Lankan stock indices (S&P SL20 and ASPI) by integrating Environmental, Social, and Governance (ESG) sentiment analysis with macroeconomic indicators and advanced time-series forecasting. Designed to leverage quantitative techniques for enhanced risk assessment, portfolio optimization, and trading strategies in volatile environments, the architecture employs FinBERT, a transformer-based NLP model, to extract sentiment from ESG texts, followed by unsupervised clustering (UMAP/HDBSCAN) to identify 5 latent ESG regimes, validated via PCA. These regimes are mapped to economic conditions using a dense neural network and gradient boosting classifier, achieving 84.04% training and 82.0% validation accuracy. Concurrently, time-series models (SRNN, MLP, LSTM, GRU) forecast daily closing prices, with GRU attaining an R-squared of 0.801 and LSTM delivering 52.78% directional accuracy on intraday data. A strong correlation between S&P SL20 and S&P 500, observed through moving average and volatility trend plots, further bolsters forecasting precision. A rule-based fusion logic merges ESG and time-series outputs for final market signals. By addressing literature gaps that overlook emerging markets and holistic integration, this quant-driven framework combines global correlations and local sentiment analysis to offer scalable, accurate tools for quantitative finance professionals navigating complex markets like Sri Lanka.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该研究为斯里兰卡市场（S&P SL20和ASPI指数）提出了一种新颖的量化金融建模方法，通过整合ESG情感分析（使用FinBERT模型）、宏观经济指标和先进的时间序列预测技术（包括SRNN、MLP、LSTM、GRU），旨在预测经济状态和市场信号。该方法采用无监督聚类（UMAP/HDBSCAN）识别出5个潜在的ESG状态，并通过密集神经网络和梯度提升分类器将其映射到经济条件，训练和验证准确率分别达到84.04%和82.0%。在时间序列预测方面，GRU模型的R平方为0.801，LSTM在日内数据上的方向准确性为52.78%。研究还观察到S&P SL20与S&P 500之间存在强相关性。该框架旨在增强风险评估、投资组合优化和交易策略，适用于波动环境中的新兴市场。",
    "fetch_date": "2025-12-24",
    "id": "20251224_6cec85b9"
  },
  {
    "title": "Pricing of wrapped Bitcoin and Ethereum on-chain options",
    "url": "https://arxiv.org/pdf/2512.20190v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "This paper measures price differences between Hegic option quotes on Arbitrum and a model-based benchmark built on Black--Scholes model with regime-sensitive volatility estimated via a two-regime MS-AR-(GJR)-GARCH model. Using option-level feasible GLS, we find benchmark prices exceed Hegic quotes on average, especially for call options. The price spread rises with order size, strike, maturity, and estimated volatility, and falls with trading volume. By underlying, wrapped Bitcoin options show larger and more persistent spreads, while Ethereum options are closer to the benchmark. The framework offers a data-driven analysis for monitoring and calibrating on-chain option pricing logic.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过对比Hegic在Arbitrum上的期权报价与基于两区制MS-AR-(GJR)-GARCH模型估计波动率的Black-Scholes基准模型，发现基准价格普遍高于Hegic报价（尤其是看涨期权）。价差随订单规模、行权价、期限和估计波动率增加而扩大，随交易量增加而缩小。其中，包装比特币期权价差更大且更持久，以太坊期权则更接近基准。该框架为监控和校准链上期权定价逻辑提供了数据驱动的分析方法。",
    "fetch_date": "2025-12-24",
    "id": "20251224_6243b8b8"
  },
  {
    "title": "Equilibrium Liquidity and Risk Offsetting in Decentralised Markets",
    "url": "https://arxiv.org/pdf/2512.19838v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "We develop an economic model of decentralised exchanges (DEXs) in which risk-averse liquidity providers (LPs) manage risk in a centralised exchange (CEX) based on preferences, information, and trading costs. Rational, risk-averse LPs anticipate the frictions associated with replication and manage risk primarily by reducing the reserves supplied to the DEX. Greater aversion reduces the equilibrium viability of liquidity provision, resulting in thinner markets and lower trading volumes. Greater uninformed demand supports deeper liquidity, whereas higher fundamental price volatility erodes it. Finally, while moderate anticipated price changes can improve LP performance, larger changes require more intensive trading in the CEX, generate higher replication costs, and induce LPs to reduce liquidity supply.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个去中心化交易所（DEX）的经济模型，研究风险厌恶的流动性提供者（LPs）如何基于偏好、信息与交易成本，在中心化交易所（CEX）管理风险。理性且风险厌恶的LPs会预判与复制相关的摩擦，主要通过减少向DEX提供的储备来管理风险。更高的风险厌恶会降低流动性提供的均衡可行性，导致市场更薄、交易量更低。更大的非知情需求支持更深的流动性，而更高的基础价格波动则会削弱流动性。最后，虽然适度的预期价格变动可能改善LP表现，但更大的变动需要在CEX进行更密集的交易，产生更高的复制成本，并促使LPs减少流动性供给。",
    "fetch_date": "2025-12-24",
    "id": "20251224_27c66c3f"
  },
  {
    "title": "How to choose my stochastic volatility parameters? A review",
    "url": "https://arxiv.org/pdf/2512.19821v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Based on the existing literature, this article presents the different ways of choosing the parameters of stochastic volatility models in general, in the context of pricing financial derivative contracts. This includes the use of stochastic volatility inside stochastic local volatility models.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "基于现有文献，本文综述了在金融衍生品定价背景下，选择随机波动率模型参数的各种方法，包括在随机局部波动率模型中使用随机波动率。",
    "fetch_date": "2025-12-24",
    "id": "20251224_faddc8fe"
  },
  {
    "title": "Counterexamples for FX Options Interpolations -- Part II",
    "url": "https://arxiv.org/pdf/2512.19625v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "This follow-up article analyzes the impact of foreign exchange option interpolation on the vanilla option implied volatilities. In particular different exact interpolations of broker quotes may lead to different implied volatilities at the 10$Δ$ and 25$Δ$ Puts and Calls.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "这篇后续文章分析了外汇期权插值对普通期权隐含波动率的影响。具体而言，经纪商报价的不同精确插值方法可能导致10Δ和25Δ看跌期权与看涨期权的隐含波动率出现差异。",
    "fetch_date": "2025-12-24",
    "id": "20251224_bee73e28"
  },
  {
    "title": "Counterexamples for FX Options Interpolations -- Part I",
    "url": "https://arxiv.org/pdf/2512.19621v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "This article provides a list of counterexamples, where some of the popular fx option interpolations break down. Interpolation of FX option prices (or equivalently volatilities), is key to risk-manage not only vanilla FX option books, but also more exotic derivatives which are typically valued with local volatility or local stochastic volatilility models.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文列举了外汇期权插值方法失效的反例，指出插值对于风险管理至关重要，不仅涉及普通外汇期权，也影响使用局部波动率或局部随机波动率模型定价的奇异衍生品。",
    "fetch_date": "2025-12-24",
    "id": "20251224_43d5d092"
  },
  {
    "title": "Heston vol-of-vol and the VVIX",
    "url": "https://arxiv.org/pdf/2512.19611v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "The Heston stochastic volatility model is arguably, the most popular stochastic volatility model used to price and risk manage exotic derivatives. In spite of this, it is not necessarily easy to calibrate to the market and obtain stable exotic option prices with this model. This paper focuses on the vol-of-vol parameter and its relation with the volatility of volatility index (VVIX) level. Four different approaches to estimate the VVIX in the Heston model are presented: two based on the known transition density of the variance, one analytical approximation, and one based on the Heston PDE which computes the value directly out of the underlying SPX500. Finally we explore their use to improve calibration stability.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文聚焦于Heston随机波动率模型中的波动率波动率参数及其与波动率波动率指数（VVIX）水平的关系。提出了四种在Heston模型中估计VVIX的方法：两种基于已知的方差转移密度，一种解析近似方法，以及一种基于Heston偏微分方程直接从标普500指数计算的方法。最后探讨了这些方法在提高模型校准稳定性方面的应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_dad4d8e3"
  },
  {
    "title": "Learning General Policies with Policy Gradient Methods",
    "url": "https://arxiv.org/pdf/2512.19366v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了强化学习方法（特别是策略梯度方法）在何种条件下能学习到具有泛化能力的策略，类似于组合方法在经典规划中所实现的。作者结合了组合方法与深度学习的思路，将策略建模为状态转移分类器（因为具体动作会随问题实例变化），并使用图神经网络（GNNs）来处理关系结构以表示规划状态的价值函数和策略。研究旨在弥合强化学习与可证明泛化的组合方法之间的差距，但重点在于理论框架与条件分析，而非直接应用于实战交易场景。",
    "fetch_date": "2025-12-24",
    "id": "20251224_5412cd38"
  },
  {
    "title": "Institutional Backing and Crypto Volatility: A Hybrid Framework for DeFi Stabilization",
    "url": "https://arxiv.org/pdf/2512.19251v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Decentralized finance (DeFi) lacks centralized oversight, often resulting in heightened volatility. In contrast, centralized finance (CeFi) offers a more stable environment with institutional safeguards. Institutional backing can play a stabilizing role in a hybrid structure (HyFi), enhancing transparency, governance, and market discipline. This study investigates whether HyFi-like cryptocurrencies, those backed by institutions, exhibit lower price risk than fully decentralized counterparts. Using daily data for 18 major cryptocurrencies from January 2020 to November 2024, we estimate panel EGLS models with fixed, random, and dynamic specifications. Results show that HyFi-like assets consistently experience lower price risk, with this effect intensifying during periods of elevated market volatility. The negative interaction between HyFi status and market-wide volatility confirms their stabilizing role. Conversely, greater decentralization is strongly associated with increased volatility, particularly during periods of market stress. Robustness checks using quantile regressions and pre-/post-Terra Luna subsamples reinforce these findings, with stronger effects observed in high-volatility quantiles and post-crisis conditions. These results highlight the importance of institutional architecture in enhancing the resilience of digital asset markets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了去中心化金融（DeFi）因缺乏中心化监管而波动性较高，而中心化金融（CeFi）通过机构保障提供更稳定环境。研究发现，在混合结构（HyFi）中，机构支持能通过增强透明度、治理和市场纪律发挥稳定作用。基于2020年1月至2024年11月18种主要加密货币的日度数据，面板EGLS模型显示，具有机构支持的HyFi类资产价格风险较低，且在市场波动加剧时期稳定作用更明显。相反，去中心化程度越高，波动性越强，尤其在市场压力时期。分位数回归及Terra Luna事件前后子样本的稳健性检验支持了这些结论。",
    "fetch_date": "2025-12-24",
    "id": "20251224_36795ecf"
  }
]