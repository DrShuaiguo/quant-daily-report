[
  {
    "title": "MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks",
    "url": "https://arxiv.org/pdf/2601.17773v1",
    "source": "ArXiv",
    "date": "2026-01-25",
    "abstract": "This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文提出MarketGAN，一种基于因子的生成框架，用于在数据严重稀缺的情况下生成高维资产收益。该框架将显式的资产定价因子结构作为经济归纳偏置嵌入，并以单个联合向量的形式生成收益，从而在保留截面依赖性和尾部共动性的同时，捕捉跨期动态。MarketGAN采用以时间卷积网络（TCN）为骨干的生成对抗学习，建模随机的、时变的因子载荷和波动率，并捕获长程时间依赖性。基于美国大型股票的日收益数据，研究发现，与传统基于因子模型的bootstrap方法相比，MarketGAN生成的收益更贴近资产收益的经验化事实，包括厚尾边际分布、波动率聚类、杠杆效应，以及尤为显著的高维截面相关结构和跨资产尾部共动。在投资组合应用中，当因子信息至少具有弱信息性时，基于MarketGAN生成样本得到的协方差估计优于其他方法，展示了切实的经济效益。",
    "fetch_date": "2026-01-28",
    "id": "20260128_2f7ca671"
  },
  {
    "title": "Learning Market Making with Closing Auctions",
    "url": "https://arxiv.org/pdf/2601.17247v1",
    "source": "ArXiv",
    "date": "2026-01-24",
    "abstract": "In this work, we investigate the market-making problem on a trading session in which a continuous phase on a limit order book is followed by a closing auction. Whereas standard optimal market-making models typically rely on terminal inventory penalties to manage end-of-day risk, ignoring the significant liquidity events available in closing auctions, we propose a Deep Q-Learning framework that explicitly incorporates this mechanism. We introduce a market-making framework designed to explicitly anticipate the closing auction, continuously refining the projected clearing price as the trading session evolves. We develop a generative stochastic market model to simulate the trading session and to emulate the market. Our theoretical model and Deep Q-Learning method is applied on the generator in two settings: (1) when the mid price follows a rough Heston model with generative data from this stochastic model; and (2) when the mid price corresponds to historical data of assets from the S&P 500 index and the performance of our algorithm is compared with classical benchmarks from optimal market making.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文提出了一种结合收盘竞价机制的做市商深度学习框架，通过深度Q学习在包含连续交易阶段和收盘竞价阶段的交易会话中进行做市决策。该方法利用生成随机市场模型模拟交易会话，并在两种设置下验证：一是基于粗糙Heston模型生成中间价数据；二是基于标普500指数成分股历史数据，并与经典最优做市基准进行比较。该研究对实战交易具有较高价值，因为它直接针对收盘竞价这一重要流动性事件进行优化，而非依赖传统的终端库存惩罚机制。",
    "fetch_date": "2026-01-28",
    "id": "20260128_42f61a11"
  },
  {
    "title": "Optimal strategy and deep hedging for share repurchase programs",
    "url": "https://arxiv.org/pdf/2601.18686v1",
    "source": "ArXiv",
    "date": "2026-01-26",
    "abstract": "In recent decades, companies have frequently adopted share repurchase programs to return capital to shareholders or for other strategic purposes, instructing investment banks to rapidly buy back shares on their behalf. When the executing institution is allowed to hedge its exposure, it encounters several challenges due to the intrinsic features of the product. Moreover, contractual clauses or market regulations on trading activity may make it infeasible to rely on Greeks. In this work, we address the hedging of these products by developing a machine-learning framework that determines the optimal execution of the buyback while explicitly accounting for the bank's actual trading capabilities. This unified treatment of execution and hedging yields substantial performance improvements, resulting in an optimized policy that provides a feasible and realistic hedging approach. The pricing of these programs can be framed in terms of the discount that banks offer to the client on the price at which the shares are delivered. Since, in our framework, risk measures serve as objective functions, we exploit the concept of indifference pricing to compute this discount, thereby capturing the actual execution performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "近年来，企业频繁采用股票回购计划向股东返还资本或实现其他战略目的，委托投资银行快速代其回购股票。当执行机构被允许对冲其风险敞口时，由于产品的内在特性，会面临若干挑战。此外，合同条款或市场交易活动监管可能使得依赖希腊字母（Greeks）变得不可行。本研究通过开发一个机器学习框架来解决这些产品的对冲问题，该框架在明确考虑银行实际交易能力的同时，确定最优的回购执行策略。这种执行与对冲的统一处理方法带来了显著的性能提升，产生了一种优化的策略，提供了一种可行且现实的对冲方法。这些计划的定价可以依据银行在股票交付价格上为客户提供的折扣来构建。在我们的框架中，风险度量作为目标函数，我们利用无差异定价（indifference pricing）的概念来计算这一折扣，从而捕捉实际风险成本。",
    "fetch_date": "2026-01-28",
    "id": "20260128_8f4d3f12"
  },
  {
    "title": "The Compounded BSDE method: A fully-forward method for option pricing and optimal stopping problems in finance",
    "url": "https://arxiv.org/pdf/2601.18634v1",
    "source": "ArXiv",
    "date": "2026-01-26",
    "abstract": "We propose the Compound BSDE method, a fully forward, deep-learning-based approach for solving a broad class of problems in financial mathematics, including optimal stopping. The method is based on a reformulation of option pricing problems in terms of a system of backward stochastic differential equations (BSDEs), which offers a new perspective on the numerical treatment of compound options and optimal stopping problems such as Bermudan option pricing. Building on the classical deep BSDE method for a single BSDE, we develop an algorithm for compound BSDEs and establish its convergence properties. In particular, we derive an \\emph{a posteriori} error estimate for the proposed method. Numerical experiments demonstrate the accuracy and computational efficiency of the approach, and illustrate its effectiveness for high-dimensional option pricing and optimal stopping problems.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为'复合BSDE方法'的完全前向、基于深度学习的算法，用于解决金融数学中的一类广泛问题，包括最优停止问题（如百慕大期权定价）。该方法基于将期权定价问题重新表述为反向随机微分方程（BSDEs）系统，为复合期权和最优停止问题的数值处理提供了新视角。在经典深度BSDE方法的基础上，开发了复合BSDEs的算法并建立了其收敛性，推导了后验误差估计。数值实验验证了该方法在高维期权定价和最优停止问题中的准确性和计算效率。",
    "fetch_date": "2026-01-28",
    "id": "20260128_57089a4d"
  },
  {
    "title": "Regret-Driven Portfolios: LLM-Guided Smart Clustering for Optimal Allocation",
    "url": "https://arxiv.org/pdf/2601.17021v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "We attempt to mitigate the persistent tradeoff between risk and return in medium- to long-term portfolio management. This paper proposes a novel LLM-guided no-regret portfolio allocation framework that integrates online learning dynamics, market sentiment indicators, and large language model (LLM)-based hedging to construct high-Sharpe ratio portfolios tailored for risk-averse investors and institutional fund managers. Our approach builds on a follow-the-leader approach, enriched with sentiment-based trade filtering and LLM-driven downside protection. Empirical results demonstrate that our method outperforms a SPY buy-and-hold baseline by 69% in annualized returns and 119% in Sharpe ratio.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种新颖的LLM引导的无悔投资组合分配框架，旨在缓解中长期投资管理中风险与回报之间的持续权衡。该方法整合了在线学习动态、市场情绪指标和基于大语言模型（LLM）的对冲策略，以构建高夏普比率投资组合，适用于风险厌恶型投资者和机构基金经理。实证结果显示，该方法在年化回报率上比SPY买入持有基准高出69%，夏普比率高出119%。",
    "fetch_date": "2026-01-28",
    "id": "20260128_ab81d040"
  },
  {
    "title": "Bayesian Robust Financial Trading with Adversarial Synthetic Market Data",
    "url": "https://arxiv.org/pdf/2601.17008v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种贝叶斯鲁棒金融交易框架，旨在解决算法交易中机器学习模型在实际市场环境下的性能退化问题。核心创新包括：1）数据层面，提出基于宏观经济指标控制的生成对抗网络（GAN）来合成具有真实时间序列、跨资产和宏观相关性的多样化市场数据；2）策略层面，将交易过程建模为双人零和贝叶斯马尔可夫博弈，以学习对抗市场波动的鲁棒交易策略。该方法直接针对实战交易中的模型过拟合和市场机制变化挑战，具有较高的应用潜力。",
    "fetch_date": "2026-01-28",
    "id": "20260128_d4d1f96f"
  },
  {
    "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
    "url": "https://arxiv.org/pdf/2601.08641v2",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market's extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《在模因币跟单交易中抵抗操纵性机器人：一种基于思维链推理的多智能体方法》针对模因币市场跟单交易中普遍存在的机器人操纵问题（如抢先交易、隐藏仓位、伪造情绪），提出了一种基于多模态可解释大语言模型的多智能体防御框架。该系统将跟单交易分解为币种评估、钱包选择和时机评估三个专门智能体，在超过6000种模因币的历史数据测试中，其预测准确性和经济表现均优于零样本及多数统计基线，在真实市场摩擦下，智能资金交易平均回报达14%，跟单者每笔交易估计回报为3%。研究证明了基于智能体的防御策略在实战交易中的有效性。",
    "fetch_date": "2026-01-28",
    "id": "20260128_0e8f0ccc"
  },
  {
    "title": "Convolutional Attention in Betting Exchange Markets",
    "url": "https://arxiv.org/pdf/2510.16008v1",
    "source": "ArXiv",
    "date": "2025-10-14",
    "abstract": "This study presents the implementation of a short-term forecasting system for price movements in exchange markets, using market depth data and a systematic procedure to enable a fully automated trading system. The case study focuses on the UK to Win Horse Racing market during the pre-live stage on the world's leading betting exchange, Betfair. Innovative convolutional attention mechanisms are introduced and applied to multiple recurrent neural networks and bi-dimensional convolutional recurrent neural network layers. Additionally, a novel padding method for convolutional layers is proposed, specifically designed for multivariate time series processing. These innovations are thoroughly detailed, along with their execution process. The proposed architectures follow a standard supervised learning approach, involving model training and subsequent testing on new data, which requires extensive pre-processing and data analysis. The study also presents a complete end-to-end framework for automated feature engineering and market interactions using the developed models in production. The key finding of this research is that all proposed innovations positively impact the performance metrics of the classification task under examination, thereby advancing the current state-of-the-art in convolutional attention mechanisms and padding methods applied to multivariate time series problems.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究提出了一种针对交易所市场价格变动的短期预测系统，利用市场深度数据和系统化流程实现全自动交易系统。案例研究聚焦于全球领先的投注交易所Betfair上英国赛马市场在赛前阶段的“Win”市场。研究引入了创新的卷积注意力机制，并将其应用于多个循环神经网络和二维卷积循环神经网络层。此外，还提出了一种专门为多变量时间序列处理设计的卷积层新型填充方法。这些创新及其执行过程均得到详细阐述。所提出的架构遵循标准的监督学习方法，包括模型训练及在新数据上的测试，这需要大量的预处理和数据分析。研究还展示了一个完整的端到端框架，用于在生产环境中使用所开发模型进行自动化特征工程和市场交互。本研究的关键发现是，所有提出的创新均对性能指标产生积极影响。",
    "fetch_date": "2026-01-28",
    "id": "20260128_0c59dfaa"
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning Under Constrained Communications",
    "url": "https://arxiv.org/pdf/2601.17069v1",
    "source": "ArXiv",
    "date": "2026-01-22",
    "abstract": "Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "集中训练分散执行（CTDE）是多智能体强化学习（MARL）的主流范式，但其依赖训练时的全局状态信息，存在可扩展性、鲁棒性和泛化性瓶颈。在实际场景中，如队友增减或环境动态变化时，CTDE方法脆弱且重训练成本高，而分布式方法允许智能体仅使用本地信息和点对点通信进行适应。本文提出一种无需集中式评论家或全局信息的分布式MARL框架。首先，开发了新颖的分布式图注意力网络（D-GAT），通过多跳通信进行全局状态推断，智能体以完全分布式方式通过输入依赖的注意力权重整合邻居特征。基于D-GAT，开发了分布式图注意力MAPPO（DG-MAPPO）——一种分布式MARL框架，智能体使用本地观察、多跳通信和共享/平均奖励优化本地策略和价值函数。在星际争霸等环境中的实证评估表明其有效性。",
    "fetch_date": "2026-01-28",
    "id": "20260128_0803a3c0"
  },
  {
    "title": "Beyond Returns: A Candlestick-Based Approach to Spot Covariance Estimation",
    "url": "https://arxiv.org/pdf/2510.12911v1",
    "source": "ArXiv",
    "date": "2025-10-14",
    "abstract": "Spot covariance estimation is commonly based on high-frequency open-to-close return data over short time windows, but such approaches face a trade-off between statistical accuracy and localization. In this paper, I introduce a new estimation framework using high-frequency candlestick data, which include open, high, low, and close prices, effectively addressing this trade-off. By exploiting the information contained in candlesticks, the proposed method improves estimation accuracy relative to benchmarks while preserving local structure. I further develop a test for spot covariance inference based on candlesticks that demonstrates reasonable size control and a notable increase in power, particularly in small samples. Motivated by recent work in the finance literature, I empirically test the market neutrality of the iShares Bitcoin Trust ETF (IBIT) using 1-minute candlestick data for the full year of 2024. The results show systematic deviations from market neutrality, especially in periods of market stress. An event study around FOMC announcements further illustrates the new method's ability to detect subtle shifts in response to relatively mild information events.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于高频K线（包含开盘、最高、最低、收盘价）的现货协方差估计新框架，旨在解决传统基于高频开盘至收盘收益率数据方法在统计精度与局部化之间的权衡问题。该方法利用K线包含的额外信息，在保持局部结构的同时提高了估计精度，并开发了基于K线的统计推断检验，在小样本中表现出良好的尺寸控制与更高的检验功效。作为实证应用，作者使用2024年全年1分钟K线数据检验了iShares比特币信托ETF（IBIT）的市场中性，发现其在市场压力时期存在系统性偏离；围绕FOMC公告的事件研究进一步展示了该方法能检测到对相对温和信息事件的细微响应变化。",
    "fetch_date": "2026-01-28",
    "id": "20260128_5fa9ede8"
  },
  {
    "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2601.18626v1",
    "source": "ArXiv",
    "date": "2026-01-26",
    "abstract": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "自然梯度在深度强化学习中因快速收敛和协变权重更新而备受关注，但其计算需在每次迭代中求逆Fisher信息矩阵（FIM），计算成本高昂。本文提出一种高效可扩展的自然策略优化技术，利用秩-1近似替代完整逆FIM。理论证明，在特定条件下，该近似比策略梯度收敛更快，且在某些情况下与随机策略梯度方法具有相同的样本复杂度。实验表明，该方法在多样环境中优于标准演员-评论家和信任区域基线。",
    "fetch_date": "2026-01-28",
    "id": "20260128_6e1786c6"
  },
  {
    "title": "The Sherman-Morrison-Markowitz Portfolio",
    "url": "https://arxiv.org/pdf/2601.18124v1",
    "source": "ArXiv",
    "date": "2026-01-26",
    "abstract": "We show that the Markowitz portfolio is a scalar multiple of another portfolio which replaces the covariance with the second moment matrix, via simple application of the Sherman-Morrison identity. Moreover it is shown that when using conditional estimates of the first two moments, this \"Sherman-Morrison-Markowitz\" portfolio solves the standard unconditional portfolio optimization problems. We argue that in multi-period portfolio optimization problems it is more natural to replace variance and covariance with their uncentered counterparts. We extend the theory to deal with constraints in expectation, where we find a decomposition of squared effects into spanned and orthogonal components. Compared to the Markowitz portfolio, the Sherman-Morrison-Markowitz portfolio downlevers by a small amount that depends on the conditional squared maximal Sharpe ratio; the practical impact will be fairly small, however. We present some example use cases for the theory.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种名为“Sherman-Morrison-Markowitz”的投资组合，通过应用Sherman-Morrison恒等式，将Markowitz投资组合中的协方差矩阵替换为二阶矩矩阵，并证明在条件估计下该组合可解决标准无条件优化问题。作者认为在多期优化中使用未中心化的矩更自然，并扩展了带约束的理论。相比Markowitz组合，该组合会小幅降杠杆（取决于条件最大夏普比的平方），但实际影响较小。论文主要贡献在于理论推导和数学框架扩展，而非提供可直接部署的实战策略或实证交易信号。",
    "fetch_date": "2026-01-28",
    "id": "20260128_af3d6b61"
  },
  {
    "title": "VIX and European options with jumps in the short-maturity regime",
    "url": "https://arxiv.org/pdf/2601.17248v1",
    "source": "ArXiv",
    "date": "2026-01-24",
    "abstract": "We present a study of the short-maturity asymptotics for VIX and European option prices in local-stochastic volatility models with compound Poisson jumps. Both out-of-the-money (OTM) and at-the-money (ATM) asymptotics are considered. The leading-order asymptotics are obtained in closed-form. We apply our results to three examples: the Eraker model, a Kou-type model, and a folded normal model. Numerical illustrations are provided for these three examples that show the accuracy of predictions based on the asymptotic results.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文研究了在具有复合泊松跳跃的局部随机波动率模型中，VIX和欧式期权价格的短期到期渐近行为，涵盖了价外和价内两种情况，并给出了封闭形式的渐近解。通过Eraker模型、Kou型模型和折叠正态模型三个示例进行了数值验证。",
    "fetch_date": "2026-01-28",
    "id": "20260128_c8d0fd50"
  },
  {
    "title": "SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2510.13262v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for cooperative and competitive tasks such as autonomous driving and strategic gaming. However, models trained by MADRL are vulnerable to adversarial perturbations on states and actions. Therefore, it is essential to investigate the robustness of MADRL models from an attack perspective. Existing studies focus on either state-only attacks or action-only attacks, but do not consider how to effectively joint them. Simply combining state and action perturbations such as randomly perturbing states and actions does not exploit their potential synergistic effects. In this paper, we propose the State-Action Joint Attack (SAJA) framework that has a good synergistic effects. SAJA consists of two important phases: (1) In the state attack phase, a multi-step gradient ascent method utilizes both the actor network and the critic network to compute an adversarial state, and (2) in the action attack phase, based on the perturbed state, a second gradient ascent uses the critic network to craft the final adversarial action. Additionally, a heuristic regularizer measuring the distance between the perturbed actions and the original clean ones is added into the loss function to enhance the effectiveness of the critic's guidance. We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating that (1) it outperforms and is more stealthy than state-only or action-only attacks, and (2) existing state or action defense methods cannot defend its attacks.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种针对多智能体深度强化学习（MADRL）的状态-动作联合攻击框架SAJA，通过梯度上升方法协同扰动状态和动作，并引入启发式正则化器。该研究主要从攻击角度探讨MADRL模型的鲁棒性，属于对抗性攻击的理论方法研究，对实战交易的直接应用价值有限。",
    "fetch_date": "2026-01-28",
    "id": "20260128_16175cb0"
  },
  {
    "title": "Institutional Differences, Crisis Shocks, and Volatility Structure: A By-Window EGARCH/TGARCH Analysis of ASEAN Stock Markets",
    "url": "https://arxiv.org/pdf/2510.16010v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "This study examines how institutional differences and external crises shape volatility dynamics in emerging Asian stock markets. Using daily stock index returns for Indonesia, Malaysia, and the Philippines from 2010 to 2024, we estimate EGARCH(1,1) and TGARCH(1,1) models in a by-window design. The sample is split into the 2013 Taper Tantrum, the 2020-2021 COVID-19 period, the 2022-2023 rate-hike cycle, and tranquil phases. Prior work typically studies a single market or a static period; to our knowledge no study unifies institutional comparison with multi-crisis dynamics within one GARCH framework. We address this gap and show that all three markets display strong volatility persistence and fat-tailed returns. During crises both persistence and asymmetry increase, while tail thickness rises, implying more frequent extreme moves. After crises, parameters revert toward pre-shock levels. Cross-country evidence indicates a buffering role of institutional maturity: Malaysias stronger regulatory and information systems dampen amplification and speed recovery, whereas the Philippines thinner market structure prolongs instability. We conclude that crises amplify volatility structures, while institutional robustness governs recovery speed. The results provide policy guidance on transparency, macroprudential communication, and liquidity support to reduce volatility persistence during global shocks.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究通过分窗口EGARCH/TGARCH模型分析东盟新兴股市（印尼、马来西亚、菲律宾）在2010-2024年间的波动率动态，考察制度差异与外部危机（2013年缩减恐慌、2020-2021年新冠疫情、2022-2023年加息周期）的影响。研究发现：所有市场均呈现强波动率持续性（volatility persistence）和厚尾收益（fat-tailed returns）；危机期间持续性、非对称性（asymmetry）和尾部厚度（tail thickness）均增加，极端波动更频繁；危机后参数向冲击前水平回归。跨国比较表明制度成熟度具有缓冲作用：马来西亚更强的监管与信息系统能抑制波动放大并加速恢复，而菲律宾较薄的市场结构（thin market structure）会延长不稳定期。结论指出危机会放大波动结构，而制度质量影响市场韧性。",
    "fetch_date": "2026-01-28",
    "id": "20260128_b3e42f8e"
  },
  {
    "title": "Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation",
    "url": "https://arxiv.org/pdf/2601.16677v1",
    "source": "ArXiv",
    "date": "2026-01-23",
    "abstract": "The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in real environments without additional tuning, is particularly desirable for its efficiency and practical value. This work proposes a novel domain adaptation approach relying on a Style-Identified Cycle Consistent Generative Adversarial Network (StyleID-CycleGAN or SICGAN), an original Cycle Consistent Generative Adversarial Network (CycleGAN) based model. SICGAN translates raw virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with real-like visual inputs. Following virtual training, the agent can be directly deployed, bypassing the need for real-world training. The pipeline is validated with two distinct industrial robots in the approaching phase of a pick-and-place operation. In virtual environments agents achieve success rates of 90 to 100\\%, and real-world deployment confirms robust zero-shot transfer (i.e., without additional training in the physical environment) with accuracies above 95\\% for most workspace regions. We use augmented reality targets to improve the evaluation process efficiency, and experimentally demonstrate that the agent successfully generalizes to real objects of varying colors and shapes, including LEGO\\textsuperscript{\\textregistered}~cubes and a mug. These results establish the proposed pipeline as an efficient, scalable solution to the sim-to-real problem.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种基于风格识别循环一致生成对抗网络（StyleID-CycleGAN）的域适应方法，用于解决深度强化学习（DRL）中模拟到现实（sim-to-real）的转移难题。通过将虚拟观察转换为真实合成图像，创建结合虚拟动态与真实视觉输入的混合域，使DRL智能体在虚拟环境中训练后能够实现零次部署（zero-shot transfer），无需在真实环境中进行额外调优。该方法在工业机器人拾放操作的接近阶段进行了验证，具有显著的实战价值。",
    "fetch_date": "2026-01-27",
    "id": "20260127_8e2e90bf"
  },
  {
    "title": "Brownian ReLU(Br-ReLU): A New Activation Function for a Long-Short Term Memory (LSTM) Network",
    "url": "https://arxiv.org/pdf/2601.16446v1",
    "source": "ArXiv",
    "date": "2026-01-23",
    "abstract": "Deep learning models are effective for sequential data modeling, yet commonly used activation functions such as ReLU, LeakyReLU, and PReLU often exhibit gradient instability when applied to noisy, non-stationary financial time series. This study introduces BrownianReLU, a stochastic activation function induced by Brownian motion that enhances gradient propagation and learning stability in Long Short-Term Memory (LSTM) networks. Using Monte Carlo simulation, BrownianReLU provides a smooth, adaptive response for negative inputs, mitigating the dying ReLU problem. The proposed activation is evaluated on financial time series from Apple, GCB, and the S&P 500, as well as LendingClub loan data for classification. Results show consistently lower Mean Squared Error and higher $R^2$ values, indicating improved predictive accuracy and generalization. Although ROC-AUC metric is limited in classification tasks, activation choice significantly affects the trade-off between accuracy and sensitivity, with Brownian ReLU and the selected activation functions yielding practically meaningful performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "深度学习模型在序列数据建模中表现优异，但ReLU、LeakyReLU和PReLU等常用激活函数在处理噪声大、非平稳的金融时间序列时，常出现梯度不稳定问题。本研究提出BrownianReLU，一种由布朗运动驱动的随机激活函数，旨在增强长短期记忆网络中的梯度传播和学习稳定性。通过蒙特卡洛模拟，BrownianReLU为负输入提供了平滑、自适应的响应，缓解了“死亡ReLU”问题。该激活函数在苹果、GCB、标普500的金融时间序列以及LendingClub贷款分类数据上进行了评估。结果显示，其均方误差持续降低，R²值提高，表明预测准确性和泛化能力得到改善。尽管分类任务中ROC-AUC指标有限，但激活函数的选择显著影响了准确性与敏感性之间的权衡，BrownianReLU及所选激活函数在实际应用中表现出有意义的性能。",
    "fetch_date": "2026-01-27",
    "id": "20260127_dc02ddc6"
  },
  {
    "title": "A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization",
    "url": "https://arxiv.org/pdf/2601.13435v1",
    "source": "ArXiv",
    "date": "2026-01-19",
    "abstract": "Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \\emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \\pm 0.045$ and a Sharpe ratio of $2.157 \\pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文提出了一种名为WaveLSFormer的可学习小波变换长短期交易模型，旨在解决金融时间序列中的噪声、非平稳性和资产间强横截面依赖等挑战。该模型通过可学习小波前端生成低频/高频分量，结合低引导高频注入模块融合多尺度信息，并输出经风险预算调整的多空头寸组合，直接优化交易目标和风险感知正则化。在五年小时数据上的实验表明，该模型在多个行业组中持续优于MLP、LSTM和Transformer基线模型。",
    "fetch_date": "2026-01-27",
    "id": "20260127_81dbccdc"
  },
  {
    "title": "On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model",
    "url": "https://arxiv.org/pdf/2510.14156v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "Quantitative trading strategies rely on accurately ranking stocks to identify profitable investments. Effective portfolio management requires models that can reliably order future stock returns. Transformer models are promising for understanding financial time series, but how different training loss functions affect their ability to rank stocks well is not yet fully understood. Financial markets are challenging due to their changing nature and complex relationships between stocks. Standard loss functions, which aim for simple prediction accuracy, often aren't enough. They don't directly teach models to learn the correct order of stock returns. While many advanced ranking losses exist from fields such as information retrieval, there hasn't been a thorough comparison to see how well they work for ranking financial returns, especially when used with modern Transformer models for stock selection. This paper addresses this gap by systematically evaluating a diverse set of advanced loss functions including pointwise, pairwise, listwise for daily stock return forecasting to facilitate rank-based portfolio selection on S&P 500 data. We focus on assessing how each loss function influences the model's ability to discern profitable relative orderings among assets. Our research contributes a comprehensive benchmark revealing how different loss functions impact a model's ability to learn cross-sectional and temporal patterns crucial for portfolio selection, thereby offering practical guidance for optimizing ranking-based trading strategies.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《关于评估股票排序损失函数：基于Transformer模型的实证分析》针对量化交易策略中股票排序这一核心问题，系统评估了多种先进的损失函数（点式、成对式、列表式）在Transformer模型中的应用效果。研究聚焦于如何通过不同损失函数提升模型对股票未来收益的排序能力，以支持基于排序的投资组合选择，对实战交易中模型优化与策略构建具有直接指导价值。",
    "fetch_date": "2026-01-27",
    "id": "20260127_9ba90577"
  },
  {
    "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents",
    "url": "https://arxiv.org/pdf/2510.13704v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "Recent works have proposed accelerating the wall-clock training time of actor-critic methods via the use of large-scale environment parallelization; unfortunately, these can sometimes still require large number of environment interactions to achieve a desired level of performance. Noting that well-structured representations can improve the generalization and sample efficiency of deep reinforcement learning (RL) agents, we propose the use of simplicial embeddings: lightweight representation layers that constrain embeddings to simplicial structures. This geometric inductive bias results in sparse and discrete features that stabilize critic bootstrapping and strengthen policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial embeddings consistently improve sample efficiency and final performance across a variety of continuous- and discrete-control environments, without any loss in runtime speed.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "近期研究通过大规模环境并行化加速了actor-critic方法的训练时间，但这些方法有时仍需要大量环境交互才能达到预期性能水平。注意到结构良好的表示可以提升深度强化学习（RL）智能体的泛化能力和样本效率，我们提出使用单纯形嵌入：轻量级的表示层，将嵌入约束在单纯形结构中。这种几何归纳偏置产生了稀疏且离散的特征，稳定了critic的自举过程并增强了策略梯度。当应用于FastTD3、FastSAC和PPO时，单纯形嵌入在各种连续和离散控制环境中持续提升了样本效率和最终性能，且未损失运行速度。",
    "fetch_date": "2026-01-27",
    "id": "20260127_18b545ae"
  },
  {
    "title": "Deep Learning-Based Financial Time Series Forecasting and Quantitative Trading Strategy Optimization",
    "url": "https://dl.acm.org/doi/abs/10.1145/3785706.3785719",
    "source": "Scholar",
    "date": "2026-01-27",
    "abstract": "… integrating deep learning (DL) models for time series forecasting and quantitative trading strategy … Second, we design a rule-based quantitative trading strategy based on the DL model’s …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度学习（DL）模型的金融时间序列预测方法，并将其与基于规则的量化交易策略相结合进行优化。研究重点在于将深度学习技术应用于实际交易场景，通过模型预测指导策略执行，旨在提升量化交易策略的性能和适应性。",
    "fetch_date": "2026-01-27",
    "id": "20260127_c9626cba"
  },
  {
    "title": "Toward Black Scholes for Prediction Markets: A Unified Kernel and Market Maker's Handbook",
    "url": "https://arxiv.org/pdf/2510.15205v1",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "Prediction markets, such as Polymarket, aggregate dispersed information into tradable probabilities, but they still lack a unifying stochastic kernel comparable to the one options gained from Black-Scholes. As these markets scale with institutional participation, exchange integrations, and higher volumes around elections and macro prints, market makers face belief volatility, jump, and cross-event risks without standardized tools for quoting or hedging. We propose such a foundation: a logit jump-diffusion with risk-neutral drift that treats the traded probability p_t as a Q-martingale and exposes belief volatility, jump intensity, and dependence as quotable risk factors. On top, we build a calibration pipeline that filters microstructure noise, separates diffusion from jumps using expectation-maximization, enforces the risk-neutral drift, and yields a stable belief-volatility surface. We then define a coherent derivative layer (variance, correlation, corridor, and first-passage instruments) analogous to volatility and correlation products in option markets. In controlled experiments on synthetic risk-neutral paths and real event data, the model reduces short-horizon belief-variance forecast error relative to diffusion-only and probability-space baselines, supporting both causal calibration and economic interpretability. Conceptually, the logit jump-diffusion kernel supplies an implied-volatility analogue for prediction markets: a tractable, tradable language for quoting, hedging, and transferring belief risk across venues such as Polymarket.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文针对预测市场（如Polymarket）提出了一种统一的理论框架，旨在为做市商提供标准化的报价和对冲工具，以应对信念波动、跳跃和跨事件风险。其核心贡献包括：1）提出一个将交易概率p_t视为Q-鞅的对数跳跃扩散模型，以量化信念波动率、跳跃强度和相关性等风险因子；2）开发一个校准流程，用于过滤微观结构噪声、分离扩散与跳跃（使用期望最大化算法）、强制风险中性漂移，并生成稳定的信念波动率曲面；3）定义了一个与期权市场中波动率和相关性产品类似的衍生品层（方差、相关性、区间和首次通过工具）。在合成风险中性路径和真实事件数据的实验中，该模型降低了短期信念方差。",
    "fetch_date": "2026-01-27",
    "id": "20260127_5ed11b62"
  },
  {
    "title": "Cryptocurrency as an Investable Asset Class: Coming of Age",
    "url": "https://arxiv.org/pdf/2510.14435v2",
    "source": "ArXiv",
    "date": "2025-10-16",
    "abstract": "Cryptocurrencies are coming of age. We organize empirical regularities into ten stylized facts and analyze cryptocurrency through the lens of empirical asset pricing. We find important similarities with traditional markets -- risk-adjusted performance is broadly comparable, and the cross-section of returns can be summarized by a small set of factors. However, cryptocurrency also has its own distinct character: jumps are frequent and large, and blockchain information helps drive prices. This common set of facts provides evidence that cryptocurrency is emerging as an investable asset class.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "加密货币正趋于成熟。本文通过实证资产定价的视角，将经验规律归纳为十个典型事实并进行分析。研究发现加密货币与传统市场存在重要相似性——风险调整后表现大致可比，且横截面收益可由少量因子概括。然而，加密货币亦具独特性：价格跳跃频繁且幅度大，区块链信息对价格具有驱动作用。这组共同事实表明，加密货币正逐渐成为可投资的资产类别。",
    "fetch_date": "2026-01-27",
    "id": "20260127_8775f112"
  },
  {
    "title": "AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions",
    "url": "https://arxiv.org/pdf/2510.13343v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "Multi-agent reinforcement learning focuses on training the behaviors of multiple learning agents that coexist in a shared environment. Recently, MARL models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep Q-learning (ACE), have significantly improved performance by leveraging sequential decision-making processes. Although these models can enhance performance, they do not explicitly consider the importance of the order in which agents make decisions. In this paper, we propose an Agent Order of Action Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which agents make decisions. The proposed model explicitly incorporates the sequence of action decisions into the learning process, allowing the model to learn and predict the optimal order of agent actions. The AOAD-MAT model leverages a Transformer-based actor-critic architecture that dynamically adjusts the sequence of agent actions. To achieve this, we introduce a novel MARL architecture that cooperates with a subtask focused on predicting the next agent to act, integrated into a Proximal Policy Optimization based loss function to synergistically maximize the advantage of the sequential decision-making. The proposed method was validated through extensive experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo benchmarks. The experimental results show that the proposed AOAD-MAT model outperforms existing MAT and other baseline models, demonstrating the effectiveness of adjusting the AOAD order in MARL.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种名为AOAD-MAT的新型多智能体强化学习模型，该模型基于Transformer架构，并特别考虑了智能体行动决策的顺序。通过将行动决策序列显式地整合到学习过程中，模型能够学习并预测最优的智能体行动顺序。AOAD-MAT采用基于Transformer的actor-critic架构，动态调整智能体行动序列，并引入了一种新颖的MARL架构，该架构与一个专注于预测下一个行动智能体的子任务协同工作，集成到基于近端策略优化的损失函数中，以协同最大化顺序决策的优势。",
    "fetch_date": "2026-01-27",
    "id": "20260127_988890ab"
  },
  {
    "title": "A Nonlinear Target-Factor Model with Attention Mechanism for Mixed-Frequency Data",
    "url": "https://arxiv.org/pdf/2601.16274v1",
    "source": "ArXiv",
    "date": "2026-01-22",
    "abstract": "We propose Mixed-Panels-Transformer Encoder (MPTE), a novel framework for estimating factor models in panel datasets with mixed frequencies and nonlinear signals. Traditional factor models rely on linear signal extraction and require homogeneous sampling frequencies, limiting their applicability to modern high-dimensional datasets where variables are observed at different temporal resolutions. Our approach leverages Transformer-style attention mechanisms to enable context-aware signal construction through flexible, data-dependent weighting schemes that replace fixed linear combinations with adaptive reweighting based on similarity and relevance. We extend classical principal component analysis (PCA) to accommodate general temporal and cross-sectional attention matrices, allowing the model to learn how to aggregate information across frequencies without manual alignment or pre-specified weights. For linear activation functions, we establish consistency and asymptotic normality of factor and loading estimators, showing that our framework nests Target PCA as a special case while providing efficiency gains through transfer learning across auxiliary datasets. The nonlinear extension uses a Transformer architecture to capture complex hierarchical interactions while preserving the theoretical foundations. In simulations, MPTE demonstrates superior performance in nonlinear environments, and in an empirical application to 13 macroeconomic forecasting targets using a selected set of 48 monthly and quarterly series from the FRED-MD and FRED-QD databases, our method achieves competitive performance against established benchmarks. We further analyze attention patterns and systematically ablate model components to assess variable importance and temporal dependence. The resulting patterns highlight which indicators and horizons are most influential for forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于混合频率面板数据的非线性目标因子模型（MPTE框架），通过Transformer注意力机制实现上下文感知的信号构建，将固定线性组合替换为基于相似性和相关性的自适应重加权。该模型扩展了经典PCA方法，允许学习跨频率信息聚合，在线性激活函数下证明了估计量的一致性和渐近正态性，并将目标PCA作为特例包含其中。非线性扩展部分未完整展示。",
    "fetch_date": "2026-01-27",
    "id": "20260127_ef5355d8"
  },
  {
    "title": "On Time-subordinated Brownian Motion Processes for Financial Markets",
    "url": "https://arxiv.org/pdf/2510.14108v2",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "In the context of time-subordinated Brownian motion models, Fourier theory and methodology are proposed to modelling the stochastic distribution of time increments. Gaussian Variance-Mean mixtures and time-subordinated models are reviewed with a key example being the Variance-Gamma process. A non-parametric characteristic function decomposition of subordinated Brownian motion is presented. The theory requires an extension of the real domain of certain characteristic functions to the complex plane, the validity of which is proven here. This allows one to characterise and study the stochastic time-change directly from the full process. An empirical decomposition of S\\&P log-returns is provided to illustrate the methodology.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在时间从属布朗运动模型的背景下，本文提出应用傅里叶理论和方法来建模时间增量的随机分布。回顾了高斯方差-均值混合模型和时间从属模型，其中方差-伽马过程是关键示例。提出了从属布朗运动的非参数特征函数分解方法。该理论需要将某些特征函数的实域扩展到复平面，本文证明了这种扩展的有效性。这使得能够直接从完整过程中表征和研究随机时间变化。提供了标普对数收益的经验分解以说明该方法。",
    "fetch_date": "2026-01-27",
    "id": "20260127_b503ef91"
  },
  {
    "title": "The Variance-Gamma Process for Option Pricing",
    "url": "https://arxiv.org/pdf/2510.14093v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "This paper explores the concept of random-time subordination in modelling stock-price dynamics, and We first present results on the Laplace distribution as a Gaussian variance-mixture, in particular a more efficient volatility estimation procedure through the absolute moments. We generalise the Laplace model to characterise the powerful variance gamma model of Madan et al. as a Gamma time-subordinated Brownian motion to price European call options via an Esscher transform method. We find that the Variance Gamma model is able to empirically explain excess kurtosis found in log-returns data, rejecting a Black-Scholes assumption in a hypothesis test.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了随机时间从属在股票价格动态建模中的应用。首先介绍了拉普拉斯分布作为高斯方差混合模型的结果，特别是通过绝对矩实现更高效的波动率估计方法。随后将拉普拉斯模型推广为Madan等人提出的强大方差伽马模型，该模型通过伽马时间从属的布朗运动，并采用Esscher变换方法对欧式看涨期权进行定价。研究发现，方差伽马模型能够实证解释对数收益率数据中存在的超额峰度，在假设检验中拒绝了布莱克-斯科尔斯模型的假设。",
    "fetch_date": "2026-01-27",
    "id": "20260127_a23ec096"
  },
  {
    "title": "Market-Based Variance of Market Portfolio and of Entire Market",
    "url": "https://arxiv.org/pdf/2510.13790v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "We present the unified market-based description of returns and variances of the trades with shares of a particular security, of the trades with shares of all securities in the market, and of the trades with the market portfolio. We consider the investor who doesn't trade the shares of his portfolio he collected at time t0 in the past. The investor observes the time series of the current trades with all securities made in the market during the averaging interval. The investor may convert these time series into the time series that model the trades with all securities as the trades with a single security and into the time series that model the trades with the market portfolio as the trades with a single security. That establishes the same description of the returns and variances of the trades with a single security, the trades with all securities in the market, and the market portfolio. We show that the market-based variance, which accounts for the impact of random change of the volumes of consecutive trades with securities, takes the form of Markowitz's (1952) portfolio variance if the volumes of consecutive trades with all market securities are assumed constant. That highlights that Markowitz's (1952) variance ignores the effects of random volumes of consecutive trades. We compare the market-based variances of the market portfolio and of the trades with all market securities, consider the importance of the duration of the averaging interval, and explain the economic obstacles that limit the accuracy of the predictions of the returns and variances at best by Gaussian distributions. The same methods describe the returns and variances of any portfolio and the trades with its securities.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于市场的统一描述方法，用于分析单一证券交易、市场中所有证券交易以及市场投资组合交易的回报与方差。作者考虑了一位不交易其过去在t0时刻构建的投资组合的投资者。该投资者观察在平均区间内市场上所有证券的当前交易时间序列，并可将这些时间序列转换为模拟所有证券交易为单一证券交易的时间序列，以及模拟市场投资组合交易为单一证券交易的时间序列。这建立了对单一证券交易、市场中所有证券交易和市场投资组合交易的回报与方差的相同描述框架。研究表明，若假设所有市场证券的连续交易量恒定，则考虑连续交易量随机变化影响的市场方差将呈现为Markowitz（1952）投资组合方差的形式。",
    "fetch_date": "2026-01-27",
    "id": "20260127_311fcaaa"
  },
  {
    "title": "Multifractality and its sources in the digital currency market",
    "url": "https://arxiv.org/pdf/2510.13785v1",
    "source": "ArXiv",
    "date": "2025-10-15",
    "abstract": "Multifractality in time series analysis characterizes the presence of multiple scaling exponents, indicating heterogeneous temporal structures and complex dynamical behaviors beyond simple monofractal models. In the context of digital currency markets, multifractal properties arise due to the interplay of long-range temporal correlations and heavy-tailed distributions of returns, reflecting intricate market microstructure and trader interactions. Incorporating multifractal analysis into the modeling of cryptocurrency price dynamics enhances the understanding of market inefficiencies, may improve volatility forecasting and facilitate the detection of critical transitions or regime shifts. Based on the multifractal cross-correlation analysis (MFCCA) whose spacial case is the multifractal detrended fluctuation analysis (MFDFA), as the most commonly used practical tools for quantifying multifractality, in the present contribution a recently proposed method of disentangling sources of multifractality in time series was applied to the most representative instruments from the digital market. They include Bitcoin (BTC), Ethereum (ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results indicate the significant role of heavy tails in generating a broad multifractal spectrum. However, they also clearly demonstrate that the primary source of multifractality are temporal correlations in the series, and without them, multifractality fades out. It appears characteristic that these temporal correlations, to a large extent, do not depend on the thickness of the tails of the fluctuation distribution. These observations, made here in the context of the digital currency market, provide a further strong argument for the validity of the proposed methodology of disentangling sources of multifractality in time series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了数字货币市场中时间序列的多重分形特性，指出多重分形源于长期时间相关性和收益率的厚尾分布，反映了复杂的市场微观结构和交易者互动。通过应用多重分形去趋势波动分析（MFDFA）及其扩展方法多重分形互相关分析（MFCCA），论文旨在分离多重分形的来源，并分析了比特币（BTC）、以太坊（ETH）、去中心化交易所（DEX）和非同质化代币（NFT）等代表性工具。研究认为，将多重分形分析纳入加密货币价格动态建模，可增强对市场低效性的理解，可能改进波动率预测，并有助于检测关键转折或制度转换。",
    "fetch_date": "2026-01-27",
    "id": "20260127_8d5486d1"
  },
  {
    "title": "Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction",
    "url": "https://arxiv.org/pdf/2510.15691v3",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three methods of different architectural complexities: representation combination, representation summation, and attentive representations. Next, building on the limitation of fusion learning observed in empirical comparison, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability of the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文探讨了量化因子与基于大型语言模型（LLM）生成的新闻流表征在股票收益预测中的协同作用。作者提出了融合学习框架，比较了表征组合、表征求和及注意力表征三种方法，并基于实证比较中融合学习的局限性，探索了自适应结合单模态预测及其融合的混合模型。为缓解混合模型训练不稳定性，引入了具有理论见解的解耦训练方法。实验基于真实投资数据，旨在提升股票选择、组合优化及风险管理等实战交易任务。",
    "fetch_date": "2026-01-26",
    "id": "20260126_b33e0684"
  },
  {
    "title": "3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy, and Selection in Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2510.17393v1",
    "source": "ArXiv",
    "date": "2025-10-20",
    "abstract": "Large Language Models (LLMs) have recently gained popularity in stock trading for their ability to process multimodal financial data. However, most existing methods focus on single-stock trading and lack the capacity to reason over multiple candidates for portfolio construction. Moreover, they typically lack the flexibility to revise their strategies in response to market shifts, limiting their adaptability in real-world trading. To address these challenges, we propose 3S-Trader, a training-free framework that incorporates scoring, strategy, and selection modules for stock portfolio construction. The scoring module summarizes each stock's recent signals into a concise report covering multiple scoring dimensions, enabling efficient comparison across candidates. The strategy module analyzes historical strategies and overall market conditions to iteratively generate an optimized selection strategy. Based on this strategy, the selection module identifies and assembles a portfolio by choosing stocks with higher scores in relevant dimensions. We evaluate our framework across four distinct stock universes, including the Dow Jones Industrial Average (DJIA) constituents and three sector-specific stock sets. Compared with existing multi-LLM frameworks and time-series-based baselines, 3S-Trader achieves the highest accumulated return of 131.83% on DJIA constituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also delivering consistently strong results across other sectors.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出3S-Trader框架，通过评分、策略和选择三个模块实现股票组合优化。该框架无需训练，能处理多模态金融数据，生成股票评分报告，分析市场条件迭代优化策略，并基于相关维度选择高评分股票构建组合。针对现有方法在组合构建和策略适应性上的不足，该框架在多个股票池（如道琼斯工业平均指数成分股）中进行了评估，显示出对实战交易的价值。",
    "fetch_date": "2026-01-26",
    "id": "20260126_e2bfe9df"
  },
  {
    "title": "Trading with the Devil: Risk and Return in Foundation Model Strategies",
    "url": "https://arxiv.org/pdf/2510.17165v1",
    "source": "ArXiv",
    "date": "2025-10-20",
    "abstract": "Foundation models - already transformative in domains such as natural language processing - are now starting to emerge for time-series tasks in finance. While these pretrained architectures promise versatile predictive signals, little is known about how they shape the risk profiles of the trading strategies built atop them, leaving practitioners reluctant to commit serious capital. In this paper, we propose an extension to the Capital Asset Pricing Model (CAPM) that disentangles the systematic risk introduced by a shared foundation model - potentially capable of generating alpha if the underlying model is genuinely predictive - from the idiosyncratic risk attributable to custom fine-tuning, which typically accrues no systematic premium. To enable a practical estimation of these separate risks, we align this decomposition with the concepts of uncertainty disentanglement, casting systematic risk as epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk as aleatory uncertainty (introduced during custom adaptations). Under the Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among other methods in the uncertainty-quantization toolkit - can directly measure the epistemic risk, thereby mapping trading strategies to a more transparent risk-return plane. Our experiments show that isolating these distinct risk factors yields deeper insights into the performance limits of foundation-model-based strategies, their model degradation over time, and potential avenues for targeted refinements. Taken together, our results highlight both the promise and the pitfalls of deploying large pretrained models in competitive financial markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "论文《与魔鬼交易：基础模型策略的风险与回报》探讨了金融时间序列任务中新兴的基础模型（如预训练架构）对交易策略风险特征的影响。针对实践中因风险不明而难以投入大额资金的问题，作者提出了一种CAPM扩展模型，旨在分离基础模型引入的系统性风险（若模型具有预测性可产生阿尔法）与定制微调带来的异质性风险（通常无系统性溢价）。通过将系统性风险对应为认知不确定性（源于预训练模型），异质性风险对应为偶然不确定性（来自定制适应），并结合不确定性解耦概念，论文展示了如何利用蒙特卡洛dropout等不确定性量化工具进行实际风险估计。该研究为量化交易中基础模型的风险评估提供了理论框架和实用方法，对实战交易具有参考价值。",
    "fetch_date": "2026-01-26",
    "id": "20260126_228a6072"
  },
  {
    "title": "A Topological Approach to Parameterizing Deep Hedging Networks",
    "url": "https://arxiv.org/pdf/2510.16938v1",
    "source": "ArXiv",
    "date": "2025-10-19",
    "abstract": "Deep hedging uses recurrent neural networks to hedge financial products that cannot be fully hedged in incomplete markets. Previous work in this area focuses on minimizing some measure of quadratic hedging error by calculating pathwise gradients, but doing so requires large batch sizes and can make training effective models in a reasonable amount of time challenging. We show that by adding certain topological features, we can reduce batch sizes substantially and make training these models more practically feasible without greatly compromising hedging performance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种拓扑学方法来参数化深度对冲网络，旨在解决不完全市场中金融产品的对冲问题。传统深度对冲方法通过计算路径梯度最小化二次对冲误差，但需要大批量数据且训练耗时。本文通过引入特定拓扑特征，显著减少批量大小，使模型训练更可行，同时保持对冲性能。",
    "fetch_date": "2026-01-26",
    "id": "20260126_219233d5"
  },
  {
    "title": "A high-frequency approach to Realized Risk Measures",
    "url": "https://arxiv.org/pdf/2510.16526v1",
    "source": "ArXiv",
    "date": "2025-10-18",
    "abstract": "We propose a new approach, termed Realized Risk Measures (RRM), to estimate Value-at-Risk (VaR) and Expected Shortfall (ES) using high-frequency financial data. It extends the Realized Quantile (RQ) approach proposed by Dimitriadis and Halbleib by lifting the assumption of return self-similarity, which displays some limitations in describing empirical data. More specifically, as the RQ, the RRM method transforms intra-day returns in intrinsic time using a subordinator process, in order to capture the inhomogeneity of trading activity and/or volatility clustering. Then, microstructural effects resulting in non-zero autocorrelation are filtered out using a suitable moving average process. Finally, a fat-tailed distribution is fitted on the cleaned intra-day returns. The return distribution at low frequency (daily) is then extrapolated via either a characteristic function approach or Monte Carlo simulations. VaR and ES are estimated as the quantile and the tail mean of the distribution, respectively. The proposed approach is benchmarked against the RQ through several experiments. Extensive numerical simulations and an empirical study on 18 US stocks show the outperformance of our method, both in terms of the in-sample estimated risk measures and in the out-of-sample risk forecasting",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种名为“已实现风险度量”的新方法，用于利用高频金融数据估计风险价值与预期损失。该方法通过引入次过程转换日内收益至内在时间，以捕捉交易活动或波动率聚集的非均匀性，并采用移动平均过程过滤微观结构效应导致的非零自相关，最后对清洗后的日内收益拟合厚尾分布，通过特征函数法或蒙特卡洛模拟外推低频收益分布，进而估计风险价值与预期损失。实验表明，该方法在18只美股上优于现有方法。",
    "fetch_date": "2026-01-26",
    "id": "20260126_1e20b3b8"
  },
  {
    "title": "Sentiment and Volatility in Financial Markets: A Review of BERT and GARCH Applications during Geopolitical Crises",
    "url": "https://arxiv.org/pdf/2510.16503v1",
    "source": "ArXiv",
    "date": "2025-10-18",
    "abstract": "Artificial intelligence techniques have increasingly been applied to understand the complex relationship between public sentiment and financial market behaviour. This study explores the relationship between the sentiment of news related to the Russia-Ukraine war and the volatility of the stock market. A comprehensive dataset of news articles from major US platforms, published between January 1 and July 17, 2024, was analysed using a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model adapted for financial language. We extracted sentiment scores and applied a Generalised Autoregressive Conditional Heteroscedasticity (GARCH) model, enhanced with a Student-t distribution to capture the heavy-tailed nature of financial returns data. The results reveal a statistically significant negative relationship between negative news sentiment and market stability, suggesting that pessimistic war coverage is associated with increased volatility in the S&P 500 index. This research demonstrates how artificial intelligence and natural language processing can be integrated with econometric modelling to assess real-time market dynamics, offering valuable tools for financial risk analysis during geopolitical crises.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究探讨了俄乌战争相关新闻情绪与股市波动率之间的关系。通过分析2024年1月1日至7月17日期间美国主要新闻平台的文章，使用针对金融语言微调的BERT模型提取情绪分数，并应用基于学生t分布的GARCH模型来捕捉金融收益数据的厚尾特征。结果表明，负面新闻情绪与市场稳定性之间存在显著的负相关关系，表明悲观的战争报道与标普500指数波动率增加相关。该研究展示了如何将人工智能、自然语言处理与计量经济学建模相结合，以评估实时市场动态，为金融领域提供有价值的工具。",
    "fetch_date": "2026-01-26",
    "id": "20260126_d1b54fbf"
  },
  {
    "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows",
    "url": "https://arxiv.org/pdf/2510.15458v1",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "In this paper, we show that interventionally robust optimization problems in causal models are continuous under the $G$-causal Wasserstein distance, but may be discontinuous under the standard Wasserstein distance. This highlights the importance of using generative models that respect the causal structure when augmenting data for such tasks. To this end, we propose a new normalizing flow architecture that satisfies a universal approximation property for causal structural models and can be efficiently trained to minimize the $G$-causal Wasserstein distance. Empirically, we demonstrate that our model outperforms standard (non-causal) generative models in data augmentation for causal regression and mean-variance portfolio optimization in causal factor models.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种新的归一化流架构，该架构满足因果结构模型的通用逼近特性，并能高效训练以最小化G-因果Wasserstein距离。实证表明，在因果回归和因果因子模型中的均值-方差投资组合优化的数据增强任务中，该模型优于标准（非因果）生成模型。",
    "fetch_date": "2026-01-26",
    "id": "20260126_32d5c23d"
  },
  {
    "title": "Centered MA Dirichlet ARMA for Financial Compositions: Theory & Empirical Evidence",
    "url": "https://arxiv.org/pdf/2510.18903v2",
    "source": "ArXiv",
    "date": "2025-10-20",
    "abstract": "Observation-driven Dirichlet models for compositional time series commonly use the additive log-ratio (ALR) link and include a moving-average (MA) term based on ALR residuals. In the standard Bayesian Dirichlet Auto-Regressive Moving-Average (B-DARMA) recursion, this MA regressor has a nonzero conditional mean under the Dirichlet likelihood, which biases the mean path and complicates interpretation of the MA coefficients. We propose a minimal change: replace the raw regressor with a centered innovation equal to the ALR residual minus its conditional expectation, computable in closed form using digamma functions. Centering restores mean-zero innovations for the MA block without altering either the likelihood or the ALR link. We provide closed-form identities for the conditional mean and forecast recursion, show first-order equivalence to a digamma-link DARMA while retaining a simple inverse back to the mean composition, and supply ready-to-use code. In a weekly application to the Federal Reserve H.8 bank-asset composition, the centered specification improves log predictive scores with virtually identical point accuracy and markedly cleaner Hamiltonian Monte Carlo diagnostics.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种针对成分时间序列的改进型贝叶斯狄利克雷自回归移动平均（B-DARMA）模型。核心创新在于将标准模型中基于加性对数比（ALR）残差的移动平均（MA）项替换为中心化创新项，即ALR残差减去其条件期望（可通过digamma函数闭式计算）。这一最小化改动解决了原MA回归量在狄利克雷似然下条件均值非零的问题，从而消除了对均值路径的偏差，并简化了MA系数的解释。该方法保持了似然函数和ALR链接不变，恢复了MA块的零均值创新，同时提供了条件均值和预测递推的闭式解，并证明其一阶等价于digamma链接的DARMA模型，且保留了回推至平均成分的简易性。论文在美国联邦储备银行H.8银行资产成分的周度数据应用中表明，中心化设定提高了对数预测分数，点预测精度几乎相同，且哈密顿蒙特卡洛诊断结果显著更清晰。",
    "fetch_date": "2026-01-26",
    "id": "20260126_a7fab262"
  },
  {
    "title": "A three-step machine learning approach to predict market bubbles with financial news",
    "url": "https://arxiv.org/pdf/2510.16636v1",
    "source": "ArXiv",
    "date": "2025-10-18",
    "abstract": "This study presents a three-step machine learning framework to predict bubbles in the S&P 500 stock market by combining financial news sentiment with macroeconomic indicators. Building on traditional econometric approaches, the proposed approach predicts bubble formation by integrating textual and quantitative data sources. In the first step, bubble periods in the S&P 500 index are identified using a right-tailed unit root test, a widely recognized real-time bubble detection method. The second step extracts sentiment features from large-scale financial news articles using natural language processing (NLP) techniques, which capture investors' expectations and behavioral patterns. In the final step, ensemble learning methods are applied to predict bubble occurrences based on high sentiment-based and macroeconomic predictors. Model performance is evaluated through k-fold cross-validation and compared against benchmark machine learning algorithms. Empirical results indicate that the proposed three-step ensemble approach significantly improves predictive accuracy and robustness, providing valuable early warning insights for investors, regulators, and policymakers in mitigating systemic financial risks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究提出了一种三步机器学习框架，通过结合金融新闻情感与宏观经济指标来预测标普500股票市场的泡沫。该方法基于传统计量经济学方法，通过整合文本和定量数据源来预测泡沫形成。第一步使用右尾单位根检验（一种广泛认可的实时泡沫检测方法）识别标普500指数的泡沫期。第二步利用自然语言处理（NLP）技术从大规模金融新闻文章中提取情感特征，捕捉投资者的预期和行为模式。第三步应用集成学习方法，基于高情感基和宏观经济预测因子来预测泡沫发生。模型性能通过k折交叉验证进行评估，并与基准机器学习算法进行比较。实证结果表明，所提出的三步集成方法显著提高了预测准确性和稳健性，为投资者、监管机构和政策制定者提供了有价值的早期预警见解，以缓解系统性风险。",
    "fetch_date": "2026-01-26",
    "id": "20260126_f01adba6"
  },
  {
    "title": "Portfolio Optimization of Indonesian Banking Stocks Using Robust Optimization",
    "url": "https://arxiv.org/pdf/2510.15288v1",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "Since the COVID-19 pandemic, the number of investors in the Indonesia Stock Exchange has steadily increased, emphasizing the importance of portfolio optimization in balancing risk and return. The classical mean-variance optimization model, while widely applied, depends on historical return and risk estimates that are uncertain and may result in suboptimal portfolios. To address this limitation, robust optimization incorporates uncertainty sets to improve portfolio reliability under market fluctuations. This study constructs such sets using moving-window and bootstrapping methods and applies them to Indonesian banking stock data with varying risk-aversion parameters. The results show that robust optimization with the moving-window method, particularly with a smaller risk-aversion parameter, provides a better risk-return trade-off compared to the bootstrapping approach. These findings highlight the potential of the moving-window method to generate more effective portfolio strategies for risk-tolerant investors.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "自COVID-19疫情以来，印尼证券交易所的投资者数量稳步增长，凸显了投资组合优化在平衡风险与收益方面的重要性。经典的均值-方差优化模型虽广泛应用，但依赖于不确定的历史收益和风险估计，可能导致次优投资组合。为克服此局限，鲁棒优化通过引入不确定性集合，提升投资组合在市场波动下的可靠性。本研究采用移动窗口和自助法构建不确定性集合，并应用于不同风险厌恶参数下的印尼银行股票数据。结果表明，采用移动窗口法的鲁棒优化（尤其配合较小的风险厌恶参数）相比自助法能提供更优的风险-收益权衡。这些发现凸显了移动窗口法为风险承受能力较强的投资者生成更有效投资组合策略的潜力。",
    "fetch_date": "2026-01-26",
    "id": "20260126_e803a1e9"
  },
  {
    "title": "Semi-analytical pricing of American options with hybrid dividends via integral equations and the GIT method",
    "url": "https://arxiv.org/pdf/2510.18159v2",
    "source": "ArXiv",
    "date": "2025-10-20",
    "abstract": "This paper introduces a semi-analytical method for pricing American options on assets (stocks, ETFs) that pay discrete and/or continuous dividends. The problem is notoriously complex because discrete dividends create abrupt price drops and affect the optimal exercise timing, making traditional continuous-dividend models unsuitable. Our approach utilizes the Generalized Integral Transform (GIT) method introduced by the author and his co-authors in a number of papers, which transforms the pricing problem from a complex partial differential equation with a free boundary into an integral Volterra equation of the second or first kind. In this paper we illustrate this approach by considering a popular GBM model that accounts for discrete cash and proportional dividends using Dirac delta functions. By reframing the problem as an integral equation, we can sequentially solve for the option price and the early exercise boundary, effectively handling the discontinuities caused by the dividends. Our methodology provides a powerful alternative to standard numerical techniques like binomial trees or finite difference methods, which can struggle with the jump conditions of discrete dividends by losing accuracy or performance. Several examples demonstrate that the GIT method is highly accurate and computationally efficient, bypassing the need for extensive computational grids or complex backward induction steps.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种半解析方法，用于对支付离散和/或连续股息的资产（股票、ETF）的美式期权进行定价。该问题因离散股息导致价格骤降并影响最优行权时机而异常复杂，使得传统的连续股息模型不适用。该方法利用广义积分变换（GIT）方法，将定价问题从具有自由边界的复杂偏微分方程转化为第二类或第一类Volterra积分方程。通过将问题重构为积分方程，可以顺序求解期权价格和提前行权边界，有效处理股息引起的不连续性。该方法为标准的数值技术（如二叉树或有限差分法）提供了强大的替代方案，这些方法在处理离散股息的跳跃条件时可能遇到困难。",
    "fetch_date": "2026-01-26",
    "id": "20260126_18855da4"
  },
  {
    "title": "Design and valuation of multi-region CoCoCat bonds",
    "url": "https://arxiv.org/pdf/2510.17221v1",
    "source": "ArXiv",
    "date": "2025-10-20",
    "abstract": "This paper introduces a novel multidimensional insurance-linked instrument: a contingent convertible bond (CoCoCat bond) whose conversion trigger is activated by predefined natural catastrophes across multiple geographical regions. We develop such a model explicitly accounting for the complex dependencies between regional catastrophe losses. Specifically, we explore scenarios ranging from complete independence to proportional loss dependencies, both with fixed and random loss amounts. Utilizing change-of-measure techniques, we derive risk-neutral pricing formulas tailored to these diverse dependence structures. By fitting our model to real-world natural catastrophe data from Property Claim Services, we demonstrate the significant impact of inter-regional dependencies on the CoCoCat bond's pricing, highlighting the importance of multidimensional risk assessment for this innovative financial instrument.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了一种新型多维保险挂钩工具：一种或有可转换债券（CoCoCat债券），其转换触发条件由多个地理区域预定义的自然灾害激活。我们开发了一个明确考虑区域灾害损失之间复杂依赖关系的模型，具体探讨了从完全独立到比例损失依赖（包括固定和随机损失金额）的各种情景。利用测度变换技术，我们推导出针对这些不同依赖结构的风险中性定价公式。通过将模型拟合到Property Claim Services提供的真实世界自然灾害数据，我们证明了区域间依赖关系对CoCoCat债券定价的显著影响，强调了多维风险评估对这一创新金融工具的重要性。",
    "fetch_date": "2026-01-26",
    "id": "20260126_f54de34c"
  },
  {
    "title": "Martingale theory for Dynkin games with asymmetric information",
    "url": "https://arxiv.org/pdf/2510.15616v1",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "This paper provides necessary and sufficient conditions for a pair of randomised stopping times to form a saddle point of a zero-sum Dynkin game with partial and/or asymmetric information across players. The framework is non-Markovian and covers essentially any information structure. Our methodology relies on the identification of suitable super and submartingales involving players' equilibrium payoffs. Saddle point strategies are characterised in terms of the dynamics of those equilibrium payoffs and are related to their Doob-Meyer decompositions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对具有部分和/或不对称信息的零和Dynkin博弈，给出了随机停止时间对构成鞍点的充要条件。该框架是非马尔可夫性的，几乎涵盖任何信息结构。方法依赖于识别涉及玩家均衡收益的合适上鞅和下鞅。鞍点策略通过均衡收益的动态及其Doob-Meyer分解来刻画。",
    "fetch_date": "2026-01-26",
    "id": "20260126_edb98a71"
  },
  {
    "title": "SoK: Market Microstructure for Decentralized Prediction Markets (DePMs)",
    "url": "https://arxiv.org/pdf/2510.15612v2",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "Decentralized prediction markets (DePMs) allow open participation in event-based wagering without fully relying on centralized intermediaries. We review the history of DePMs which date back to 2011 and includes hundreds of proposals. Perhaps surprising, modern DePMs like Polymarket deviate materially from earlier designs like Truthcoin and Augur v1. We use our review to present a modular workflow comprising eight stages: underlying infrastructure, market topic, share structure and pricing, market initialization, trading, market resolution, settlement, and archiving. For each module, we enumerate the design variants, analyzing trade-offs around decentralization, expressiveness, and manipulation resistance. We also identify open problems for researchers interested in this ecosystem.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文系统综述了去中心化预测市场（DePMs）的发展历史与设计架构。论文提出了一个包含八个阶段的模块化工作流程：底层基础设施、市场主题、份额结构与定价、市场初始化、交易、市场裁决、结算和归档。针对每个模块，作者列举了不同的设计变体，并分析了在去中心化、表达能力和抗操纵性之间的权衡。文章还指出了该领域的研究空白。虽然对理解DePMs的微观结构有理论价值，但未涉及具体的交易策略、算法或实战应用，因此对量化交易实战的直接指导意义有限。",
    "fetch_date": "2026-01-26",
    "id": "20260126_ee4a386a"
  },
  {
    "title": "On the short-time behaviour of up-and-in barrier options using Malliavin calculus",
    "url": "https://arxiv.org/pdf/2510.15423v1",
    "source": "ArXiv",
    "date": "2025-10-17",
    "abstract": "In this paper we study the short-maturity asymptotics of up-and-in barrier options under a broad class of stochastic volatility models. Our approach uses Malliavin calculus techniques, typically used for linear stochastic partial differential equations, to analyse the law of the supremum of the log-price process. We derive a concentration inequality and explicit bounds on the density of the supremum in terms of the time to maturity. These results yield an upper bound on the asymptotic decay rate of up-and-in barrier option prices as maturity vanishes. We further demonstrate the applicability of our framework to the rough Bergomi model and validate the theoretical results with numerical experiments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文利用Malliavin微积分技术，研究了一类广泛随机波动率模型下敲入障碍期权在短期限内的渐近行为。通过分析对数价格过程上确界的分布规律，推导出浓度不等式和上确界密度关于到期时间的显式边界。这些结果为敲入障碍期权价格在到期时间趋近于零时的渐近衰减率提供了上界。作者进一步展示了该框架在粗糙Bergomi模型中的适用性，并通过数值实验验证了理论结果。",
    "fetch_date": "2026-01-26",
    "id": "20260126_aded26ab"
  },
  {
    "title": "News-Aware Direct Reinforcement Trading for Financial Markets",
    "url": "https://arxiv.org/pdf/2510.19173v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "The financial market is known to be highly sensitive to news. Therefore, effectively incorporating news data into quantitative trading remains an important challenge. Existing approaches typically rely on manually designed rules and/or handcrafted features. In this work, we directly use the news sentiment scores derived from large language models, together with raw price and volume data, as observable inputs for reinforcement learning. These inputs are processed by sequence models such as recurrent neural networks or Transformers to make end-to-end trading decisions. We conduct experiments using the cryptocurrency market as an example and evaluate two representative reinforcement learning algorithms, namely Double Deep Q-Network (DDQN) and Group Relative Policy Optimization (GRPO). The results demonstrate that our news-aware approach, which does not depend on handcrafted features or manually designed rules, can achieve performance superior to market benchmarks. We further highlight the critical role of time-series information in this process.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场对新闻高度敏感，将新闻数据有效融入量化交易是一大挑战。现有方法通常依赖人工设计的规则和特征。本研究直接使用大语言模型生成的新闻情感分数，结合原始价格和交易量数据，作为强化学习的观测输入。这些输入通过循环神经网络或Transformer等序列模型处理，实现端到端的交易决策。以加密货币市场为例，评估了双深度Q网络和组相对策略优化两种代表性强化学习算法。结果表明，这种不依赖人工特征或规则、具备新闻感知能力的方法，其表现优于市场基准。研究进一步强调了时间序列信息在此过程中的关键作用。",
    "fetch_date": "2026-01-25",
    "id": "20260125_4a5c2f20"
  },
  {
    "title": "Aligning Multilingual News for Stock Return Prediction",
    "url": "https://arxiv.org/pdf/2510.19203v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "News spreads rapidly across languages and regions, but translations may lose subtle nuances. We propose a method to align sentences in multilingual news articles using optimal transport, identifying semantically similar content across languages. We apply this method to align more than 140,000 pairs of Bloomberg English and Japanese news articles covering around 3500 stocks in Tokyo exchange over 2012-2024. Aligned sentences are sparser, more interpretable, and exhibit higher semantic similarity. Return scores constructed from aligned sentences show stronger correlations with realized stock returns, and long-short trading strategies based on these alignments achieve 10\\% higher Sharpe ratios than analyzing the full text sample.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种利用最优传输对齐多语言新闻句子的方法，旨在捕捉跨语言语义相似内容。该方法应用于2012-2024年间覆盖东京交易所约3500只股票的超过14万对彭博英文和日文新闻文章。对齐后的句子更稀疏、可解释性更强且语义相似度更高。基于对齐句子构建的收益分数与实际股票收益表现出更强的相关性，基于这些对齐的多空交易策略相比分析全文样本实现了10%的夏普比率提升。",
    "fetch_date": "2026-01-25",
    "id": "20260125_86acc467"
  },
  {
    "title": "Denoising Complex Covariance Matrices with Hybrid ResNet and Random Matrix Theory: Cryptocurrency Portfolio Applications",
    "url": "https://arxiv.org/pdf/2510.19130v2",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "Covariance matrices estimated from short, noisy, and non-Gaussian financial time series are notoriously unstable. Empirical evidence suggests that such covariance structures often exhibit power-law scaling, reflecting complex, hierarchical interactions among assets. Motivated by this observation, we introduce a power-law covariance model to characterize collective market dynamics and propose a hybrid estimator that integrates Random Matrix Theory (RMT) with deep Residual Neural Networks (ResNets). The RMT component regularizes the eigenvalue spectrum in high-dimensional noisy settings, while the ResNet learns data-driven corrections that recover latent structural dependencies encoded in the eigenvectors. Monte Carlo simulations show that the proposed ResNet-based estimators consistently minimize both Frobenius and minimum-variance losses across a range of population covariance models. Empirical experiments on 89 cryptocurrencies over the period 2020-2025, using a training window ending at the local Bitcoin peak in November 2021 and testing through the subsequent bear market, demonstrate that a two-step estimator combining hierarchical filtering with ResNet corrections produces the most profitable and well-balanced portfolios, remaining robust across market regime shifts. Beyond finance, the proposed hybrid framework applies broadly to high-dimensional systems described by low-rank deformations of Wishart ensembles, where incorporating eigenvector information enables the detection of multiscale and hierarchical structure that is inaccessible to purely eigenvalue-based methods.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种结合随机矩阵理论（RMT）与深度残差神经网络（ResNet）的混合协方差矩阵去噪方法，用于处理金融时间序列中的噪声和非高斯特性。RMT部分在高维噪声环境下正则化特征值谱，ResNet则学习数据驱动的修正以恢复特征向量中编码的潜在结构依赖。蒙特卡洛模拟显示，该方法在不同总体协方差模型下能最小化Frobenius和最小方差损失。在2020-2025年89种加密货币的实证实验中，采用分层过滤与ResNet修正的两步估计器在训练窗口（截至2021年11月比特币局部峰值）和后续熊市测试中表现出色，对实战交易中的投资组合优化具有直接应用价值。",
    "fetch_date": "2026-01-25",
    "id": "20260125_a9f6ce4a"
  },
  {
    "title": "An Efficient Calibration Framework for Volatility Derivatives under Rough Volatility with Jumps",
    "url": "https://arxiv.org/pdf/2510.19126v1",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "We present a fast and robust calibration method for stochastic volatility models that admit Fourier-analytic transform-based pricing via characteristic functions. The design is structure-preserving: we keep the original pricing transform and (i) split the pricing formula into data-independent inte- grals and a market-dependent remainder; (ii) precompute those data-independent integrals with GPU acceleration; and (iii) approximate only the remaining, market-dependent pricing map with a small neural network. We instantiate the workflow on a rough volatility model with tempered-stable jumps tailored to power-type volatility derivatives and calibrate it to VIX options with a global-to-local search. We verify that a pure-jump rough volatility model adequately captures the VIX dynamics, consistent with prior empirical findings, and demonstrate that our calibration method achieves high accuracy and speed.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种针对粗糙波动率跳跃模型的快速稳健校准框架，通过GPU加速预计算数据无关积分，并用小型神经网络近似市场相关定价映射，实现了对VIX期权的高精度快速校准，对波动率衍生品实战交易具有直接应用价值。",
    "fetch_date": "2026-01-25",
    "id": "20260125_6e6421ec"
  },
  {
    "title": "QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework",
    "url": "https://arxiv.org/pdf/2510.18569v1",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "QuantEvolve：通过多智能体进化框架自动化量化策略发现。该框架结合质量-多样性优化与假设驱动的策略生成，利用与投资者偏好（如策略类型、风险特征、换手率、收益特性）对齐的特征图，维持多样化且有效的策略集合。通过迭代生成与评估的系统性探索，产生能适应市场状态转换和个性化投资需求的复杂策略。实证结果表明其优于传统基准方法。",
    "fetch_date": "2026-01-25",
    "id": "20260125_e7438d9b"
  },
  {
    "title": "BondBERT: What we learn when assigning sentiment in the bond market",
    "url": "https://arxiv.org/pdf/2511.01869v2",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "Bond markets respond differently to macroeconomic news compared to equity markets, yet most sentiment models are trained primarily on general financial or equity news data. However, bond prices often move in the opposite direction to economic optimism, making general or equity-based sentiment tools potentially misleading. We introduce BondBERT, a transformer-based language model fine-tuned on bond-specific news. BondBERT can act as the perception and reasoning component of a financial decision-support agent, providing sentiment signals that integrate with forecasting models. We propose a generalisable framework for adapting transformers to low-volatility, domain-inverse sentiment tasks by compiling and cleaning 30,000 UK bond market articles (2018-2025). BondBERT's sentiment predictions are compared against FinBERT, FinGPT, and Instruct-FinGPT using event-based correlation, up/down accuracy analyses, and LSTM forecasting across ten UK sovereign bonds. We find that BondBERT consistently produces positive correlations with bond returns, and achieves higher alignment and forecasting accuracy than the three baseline models. These results demonstrate that domain-specific sentiment adaptation better captures fixed income dynamics, bridging a gap between NLP advances and bond market analytics.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "债券市场对宏观经济新闻的反应与股票市场不同，大多数情绪模型主要基于一般金融或股票新闻数据进行训练，而债券价格常与经济乐观情绪反向变动，导致通用或基于股票的情绪工具可能产生误导。本文介绍了BondBERT，这是一种基于Transformer的语言模型，专门针对债券新闻进行微调。BondBERT可作为金融决策支持代理的感知和推理组件，提供与预测模型集成的情绪信号。我们提出了一个通用框架，通过整理和清洗30,000篇英国债券市场文章（2018-2025年），将Transformer适应于低波动性、领域反向情绪任务。BondBERT的情绪预测与FinBERT、FinGPT和Instruct-FinGPT进行了基于事件的相关性、涨跌准确性分析和LSTM预测比较，涉及十种英国主权债券。结果表明，BondBERT始终与债券回报呈正相关，并在对齐和预测准确性方面优于三个基线模型。这些结果证明了领域特定情绪适应的重要性。",
    "fetch_date": "2026-01-25",
    "id": "20260125_89b826a0"
  },
  {
    "title": "Tailoring Portfolio Choice via Quantile-Targeted Policies",
    "url": "https://arxiv.org/pdf/2510.19271v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "We study the dynamic investment decisions of investors who prioritise specific quantiles of outcomes over their expected values. Downside-focused agents targeting low quantiles reduce risk in states with high variance, while those with a preference for high quantiles concentrate in sleeves with high dispersion when there is potential for upside. These results provide a microfoundation for volatility management, demonstrating that reducing exposure in volatile states is an optimal response for risk-averse investors and rationalising inverse-variance heuristics. We propose a distributional actor-critic algorithm that learns time-consistent policies tailored to these specific risks, irrespective of the utilitys functional form. The quantile value can be mapped onto interpretable tilts, and the performance of empirically chosen portfolios aligns with investors objectives.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究投资者在动态投资决策中优先考虑特定结果分位数而非期望值的情况。关注下行风险的投资者（针对低分位数）在高波动状态下降低风险敞口，而偏好高分位数的投资者则在有上行潜力时集中于高离散度的资产类别。这些结果为波动率管理提供了微观基础，表明在波动状态下减少风险敞口是风险厌恶投资者的最优反应，并为逆方差启发式方法提供了理论依据。作者提出了一种分布型演员-评论家算法，该算法能够学习与这些特定风险相匹配的时间一致性策略，而不依赖于效用函数的具体形式。分位数值可映射为可解释的倾斜配置，实证选择的投资组合表现与投资者目标一致。",
    "fetch_date": "2026-01-25",
    "id": "20260125_43b30ca8"
  },
  {
    "title": "Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations",
    "url": "https://arxiv.org/pdf/2510.18995v1",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "Estimating risk measures such as large loss probabilities and Value-at-Risk is fundamental in financial risk management and often relies on computationally intensive nested Monte Carlo methods. While Multi-Level Monte Carlo (MLMC) techniques and their weighted variants are typically more efficient, their effectiveness tends to deteriorate when dealing with irregular functions, notably indicator functions, which are intrinsic to these risk measures. We address this issue by introducing a novel MLMC parametrization that significantly improves performance in practical, non-asymptotic settings while maintaining theoretical asymptotic guarantees. We also prove that antithetic sampling of MLMC levels enhances efficiency regardless of the regularity of the underlying function. Numerical experiments motivated by the calculation of economic capital in a life insurance context confirm the practical value of our approach for estimating loss probabilities and quantiles, bridging theoretical advances and practical requirements in financial risk estimation.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文针对金融风险管理中计算密集型嵌套蒙特卡洛方法（如大额损失概率和风险价值VaR估计）的效率问题，提出了一种优化的多级蒙特卡洛（MLMC）参数化方法和反变量采样技术。通过改进非渐近实际场景下的性能（尤其在处理指示函数等不规则函数时），同时保持理论渐近保证，并在寿险经济资本计算等数值实验中验证了其对于损失概率和分位数估计的实用价值。",
    "fetch_date": "2026-01-25",
    "id": "20260125_97f74480"
  },
  {
    "title": "Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators",
    "url": "https://arxiv.org/pdf/2510.20017v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "Traditional mean-field game (MFG) solvers operate on an instance-by-instance basis, which becomes infeasible when many related problems must be solved (e.g., for seeking a robust description of the solution under perturbations of the dynamics or utilities, or in settings involving continuum-parameterized agents.). We overcome this by training neural operators (NOs) to learn the rules-to-equilibrium map from the problem data (``rules'': dynamics and cost functionals) of LQ MFGs defined on separable Hilbert spaces to the corresponding equilibrium strategy. Our main result is a statistical guarantee: an NO trained on a small number of randomly sampled rules reliably solves unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO parameters needed remains controlled under appropriate rule sampling during training.\n  Our guarantee follows from three results: (i) local-Lipschitz estimates for the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation theorem using NOs with a prespecified Lipschitz regularity (unlike traditional NO results where the NO's Lipschitz constant can diverge as the approximation error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz learners in infinite dimensions, directly applicable as the Lipschitz constants of our approximating NOs are controlled in (ii).",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种使用神经算子（NOs）同时求解无限多个线性二次平均场博弈（LQ MFG）的方法，通过训练NOs学习从问题数据（动态和成本泛函）到均衡策略的映射。主要贡献包括：统计保证（在少量随机采样规则上训练的NO能可靠解决未见过的LQ MFG变体）、局部Lipschitz估计、具有预设Lipschitz正则性的通用逼近定理，以及新的样本复杂度分析。该研究主要针对无限维希尔伯特空间中的理论问题，侧重于算法泛化性和数学保证，而非直接的市场应用或交易策略设计。",
    "fetch_date": "2026-01-25",
    "id": "20260125_66784c24"
  },
  {
    "title": "An Empirical study on Mutual fund factor-risk-shifting and its intensity on Indian Equity Mutual funds",
    "url": "https://arxiv.org/pdf/2510.19619v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "Investment style groups investment approaches to predict portfolio return variations. This study examines the relationship between investment style, style consistency, and risk-adjusted returns of Indian equity mutual funds. The methodology involves estimating size and style beta coefficients, identifying breakpoints, analysing investment styles, and assessing risk-shifting intensity. Funds transition across styles over time, reflecting rotation, drift, or strengthening trends. Many Mid Blend funds remain in the same category, while others shift to Large Blend or Mid Value, indicating value-oriented strategies or large-cap exposure. Some funds adopt high-return styles like Small Value and Small Blend, aiming for alpha through small-cap equities. Performance changes following risk structure shifts are analyzed by comparing pre- and post-shift metrics, showing that style adjustments can enhance returns based on market conditions. This study contributes to mutual fund evaluation literature by highlighting the impact of style transitions on returns.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究对印度股票型共同基金的投资风格、风格一致性及风险调整后收益之间的关系进行了实证分析。方法包括估计规模与风格贝塔系数、识别断点、分析投资风格及评估风险转移强度。研究发现基金风格会随时间推移发生转换，反映轮动、漂移或趋势强化现象。许多中盘混合型基金保持原类别，而部分转向大盘混合型或中盘价值型，表明价值导向策略或大盘股敞口。部分基金采用小盘价值型和小盘混合型等高收益风格，旨在通过小盘股获取阿尔法。通过比较风格转换前后的绩效指标，分析风险结构变化后的表现，显示风格调整可根据市场条件提升收益。本研究通过强调风格转换对收益的影响，为共同基金评估文献做出贡献。",
    "fetch_date": "2026-01-25",
    "id": "20260125_5a7704eb"
  },
  {
    "title": "Compensation-based risk-sharing",
    "url": "https://arxiv.org/pdf/2510.19511v3",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "This paper studies the mathematical problem of allocating payouts (compensations) in an endowment contingency fund using a risk-sharing rule that satisfies full allocation. Besides the participants, an administrator manages the fund by collecting ex-ante contributions to establish the fund and distributing ex-post payouts to members. Two types of administrators are considered. An 'active' administrator both invests in the fund and receives the payout of the fund when no participant receives a payout. A 'passive' administrator performs only administrative tasks and neither invests in nor receives a payout from the fund. We analyze the actuarial fairness of both compensation-based risk-sharing schemes and provide general conditions under which fairness is achieved. The results extend earlier work by Denuit and Robert (2025) and Dhaene and Milevsky (2024), who focused on payouts based on Bernoulli distributions, by allowing for general non-negative loss distributions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一种基于补偿的风险分担机制，用于在捐赠型应急基金中分配赔付（补偿），该机制满足全额分配原则。除参与者外，由一名管理员负责管理基金：收集事前缴费以建立基金，并在事后向成员分配赔付。研究考虑了两种类型的管理员：'主动'型管理员既向基金投资，也在无参与者获得赔付时接收基金赔付；'被动'型管理员仅执行管理任务，既不投资也不从基金接收赔付。文章分析了两种基于补偿的风险分担方案的精算公平性，并给出了实现公平的一般条件。该结果扩展了Denuit与Robert（2025）以及Dhaene与Milevsky（2024）先前的工作（他们主要关注基于伯努利分布的赔付），允许更一般的非负损失分布。",
    "fetch_date": "2026-01-25",
    "id": "20260125_baceddc6"
  },
  {
    "title": "Overprocurement of balancing capacity may increase the welfare in the cross-zonal energy-reserve coallocation problem",
    "url": "https://arxiv.org/pdf/2511.01877v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "When the traded energy and reserve products between zones are co-allocated to optimize the infrastructure usage, both deterministic and stochastic flows have to be accounted for on interconnector lines. We focus on allocation models, which guarantee deliverability in the context of the portfolio bidding European day-ahead market framework, assuming a flow-based description of network constraints. In such models, as each unit of allocated reserve supply implies additional cost, it is straightforward to assume that the amount of allocated reserve is equal to the accepted reserve demand quantity. However, as it is illustrated by the proposed work, overprocurement of reserves may imply counterintuitive benefits. Reserve supplies not used for balancing may be used for congestion management, thus allowing valuable additional flows in the network.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在跨区域能源与备用容量协同分配问题中，当采用基于潮流的网络约束描述时，论文研究表明超额采购备用容量可能带来反直觉的福利提升。未用于平衡的备用容量可用于缓解网络阻塞，从而允许更有价值的额外电力传输。该研究基于欧洲日前市场框架下的组合投标机制，重点探讨了保证可交付性的分配模型。",
    "fetch_date": "2026-01-25",
    "id": "20260125_8bcb87c9"
  },
  {
    "title": "Quantum Machine Learning methods for Fourier-based distribution estimation with application in option pricing",
    "url": "https://arxiv.org/pdf/2510.19494v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "The ongoing progress in quantum technologies has fueled a sustained exploration of their potential applications across various domains. One particularly promising field is quantitative finance, where a central challenge is the pricing of financial derivatives-traditionally addressed through Monte Carlo integration techniques. In this work, we introduce two hybrid classical-quantum methods to address the option pricing problem. These approaches rely on reconstructing Fourier series representations of statistical distributions from the outputs of Quantum Machine Learning (QML) models based on Parametrized Quantum Circuits (PQCs). We analyze the impact of data size and PQC dimensionality on performance. Quantum Accelerated Monte Carlo (QAMC) is employed as a benchmark to quantitatively assess the proposed models in terms of computational cost and accuracy in the extraction of Fourier coefficients. Through the numerical experiments, we show that the proposed methods achieve remarkable accuracy, becoming a competitive quantum alternative for derivatives valuation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "量子技术的持续进步推动了对各领域潜在应用的探索。在量化金融领域，金融衍生品定价是一个核心挑战，传统上通过蒙特卡洛积分技术解决。本研究提出了两种混合经典-量子方法来解决期权定价问题。这些方法基于参数化量子电路（PQCs）的量子机器学习（QML）模型输出，重建统计分布的傅里叶级数表示。我们分析了数据规模和PQC维度对性能的影响，并使用量子加速蒙特卡洛（QAMC）作为基准，从计算成本和傅里叶系数提取精度两方面定量评估所提出的模型。数值实验表明，所提出的方法实现了显著的精度，成为衍生品估值的竞争性量子替代方案。",
    "fetch_date": "2026-01-25",
    "id": "20260125_050fda92"
  },
  {
    "title": "A Natural Hedging Framework for Longevity Risk with Graphical Risk Assessment",
    "url": "https://arxiv.org/pdf/2510.18721v1",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "Natural hedging allows life insurers to manage longevity risk internally by offsetting the opposite exposures of life insurance and annuity liabilities. Although many studies have proposed natural hedging strategies under different settings, calibration methods, and mortality models, a unified framework for constructing and evaluating such hedges remains undeveloped. While graphical risk assessment has been explored for index-based longevity hedges, no comparable metric exists for natural hedging. This paper proposes a structured natural hedging framework paired with a graphical risk metric for hedge evaluation. The framework integrates valuation, calibration, and evaluation, while the graphical metric provides intuitive insights into residual dependencies and hedge performance. Applied to multiple hedging scenarios, the proposed methods demonstrate flexibility, interpretability, and practical value for longevity risk management.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个结构化自然对冲框架，结合图形化风险度量用于对冲评估。该框架整合了估值、校准和评估，而图形化度量则直观展示了剩余依赖关系和对冲表现。应用于多种对冲场景，所提方法展示了在长寿风险管理方面的灵活性、可解释性和实用价值。",
    "fetch_date": "2026-01-25",
    "id": "20260125_0c0713a4"
  },
  {
    "title": "Optimal allocations with distortion risk measures and mixed risk attitudes",
    "url": "https://arxiv.org/pdf/2510.18236v1",
    "source": "ArXiv",
    "date": "2025-10-21",
    "abstract": "We study Pareto-optimal risk sharing in economies with heterogeneous attitudes toward risk, where agents' preferences are modeled by distortion risk measures. Building on comonotonic and counter-monotonic improvement results, we show that agents with similar attitudes optimally share risks comonotonically (risk-averse) or counter-monotonically (risk-seeking). We show how the general $n$-agent problem can be reduced to a two-agent formulation between representative risk-averse and risk-seeking agents, characterized by the infimal convolution of their distortion risk measures. Within this two-agent framework, we establish necessary and sufficient conditions for the existence of optimal allocations, and we identify when the infimal convolution yields an unbounded value. When existence fails, we analyze the problem under nonnegative allocation constraints, and we characterize optima explicitly, under piecewise-linear distortion functions and Bernoulli-type risks. Our findings suggest that the optimal allocation structure is governed by the relative strength of risk aversion versus risk seeking behavior, as intuition would suggest.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究在具有异质风险态度的经济体中，代理人偏好由失真风险度量建模时的帕累托最优风险分担问题。基于共单调和反单调改进结果，我们证明具有相似态度的代理人会最优地以共单调（风险厌恶）或反单调（风险寻求）方式分担风险。我们展示了如何将一般的n-代理人问题简化为代表性风险厌恶和风险寻求代理人之间的两代理人表述，其特征是它们失真风险度量的下卷积。在此两代理人框架内，我们建立了最优分配存在的必要和充分条件，并确定了何时下卷积产生无界值。当存在性失败时，我们在非负分配约束下分析问题，并在分段线性失真函数和伯努利型风险下明确表征最优解。我们的发现表明，最优分配结构由风险厌恶与风险寻求行为的相对强度决定，正如直觉所暗示的那样。",
    "fetch_date": "2026-01-25",
    "id": "20260125_c133fc52"
  },
  {
    "title": "Fusing Narrative Semantics for Financial Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2510.20699v1",
    "source": "ArXiv",
    "date": "2025-10-23",
    "abstract": "We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们介绍了M2VN：多模态波动率网络，这是一个基于深度学习的新型金融波动率预测框架，将时间序列特征与非结构化新闻数据统一起来。M2VN利用深度神经网络的表示能力来解决该领域的两个关键挑战：(i) 对齐和融合异构数据模态（数值金融数据和文本信息），以及(ii) 减轻可能损害金融模型有效性的前瞻性偏差。为实现这一目标，M2VN将开源市场特征与Time Machine GPT（一种最近引入的时点大语言模型）生成的新闻嵌入相结合，确保时间完整性。引入辅助对齐损失以增强深度学习架构中结构化和非结构化数据的整合。大量实验表明，M2VN始终优于现有基线，突显了其在动态市场中风险管理和金融决策方面的实用价值。",
    "fetch_date": "2026-01-24",
    "id": "20260124_02a6663d"
  },
  {
    "title": "Robust Yield Curve Estimation for Mortgage Bonds Using Neural Networks",
    "url": "https://arxiv.org/pdf/2510.21347v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "Robust yield curve estimation is crucial in fixed-income markets for accurate instrument pricing, effective risk management, and informed trading strategies. Traditional approaches, including the bootstrapping method and parametric Nelson-Siegel models, often struggle with overfitting or instability issues, especially when underlying bonds are sparse, bond prices are volatile, or contain hard-to-remove noise. In this paper, we propose a neural networkbased framework for robust yield curve estimation tailored to small mortgage bond markets. Our model estimates the yield curve independently for each day and introduces a new loss function to enforce smoothness and stability, addressing challenges associated with limited and noisy data. Empirical results on Swedish mortgage bonds demonstrate that our approach delivers more robust and stable yield curve estimates compared to existing methods such as Nelson-Siegel-Svensson (NSS) and Kernel-Ridge (KR). Furthermore, the framework allows for the integration of domain-specific constraints, such as alignment with risk-free benchmarks, enabling practitioners to balance the trade-off between smoothness and accuracy according to their needs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "稳健的收益率曲线估计对于固定收益市场中的精确工具定价、有效风险管理和明智交易策略至关重要。传统方法（如自举法和参数化Nelson-Siegel模型）常面临过拟合或不稳定性问题，尤其是在基础债券稀疏、债券价格波动或包含难以去除的噪声时。本文提出了一种基于神经网络的框架，专门用于小型抵押贷款债券市场的稳健收益率曲线估计。该模型独立估计每日收益率曲线，并引入新的损失函数以强制平滑性和稳定性，解决了与有限和噪声数据相关的挑战。在瑞典抵押贷款债券上的实证结果表明，与现有方法（如Nelson-Siegel-Svensson和Kernel-Ridge）相比，我们的方法提供了更稳健和稳定的收益率曲线估计。此外，该框架允许集成特定领域约束（如与无风险基准对齐），使从业者能够根据需求平衡平滑性和准确性之间的权衡。",
    "fetch_date": "2026-01-24",
    "id": "20260124_c5397b1e"
  },
  {
    "title": "Jump risk premia in the presence of clustered jumps",
    "url": "https://arxiv.org/pdf/2510.21297v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "This paper presents an option pricing model that incorporates clustered jumps using a bivariate Hawkes process. The process captures both self- and cross-excitation of positive and negative jumps, enabling the model to generate return dynamics with asymmetric, time-varying skewness and to produce positive or negative implied volatility skews. This feature is especially relevant for assets such as cryptocurrencies, so-called ``meme'' stocks, G-7 currencies, and certain commodities, where implied volatility skews may change sign depending on prevailing sentiment. We introduce two additional parameters, namely the positive and negative jump premia, to model the market risk preferences for positive and negative jumps, inferred from options data. This enables the model to flexibly match observed skew dynamics. Using Bitcoin (BTC) options, we empirically demonstrate how inferred jump risk premia exhibit predictive power for both the cost of carry in BTC futures and the performance of delta-hedged option strategies.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种采用双变量Hawkes过程的期权定价模型，该模型通过捕捉正负跳跃的自激和互激效应，能够生成具有不对称、时变偏度的收益动态，并产生正或负的隐含波动率偏斜。这一特性尤其适用于加密货币、所谓“迷因”股票、G-7货币及某些大宗商品等资产，其隐含波动率偏斜可能随市场情绪变化而改变符号。模型引入了正负跳跃溢价两个额外参数，从期权数据中推断市场对正负跳跃的风险偏好，从而灵活匹配观测到的偏斜动态。基于比特币（BTC）期权的实证研究表明，推断出的跳跃风险溢价对BTC期货的持有成本以及Delta对冲期权策略的表现具有预测能力。",
    "fetch_date": "2026-01-24",
    "id": "20260124_5b1e48dd"
  },
  {
    "title": "Hierarchical AI Multi-Agent Fundamental Investing: Evidence from China's A-Share Market",
    "url": "https://arxiv.org/pdf/2510.21147v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "We present a multi-agent, AI-driven framework for fundamental investing that integrates macro indicators, industry-level and firm-specific information to construct optimized equity portfolios. The architecture comprises: (i) a Macro agent that dynamically screens and weights sectors based on evolving economic indicators and industry performance; (ii) four firm-level agents -- Fundamental, Technical, Report, and News -- that conduct in-depth analyses of individual firms to ensure both breadth and depth of coverage; (iii) a Portfolio agent that uses reinforcement learning to combine the agent outputs into a unified policy to generate the trading strategy; and (iv) a Risk Control agent that adjusts portfolio positions in response to market volatility. We evaluate the system on the constituents by the CSI 300 Index of China's A-share market and find that it consistently outperforms standard benchmarks and a state-of-the-art multi-agent trading system on risk-adjusted returns and drawdown control. Our core contribution is a hierarchical multi-agent design that links top-down macro screening with bottom-up fundamental analysis, offering a robust and extensible approach to factor-based portfolio construction.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于基本面投资的分层多智能体AI框架，整合宏观指标、行业级和公司特定信息以构建优化的股票投资组合。该架构包括：(i) 宏观智能体，基于动态变化的经济指标和行业表现筛选和加权板块；(ii) 四个公司级智能体（基本面、技术面、报告、新闻），对个股进行深入分析以确保覆盖的广度和深度；(iii) 投资组合智能体，使用强化学习将各智能体输出整合为统一策略以生成交易策略；(iv) 风险控制智能体，根据市场波动调整投资组合头寸。该系统在中国A股市场沪深300指数成分股上进行了评估，结果显示其在风险调整后收益和回撤控制方面持续优于标准基准和最先进的多智能体交易系统。核心贡献在于将自上而下的宏观筛选与自下而上的基本面分析相结合的分层多智能体设计，为基于因子的投资组合构建提供了稳健且可扩展的方法。",
    "fetch_date": "2026-01-24",
    "id": "20260124_2aad73cf"
  },
  {
    "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach",
    "url": "https://arxiv.org/pdf/2510.21110v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文针对强化学习在复杂高维环境中从存在未观测混杂偏倚的数据中进行离策略学习的问题，提出了一种基于因果推断的深度强化学习算法。该算法在经典的深度Q网络（DQN）基础上，通过寻找与观测数据兼容的最坏情况环境下的安全策略，增强了对混杂偏倚的鲁棒性。在12个存在混杂的Atari游戏测试中，该方法在行为策略与目标策略输入不匹配且存在未观测混杂因子的场景下，均显著优于标准DQN。",
    "fetch_date": "2026-01-24",
    "id": "20260124_03dad136"
  },
  {
    "title": "Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach",
    "url": "https://arxiv.org/pdf/2510.20235v1",
    "source": "ArXiv",
    "date": "2025-10-23",
    "abstract": "In this paper, we propose a provably convergent and practical framework for multi-objective reinforcement learning with max-min criterion. From a game-theoretic perspective, we reformulate max-min multi-objective reinforcement learning as a two-player zero-sum regularized continuous game and introduce an efficient algorithm based on mirror descent. Our approach simplifies the policy update while ensuring global last-iterate convergence. We provide a comprehensive theoretical analysis on our algorithm, including iteration complexity under both exact and approximate policy evaluations, as well as sample complexity bounds. To further enhance performance, we modify the proposed algorithm with adaptive regularization. Our experiments demonstrate the convergence behavior of the proposed algorithm in tabular settings, and our implementation for deep reinforcement learning significantly outperforms previous baselines in many MORL environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于博弈论视角的可证明收敛且实用的多目标强化学习框架，采用最大最小准则。该方法将多目标强化学习重构为两人零和正则化连续博弈，并引入基于镜像下降的高效算法，简化策略更新同时确保全局最后迭代收敛。论文提供了完整的理论分析，包括精确和近似策略评估下的迭代复杂度以及样本复杂度界限，并通过自适应正则化进一步优化性能。实验表明，该算法在表格设置中展现收敛行为，在深度强化学习实现中显著超越先前基准，适用于多种MORL环境。",
    "fetch_date": "2026-01-24",
    "id": "20260124_dff5929a"
  },
  {
    "title": "An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring",
    "url": "https://arxiv.org/pdf/2601.15304v1",
    "source": "ArXiv",
    "date": "2026-01-10",
    "abstract": "Market integrity monitoring is difficult because suspicious price/volume behavior can arise from many benign mechanisms, while modern detection systems often rely on opaque models that are hard to audit and communicate. We present AIMM-X, an explainable monitoring pipeline that combines market microstructure-style signals derived from OHLCV time series with multi-source public attention signals (e.g., news and online discussion proxies) to surface time windows that merit analyst review. The system detects candidate anomalous windows using transparent thresholding and aggregation, then assigns an interpretable integrity score decomposed into a small set of additive components, allowing practitioners to trace why a window was flagged and which factors drove the score. We provide an end-to-end, reproducible implementation that downloads data, constructs attention features, builds unified panels, detects windows, computes component signals, and generates summary figures/tables. Our goal is not to label manipulation, but to provide a practical, auditable screening tool that supports downstream investigation by compliance teams, exchanges, or researchers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "《一种可解释的市场完整性监控系统：融合多源注意力信号与透明评分》提出AIMM-X系统，将市场微观结构信号（源自OHLCV时间序列）与多源公众注意力信号（如新闻和在线讨论代理）相结合，筛选出值得分析师审查的时间窗口。该系统通过透明阈值和聚合检测候选异常窗口，并分配可解释的完整性评分，该评分分解为少量可加性成分，使从业者能够追溯标记原因及驱动因素。该工具旨在提供实用、可审计的筛查支持，而非直接判定操纵行为，适用于合规团队、交易所或研究人员的下游调查。",
    "fetch_date": "2026-01-24",
    "id": "20260124_6f3e8f21"
  },
  {
    "title": "Portfolio selection with exogenous and endogenous transaction costs under a two-factor stochastic volatility model",
    "url": "https://arxiv.org/pdf/2510.21156v2",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "In this paper, we investigate a portfolio selection problem with transaction costs under a two-factor stochastic volatility structure, where volatility follows a mean-reverting process with a stochastic mean-reversion level. The model incorporates both proportional exogenous transaction costs and endogenous costs modeled by a stochastic liquidity risk process. Using an option-implied approach, we extract an S-shaped utility function that reflects investor behavior and apply its concave envelope transformation to handle the non-concavity. The resulting problem reduces to solving a five-dimensional nonlinear Hamilton-Jacobi-Bellman equation. We employ a deep learning-based policy iteration scheme to numerically compute the value function and the optimal policy. Numerical experiments are conducted to analyze how both types of transaction costs and stochastic volatility affect optimal investment decisions.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究在双因子随机波动率模型下的投资组合选择问题，该模型中波动率遵循具有随机均值回复水平的均值回复过程。模型同时纳入了比例外生交易成本和由随机流动性风险过程建模的内生成本。采用期权隐含方法提取反映投资者行为的S形效用函数，并应用其凹包络变换处理非凹性。最终问题简化为求解一个五维非线性Hamilton-Jacobi-Bellman方程。作者采用基于深度学习的策略迭代方案数值计算价值函数和最优策略，并通过数值实验分析两类交易成本及随机波动率对最优投资决策的影响。",
    "fetch_date": "2026-01-24",
    "id": "20260124_8ebf4295"
  },
  {
    "title": "Market-Implied Sustainability: Insights from Funds' Portfolio Holdings",
    "url": "https://arxiv.org/pdf/2510.20434v1",
    "source": "ArXiv",
    "date": "2025-10-23",
    "abstract": "In this work, we aim to develop a market-implied sustainability score for companies, based on the extent to which a stock is over- or under-represented in sustainable funds compared to traditional ones. To identify sustainable funds, we rely on the Sustainable Finance Disclosure Regulation (SFDR), a European framework designed to clearly categorize investment funds into different classes according to their commitment to sustainability. In our analysis, we classify as sustainable those funds categorized as Article 9 - also known as \"dark green\" - and compare them to funds categorized as Article 8 or Article 6.\n  We compute an SFDR Market-Implied Sustainability (SMIS) score for a large set of European companies. We then conduct an econometric analysis to identify the factors influencing SMIS and compare them with state-of-the-art ESG (Environmental, Social, and Governance) scores provided by Refinitiv. Finally, we assess the realized risk-adjusted performance of stocks using portfolio-tilting strategies.\n  Our results show that SMIS scores deviate substantially from traditional ESG scores and that, over the period 2010-2023, companies with high SMIS have been associated with significant financial outperformance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于市场隐含的可持续性评分方法（SMIS），通过比较可持续基金与传统基金的持仓差异来评估公司的可持续性表现。研究利用欧盟《可持续金融信息披露条例》（SFDR）对基金进行分类，将第9条（深绿）基金视为可持续基金，并与第8条或第6条基金进行对比。通过计算大量欧洲公司的SMIS评分，并进行计量经济学分析，研究发现SMIS评分与传统ESG评分存在显著差异，且在2010-2023年期间，高SMIS评分的公司表现出显著的风险调整后收益。该研究为量化交易提供了基于市场数据的可持续性因子，可用于构建投资组合倾斜策略，具有实战应用潜力。",
    "fetch_date": "2026-01-24",
    "id": "20260124_da6a95b8"
  },
  {
    "title": "FinCARE: Financial Causal Analysis with Reasoning and Evidence",
    "url": "https://arxiv.org/pdf/2510.20221v1",
    "source": "ArXiv",
    "date": "2025-10-23",
    "abstract": "Portfolio managers rely on correlation-based analysis and heuristic methods that fail to capture true causal relationships driving performance. We present a hybrid framework that integrates statistical causal discovery algorithms with domain knowledge from two complementary sources: a financial knowledge graph extracted from SEC 10-K filings and large language model reasoning. Our approach systematically enhances three representative causal discovery paradigms, constraint-based (PC), score-based (GES), and continuous optimization (NOTEARS), by encoding knowledge graph constraints algorithmically and leveraging LLM conceptual reasoning for hypothesis generation. Evaluated on a synthetic financial dataset of 500 firms across 18 variables, our KG+LLM-enhanced methods demonstrate consistent improvements across all three algorithms: PC (F1: 0.622 vs. 0.459 baseline, +36%), GES (F1: 0.735 vs. 0.367, +100%), and NOTEARS (F1: 0.759 vs. 0.163, +366%). The framework enables reliable scenario analysis with mean absolute error of 0.003610 for counterfactual predictions and perfect directional accuracy for intervention effects. It also addresses critical limitations of existing methods by grounding statistical discoveries in financial domain expertise while maintaining empirical validation, providing portfolio managers with the causal foundation necessary for proactive risk management and strategic decision-making in dynamic market environments.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "FinCARE：一种结合金融因果分析与推理证据的混合框架。该方法通过整合统计因果发现算法与两个互补领域的知识——从SEC 10-K文件中提取的金融知识图谱和大语言模型推理——来增强三种代表性因果发现范式（基于约束的PC、基于评分的GES和连续优化NOTEARS）。通过在合成金融数据集（500家公司，18个变量）上的评估，KG+LLM增强方法在所有三种算法中均表现出持续改进：PC（F1：0.622 vs. 0.459基准，+36%）、GES（F1：0.735 vs. 0.367，+100%）和NOTEARS（F1：0.759 vs. 0.163，+366%）。该框架支持可靠的场景分析，反事实预测的平均绝对误差为0.003610，干预效应的方向准确性完美。",
    "fetch_date": "2026-01-24",
    "id": "20260124_7d5f3fd2"
  },
  {
    "title": "Goal-based portfolio selection with fixed transaction costs",
    "url": "https://arxiv.org/pdf/2510.21650v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "We study a goal-based portfolio selection problem in which an investor aims to meet multiple financial goals, each with a specific deadline and target amount. Trading the stock incurs a strictly positive transaction cost. Using the stochastic Perron's method, we show that the value function is the unique viscosity solution to a system of quasi-variational inequalities. The existence of an optimal trading strategy and goal funding scheme is established. Numerical results reveal complex optimal trading regions and show that the optimal investment strategy differs substantially from the V-shaped strategy observed in the frictionless case.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究基于目标的投资组合选择问题，投资者需在特定截止日期前达成多个财务目标，且股票交易会产生严格正的固定交易成本。运用随机Perron方法，证明价值函数是拟变分不等式系统的唯一粘性解，并确立了最优交易策略和目标融资方案的存在性。数值结果显示复杂的最优交易区域，且最优投资策略与无摩擦情况下的V形策略存在显著差异。",
    "fetch_date": "2026-01-24",
    "id": "20260124_9d79adfa"
  },
  {
    "title": "Personalized Chain-of-Thought Summarization of Financial News for Investor Decision Support",
    "url": "https://arxiv.org/pdf/2511.05508v2",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "Financial advisors and investors struggle with information overload from financial news, where irrelevant content and noise obscure key market signals and hinder timely investment decisions. To address this, we propose a novel Chain-of-Thought (CoT) summarization framework that condenses financial news into concise, event-driven summaries. The framework integrates user-specified keywords to generate personalized outputs, ensuring that only the most relevant contexts are highlighted. These personalized summaries provide an intermediate layer that supports language models in producing investor-focused narratives, bridging the gap between raw news and actionable insights.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "金融顾问和投资者面临金融新闻信息过载的问题，无关内容和噪音掩盖了关键市场信号，阻碍了及时的投资决策。为此，我们提出了一种新颖的思维链（Chain-of-Thought, CoT）摘要框架，将金融新闻压缩为简洁、事件驱动的摘要。该框架整合了用户指定的关键词，以生成个性化输出，确保仅突出最相关的上下文。这些个性化摘要提供了一个中间层，支持语言模型生成以投资者为中心的叙述，弥合原始新闻与可操作见解之间的差距。",
    "fetch_date": "2026-01-24",
    "id": "20260124_bd5f5173"
  },
  {
    "title": "The local Gaussian correlation networks among return tails in the Chinese stock market",
    "url": "https://arxiv.org/pdf/2510.21165v1",
    "source": "ArXiv",
    "date": "2025-10-24",
    "abstract": "Financial networks based on Pearson correlations have been intensively studied. However, previous studies may have led to misleading and catastrophic results because of several critical shortcomings of the Pearson correlation. The local Gaussian correlation coefficient, a new measurement of statistical dependence between variables, has unique advantages including capturing local nonlinear dependence and handling heavy-tailed distributions. This study constructs financial networks using the local Gaussian correlation coefficients between tail regions of stock returns in the Shanghai Stock Exchange. The work systematically analyzes fundamental network metrics including node centrality, average shortest path length, and entropy. Compared with the local Gaussian correlation network among positive tails and the conventional Pearson correlation network, the properties of the local Gaussian correlation network among negative tails are more sensitive to the stock market risks. This finding suggests researchers should prioritize the local Gaussian correlation network among negative tails. Future work should reevaluate existing findings using the local Gaussian correlation method.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "基于皮尔逊相关性的金融网络研究广泛，但其存在关键缺陷可能导致误导性结果。局部高斯相关系数作为一种新的统计依赖度量，具有捕捉局部非线性依赖和处理厚尾分布的独特优势。本研究使用上海证券交易所股票收益率尾部区域的局部高斯相关系数构建金融网络，系统分析了节点中心性、平均最短路径长度和熵等基本网络指标。与正尾部局部高斯相关网络和传统皮尔逊相关网络相比，负尾部局部高斯相关网络的性质对股市风险更为敏感。这一发现表明，研究者应优先关注负尾部局部高斯相关网络。未来工作应使用局部高斯相关方法重新评估现有发现。",
    "fetch_date": "2026-01-24",
    "id": "20260124_a0afce20"
  },
  {
    "title": "Consumption-Investment Problem in Rank-Based Models",
    "url": "https://arxiv.org/pdf/2510.20763v1",
    "source": "ArXiv",
    "date": "2025-10-23",
    "abstract": "We study a consumption-investment problem in a multi-asset market where the returns follow a generic rank-based model. Our main result derives an HJB equation with Neumann boundary conditions for the value function and proves a corresponding verification theorem. The control problem is nonstandard due to the discontinuous nature of the coefficients in rank-based models, requiring a bespoke approach of independent mathematical interest. The special case of first-order models, prescribing constant drift and diffusion coefficients for the ranked returns, admits explicit solutions when the investor is either (a) unconstrained, (b) abides by open market constraints or (c) is fully invested in the market. The explicit optimal strategies in all cases are related to the celebrated solution to Merton's problem, despite the intractability of constraint (b) in that setting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了多资产市场中收益遵循通用排序模型的消费-投资问题。主要成果是推导了价值函数的带诺伊曼边界条件的HJB方程，并证明了相应的验证定理。由于排序模型中系数的不连续性，该控制问题具有非标准特性，需要独立数学意义的定制方法。在一阶模型（规定排序收益具有恒定漂移和扩散系数）的特殊情况下，当投资者（a）无约束、（b）遵守开放市场约束或（c）完全投资于市场时，可获得显式解。所有情况下的显式最优策略均与默顿问题的著名解相关，尽管在该设定中约束（b）通常难以处理。",
    "fetch_date": "2026-01-24",
    "id": "20260124_83c9058d"
  },
  {
    "title": "Multivariate Variance Swap Using Generalized Variance Method for Stochastic Volatility models",
    "url": "https://arxiv.org/pdf/2510.20047v1",
    "source": "ArXiv",
    "date": "2025-10-22",
    "abstract": "This paper develops a novel framework for modeling the variance swap of multi-asset portfolios by employing the generalized variance approach, which utilizes the determinant of the covariance matrix of the underlying assets. By specifying the distribution of the log returns of the underlying assets under the Heston and Barndorff-Nielsen and Shephard (BNS) stochastic volatility frameworks, we derive closed-form solutions for the realized variance through the computation of the covariance generalization of multi-assets. To evaluate the robustness of the proposed model, we conduct simulations using nine different assets generated via the quantmod package. For a three-asset portfolio, analytical expressions for the multivariate variance swap are obtained under both the Heston and BNS models. Numerical experiments further demonstrate the effectiveness of the proposed model through parameter testing, calibration, and validation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于广义方差方法的多资产组合方差互换建模新框架，该方法利用标的资产协方差矩阵的行列式。通过在Heston和Barndorff-Nielsen与Shephard（BNS）随机波动率框架下指定标的资产对数收益的分布，我们通过计算多资产协方差广义化，推导出实现方差的闭式解。为评估所提模型的稳健性，我们使用quantmod包生成的九种不同资产进行模拟。对于三资产组合，在Heston和BNS模型下均获得了多元方差互换的解析表达式。数值实验通过参数测试、校准和验证进一步证明了所提模型的有效性。",
    "fetch_date": "2026-01-24",
    "id": "20260124_76747fc6"
  },
  {
    "title": "Causal and Predictive Modeling of Short-Horizon Market Risk and Systematic Alpha Generation Using Hybrid Machine Learning Ensembles",
    "url": "https://arxiv.org/pdf/2510.22348v1",
    "source": "ArXiv",
    "date": "2025-10-25",
    "abstract": "We present a systematic trading framework that forecasts short-horizon market risk, identifies its underlying drivers, and generates alpha using a hybrid machine learning ensemble built to trade on the resulting signal. The framework integrates neural networks with tree-based voting models to predict five-day drawdowns in the S&P 500 ETF, leveraging a cross-asset feature set spanning equities, fixed income, foreign exchange, commodities, and volatility markets. Interpretable feature attribution methods reveal the key macroeconomic and microstructural factors that differentiate high-risk (crash) from benign (non-crash) weekly regimes. Empirical results show a Sharpe ratio of 2.51 and an annualized CAPM alpha of +0.28, with a market beta of 0.51, indicating that the model delivers substantial systematic alpha with limited directional exposure during the 2005--2025 backtest period. Overall, the findings underscore the effectiveness of hybrid ensemble architectures in capturing nonlinear risk dynamics and identifying interpretable, potentially causal drivers, providing a robust blueprint for machine learning-driven alpha generation in systematic trading.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文提出了一种用于实战交易的系统性框架，该框架通过混合机器学习集成模型预测短期市场风险、识别其根本驱动因素，并基于生成的信号产生阿尔法。该模型整合了神经网络与基于树的投票模型，利用跨资产（股票、固定收益、外汇、大宗商品、波动率市场）特征集来预测标普500 ETF的五日回撤。通过可解释的特征归因方法，揭示了区分高风险（崩盘）与低风险（非崩盘）周度区制的关键宏观与微观结构因素。实证结果显示，在2005-2025年回测期内，该模型实现了2.51的夏普比率、年化+0.28的CAPM阿尔法以及0.51的市场贝塔，表明其在有限方向性暴露下产生了显著的系统性阿尔法。总体而言，研究结果强调了混合集成架构在捕捉非线性风险动态和识别可解释、潜在因果驱动因素方面的有效性，为系统性交易中机器学习驱动的阿尔法生成提供了稳健的蓝图。",
    "fetch_date": "2026-01-23",
    "id": "20260123_c641345c"
  },
  {
    "title": "Revisiting the Structure of Trend Premia: When Diversification Hides Redundancy",
    "url": "https://arxiv.org/pdf/2510.23150v2",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "Recent work has emphasized the diversification benefits of combining trend signals across multiple horizons, with the medium-term window-typically six months to one year-long viewed as the \"sweet spot\" of trend-following. This paper revisits this conventional view by reallocating exposure dynamically across horizons using a Bayesian optimization framework designed to learn the optimal weights assigned to each trend horizon at the asset level. The common practice of equal weighting implicitly assumes that all assets benefit equally from all horizons; we show that this assumption is both theoretically and empirically suboptimal. We first optimize the horizon-level weights at the asset level to maximize the informativeness of trend signals before applying Bayesian graphical models-with sparsity and turnover control-to allocate dynamically across assets. The key finding is that the medium-term band contributes little incremental performance or diversification once short- and long-term components are included. Removing the 125-day layer improves Sharpe ratios and drawdown efficiency while maintaining benchmark correlation. We then rationalize this outcome through a minimum-variance formulation, showing that the medium-term horizon largely overlaps with its neighboring horizons. The resulting \"barbell\" structure-combining short- and long-term trends-captures most of the performance while reducing model complexity. This result challenges the common belief that more horizons always improve diversification and suggests that some forms of time-scale diversification may conceal unnecessary redundancy in trend premia.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视了趋势溢价的结构，挑战了传统观点——即认为中期（通常为6个月至1年）是趋势跟踪的“最佳区间”。通过采用贝叶斯优化框架，在资产层面动态调整不同时间跨度的暴露，研究发现，等权重分配假设所有资产均等地受益于所有时间跨度，这在理论和实证上均非最优。关键发现是：一旦包含短期和长期成分，中期区间对增量绩效或分散化的贡献微乎其微。移除125天层可提升夏普比率和回撤效率，同时保持基准相关性。研究通过最小方差形式合理化这一结果，并应用贝叶斯图模型（具有稀疏性和换手率控制）在资产间进行动态配置。",
    "fetch_date": "2026-01-23",
    "id": "20260123_6203c40d"
  },
  {
    "title": "Right Place, Right Time: Market Simulation-based RL for Execution Optimisation",
    "url": "https://arxiv.org/pdf/2510.22206v1",
    "source": "ArXiv",
    "date": "2025-10-25",
    "abstract": "Execution algorithms are vital to modern trading, they enable market participants to execute large orders while minimising market impact and transaction costs. As these algorithms grow more sophisticated, optimising them becomes increasingly challenging. In this work, we present a reinforcement learning (RL) framework for discovering optimal execution strategies, evaluated within a reactive agent-based market simulator. This simulator creates reactive order flow and allows us to decompose slippage into its constituent components: market impact and execution risk. We assess the RL agent's performance using the efficient frontier based on work by Almgren and Chriss, measuring its ability to balance risk and cost. Results show that the RL-derived strategies consistently outperform baselines and operate near the efficient frontier, demonstrating a strong ability to optimise for risk and impact. These findings highlight the potential of reinforcement learning as a powerful tool in the trader's toolkit.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《Right Place, Right Time: Market Simulation-based RL for Execution Optimisation》提出了一种基于强化学习（RL）的执行优化框架，通过反应式代理市场模拟器评估最优执行策略。该模拟器能生成反应式订单流，并将滑点分解为市场冲击和执行风险两部分。研究使用Almgren和Chriss提出的有效前沿评估RL代理性能，结果显示RL衍生的策略持续优于基线，并在有效前沿附近运行，展现出优化风险和冲击的强能力。这些发现突显了强化学习作为交易工具的强大潜力。",
    "fetch_date": "2026-01-23",
    "id": "20260123_09e96edc"
  },
  {
    "title": "Beyond Carr Madan: A Projection Approach to Risk-Neutral Moment Estimation",
    "url": "https://arxiv.org/pdf/2601.14852v1",
    "source": "ArXiv",
    "date": "2026-01-21",
    "abstract": "We propose a projection method to estimate risk-neutral moments from option prices. We derive a finite-sample bound implying that the projection estimator attains (up to a constant) the smallest pricing error within the span of traded option payoffs. This finite-sample optimality is not available for the widely used Carr--Madan approximation. Simulations show sizable accuracy gains for key quantities such as VIX and SVIX. We then extend the framework to multiple underlyings, deriving necessary and sufficient conditions under which simple options complete the market in higher dimensions, and providing estimators for joint moments. In our empirical application, we recover risk-neutral correlations and joint tail risk from FX options alone, addressing a longstanding measurement problem raised by Ross (1976). Our joint tail-risk measure predicts future joint currency crashes and identifies periods in which currency portfolios are particularly useful for hedging.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于投影法的风险中性矩估计方法，直接从期权价格中提取信息。该方法在有限样本下具有最优定价误差界，优于广泛使用的Carr-Madan近似法，在VIX和SVIX等关键指标上显示出显著的精度提升。研究还将框架扩展到多资产情形，推导了简单期权在高维市场中完备性的充要条件，并提供了联合矩估计器。实证应用中，仅使用外汇期权即可恢复风险中性相关性和联合尾部风险，解决了Ross（1976）提出的长期测量问题。该联合尾部风险指标能够预测未来联合货币崩盘，并识别货币投资组合对冲效果特别显著的时期。",
    "fetch_date": "2026-01-23",
    "id": "20260123_ee535131"
  },
  {
    "title": "Deviations from Tradition: Stylized Facts in the Era of DeFi",
    "url": "https://arxiv.org/pdf/2510.22834v1",
    "source": "ArXiv",
    "date": "2025-10-26",
    "abstract": "Decentralized Exchanges (DEXs) are now a significant component of the financial world where billions of dollars are traded daily. Differently from traditional markets, which are typically based on Limit Order Books, DEXs typically work as Automated Market Makers, and, since the implementation of Uniswap v3, feature concentrated liquidity. By investigating the twenty-four most active pools in Uniswap v3 during 2023 and 2024, we empirically study how this structural change in the organization of the markets modifies the well-studied stylized facts of prices, liquidity, and order flow observed in traditional markets. We find a series of new statistical regularities in the distributions and cross-autocorrelation functions of these variables that we are able to associate either with the market structure (e.g., the execution of orders in blocks) or with the intense activity of Maximal Extractable Value searchers, such as Just-in-Time liquidity providers and sandwich attackers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "《偏离传统：DeFi时代的典型化事实》一文通过实证研究2023-2024年Uniswap v3上24个最活跃的流动性池，分析了去中心化交易所（DEXs）基于自动做市商（AMM）和集中流动性（concentrated liquidity）的市场结构如何改变传统市场中价格、流动性和订单流的典型化事实（stylized facts）。研究发现了一系列与市场结构（如订单块执行）及最大可提取价值（MEV）搜索者（如即时流动性提供者和三明治攻击者）活动相关的新统计规律。该研究对实战交易的价值在于：为量化策略开发提供了DeFi市场特有的统计特征基础，揭示了MEV活动对市场微观结构的影响，有助于构建针对AMM和集中流动性的alpha因子或风险模型。",
    "fetch_date": "2026-01-23",
    "id": "20260123_11750f95"
  },
  {
    "title": "Understanding Carbon Trade Dynamics: A European Union Emissions Trading System Perspective",
    "url": "https://arxiv.org/pdf/2510.22341v1",
    "source": "ArXiv",
    "date": "2025-10-25",
    "abstract": "The European Union Emissions Trading System (EU ETS), the worlds largest cap-and-trade carbon market, is central to EU climate policy. This study analyzes its efficiency, price behavior, and market structure from 2010 to 2020. Using an AR-GARCH framework, we find pronounced price clustering and short-term return predictability, with 60.05 percent directional accuracy and a 70.78 percent hit rate within forecast intervals. Network analysis of inter-country transactions shows a concentrated structure dominated by a few registries that control most high-value flows. Country-specific log-log regressions of price on traded quantity reveal heterogeneous and sometimes positive elasticities exceeding unity, implying that trading volumes often rise with prices. These results point to persistent inefficiencies in the EU ETS, including partial predictability, asymmetric market power, and unconventional price-volume relationships, suggesting that while the system contributes to decarbonization, its trading dynamics and price formation remain imperfect.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究从欧盟排放交易体系（EU ETS）视角分析碳交易动态，这是全球最大的限额与交易碳市场。研究分析了2010年至2020年间的市场效率、价格行为和市场结构。采用AR-GARCH框架发现显著的价格聚类和短期收益可预测性，方向准确性达60.05%，预测区间内命中率为70.78%。国家间交易的网络分析显示市场结构集中，少数注册机构控制大部分高价值流动。特定国家的价格与交易量对数回归显示异质性弹性，有时超过1，表明交易量常随价格上涨。这些结果指向EU ETS中持续存在的低效率，包括部分可预测性、不对称市场力量和非传统价量关系，暗示虽然系统有助于脱碳，但其交易动态和价格形成仍不完善。",
    "fetch_date": "2026-01-23",
    "id": "20260123_9e87199a"
  },
  {
    "title": "An uncertainty-aware physics-informed neural network solution for the Black-Scholes equation: a novel framework for option pricing",
    "url": "https://arxiv.org/pdf/2511.05519v1",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "We present an uncertainty-aware, physics-informed neural network (PINN) for option pricing that solves the Black--Scholes (BS) partial differential equation (PDE) as a mesh-free, global surrogate over $(S,t)$. The model embeds the BS operator and boundary/terminal conditions in a residual-based objective and requires no labeled prices. For American options, early exercise is handled via an obstacle-style relaxation while retaining the BS residual in the continuation region. To quantify \\emph{epistemic} uncertainty, we introduce an anchored-ensemble fine-tuning stage (AT--PINN) that regularizes each model toward a sampled anchor and yields prediction bands alongside point estimates. On European calls/puts, the approach attains low errors (e.g., MAE $\\sim 5\\times10^{-2}$, RMSE $\\sim 7\\times10^{-2}$, explained variance $\\approx 0.999$ in representative settings) and tracks ground truth closely across strikes and maturities. For American puts, the method remains accurate (MAE/RMSE on the order of $10^{-1}$ with EV $\\approx 0.999$) and does not exhibit the error accumulation associated with time-marching schemes. Against data-driven baselines (ANN, RNN) and a Kolmogorov--Arnold FINN variant (KAN), our PINN matches or outperforms on accuracy while training more stably; anchored ensembles provide uncertainty bands that align with observed error scales. We discuss design choices (loss balancing, sampling near the payoff kink), limitations, and extensions to higher-dimensional BS settings and alternative dynamics.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种不确定性感知的物理信息神经网络（PINN）用于期权定价，通过求解Black-Scholes偏微分方程作为无网格全局代理模型。该方法将BS算子和边界/终端条件嵌入基于残差的目标函数中，无需标注价格数据。对于美式期权，通过障碍式松弛处理提前行权，同时在继续区域保留BS残差。为量化认知不确定性，引入了锚定集成微调阶段（AT-PINN），使每个模型向采样锚点正则化，并提供预测区间和点估计。在欧洲看涨/看跌期权上，该方法实现了低误差（如MAE约5×10^-2，RMSE约7×10^-2，解释方差约0.999），并在不同行权价和到期日上紧密跟踪真实值。对于美式看跌期权，方法保持准确性（MAE/RMSE约10^-1，EV约0.999），且未出现时间推进方案相关的误差累积。",
    "fetch_date": "2026-01-23",
    "id": "20260123_9f68c927"
  },
  {
    "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics",
    "url": "https://arxiv.org/pdf/2510.22158v1",
    "source": "ArXiv",
    "date": "2025-10-25",
    "abstract": "Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新颖的深度强化学习（DRL）算法，专门用于解决非平稳连续均值场博弈（MFG）。该方法基于虚构博弈（FP）框架，利用DRL计算最优响应，并通过监督学习表示平均策略。此外，它使用条件归一化流学习时间依赖的种群分布表示。通过在三个复杂度递增的示例上验证，该方法在可扩展性和密度近似方面取得显著进展，推动了DRL在复杂MFG问题中的应用，使其更接近现实世界的多智能体系统。",
    "fetch_date": "2026-01-23",
    "id": "20260123_9de9d7f0"
  },
  {
    "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding",
    "url": "https://arxiv.org/pdf/2601.15131v1",
    "source": "ArXiv",
    "date": "2026-01-21",
    "abstract": "In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究有限时间范围内的车辆路径规划问题，目标是最大化有限时间内服务的客户请求数量。提出了一种新颖的路由网络嵌入模块，该模块创建局部节点嵌入向量和上下文感知的全局图表示。将有限剩余时间纳入网络嵌入模块，为嵌入模块提供适当的路径规划上下文。将该嵌入模块与基于策略梯度的深度强化学习框架集成，以解决有限时间范围内的车辆路径规划问题。在真实世界路由网络和合成欧几里得网络上进行了训练和验证，实验结果表明该方法比现有路由方法实现了更高的客户服务率，且求解时间显著降低。",
    "fetch_date": "2026-01-23",
    "id": "20260123_7d94bf8a"
  },
  {
    "title": "The Limits of Lognormal: Assessing Cryptocurrency Volatility and VaR using Geometric Brownian Motion",
    "url": "https://arxiv.org/pdf/2601.14272v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "The integration of cryptocurrencies into institutional portfolios necessitates the adoption of robust risk modeling frameworks. This study is a part of a series of subsequent works to fine-tune model risk analysis for cryptocurrencies. Through this first research work, we establish a foundational benchmark by applying the traditional industry-standard Geometric Brownian Motion (GBM) model. Popularly used for non-crypto financial assets, GBM assumes Lognormal return distributions for a multi-asset cryptocurrency portfolio (XRP, SOL, ADA). This work utilizes Maximum Likelihood Estimation and a correlated Monte Carlo Simulation incorporating the Cholesky decomposition of historical covariance. We present our stock portfolio model as a Minimum Variance Portfolio (MVP). We observe the model's structural shift within the heavy-tailed, non-Gaussian cryptocurrency environment. The results reveal limitations of the Lognormal assumption: the calculated Value-at-Risk at the 5% confidence level over the one-year horizon. For baselining our results, we also present a holistic comparative analysis with an equity portfolio (AAPL, TSLA, NVDA), demonstrating a significantly lower failure rate. This performance provides conclusive evidence that the GBM model is fundamentally the perfect benchmark for our subsequent works. Results from this novel work will be an indicator for the success criteria in our future model for crypto risk management, rigorously motivating the development and application of advanced models.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "将传统行业标准几何布朗运动（GBM）模型应用于加密货币投资组合（XRP、SOL、ADA），作为风险建模的基准。该模型假设对数正态收益分布，并采用最大似然估计和基于历史协方差Cholesky分解的蒙特卡洛模拟。研究发现，在加密货币的重尾、非高斯环境中，对数正态假设存在局限性，导致在5%置信水平下计算的一年期风险价值（VaR）表现不佳。通过与股票投资组合（AAPL、TSLA、NVDA）的对比分析，进一步突显了该模型在加密货币领域的适用性不足。",
    "fetch_date": "2026-01-23",
    "id": "20260123_6d56face"
  },
  {
    "title": "The Breadth Premium: Measuring the Firm-level Impact of CEO Career Breadth",
    "url": "https://arxiv.org/pdf/2511.05515v1",
    "source": "ArXiv",
    "date": "2025-10-26",
    "abstract": "Prevailing career and education systems continue to reward early specialization and deep expertise within narrow domains. While such depth promotes efficiency, it may also limit adaptability in complex and rapidly changing environments. Building on research showing that variability in training inputs enhances learning outcomes across cognitive and behavioral domains, this study explores whether the same principle applies to executive performance.\n  Using an original dataset of 650 CEOs leading firms that together represent roughly 85% of US market capitalization, we construct a composite Breadth Index capturing cross-domain educational and professional breadth. Preliminary analyses reveal that firms led by higher-breadth CEOs outperform their industry peers by an average of 9.8 percentage points over a three-year window. Regression results indicate that each one-point increase on the five-point Range Index corresponds to a 1.8-point gain in abnormal returns (p < 0.03), with effects remaining robust across industries, firm sizes, and CEO age groups.\n  These early findings suggest that leadership breadth, defined as experience spanning multiple functions, disciplines, and sectors, is positively associated with firm-level performance. While the dataset remains under validation, the pattern observed supports the emerging view that as specialization deepens, the marginal value of lateral insight rises. Breadth, in this light, functions as a form of adaptive capital; it enhances leaders' capacity for integrative reasoning, organizational translation, and strategic flexibility in uncertain environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "标题：广度溢价：衡量CEO职业广度对公司层面的影响。摘要：主流职业与教育体系仍奖励早期专业化与狭窄领域的精深专长。虽然这种深度能提升效率，但也可能限制在复杂快速变化环境中的适应能力。本研究基于显示训练输入的变异性能提升认知与行为领域学习成果的研究，探讨这一原则是否适用于高管绩效。通过使用包含650位CEO的原创数据集（这些CEO领导的公司合计约占美国市值的85%），我们构建了一个综合广度指数，以捕捉跨领域的教育与职业广度。初步分析显示，由广度较高的CEO领导的公司，在三年窗口期内平均比行业同行表现高出9.8个百分点。回归结果表明，在五分制广度指数上每增加一分，异常回报相应增加1.8个百分点（p < 0.03），且该效应在不同行业、公司规模和CEO年龄组中保持稳健。这些早期发现表明，领导力广度（定义为跨越多个职能、学科和部门的经验）可能是一种被低估的战略资产。",
    "fetch_date": "2026-01-23",
    "id": "20260123_9f15ebed"
  },
  {
    "title": "TABL-ABM: A Hybrid Framework for Synthetic LOB Generation",
    "url": "https://arxiv.org/pdf/2510.22685v1",
    "source": "ArXiv",
    "date": "2025-10-26",
    "abstract": "The recent application of deep learning models to financial trading has heightened the need for high fidelity financial time series data. This synthetic data can be used to supplement historical data to train large trading models. The state-of-the-art models for the generative application often rely on huge amounts of historical data and large, complicated models. These models range from autoregressive and diffusion-based models through to architecturally simpler models such as the temporal-attention bilinear layer. Agent-based approaches to modelling limit order book dynamics can also recreate trading activity through mechanistic models of trader behaviours. In this work, we demonstrate how a popular agent-based framework for simulating intraday trading activity, the Chiarella model, can be combined with one of the most performant deep learning models for forecasting multi-variate time series, the TABL model. This forecasting model is coupled to a simulation of a matching engine with a novel method for simulating deleted order flow. Our simulator gives us the ability to test the generative abilities of the forecasting model using stylised facts. Our results show that this methodology generates realistic price dynamics however, when analysing deeper, parts of the markets microstructure are not accurately recreated, highlighting the necessity for including more sophisticated agent behaviors into the modeling framework to help account for tail events.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种名为TABL-ABM的混合框架，用于合成限价订单簿（LOB）数据生成。该框架将基于代理的Chiarella模型（模拟日内交易活动）与TABL深度学习模型（用于预测多变量时间序列）相结合，并耦合了匹配引擎模拟及一种新颖的模拟删除订单流的方法。该模拟器能够使用程式化事实测试预测模型的生成能力。论文主要关注于生成高保真合成数据以补充历史数据，用于训练大型交易模型，属于方法论研究，对实战交易的价值主要体现在数据生成和模型测试层面，而非直接提供交易策略或Alpha信号。",
    "fetch_date": "2026-01-23",
    "id": "20260123_6c3ccac7"
  },
  {
    "title": "SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions",
    "url": "https://arxiv.org/pdf/2510.22568v1",
    "source": "ArXiv",
    "date": "2025-10-26",
    "abstract": "This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for Learning), a novel approach for training autonomous drones in multi-agent racing competitions. SPIRAL distinctively employs a self-play mechanism to incrementally cultivate complex racing behaviors within a challenging, dynamic environment. Through this self-play core, drones continuously compete against increasingly proficient versions of themselves, naturally escalating the difficulty of competitive interactions. This progressive learning journey guides agents from mastering fundamental flight control to executing sophisticated cooperative multi-drone racing strategies. Our method is designed for versatility, allowing integration with any state-of-the-art Deep Reinforcement Learning (DRL) algorithms within its self-play framework. Simulations demonstrate the significant advantages of SPIRAL and benchmark the performance of various DRL algorithms operating within it. Consequently, we contribute a versatile, scalable, and self-improving learning framework to the field of autonomous drone racing. SPIRAL's capacity to autonomously generate appropriate and escalating challenges through its self-play dynamic offers a promising direction for developing robust and adaptive racing strategies in multi-agent environments. This research opens new avenues for enhancing the performance and reliability of autonomous racing drones in increasingly complex and competitive scenarios.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出SPIRAL（自我博弈增量赛车算法），一种用于训练多智能体无人机竞赛中自主无人机的新方法。SPIRAL独特地采用自我博弈机制，在具有挑战性的动态环境中逐步培养复杂的赛车行为。通过这一核心机制，无人机持续与日益熟练的自身版本竞争，自然地提升竞争互动的难度。这一渐进式学习过程引导智能体从掌握基本飞行控制发展到执行复杂的协作多无人机赛车策略。该方法设计灵活，允许在其自我博弈框架内集成任何最先进的深度强化学习算法。仿真实验展示了SPIRAL的显著优势，并对其中运行的各种DRL算法进行了性能基准测试。因此，我们为自主无人机赛车领域贡献了一个通用、可扩展且自我改进的学习框架。SPIRAL通过其自我博弈动态自主生成适当且逐步升级挑战的能力。",
    "fetch_date": "2026-01-23",
    "id": "20260123_9a1291f0"
  },
  {
    "title": "General Equilibrium Amplification and Crisis Vulnerability: Cross-Crisis Evidence from Global Banks",
    "url": "https://arxiv.org/pdf/2510.24775v2",
    "source": "ArXiv",
    "date": "2025-10-25",
    "abstract": "This paper develops a continuous framework for analyzing financial contagion that incorporates both geographic proximity and interbank network linkages. The framework characterizes stress propagation through a master equation whose solution admits a Feynman-Kac representation as expected cumulative stress along stochastic paths through spatial-network space. From this representation, I derive the General Equilibrium Amplification Factor -- a structural measure of systemic importance that captures the ratio of total system-wide effects to direct effects following a localized shock. The amplification factor decomposes naturally into spatial, network, and interaction components, revealing which transmission channels contribute most to each institution's systemic importance. The framework nests discrete cascade models as a limiting case when jump intensity becomes infinite above default thresholds, clarifying that continuous and discrete approaches describe different regimes of the same phenomenon. Empirical validation using 38 global banks across the 2008 financial crisis and COVID-19 pandemic demonstrates that the amplification factor correctly identifies systemically important institutions (Pearson correlation $ρ= -0.450$, $p = 0.080$ between amplification factor and crisis drawdowns) and predicts crisis outcomes out-of-sample ($ρ= -0.352$ for COVID-19). Robustness analysis using cumulative abnormal returns -- a measure more directly connected to the Feynman-Kac integral -- strengthens these findings ($ρ= -0.512$, $p = 0.042$). Time-series analysis confirms that average pairwise bank correlations track macroeconomic stress indicators ($ρ= 0.265$ with VIX, $p < 0.001$). Comparing the two crises reveals that COVID-19 produced a sharper correlation spike (+93%) despite smaller equity losses, reflecting different contagion dynamics for exogenous versus endogenous shocks.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文开发了一个连续框架来分析金融传染，该框架结合了地理邻近性和银行间网络联系。该框架通过主方程描述压力传播，其解采用费曼-卡茨表示作为通过空间-网络空间的随机路径上的预期累积压力。由此表示，作者推导出一般均衡放大因子——一种系统性重要性的结构性度量，捕捉局部冲击后系统范围总效应与直接效应的比率。放大因子自然分解为空间、网络和相互作用分量，揭示哪些传播渠道对每个机构的系统性重要性贡献最大。当跳跃强度在违约阈值以上变为无限时，该框架将离散级联模型作为极限情况嵌套，阐明连续和离散方法描述同一现象的不同机制。使用2008年金融危机和COVID-19大流行期间的38家全球银行进行实证验证表明，放大因子正确识别了系统重要性机构。",
    "fetch_date": "2026-01-23",
    "id": "20260123_d41b445c"
  },
  {
    "title": "Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations",
    "url": "https://arxiv.org/pdf/2601.12839v1",
    "source": "ArXiv",
    "date": "2026-01-19",
    "abstract": "Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文提出了一种名为关系领域逻辑集成（RDLI）的框架，用于解决去中心化加密货币网络中异常检测面临的极端标签稀缺（0.01%）和复杂洗钱模式（如资金分散、分层）的挑战。RDLI将专家启发式知识嵌入为可微分的逻辑感知潜在信号，增强了图神经网络（GNN）对多跳、逻辑驱动模式的学习能力。此外，论文还引入了检索式上下文（RGC）模块，结合监管和宏观经济背景调整异常评分，减少因市场波动导致的误报。在极端标签稀缺条件下，RDLI的F1分数比现有GNN基准模型提高了28.9%。该研究对实战交易中的反洗钱监控、风险管理和合规性具有直接应用价值。",
    "fetch_date": "2026-01-22",
    "id": "20260122_3cab7c3f"
  },
  {
    "title": "Autonomous Market Intelligence: Agentic AI Nowcasting Predicts Stock Returns",
    "url": "https://arxiv.org/pdf/2601.11958v1",
    "source": "ArXiv",
    "date": "2026-01-17",
    "abstract": "Can fully agentic AI nowcast stock returns? We deploy a state-of-the-art Large Language Model to evaluate the attractiveness of each Russell 1000 stock daily, starting from April 2025 when AI web interfaces enabled real-time search. Our data contribution is unique along three dimensions. First, the nowcasting framework is completely out-of-sample and free of look-ahead bias by construction: predictions are collected at the current edge of time, ensuring the AI has no knowledge of future outcomes. Second, this temporal design is irreproducible -- once the information environment passes, it can never be recreated. Third, our framework is 100% agentic: we do not feed the model news, disclosures, or curated text; it autonomously searches the web, filters sources, and synthesises information into quantitative predictions. We find that AI possesses genuine stock selection ability, but only for identifying top winners. Longing the 20 highest-ranked stocks generates a daily Fama-French five-factor plus momentum alpha of 18.4 basis points and an annualised Sharpe ratio of 2.43. Critically, these returns derive from an implementable strategy trading highly liquid Russell 1000 constituents, with transaction costs representing less than 10\\% of gross alpha. However, this predictability is highly concentrated: expanding beyond the top tier rapidly dilutes alpha, and bottom-ranked stocks exhibit returns statistically indistinguishable from the market. We hypothesise that this asymmetry reflects online information structure: genuinely positive news generates coherent signals, while negative news is contaminated by strategic corporate obfuscation and social media noise.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "论文《自主市场情报：智能体AI即时预测股票收益》提出了一种完全自主的AI即时预测框架，用于评估罗素1000指数成分股的吸引力。该框架具有三个独特优势：完全样本外且无前瞻性偏差的预测设计、不可复现的时间窗口、100%自主的信息获取与处理能力。研究发现，AI在识别顶级赢家股票方面具有真实选股能力，做多排名前20的股票可获得每日18.4个基点的五因子加动量超额收益和2.43的年化夏普比率。这些收益来自对高流动性罗素1000成分股的可执行交易策略，对实战交易具有显著价值。",
    "fetch_date": "2026-01-22",
    "id": "20260122_9fb33dba"
  },
  {
    "title": "Leveraged positions on decentralized lending platforms",
    "url": "https://arxiv.org/pdf/2601.14005v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "We develop a mathematical framework to optimize leveraged staking (\"loopy\") strategies in Decentralized Finance (DeFi), in which a staked asset is supplied as collateral, the underlying is borrowed and re-staked, and the loop can be repeated across multiple lending markets. Exploiting the fact that DeFi borrow rates are deterministic functions of pool utilization, we reduce the multi-market problem to a convex allocation over market exposures and obtain closed-form solutions under three interest-rate models: linear, kinked, and adaptive (Morpho's AdaptiveCurveIRM). The framework incorporates market-specific leverage limits, utilization-dependent borrowing costs, and transaction fees. Backtests on the Ethereum and Base blockchains using the largest Morpho wstETH/WETH markets (from January 1 to April 1, 2025) show that rebalanced leveraged positions can reach up to 6.2% APY versus 3.1% for unleveraged staking, with strong dependence on position size and rebalancing frequency. Our results provide a mathematical basis for transparent, automated DeFi portfolio optimization.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文开发了一个数学框架，用于优化去中心化金融（DeFi）中的杠杆质押（“循环”）策略，其中质押资产作为抵押品，借入底层资产并重新质押，该循环可在多个借贷市场中重复。利用DeFi借款利率是资金池利用率的确定性函数这一事实，我们将多市场问题简化为市场敞口的凸分配问题，并在三种利率模型（线性、拐点和自适应（Morpho的AdaptiveCurveIRM））下获得闭式解。该框架结合了市场特定的杠杆限制、利用率依赖的借款成本和交易费用。在以太坊和Base区块链上使用最大的Morpho wstETH/WETH市场（从2025年1月1日至4月1日）进行的回测显示，再平衡的杠杆头寸年化收益率可达6.2%，而非杠杆质押为3.1%，且对头寸规模和再平衡频率有强依赖性。我们的结果为透明、自动化的DeFi投资组合优化提供了数学基础。",
    "fetch_date": "2026-01-22",
    "id": "20260122_8d0c7743"
  },
  {
    "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance",
    "url": "https://arxiv.org/pdf/2601.13770v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文介绍了Look-Ahead-Bench，这是一个用于衡量金融工作流中Point-in-Time（PiT）大语言模型（LLMs）前瞻性偏差的标准化基准。与主要通过问答测试内部前瞻知识的现有方法不同，该基准评估模型在实际场景中的行为。为区分真实预测能力与基于记忆的性能，我们分析了不同时间市场制度下的性能衰减，并引入多个量化基线以建立性能阈值。我们评估了主流开源LLMs（Llama 3.1的8B和70B版本、DeepSeek 3.2）与PiT-Inference的Point-in-Time LLMs系列（Pitinf-Small、Pitinf-Medium及前沿模型Pitinf-Large）。结果显示，标准LLMs存在显著的前瞻性偏差（通过alpha衰减衡量），而Pitinf模型则随着规模扩大展现出更好的泛化与推理能力。这项工作为金融LLMs时间偏差的标准化评估奠定了基础，并为识别适合实际部署的模型提供了实用框架。",
    "fetch_date": "2026-01-22",
    "id": "20260122_9b482fc1"
  },
  {
    "title": "Beyond Visual Realism: Toward Reliable Financial Time Series Generation",
    "url": "https://arxiv.org/pdf/2601.12990v1",
    "source": "ArXiv",
    "date": "2026-01-19",
    "abstract": "Generative models for financial time series often create data that look realistic and even reproduce stylized facts such as fat tails or volatility clustering. However, these apparent successes break down under trading backtests: models like GANs or WGAN-GP frequently collapse, yielding extreme and unrealistic results that make the synthetic data unusable in practice. We identify the root cause in the neglect of financial asymmetry and rare tail events, which strongly affect market risk but are often overlooked by objectives focusing on distribution matching. To address this, we introduce the Stylized Facts Alignment GAN (SFAG), which converts key stylized facts into differentiable structural constraints and jointly optimizes them with adversarial loss. This multi-constraint design ensures that generated series remain aligned with market dynamics not only in plots but also in backtesting. Experiments on the Shanghai Composite Index (2004--2024) show that while baseline GANs produce unstable and implausible trading outcomes, SFAG generates synthetic data that preserve stylized facts and support robust momentum strategy performance. Our results highlight that structure-preserving objectives are essential to bridge the gap between superficial realism and practical usability in financial generative modeling.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《超越视觉真实：迈向可靠的金融时间序列生成》指出，现有生成模型（如GANs、WGAN-GP）虽能生成视觉上逼真且符合厚尾、波动聚集等典型事实的金融时间序列，但在实战回测中常因忽略金融不对称性和罕见尾部事件而失效，导致合成数据不实用。为此，作者提出Stylized Facts Alignment GAN（SFAG），将关键典型事实转化为可微的结构约束，并与对抗损失联合优化，确保生成序列在回测中仍与市场动态保持一致。基于上证综指（2004-2024）的实验表明，SFAG生成的合成数据能保持典型事实并支持稳健的动量策略表现，突显了结构保留目标对实战交易的价值。",
    "fetch_date": "2026-01-22",
    "id": "20260122_69b7dcda"
  },
  {
    "title": "SANOS Smooth strictly Arbitrage-free Non-parametric Option Surfaces",
    "url": "https://arxiv.org/pdf/2601.11209v2",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "We present a simple, numerically efficient but highly flexible non-parametric method to construct representations of option price surfaces which are both smooth and strictly arbitrage-free across time and strike. The method can be viewed as a smooth generalization of the widely-known linear interpolation scheme, and retains the simplicity and transparency of that baseline. Calibration of the model to observed market quotes is formulated as a linear program, allowing bid-ask spreads to be incorporated directly via linear penalties or inequalities, and delivering materially lower computational cost than most of the currently available implied-volatility surface fitting routines. As a further contribution, we derive an equivalent parameterization of the proposed surface in terms of strictly positive \"discrete local volatility\" variables. This yields, to our knowledge, the first construction of smooth, strictly arbitrage-free option price surfaces while requiring only trivial parameter constraints (positivity). We illustrate the approach using S&P 500 index options",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种简单、计算高效且高度灵活的非参数化方法，用于构建在时间和行权价维度上既平滑又严格无套利的期权价格曲面。该方法可视为广泛使用的线性插值方案的平滑推广，保留了基线的简洁性和透明度。模型对市场报价的校准被表述为线性规划问题，允许通过线性惩罚或不等式直接纳入买卖价差，计算成本显著低于当前大多数隐含波动率曲面拟合方法。作为进一步贡献，我们推导了所提出曲面在严格正的“离散局部波动率”变量下的等效参数化。据我们所知，这是首次在仅需简单参数约束（正性）的情况下，构建平滑且严格无套利的期权价格曲面。该方法以标普500指数期权为例进行了说明。",
    "fetch_date": "2026-01-22",
    "id": "20260122_cf225e00"
  },
  {
    "title": "Demystifying the trend of the healthcare index: Is historical price a key driver?",
    "url": "https://arxiv.org/pdf/2601.14062v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "Healthcare sector indices consolidate the economic health of pharmaceutical, biotechnology, and healthcare service firms. The short-term movements in these indices are closely intertwined with capital allocation decisions affecting research and development investment, drug availability, and long-term health outcomes. This research investigates whether historical open-high-low-close (OHLC) index data contain sufficient information for predicting the directional movement of the opening index on the subsequent trading day. The problem is formulated as a supervised classification task involving a one-step-ahead rolling window. A diverse feature set is constructed, comprising original prices, volatility-based technical indicators, and a novel class of nowcasting features derived from mutual OHLC ratios. The framework is evaluated on data from healthcare indices in the U.S. and Indian markets over a five-year period spanning multiple economic phases, including the COVID-19 pandemic. The results demonstrate robust predictive performance, with accuracy exceeding 0.8 and Matthews correlation coefficients above 0.6. Notably, the proposed nowcasting features have emerged as a key determinant of the market movement. We have employed the Shapley-based explainability paradigm to further elucidate the contribution of the features: outcomes reveal the dominant role of the nowcasting features, followed by a more moderate contribution of original prices. This research offers a societal utility: the proposed features and model for short-term forecasting of healthcare indices can reduce information asymmetry and support a more stable and equitable health economy.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该研究探讨了医疗健康指数历史开盘-最高-最低-收盘(OHLC)数据是否包含足够信息来预测下一个交易日的开盘方向。研究将其构建为监督分类任务，采用滚动窗口方法，构建了包含原始价格、基于波动率的技术指标以及从OHLC比率衍生出的新型即时预测特征在内的多样化特征集。在美国和印度市场五年期数据（涵盖多个经济阶段，包括COVID-19疫情）上的评估显示，预测性能稳健，准确率超过0.8，马修斯相关系数高于0.6。研究发现，所提出的即时预测特征是关键决定因素。",
    "fetch_date": "2026-01-22",
    "id": "20260122_fb52ebfb"
  },
  {
    "title": "Spectral Dynamics and Regularization for High-Dimensional Copulas",
    "url": "https://arxiv.org/pdf/2601.13281v1",
    "source": "ArXiv",
    "date": "2026-01-19",
    "abstract": "We introduce a novel model for time-varying, asymmetric, tail-dependent copulas in high dimensions that incorporates both spectral dynamics and regularization. The dynamics of the dependence matrix' eigenvalues are modeled in a score-driven way, while biases in the unconditional eigenvalue spectrum are resolved by non-linear shrinkage. The dynamic parameterization of the copula dependence matrix ensures that it satisfies the appropriate restrictions at all times and for any dimension. The model is parsimonious, computationally efficient, easily scalable to high dimensions, and performs well for both simulated and empirical data. In an empirical application to financial market dynamics using 100 stocks from 10 different countries and 10 different industry sectors, we find that our copula model captures both geographic and industry related co-movements and outperforms recent computationally more intensive clustering-based factor copula alternatives. Both the spectral dynamics and the regularization contribute to the new model's performance. During periods of market stress, we find that the spectral dynamics reveal strong increases in international stock market dependence, which causes reductions in diversification potential and increases in systemic risk.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种新颖的高维时变、非对称、尾部相依Copula模型，融合了谱动态和正则化技术。该模型以得分驱动方式建模相依矩阵特征值的动态变化，并通过非线性收缩解决无条件特征值谱的偏差问题。Copula相依矩阵的动态参数化确保其在任何时间和维度下都满足适当约束。模型具有简约性、计算高效性、易于扩展至高维度的特点，在模拟和实证数据中均表现良好。在针对10个国家、10个不同行业的100只股票进行的金融市场动态实证应用中，该Copula模型能捕捉地理和行业相关的共同变动，并优于近期计算更密集的基于聚类的因子Copula替代方案。谱动态和正则化均对新模型的性能有所贡献。在市场压力期间，谱动态揭示了国际股票市场相依性的显著增强。",
    "fetch_date": "2026-01-22",
    "id": "20260122_aa56e62c"
  },
  {
    "title": "The Physics of Price Discovery: Deconvolving Information, Volatility, and the Critical Breakdown of Signal during Retail Herding",
    "url": "https://arxiv.org/pdf/2601.11602v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "Building on the finding that Market Cap Normalization ($\\SMC$) isolates the ``pure'' directional signal of informed trading \\citep{kang2025}, this paper investigates the physics of how that signal is transmitted -- and how it breaks down. We employ \\textbf{Tikhonov-regularized deconvolution} to recover the impulse response kernels of investor flows, revealing a dual-channel market structure: Foreign and Institutional investors act as ``architects'' of price discovery (positive permanent impact), while Individual investors act as liquidity providers (negative total impact). However, using \\textbf{Multivariate Hawkes Processes}, we demonstrate that this structure is fragile. We find that individual investor order flow exhibits near-critical self-excitation (Branching Ratio $\\approx$ 0.998). During periods of high retail herding, the market undergoes a \\textbf{phase transition} into a ``critical state.'' In this regime, the signal-to-noise ratio collapses, causing the price impact of sophisticated investors to reverse from positive to negative. These findings suggest that retail contagion acts as a physical barrier that temporarily disables efficient price discovery.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "基于市场市值归一化（$\\SMC$）可分离知情交易“纯粹”方向信号的发现，本研究运用Tikhonov正则化解卷积技术，揭示了投资者资金流的脉冲响应核，呈现双通道市场结构：外资与机构投资者充当价格发现的“建筑师”（具有正向永久冲击），而个人投资者则扮演流动性提供者角色（具有负向总冲击）。然而，通过多元Hawkes过程分析，我们发现该结构具有脆弱性。个人投资者订单流表现出近乎临界的自激励特性（分支比≈0.998）。在散户羊群行为高涨期间，市场会经历相变进入“临界状态”。在此状态下，信噪比崩溃，导致成熟投资者的价格冲击由正转负。这些发现表明，散户传染效应如同物理屏障，会暂时破坏有效的价格发现机制。",
    "fetch_date": "2026-01-22",
    "id": "20260122_b6dcedfc"
  },
  {
    "title": "Log-optimality with small liability stream",
    "url": "https://arxiv.org/pdf/2601.14139v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "In an incomplete financial market with general continuous semimartingale dynamics; we model an investor with log-utility preferences who, in addition to an initial capital, receives units of a non-traded endowment process. Using duality techniques, we derive the fourth-order expansion of the primal value function with respect to the units $ε$, held in the non-traded endowment. In turn, this lays the foundation for expanding the optimal wealth process, in this context, up to second order w.r.t. $ε$. The key processes underpinning the aforementioned results are given in terms of Kunita-Watanabe projections, mirroring the case of lower order expansions of similar nature. Both the case of finite and infinite horizons are treated in a unified manner.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在不完全金融市场中，具有对数效用偏好的投资者除了初始资本外，还持有非交易禀赋过程。利用对偶技术，推导了价值函数关于禀赋持有量ε的四阶展开，进而得到最优财富过程的二阶展开。关键过程以Kunita-Watanabe投影表示，统一处理了有限和无限时间范围的情况。",
    "fetch_date": "2026-01-22",
    "id": "20260122_2e05095a"
  },
  {
    "title": "Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2601.13657v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于深度强化学习（DRL）的控制器，用于无人机（UAV）群在通信受限环境中的集体导航。该系统采用隐式领导者-跟随者框架，仅领导者拥有目标信息，跟随者仅通过机载LiDAR感知学习策略，无需通信或识别领导者。核心DRL控制器在GPU加速的Nvidia Isaac Sim中训练，使跟随者仅通过局部感知学习复杂涌现行为（如集群与避障），实现鲁棒的隐式跟随。",
    "fetch_date": "2026-01-22",
    "id": "20260122_f3783156"
  },
  {
    "title": "LQ Mean Field Games with Common Noise in Hilbert Spaces: Small and Arbitrary Finite Time Horizons",
    "url": "https://arxiv.org/pdf/2601.13493v1",
    "source": "ArXiv",
    "date": "2026-01-20",
    "abstract": "We extend the results of (Liu and Firoozi, 2025), which develops the theory of linear-quadratic (LQ) mean field games in Hilbert spaces, by incorporating a common noise. This common noise is an infinite-dimensional Wiener process affecting the dynamics of all agents. In the presence of common noise, the mean-field consistency condition is characterized by a system of coupled forward-backward stochastic evolution equations (FBSEEs) in Hilbert spaces, whereas in its absence, it is represented by forward-backward deterministic evolution equations. We establish the existence and uniqueness of solutions to the coupled linear FBSEEs associated with the LQ MFG setting for small time horizons and prove the $ε$-Nash property of the resulting equilibrium strategy. Furthermore, for the first time in the literature, we develop an analysis that establishes the well-posedness of these coupled linear FBSEEs in Hilbert spaces, for which only mild solutions exist, over arbitrary finite time horizons.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在希尔伯特空间中，将线性二次平均场博弈理论扩展至包含共同噪声的情形。该共同噪声是一个影响所有智能体动态的无限维维纳过程。在存在共同噪声时，平均场一致性条件由希尔伯特空间中的耦合前向-后向随机演化方程系统表征，而无共同噪声时则由确定性演化方程表示。作者证明了在小时间范围内，与LQ MFG设定相关的耦合线性FBSEEs解的存在唯一性，并证明了所得均衡策略的ε-纳什性质。此外，首次在文献中建立了这些耦合线性FBSEEs在任意有限时间范围内的适定性分析，其中仅存在温和解。",
    "fetch_date": "2026-01-22",
    "id": "20260122_2c4356ac"
  },
  {
    "title": "Market Making and Transient Impact in Spot FX",
    "url": "https://arxiv.org/pdf/2601.13421v1",
    "source": "ArXiv",
    "date": "2026-01-19",
    "abstract": "Dealers in foreign exchange markets provide bid and ask prices to their clients at which they are happy to buy and sell, respectively. To manage risk, dealers can skew their quotes and hedge in the interbank market. Hedging offers certainty but comes with transaction costs and market impact. Optimal market making with execution has previously been addressed within the Almgren-Chriss market impact model, which includes instantaneous and permanent components. However, there is overwhelming empirical evidence of the transient nature of market impact, with instantaneous and permanent impacts arising as the two limiting cases. In this note, we consider an intermediate scenario and study the interplay between risk management and impact resilience.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "外汇市场的做市商向客户提供买卖报价，并通过调整报价和在银行间市场对冲来管理风险。对冲提供确定性但伴随交易成本和市场冲击。先前的最优做市执行问题在Almgren-Chriss市场冲击模型中解决，该模型包含瞬时和永久冲击成分。然而，大量实证证据表明市场冲击具有瞬态性质，瞬时和永久冲击是两种极限情况。本文考虑中间情景，研究风险管理和冲击恢复之间的相互作用。",
    "fetch_date": "2026-01-22",
    "id": "20260122_b103d782"
  },
  {
    "title": "Latent Variable Phillips Curve",
    "url": "https://arxiv.org/pdf/2601.11601v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "This paper re-examines the empirical Phillips curve (PC) model and its usefulness in the context of medium-term inflation forecasting. A latent variable Phillips curve hypothesis is formulated and tested using 3,968 randomly generated factor combinations. Evidence from US core PCE inflation between Q1 1983 and Q1 2025 suggests that latent variable PC models reliably outperform traditional PC models six to eight quarters ahead and stand a greater chance of outperforming a univariate benchmark. Incorporating an MA(1) residual process improves the accuracy of empirical PC models across the board, although the gains relative to univariate models remain small. The findings presented in this paper have two important implications: First, they corroborate a new conceptual view on the Phillips curve theory; second, they offer a novel path towards improving the competitiveness of Phillips curve forecasts in future empirical work.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文重新审视了菲利普斯曲线（PC）模型及其在中期通胀预测中的实用性。通过提出并测试基于3,968个随机生成因子组合的潜在变量菲利普斯曲线假设，基于1983年第一季度至2025年第一季度美国核心PCE通胀数据的证据表明，潜在变量PC模型在六至八个季度前的预测中可靠地优于传统PC模型，且更有可能超越单变量基准模型。引入MA(1)残差过程可普遍提升实证PC模型的准确性，但相对于单变量模型的改进幅度仍较小。本文的研究结果具有两个重要启示：一是为菲利普斯曲线理论提供了新的概念视角；二是为未来实证研究中提升菲利普斯曲线预测的竞争力提供了新路径。",
    "fetch_date": "2026-01-22",
    "id": "20260122_bbd12baa"
  },
  {
    "title": "Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management",
    "url": "https://arxiv.org/pdf/2510.26217v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QAOA (HO-QAOA) on binding sub-QUBOs (subset size n <= 16, order k <= 4) to coordinate multi-asset moves across caps and RA-induced discreteness; (iii) a weighted risk-aware objective (Movement, CVaR, funding-priced overshoot) with an explicit coverage window U <= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility and gaps, including a U-cap pre-check that reports the minimal feasible buffer B*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain couplings that defeat local swaps. On government bond datasets and multi-CSA inputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%, and 10.7% across representative harnesses, delivering better cost-movement-tail frontiers under governance settings. We release governance grade artifacts-span citations, valuation matrix audit, weight provenance, QUBO manifests, and CP-SAT traces-to make results auditable and reproducible.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于ISDA信用支持附件（CSA）下金融原生抵押品优化的可认证混合流程。该流程包含：（i）基于证据门控的大语言模型（LLM），用于将CSA条款提取为标准化JSON格式（默认弃权、跨引用）；（ii）量子启发式探索器，在绑定子QUBO（子集大小n≤16，阶数k≤4）上交错使用模拟退火与微高阶量子近似优化算法（HO-QAOA），以协调跨上限和RA引起的离散性的多资产移动；（iii）加权风险感知目标函数（考虑移动成本、条件风险价值CVaR、资金定价的超调），并设定明确的覆盖窗口U≤Reff+B；（iv）使用CP-SAT作为单一仲裁器，以验证可行性和差距，包括报告最小可行缓冲B*的上限预检查。通过将上限/舍入编码为高阶项，HO-QAOA能够针对破坏局部交换的领域耦合进行优化。在政府债券数据集和多CSA输入上，该混合方法在代表性测试中分别比强经典基线（BL-3）提升了9.1%、9.6%和10.7%，实现了更好的性能。",
    "fetch_date": "2026-01-21",
    "id": "20260121_bb2915ce"
  },
  {
    "title": "Learning to Manage Investment Portfolios beyond Simple Utility Functions",
    "url": "https://arxiv.org/pdf/2510.26165v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.\n  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.\n  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as \"growth\" and \"value,\" while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.\n  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.\n  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种生成式框架，用于学习基金经理策略的潜在表示，无需显式效用函数设定。该方法直接建模基金投资组合权重的条件概率，给定股票特征、历史收益、先前权重及代表基金策略的潜在变量。基于GAN的架构直接从观察到的持仓与市场数据的联合分布中学习，无需指定奖励或标记专家目标。在1436只美国股票共同基金数据集上验证，学习到的表示成功捕捉了已知投资风格（如“成长型”和“价值型”），并揭示了隐含的经理目标。对实战交易的价值在于：提供了一种无监督学习基金经理复杂策略的方法，可能用于策略模仿、风格分析或生成新的投资组合配置，超越了传统的风险-收益权衡模型。",
    "fetch_date": "2026-01-21",
    "id": "20260121_ffc4d8d1"
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion",
    "url": "https://arxiv.org/pdf/2510.25929v1",
    "source": "ArXiv",
    "date": "2025-10-29",
    "abstract": "Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question.\n  We propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns.\n  Experimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一个用于研究做市算法合谋的分层多智能体强化学习框架。框架包含一个在由对手塑造的不确定环境中训练的自利做市商（Agent A），以及三个底层竞争者：自利的Agent B1（目标为最大化自身盈亏）、竞争的Agent B2（目标为最小化对手盈亏）和混合型Agent B*（可在前两者行为间调节）。论文通过交互级指标量化行为不对称性和系统级动态，以分析智能体间的相互影响及对市场结果的作用。实验结果表明Agent B2取得了主导性能。该研究对理解多智能体市场交互、检测算法合谋及设计稳健做市策略具有实战参考价值。",
    "fetch_date": "2026-01-21",
    "id": "20260121_33e9e123"
  },
  {
    "title": "Adaptive Multilevel Splitting: First Application to Rare-Event Derivative Pricing",
    "url": "https://arxiv.org/pdf/2510.23461v3",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "This work investigates the computational burden of pricing binary options in rare event regimes and introduces an adaptation of the adaptive multilevel splitting (AMS) method for financial derivatives. Standard Monte Carlo becomes inefficient for deep out-of-the-money binaries due to discontinuous payoffs and extremely small exercise probabilities, requiring prohibitively large sample sizes for accurate estimation. The proposed AMS framework reformulates the rare-event problem as a sequence of conditional events and is applied under both Black-Scholes and Heston dynamics. Numerical experiments cover European, Asian, and up-and-in barrier digital options, together with a multidimensional digital payoff designed as a stress test. Across all contracts, AMS achieves substantial gains, reaching up to 200-fold improvements over standard Monte Carlo, while preserving unbiasedness and showing robust performance with respect to the choice of importance function. To the best of our knowledge, this is the first application of AMS to derivative pricing. An open-source Rcpp implementation is provided, supporting multiple discretisation schemes and alternative importance functions.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了在罕见事件情景下定价二元期权的计算负担，并引入了自适应多级分裂（AMS）方法在金融衍生品定价中的应用。由于不连续支付和极小的行权概率，标准蒙特卡洛方法对于深度虚值二元期权变得低效，需要大量样本才能准确估计。提出的AMS框架将罕见事件问题重新表述为一系列条件事件，并在Black-Scholes和Heston模型下应用。数值实验涵盖了欧式、亚式和向上敲入障碍数字期权，以及一个设计为压力测试的多维数字支付。在所有合约中，AMS实现了显著增益，相比标准蒙特卡洛方法最高可达200倍的改进，同时保持无偏性，并在重要性函数选择方面表现出稳健性能。据我们所知，这是AMS首次应用于衍生品定价。提供了开源的Rcpp实现，支持多种离散化方案和替代重要性函数。",
    "fetch_date": "2026-01-21",
    "id": "20260121_7958f6a6"
  },
  {
    "title": "Causal Deep Q Network",
    "url": "https://arxiv.org/pdf/2510.23424v1",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement learning tasks. However, their reliance on associative learning often leads to the acquisition of spurious correlations, hindering their problem-solving capabilities. In this paper, we introduce a novel approach to integrate causal principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational Causal Effect) formula for estimating causal effects. By incorporating causal reasoning during training, our proposed framework enhances the DQN's understanding of the underlying causal structure of the environment, thereby mitigating the influence of confounding factors and spurious correlations. We demonstrate that integrating DQNs with causal capabilities significantly enhances their problem-solving capabilities without compromising performance. Experimental results on standard benchmark environments showcase that our approach outperforms conventional DQNs, highlighting the effectiveness of causal reasoning in reinforcement learning. Overall, our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "深度Q网络（DQN）在强化学习中表现出色，但其依赖关联学习常导致虚假相关性，影响问题解决能力。本文提出一种将因果原理集成到DQN中的新方法，利用PEACE（概率易变分因果效应）公式估计因果效应。通过在训练中融入因果推理，该框架增强了对环境潜在因果结构的理解，从而减少混杂因素和虚假相关性的影响。实验表明，因果DQN在标准基准环境中优于传统DQN，突显了因果推理在强化学习中的有效性。",
    "fetch_date": "2026-01-21",
    "id": "20260121_11c10c50"
  },
  {
    "title": "Estimating cognitive biases with attention-aware inverse planning",
    "url": "https://arxiv.org/pdf/2510.25951v1",
    "source": "ArXiv",
    "date": "2025-10-29",
    "abstract": "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the attention-aware inverse planning problem, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种注意力感知逆向规划方法，用于从人类行为中估计认知偏差。该方法结合了深度强化学习与计算认知建模，并在Waymo开放数据集的实际驾驶场景中进行了验证，展示了其可扩展性。论文虽涉及实际应用验证，但核心仍偏向于理论方法构建与验证，对实战交易的直接应用价值有限。",
    "fetch_date": "2026-01-21",
    "id": "20260121_d6dec2de"
  },
  {
    "title": "Building Trust in Illiquid Markets: an AI-Powered Replication of Private Equity Funds",
    "url": "https://arxiv.org/pdf/2510.23201v1",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "In response to growing demand for resilient and transparent financial instruments, we introduce a novel framework for replicating private equity (PE) performance using liquid, AI-enhanced strategies. Despite historically delivering robust returns, private equity's inherent illiquidity and lack of transparency raise significant concerns regarding investor trust and systemic stability, particularly in periods of heightened market volatility. Our method uses advanced graphical models to decode liquid PE proxies and incorporates asymmetric risk adjustments that emulate private equity's unique performance dynamics. The result is a liquid, scalable solution that aligns closely with traditional quarterly PE benchmarks like Cambridge Associates and Preqin. This approach enhances portfolio resilience and contributes to the ongoing discourse on safe asset innovation, supporting market stability and investor confidence.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "针对非流动性市场中的信任问题，本文提出了一种利用人工智能增强策略复制私募股权基金表现的新框架。该方法通过高级图模型解析流动性私募股权代理指标，并引入非对称风险调整以模拟私募股权的独特绩效动态，最终生成一种与剑桥协会、Preqin等传统季度私募股权基准高度匹配的流动性、可扩展解决方案。该研究旨在提升投资组合韧性，推动安全资产创新讨论，支持市场稳定与投资者信心。",
    "fetch_date": "2026-01-21",
    "id": "20260121_83a76cc4"
  },
  {
    "title": "PEARL: Private Equity Accessibility Reimagined with Liquidity",
    "url": "https://arxiv.org/pdf/2510.23183v1",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "In this work, we introduce PEARL (Private Equity Accessibility Reimagined with Liquidity), an AI-powered framework designed to replicate and decode private equity funds using liquid, cost-effective assets. Relying on previous research methods such as Erik Stafford's single stock selection (Stafford) and Thomson Reuters - Refinitiv's sector approach (TR), our approach incorporates an additional asymmetry to capture the reduced volatility and better performance of private equity funds resulting from sale timing, leverage, and stock improvements through management changes. As a result, our model exhibits a strong correlation with well-established liquid benchmarks such as Stafford and TR, as well as listed private equity firms (Listed PE), while enhancing performance to better align with renowned quarterly private equity benchmarks like Cambridge Associates, Preqin, and Bloomberg Private Equity Fund indices. Empirical findings validate that our two-step approachdecoding liquid daily private equity proxies with a degree of negative return asymmetry outperforms the initial daily proxies and yields performance more consistent with quarterly private equity benchmarks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出PEARL框架，旨在利用流动性好、成本低的资产来复制和解读私募股权基金。该方法基于Stafford的单股选择和Thomson Reuters-Refinitiv的行业方法，通过引入额外的非对称性来捕捉私募股权基金因出售时机、杠杆和管理层变动带来的波动性降低和业绩提升。实证表明，该两步法（解码流动性私募股权代理变量并引入负收益非对称性）优于初始的日度代理变量，其表现更接近季度私募股权基准（如Cambridge Associates、Preqin和Bloomberg私募股权基金指数）。",
    "fetch_date": "2026-01-21",
    "id": "20260121_80a222a3"
  },
  {
    "title": "A mathematical study of the excess growth rate",
    "url": "https://arxiv.org/pdf/2510.25740v1",
    "source": "ArXiv",
    "date": "2025-10-29",
    "abstract": "We study the excess growth rate -- a fundamental logarithmic functional arising in portfolio theory -- from the perspective of information theory. We show that the excess growth rate can be connected to the Rényi and cross entropies, the Helmholtz free energy, L. Campbell's measure of average code length and large deviations. Our main results consist of three axiomatic characterization theorems of the excess growth rate, in terms of (i) the relative entropy, (ii) the gap in Jensen's inequality, and (iii) the logarithmic divergence that generalizes the Bregman divergence. Furthermore, we study maximization of the excess growth rate and compare it with the growth optimal portfolio. Our results not only provide theoretical justifications of the significance of the excess growth rate, but also establish new connections between information theory and quantitative finance.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文从信息论视角研究投资组合理论中的基本对数函数——超额增长率。主要贡献包括：通过相对熵、詹森不等式间隙和推广布雷格曼散度的对数散度三个公理化特征定理刻画超额增长率；建立其与Rényi熵、交叉熵、亥姆霍兹自由能、L. Campbell平均码长测度及大偏差理论的联系；探讨超额增长率最大化并与增长最优投资组合比较。研究为超额增长率的重要性提供理论依据，并建立信息论与量化金融的新联系，但未涉及具体交易策略或实证应用。",
    "fetch_date": "2026-01-21",
    "id": "20260121_46a57b4c"
  },
  {
    "title": "Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures",
    "url": "https://arxiv.org/pdf/2510.24607v2",
    "source": "ArXiv",
    "date": "2025-10-28",
    "abstract": "We introduce Entropy-Guided Multiplicative Updates (EGMU), a convex optimization framework for constructing multi-factor target-exposure portfolios by minimizing Kullback-Leibler divergence from a benchmark under linear factor constraints. We establish feasibility and uniqueness of strictly positive solutions when the benchmark and targets satisfy convex-hull conditions. We derive the dual concave formulation with explicit gradient, Hessian, and sensitivity expressions, and provide two provably convergent solvers: a damped dual Newton method with global convergence and local quadratic rate, and a KL-projection scheme based on iterative proportional fitting and Bregman-Dykstra projections. We further generalize EGMU to handle elastic targets and robust target sets, and introduce a path-following ordinary differential equation for tracing solution trajectories. Stable and scalable implementations are provided using LogSumExp stabilization, covariance regularization, and half-space KL projections. Our focus is on theory and reproducible algorithms; empirical benchmarking is optional.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于熵引导乘性更新（EGMU）的凸优化框架，用于构建多因子目标暴露投资组合，通过在线性因子约束下最小化与基准的Kullback-Leibler散度来实现。在基准与目标满足凸包条件时，证明了严格正解的可行性和唯一性。推导了具有显式梯度、Hessian和敏感性表达式的对偶凹形式，并提供了两种可证明收敛的求解器：具有全局收敛和局部二次收敛速率的阻尼对偶牛顿法，以及基于迭代比例拟合和Bregman-Dykstra投影的KL投影方案。进一步将EGMU推广到处理弹性目标和鲁棒目标集，并引入路径跟踪常微分方程来追踪解轨迹。通过LogSumExp稳定化、协方差正则化和半空间KL投影提供了稳定且可扩展的实现。重点在于理论和可复现算法，实证基准测试为可选。",
    "fetch_date": "2026-01-21",
    "id": "20260121_1d9cd422"
  },
  {
    "title": "The Evolution of Probabilistic Price Forecasting Techniques: A Review of the Day-Ahead, Intra-Day, and Balancing Markets",
    "url": "https://arxiv.org/pdf/2511.05523v1",
    "source": "ArXiv",
    "date": "2025-10-28",
    "abstract": "Electricity price forecasting has become a critical tool for decision-making in energy markets, particularly as the increasing penetration of renewable energy introduces greater volatility and uncertainty. Historically, research in this field has been dominated by point forecasting methods, which provide single-value predictions but fail to quantify uncertainty. However, as power markets evolve due to renewable integration, smart grids, and regulatory changes, the need for probabilistic forecasting has become more pronounced, offering a more comprehensive approach to risk assessment and market participation. This paper presents a review of probabilistic forecasting methods, tracing their evolution from Bayesian and distribution based approaches, through quantile regression techniques, to recent developments in conformal prediction. Particular emphasis is placed on advancements in probabilistic forecasting, including validity-focused methods which address key limitations in uncertainty estimation. Additionally, this review extends beyond the Day-Ahead Market to include the Intra-Day and Balancing Markets, where forecasting challenges are intensified by higher temporal granularity and real-time operational constraints. We examine state of the art methodologies, key evaluation metrics, and ongoing challenges, such as forecast validity, model selection, and the absence of standardised benchmarks, providing researchers and practitioners with a comprehensive and timely resource for navigating the complexities of modern electricity markets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文系统综述了电力市场中的概率价格预测技术，追踪了从贝叶斯和基于分布的方法、分位数回归技术到近期保形预测的发展历程。重点强调了概率预测的进展，包括关注有效性的方法，以解决不确定性估计中的关键限制。此外，综述不仅涵盖日前市场，还扩展至日内和平衡市场，这些市场因更高的时间粒度而面临更严峻的预测挑战。论文主要聚焦于理论方法回顾，未涉及实战交易策略或具体应用案例，因此对量化交易实战的直接价值有限。",
    "fetch_date": "2026-01-21",
    "id": "20260121_d3c06580"
  },
  {
    "title": "The Omniscient, yet Lazy, Investor",
    "url": "https://arxiv.org/pdf/2510.24467v1",
    "source": "ArXiv",
    "date": "2025-10-28",
    "abstract": "We formalize the paradox of an omniscient yet lazy investor - a perfectly informed agent who trades infrequently due to execution or computational frictions. Starting from a deterministic geometric construction, we derive a closed-form expected profit function linking trading frequency, execution cost, and path roughness. We prove existence and uniqueness of the optimal trading frequency and show that this optimum can be interpreted through the fractal dimension of the price path. A stochastic extension under fractional Brownian motion provides analytical expressions for the optimal interval and comparative statics with respect to the Hurst exponent. Empirical illustrations on equity data confirm the theoretical scaling behavior.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们形式化了一个全知但懒惰的投资者的悖论——一个拥有完美信息但因执行或计算摩擦而交易不频繁的代理人。从确定性几何构造出发，我们推导出一个封闭形式的预期利润函数，将交易频率、执行成本和路径粗糙度联系起来。我们证明了最优交易频率的存在性和唯一性，并表明这一最优解可以通过价格路径的分形维数来解释。在分数布朗运动下的随机扩展为最优区间和关于赫斯特指数的比较静态提供了解析表达式。股票数据的实证例证证实了理论标度行为。",
    "fetch_date": "2026-01-21",
    "id": "20260121_63f77ea5"
  },
  {
    "title": "Explainable Federated Learning for U.S. State-Level Financial Distress Modeling",
    "url": "https://arxiv.org/pdf/2511.08588v1",
    "source": "ArXiv",
    "date": "2025-10-28",
    "abstract": "We present the first application of federated learning (FL) to the U.S. National Financial Capability Study, introducing an interpretable framework for predicting consumer financial distress across all 50 states and the District of Columbia without centralizing sensitive data. Our cross-silo FL setup treats each state as a distinct data silo, simulating real-world governance in nationwide financial systems. Unlike prior work, our approach integrates two complementary explainable AI techniques to identify both global (nationwide) and local (state-specific) predictors of financial hardship, such as contact from debt collection agencies. We develop a machine learning model specifically suited for highly categorical, imbalanced survey data. This work delivers a scalable, regulation-compliant blueprint for early warning systems in finance, demonstrating how FL can power socially responsible AI applications in consumer credit risk and financial inclusion.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文首次将联邦学习（FL）应用于美国国家金融能力研究，提出一个可解释的框架，用于预测全美50个州和哥伦比亚特区的消费者财务困境，而无需集中敏感数据。跨孤岛FL设置将每个州视为独立的数据孤岛，模拟全国金融系统中的实际治理。与先前工作不同，该方法整合两种互补的可解释AI技术，以识别财务困境的全局（全国性）和局部（州特定）预测因子，如债务催收机构的联系。开发了一个专门适用于高度分类、不平衡调查数据的机器学习模型。这项工作为金融预警系统提供了一个可扩展、合规的蓝图，展示了FL如何在消费者信用风险和金融包容性中推动社会责任型AI应用。",
    "fetch_date": "2026-01-21",
    "id": "20260121_fff5ed3c"
  },
  {
    "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2510.23216v3",
    "source": "ArXiv",
    "date": "2025-10-27",
    "abstract": "While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testament to the impact of the approach, the method has been adopted for use in the most recent release of the series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种面向视频游戏工业的样本高效深度强化学习方法，通过利用预收集数据和增强网络可塑性来提升基于价值的DRL的样本效率。该方法在EA SPORTS FC 25中训练守门员智能体，其扑救率比游戏内置AI高10%，训练速度比标准DRL方法快50%，且被领域专家评价为比手工制作的智能体更具人类真实感。该方法已被最新游戏版本采用。",
    "fetch_date": "2026-01-21",
    "id": "20260121_07919793"
  },
  {
    "title": "RL-Exec: Impact-Aware Reinforcement Learning for Opportunistic Optimal Liquidation, Outperforms TWAP and a Book-Liquidity VWAP on BTC-USD Replays",
    "url": "https://arxiv.org/pdf/2511.07434v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "We study opportunistic optimal liquidation over fixed deadlines on BTC-USD limit-order books (LOB). We present RL-Exec, a PPO agent trained on historical replays augmented with endogenous transient impact (resilience), partial fills, maker/taker fees, and latency. The policy observes depth-20 LOB features plus microstructure indicators and acts under a sell-only inventory constraint to reach a residual target. Evaluation follows a strict time split (train: Jan-2020; test: Feb-2020) and a per-day protocol: for each test day we run ten independent start times and aggregate to a single daily score, avoiding pseudo-replication. We compare the agent to (i) TWAP and (ii) a VWAP-like baseline allocating using opposite-side order-book liquidity (top-20 levels), both executed on identical timestamps and costs. Statistical inference uses one-sided Wilcoxon signed-rank tests on daily RL-baseline differences with Benjamini-Hochberg FDR correction and bootstrap confidence intervals. On the Feb-2020 test set, RL-Exec significantly outperforms both baselines and the gap increases with the execution horizon (+2-3 bps at 30 min, +7-8 bps at 60 min, +23 bps at 120 min).\n  Code: github.com/Giafferri/RL-Exec",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "论文《RL-Exec：基于影响感知强化学习的时机性最优清算，在BTC-USD回测中超越TWAP和基于订单簿流动性的VWAP》研究了在BTC-USD限价订单簿（LOB）上固定期限内的时机性最优清算问题。作者提出了RL-Exec，这是一个基于PPO算法的智能体，训练数据来自历史回测，并加入了内生瞬时冲击（弹性）、部分成交、做市商/吃单手续费和延迟等因素。策略观察深度为20的LOB特征及微观结构指标，在仅卖出库存约束下操作以达到剩余目标。评估采用严格的时间分割（训练：2020年1月；测试：2020年2月）和每日协议：每个测试日运行十个独立起始时间并汇总为单日得分，避免伪重复。该智能体与（i）TWAP和（ii）基于对手方订单簿流动性（前20档）的类VWAP基准进行比较，两者在相同时间戳和成本下执行。统计推断使用单侧Wilcoxon符号秩检验对每日RL-基准差异进行Benjamini-Hochberg FDR校正和自助法置信区间。在2020年2月测试集上，RL-Exec显著优于两个基准，且随着执行期限延长，优势扩大（30分钟时+2-3基点，60分钟时+7-8基点，120分钟时+23基点）。代码：github.com/Giafferri/",
    "fetch_date": "2026-01-20",
    "id": "20260120_af8b1fef"
  },
  {
    "title": "An Impulse Control Approach to Market Making in a Hawkes LOB Market",
    "url": "https://arxiv.org/pdf/2510.26438v2",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "We study the optimal Market Making problem in a Limit Order Book (LOB) market simulated using a high-fidelity, mutually exciting Hawkes process. Departing from traditional Brownian-driven mid-price models, our setup captures key microstructural properties such as queue dynamics, inter-arrival clustering, and endogenous price impact. Recognizing the realistic constraint that market makers cannot update strategies at every LOB event, we formulate the control problem within an impulse control framework, where interventions occur discretely via limit, cancel, or market orders. This leads to a high-dimensional, non-local Hamilton-Jacobi-Bellman Quasi-Variational Inequality (HJB-QVI), whose solution is analytically intractable and computationally expensive due to the curse of dimensionality. To address this, we propose a novel Reinforcement Learning (RL) approximation inspired by auxiliary control formulations. Using a two-network PPO-based architecture with self-imitation learning, we demonstrate strong empirical performance with limited training, achieving Sharpe ratios above 30 in a realistic simulated LOB. In addition to that, we solve the HJB-QVI using a deep learning method inspired by Sirignano and Spiliopoulos 2018 and compare the performance with the RL agent. Our findings highlight the promise of combining impulse control theory with modern deep RL to tackle optimal execution problems in jump-driven microstructural markets.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文研究在基于霍克斯过程的高保真限价订单簿市场中，做市商的最优控制问题。模型摒弃了传统的布朗运动驱动中间价模型，捕捉了队列动态、到达聚类和内生价格影响等关键微观结构特性。考虑到做市商无法在每个LOB事件时更新策略的现实约束，将控制问题构建为脉冲控制框架，通过限价、取消或市价订单进行离散干预。这导致了一个高维、非局部的Hamilton-Jacobi-Bellman拟变分不等式，其解析解因维度诅咒而难以获得且计算昂贵。为此，提出了一种受辅助控制公式启发的强化学习近似方法，采用基于PPO的双网络架构与自模仿学习，在有限训练下实现了强劲的实证性能，在现实模拟LOB中获得了超过30的夏普比率。此外，还使用深度学习方法求解了HJB-QVI。",
    "fetch_date": "2026-01-20",
    "id": "20260120_7c23ce0f"
  },
  {
    "title": "ChatGPT in Systematic Investing -- Enhancing Risk-Adjusted Returns with LLMs",
    "url": "https://arxiv.org/pdf/2510.26228v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "This paper investigates whether large language models (LLMs) can improve cross-sectional momentum strategies by extracting predictive signals from firm-specific news. We combine daily U.S. equity returns for S&P 500 constituents with high-frequency news data and use prompt-engineered queries to ChatGPT that inform the model when a stock is about to enter a momentum portfolio. The LLM evaluates whether recent news supports a continuation of past returns, producing scores that condition both stock selection and portfolio weights. An LLM-enhanced momentum strategy outperforms a standard long-only momentum benchmark, delivering higher Sharpe and Sortino ratios both in-sample and in a truly out-of-sample period after the model's pre-training cut-off. These gains are robust to transaction costs, prompt design, and portfolio constraints, and are strongest for concentrated, high-conviction portfolios. The results suggest that LLMs can serve as effective real-time interpreters of financial news, adding incremental value to established factor-based investment strategies.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文研究大型语言模型（LLMs）能否通过从公司特定新闻中提取预测信号来改进横截面动量策略。结合标普500成分股的日度美国股票收益率与高频新闻数据，使用提示工程查询ChatGPT，告知模型某股票即将进入动量投资组合。LLM评估近期新闻是否支持过去收益的延续，生成评分以调整股票选择和投资组合权重。LLM增强的动量策略在样本内和模型预训练截止后的真实样本外期间均优于标准多头动量基准，提供更高的夏普比率和索提诺比率。这些收益对交易成本、提示设计和投资组合约束具有稳健性，且在集中、高确信度的投资组合中表现最强。结果表明，LLMs可作为金融新闻的有效实时解释器，为已建立的基于因子的投资策略增加增量价值。",
    "fetch_date": "2026-01-20",
    "id": "20260120_5b74cd65"
  },
  {
    "title": "KANHedge: Efficient Hedging of High-Dimensional Options Using Kolmogorov-Arnold Network-Based BSDE Solver",
    "url": "https://arxiv.org/pdf/2601.11097v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "High-dimensional option pricing and hedging present significant challenges in quantitative finance, where traditional PDE-based methods struggle with the curse of dimensionality. The BSDE framework offers a computationally efficient alternative to PDE-based methods, and recently proposed deep BSDE solvers, generally utilizing conventional Multi-Layer Perceptrons (MLPs), build upon this framework to provide a scalable alternative to numerical BSDE solvers. In this research, we show that although such MLP-based deep BSDEs demonstrate promising results in option pricing, there remains room for improvement regarding hedging performance. To address this issue, we introduce KANHedge, a novel BSDE-based hedger that leverages Kolmogorov-Arnold Networks (KANs) within the BSDE framework. Unlike conventional MLP approaches that use fixed activation functions, KANs employ learnable B-spline activation functions that provide enhanced function approximation capabilities for continuous derivatives. We comprehensively evaluate KANHedge on both European and American basket options across multiple dimensions and market conditions. Our experimental results demonstrate that while KANHedge and MLP achieve comparable pricing accuracy, KANHedge provides improved hedging performance. Specifically, KANHedge achieves considerable reductions in hedging cost metrics, demonstrating enhanced risk control capabilities.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出KANHedge，一种基于Kolmogorov-Arnold网络（KAN）的BSDE求解器，用于高效对冲高维期权。相比传统多层感知机（MLP）方法，KAN采用可学习的B样条激活函数，提升了连续导数的函数逼近能力，从而改善了期权对冲性能。研究在欧洲和美国篮子期权上进行了多维度、多市场条件的全面评估，证明该方法在实战交易中具有应用潜力。",
    "fetch_date": "2026-01-20",
    "id": "20260120_b7602d3a"
  },
  {
    "title": "SANOS -- Smooth strictly Arbitrage-free Non-parametric Option Surfaces",
    "url": "https://arxiv.org/pdf/2601.11209v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "We present a simple, numerically efficient but highly flexible non-parametric method to construct representations of option price surfaces which are both smooth and strictly arbitrage-free across time and strike. The method can be viewed as a smooth generalization of the widely-known linear interpolation scheme, and retains the simplicity and transparency of that baseline. Calibration of the model to observed market quotes is formulated as a linear program, allowing bid-ask spreads to be incorporated directly via linear penalties or inequalities, and delivering materially lower computational cost than most of the currently available implied-volatility surface fitting routines. As a further contribution, we derive an equivalent parameterization of the proposed surface in terms of strictly positive \"discrete local volatility\" variables. This yields, to our knowledge, the first construction of smooth, strictly arbitrage-free option price surfaces while requiring only trivial parameter constraints (positivity). We illustrate the approach using S&P 500 index options",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种简单、计算高效且高度灵活的非参数方法SANOS，用于构建在时间和行权价维度上既平滑又严格无套利的期权价格曲面。该方法可视为广泛使用的线性插值方案的平滑推广，保持了基线的简单性和透明度。模型对市场报价的校准被表述为一个线性规划问题，允许通过线性惩罚或不等式直接纳入买卖价差，计算成本显著低于当前大多数隐含波动率曲面拟合方法。作为进一步贡献，本文推导了所提出曲面在严格正“离散局部波动率”变量下的等价参数化，据我们所知，这是首次在仅需平凡参数约束（正性）的情况下构建平滑、严格无套利的期权价格曲面。该方法以标普500指数期权为例进行了说明。",
    "fetch_date": "2026-01-20",
    "id": "20260120_d2cc6df5"
  },
  {
    "title": "Fast Times, Slow Times: Timescale Separation in Financial Timeseries Data",
    "url": "https://arxiv.org/pdf/2601.11201v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "Financial time series exhibit multiscale behavior, with interaction between multiple processes operating on different timescales. This paper introduces a method for separating these processes using variance and tail stationarity criteria, framed as generalized eigenvalue problems. The approach allows for the identification of slow and fast components in asset returns and prices, with applications to parameter drift, mean reversion, and tail risk management. Empirical examples using currencies, equity ETFs and treasury yields illustrate the practical utility of the method.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融时间序列呈现多尺度行为，涉及不同时间尺度上多个过程的相互作用。本文提出一种方法，利用方差和尾部平稳性准则（表述为广义特征值问题）来分离这些过程。该方法能够识别资产收益率和价格中的慢速与快速成分，应用于参数漂移、均值回归和尾部风险管理。通过货币、股票ETF和国债收益率的实证案例展示了该方法的实际效用。",
    "fetch_date": "2026-01-20",
    "id": "20260120_07d2c35a"
  },
  {
    "title": "Probabilistic Rule Models as Diagnostic Layers: Interpreting Structural Concept Drift in Post-Crisis Finance",
    "url": "https://arxiv.org/pdf/2510.26627v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "Machine learning models used for high-stakes predictions in domains like credit risk face critical degradation due to concept drift, requiring robust and transparent adaptation mechanisms. We propose an architecture, where a dedicated correction layer is employed to efficiently capture systematic shifts in predictive scores when a model becomes outdated. The key element of this architecture is the design of a correction layer using Probabilistic Rule Models (PRMs) based on Markov Logic Networks, which guarantees intrinsic interpretability through symbolic, auditable rules. This structure transforms the correction layer from a simple scoring mechanism into a powerful diagnostic tool capable of isolating and explaining the fundamental changes in borrower riskiness. We illustrate this diagnostic capability using Fannie Mae mortgage data, demonstrating how the interpretable rules extracted by the correction layer successfully explain the structural impact of the 2008 financial crisis on specific population segments, providing essential insights for portfolio risk management and regulatory compliance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于概率规则模型（PRMs）和马尔可夫逻辑网络的校正层架构，用于捕获机器学习模型在概念漂移时的系统性预测偏移。该架构将校正层从简单的评分机制转变为强大的诊断工具，通过符号化、可审计的规则保证内在可解释性。作者以房利美抵押贷款数据为例，展示了该校正层如何解释2008年金融危机对特定人群风险的结构性影响，为投资组合风险管理和监管合规提供了关键见解。",
    "fetch_date": "2026-01-20",
    "id": "20260120_37522ae0"
  },
  {
    "title": "Automated Liquidity: Market Impact, Cycles, and De-pegging Risk",
    "url": "https://arxiv.org/pdf/2601.11375v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "Three traits of decentralized finance are studied. First, the market impact function is derived for optimal-growth liquidity providers. For a standard random walk, the classic square-root impact is recovered. An extension is then derived to fit general fractional Ornstein-Uhlenbeck processes. These findings break with the linearized liquidity models used in most decentralized exchanges. Second, a Constant Product Market Maker is viewed as a multi-phase Carnot engine, where one phase matches the exchange of tokens by a liquidity taker, and another the change of pool size by a liquidity provider. Third, stablecoin de-pegging is a form of catastrophe risk. By using growth optimization, default odds are linked to the cost of catastrophe bonds. De-pegging insurance can act as a counterweight and a key marketing tool when the law forbids the payment of interest on stablecoins.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文研究了去中心化金融的三个特性。首先，推导了最优增长流动性提供者的市场冲击函数，在标准随机游走中恢复了经典的平方根冲击模型，并扩展到一般分数阶Ornstein-Uhlenbeck过程，突破了大多数去中心化交易所使用的线性化流动性模型。其次，将恒定乘积做市商视为多相卡诺热机，其中一相匹配流动性接受者的代币交换，另一相匹配流动性提供者的资金池规模变化。第三，稳定币脱锚被视为一种灾难风险，通过增长优化将违约概率与灾难债券成本联系起来，脱锚保险可作为反制工具，并在法律禁止稳定币支付利息时成为关键营销工具。",
    "fetch_date": "2026-01-20",
    "id": "20260120_95a734f9"
  },
  {
    "title": "FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling",
    "url": "https://arxiv.org/pdf/2601.11134v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\\% vs +1.4\\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种用于信用风险建模的联邦生存学习框架（FSL-BDP），结合贝叶斯差分隐私，旨在解决传统违约预测的两个局限：二元分类忽略违约时间（将早期与晚期违约者等同处理），以及集中式训练违反数据保护法规（如GDPR、CCPA）。该框架允许金融机构在不集中敏感数据的情况下联合学习风险动态，同时提供贝叶斯（数据依赖）差分隐私保证。在三个真实信用数据集（LendingClub、SBA、Bondora）上的实验表明，联邦学习显著改变了隐私机制的有效性：贝叶斯差分隐私在联邦设置中表现更优（性能提升+7.0% vs 经典差分隐私的+1.4%），接近非隐私模型的性能。",
    "fetch_date": "2026-01-20",
    "id": "20260120_e62dfe15"
  },
  {
    "title": "Event-Driven Market Co-Movement Dynamics in Critical Mineral Equities: An Empirical Framework Using Change Point Detection and Cross-Sectional Analysis",
    "url": "https://arxiv.org/pdf/2601.10851v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "This study examines market behavior in critical mineral investments using a novel analytical framework that combines change-point detection (PELT algorithm) with cross-sectional analysis. This research analyzes ESG-ranked critical mineral ETFs from March 31, 2014, to April 19, 2024, using the S&P 500 as a benchmark to evaluate market co-movements. The findings demonstrate that different critical mineral investments experienced change points at distinct times, but three major dates, July 23, 2015; March 17, 2020; and December 1, 2020, were common and aligned with global events such as the oil market shock, the COVID-19 pandemic, and later market adjustments. Herding behavior among investors increased after these shocks, following the 2015 and 2020 crises, but shifted to anti-herding after positive vaccine news in late 2020 and after the Russian invasion of Ukraine in 2022. The sensitivity analysis shows that investor coordination is strongest during market downturns but exhibits greater variation during stable periods or after major developments, with these dynamics sensitive to the length of the observation period. Additionally, anti-herding became more apparent during crises, suggesting investors reacted to specific risks rather than moving in lockstep, especially in response to geopolitical shocks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究采用结合变点检测（PELT算法）与横截面分析的新颖分析框架，考察关键矿物投资的市场行为。研究分析了2014年3月31日至2024年4月19日期间ESG评级的关键矿物ETF，以标普500为基准评估市场联动。研究发现，不同关键矿物投资在不同时间点经历了变点，但三个主要日期（2015年7月23日、2020年3月17日、2020年12月1日）是共同的，并与全球事件（如石油市场冲击、COVID-19大流行及后续市场调整）一致。投资者羊群行为在这些冲击后（2015年和2020年危机后）增加，但在2020年末疫苗积极消息和2022年俄罗斯入侵乌克兰后转向反羊群行为。敏感性分析表明，投资者协调在市场下跌期间最强，但在稳定期或重大发展后表现出更大变化，这些动态对观察期长度敏感。此外，危机期间反羊群行为更为明显。",
    "fetch_date": "2026-01-20",
    "id": "20260120_a4793b1c"
  },
  {
    "title": "Regime-Dependent Predictive Structure Between Equity Factors: Evidence from Granger Causality",
    "url": "https://arxiv.org/pdf/2601.10732v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "We document regime-dependent predictive structure between equity factors using 35 years of Fama-French data (1990-2024). We find that Value (HML) Granger-causes Size (SMB) during crisis regimes (p < 1e-4, 9-day lag) but not during normal conditions, validating across 5 of 6 historical stress events (2008, 2011, 2015, 2018, 2020). Regimes are identified via a Student-t HMM, which detects moderate crises such as 2011 (69%) that Gaussian models miss entirely (0%). Although the relationship does not yield trading profits, the 9-day lead time may support risk management decisions. We note that Granger causality implies temporal precedence, not structural causality, and that common drivers could explain the pattern; our economic interpretation is a hypothesis rather than a verified mechanism.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文利用35年Fama-French数据（1990-2024），通过Granger因果检验揭示了股权因子间的制度依赖性预测结构。研究发现：在危机制度下（通过Student-t隐马尔可夫模型识别），价值因子（HML）对规模因子（SMB）存在Granger因果关系（p<1e-4，滞后9天），而在正常条件下则无此关系。该结论在6次历史压力事件中的5次（2008、2011、2015、2018、2020）得到验证，且Student-t HMM能检测到高斯模型完全忽略的中等危机（如2011年，概率69%）。尽管这种关系未能产生交易利润，但9天的领先时间可能支持风险管理决策。作者强调Granger因果关系仅表示时间先后性而非结构性因果，且共同驱动因素可能解释该模式；其经济解释仅为假设而非已验证机制。",
    "fetch_date": "2026-01-20",
    "id": "20260120_8fbe89e3"
  },
  {
    "title": "Estimating the Hurst parameter from the zero vanna implied volatility and its dual",
    "url": "https://arxiv.org/pdf/2510.26310v2",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "The covariance between the return of an asset and its realized volatility can be approximated as the difference between two specific implied volatilities. In this paper it is proved that in the small time-to-maturity limit the approximation error tends to zero. In addition a direct relation between the short time-to-maturity covariance and slope of the at-the-money implied volatility is established. The limit theorems are valid for stochastic volatility models with Hurst parameter $H \\in(0, 1)$. An application of the results is to accurately approximate the Hurst parameter using only a discrete set of implied volatilities. Numerical examples under the rough Bergomi model are presented.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过证明在短到期期限极限下，资产收益率与其已实现波动率之间的协方差可精确近似为两种特定隐含波动率之差，建立了该协方差与平值隐含波动率斜率的直接关系。这些极限定理适用于Hurst参数H∈(0,1)的随机波动率模型。主要应用是利用离散隐含波动率数据准确估计Hurst参数，并在粗糙Bergomi模型下提供了数值示例。",
    "fetch_date": "2026-01-20",
    "id": "20260120_a8999256"
  },
  {
    "title": "Multiscaling in the Rough Bergomi Model: A Tale of Tails",
    "url": "https://arxiv.org/pdf/2601.11305v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "The rough Bergomi (rBergomi) model, characterised by its roughness parameter $H$, has been shown to exhibit multiscaling behaviour as $H$ approaches zero. Multiscaling has profound implications for financial modelling: it affects extreme risk estimation, influences optimal portfolio allocation across different time horizons, and challenges traditional option pricing approaches that assume uniscaling behaviours. Understanding whether multiscaling arises primarily from the roughness of volatility paths or from the resulting fat-tailed returns has important implications for financial modelling, option pricing, and risk management. This paper investigates the real source of this multiscaling behaviour by introducing a novel two-stage statistical testing procedure. In the first stage, we establish the presence of multiscaling in the rBergomi model against an uniscaling fractional Brownian motion process. We quantify multiscaling by using weighted least squares regression that accounts for heteroscedastic estimation errors across moments. In the second stage, we apply shuffled surrogates that preserve return distributions while destroying temporal correlations. This is done by using distance-based permutation tests robust to asymmetric null distributions. In order to validate our procedure, we check the robustness of the results by using synthetic processes with known multifractal properties, namely the Multifractal Random Walk (MRW) and the Fractional Lévy Stable Motion (FLSM). We provide compelling evidence that multiscaling in the rBergomi model arises primarily from fat-tailed return distributions rather than memory effects. Our findings suggest that the apparent multiscaling in rough volatility models is largely attributable to distributional properties rather than genuine temporal scaling behaviour.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《粗糙Bergomi模型中的多重标度：一个关于尾部的故事》研究了粗糙Bergomi模型（rBergomi）中当粗糙度参数H趋近于零时表现出的多重标度行为。多重标度对金融建模具有深远影响：它影响极端风险估计，影响不同时间跨度的最优投资组合配置，并对假设单一标度行为的传统期权定价方法构成挑战。理解多重标度主要源于波动率路径的粗糙性还是由此产生的厚尾回报，对金融建模、期权定价和风险管理具有重要意义。本文通过引入一种新颖的两阶段统计检验程序来探究这种多重标度行为的真实来源。第一阶段，我们确立了rBergomi模型中多重标度的存在，并与单一标度的分形布朗运动过程进行对比。我们通过使用加权最小二乘回归来量化多重标度，该回归考虑了跨矩的异方差估计误差。第二阶段，我们应用了保留回报分布但破坏时间相关性的洗牌替代方法。",
    "fetch_date": "2026-01-20",
    "id": "20260120_c4b81490"
  },
  {
    "title": "Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems",
    "url": "https://arxiv.org/pdf/2601.11189v1",
    "source": "ArXiv",
    "date": "2026-01-16",
    "abstract": "This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于策略的深度强化学习超启发式框架，用于解决作业车间调度问题。该超启发式代理学习根据系统状态动态切换调度规则。我们通过两个关键机制扩展了该框架：首先，动作预过滤将决策限制在可行的低层动作上，使低层启发式方法能够独立于环境约束进行评估，并提供无偏评估；其次，承诺机制调节启发式切换的频率。我们研究了从逐步切换到全周期承诺的不同承诺策略对训练行为和完工时间的影响，并比较了策略层面的两种动作选择策略：确定性贪婪选择和随机采样。在标准JSSP基准上的计算实验表明，该方法优于传统启发式、元启发式和最近的基于神经网络的调度方法。",
    "fetch_date": "2026-01-20",
    "id": "20260120_e49bd93a"
  },
  {
    "title": "Optimal Liquidation of Perpetual Contracts",
    "url": "https://arxiv.org/pdf/2601.10812v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "An agent holds a position in a perpetual contract with payoff function $ψ$ and attempts to liquidate the position while managing transaction costs, inventory risk, and funding rate payments. By solving the agent's stochastic control problem we obtain a closed-form expression for the optimal trading strategy when the payoff function is given by $ψ(s) = s$. When the payoff function is non-linear we provide approximations to the optimal strategy which apply when the funding rate parameter is small or when the length of the trading interval is small. We further prove that when $ψ$ is non-linear, the short time approximation can be written in terms of the closed-form trading strategy corresponding to the case of the identity payoff function.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了永续合约的最优清算问题，考虑了交易成本、持仓风险和资金费率支付等因素。通过求解随机控制问题，在线性收益函数ψ(s)=s时得到了最优交易策略的闭式解；对于非线性收益函数，提供了资金费率参数较小或交易区间较短时的近似策略，并证明了非线性情况下的短时近似可用线性情况的闭式策略表示。",
    "fetch_date": "2026-01-20",
    "id": "20260120_ebea0f8a"
  },
  {
    "title": "When AI Trading Agents Compete: Adverse Selection of Meta-Orders by Reinforcement Learning-Based Market Making",
    "url": "https://arxiv.org/pdf/2510.27334v1",
    "source": "ArXiv",
    "date": "2025-10-31",
    "abstract": "We investigate the mechanisms by which medium-frequency trading agents are adversely selected by opportunistic high-frequency traders. We use reinforcement learning (RL) within a Hawkes Limit Order Book (LOB) model in order to replicate the behaviours of high-frequency market makers. In contrast to the classical models with exogenous price impact assumptions, the Hawkes model accounts for endogenous price impact and other key properties of the market (Jain et al. 2024a). Given the real-world impracticalities of the market maker updating strategies for every event in the LOB, we formulate the high-frequency market making agent via an impulse control reinforcement learning framework (Jain et al. 2025). The RL used in the simulation utilises Proximal Policy Optimisation (PPO) and self-imitation learning. To replicate the adverse selection phenomenon, we test the RL agent trading against a medium frequency trader (MFT) executing a meta-order and demonstrate that, with training against the MFT meta-order execution agent, the RL market making agent learns to capitalise on the price drift induced by the meta-order. Recent empirical studies have shown that medium-frequency traders are increasingly subject to adverse selection by high-frequency trading agents. As high-frequency trading continues to proliferate across financial markets, the slippage costs incurred by medium-frequency traders are likely to increase over time. However, we do not observe that increased profits for the market making RL agent necessarily cause significantly increased slippages for the MFT agent.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文研究高频做市商如何通过强化学习（RL）在Hawkes限价订单簿（LOB）模型中利用中频交易者的元订单执行进行逆向选择。论文采用脉冲控制强化学习框架（PPO与自模仿学习），模拟高频做市商捕捉元订单引发的价格漂移，揭示了中频交易者在实际市场中面临的逆向选择风险。",
    "fetch_date": "2026-01-19",
    "id": "20260119_aae970e9"
  },
  {
    "title": "Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.11607v1",
    "source": "ArXiv",
    "date": "2025-11-02",
    "abstract": "Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《基于聚类的权重正交化用于稳定深度强化学习》提出了一种可集成到任何RL算法策略网络中的COWM层，通过聚类技术和投影矩阵有效缓解环境非平稳性问题，提高了学习速度、减少梯度干扰，在DMControl基准测试中实现9%-12.6%的性能提升，对实战交易中处理市场非平稳性具有直接应用价值。",
    "fetch_date": "2026-01-19",
    "id": "20260119_c0731931"
  },
  {
    "title": "Technical Analysis Meets Machine Learning: Bitcoin Evidence",
    "url": "https://arxiv.org/pdf/2511.00665v1",
    "source": "ArXiv",
    "date": "2025-11-01",
    "abstract": "In this note, we compare Bitcoin trading performance using two machine learning models-Light Gradient Boosting Machine (LightGBM) and Long Short-Term Memory (LSTM)-and two technical analysis-based strategies: Exponential Moving Average (EMA) crossover and a combination of Moving Average Convergence/Divergence with the Average Directional Index (MACD+ADX). The objective is to evaluate how trading signals can be used to maximize profits in the Bitcoin market. This comparison was motivated by the U.S. Securities and Exchange Commission's (SEC) approval of the first spot Bitcoin exchange-traded funds (ETFs) on 2024-01-10. Our results show that the LSTM model achieved a cumulative return of approximately 65.23% in under a year, significantly outperforming LightGBM, the EMA and MACD+ADX strategies, as well as the baseline buy-and-hold. This study highlights the potential for deeper integration of machine learning and technical analysis in the rapidly evolving cryptocurrency landscape.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文比较了比特币交易中两种机器学习模型（LightGBM和LSTM）与两种技术分析策略（EMA交叉和MACD+ADX组合）的表现。研究动机源于美国SEC于2024年1月10日批准首批现货比特币ETF。结果显示，LSTM模型在不到一年内实现了约65.23%的累计回报，显著优于LightGBM、EMA、MACD+ADX策略及基准买入持有策略。该研究强调了在快速发展的加密货币市场中，机器学习与技术分析深度融合的潜力。",
    "fetch_date": "2026-01-19",
    "id": "20260119_2f7c2b8e"
  },
  {
    "title": "How Digital Asset Treasury Companies Can Survive Bear Markets: The Case of the Strategy and Bitcoin",
    "url": "https://arxiv.org/pdf/2511.01135v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "Digital Asset Treasury (DAT) companies, public firms that hold large crypto reserves as a core strategy, deliver levered exposure to digital assets but face acute downside risk when equity premia over net asset value multiples (mNAV) compress in bear markets. This paper develops a survival framework that couples conservative treasury policy with an operating line that monetizes holdings independent of mark-to-market gains. Using Strategy (formerly MicroStrategy) as a case, we propose a \"BTC-to-sats\" payments rail that allocates a small, risk-capped liquidity sleeve of the treasury to Lightning Network channels, generating price-agnostic fee revenue (acquiring bps, routing, hedge/FX spread) while keeping settlement exposure near zero beta to BTC. We formalize a no-forced-sale condition and show how disclosed KPIs allow investors to test whether operating cash flows can bridge an 18 to 24-month bear without liquidations. The feasibility of the rail is supported by Strategy's Lightning initiative and empirical Lightning performance. Our model generalizes across DAT types and provides implementable disclosures that can sustain an mNAV premium through cycles.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "数字资产财库（DAT）公司（如持有大量加密储备作为核心战略的上市公司）在熊市中面临股权溢价相对于净资产价值倍数（mNAV）压缩带来的下行风险。本文提出一个生存框架，将保守的财库政策与独立于市值损益的运营线相结合。以Strategy（原MicroStrategy）为例，提出“BTC-to-sats”支付通道，将财库中一小部分风险上限的流动性分配到闪电网络通道，产生与价格无关的费用收入（如基点获取、路由、对冲/外汇价差），同时保持结算风险对比特币的贝塔值接近零。该框架形式化了无强制出售条件，并展示如何通过披露的关键绩效指标（KPIs）让投资者测试运营现金流能否在18至24个月的熊市中避免清算。模型的可行性得到Strategy的闪电网络倡议和实证闪电网络性能的支持，可推广到各类DAT公司，并提供可实施的披露方案以维持周期中的mNAV溢价。",
    "fetch_date": "2026-01-19",
    "id": "20260119_8b708016"
  },
  {
    "title": "Deep reinforcement learning for optimal trading with partial information",
    "url": "https://arxiv.org/pdf/2511.00190v1",
    "source": "ArXiv",
    "date": "2025-10-31",
    "abstract": "Reinforcement Learning (RL) applied to financial problems has been the subject of a lively area of research. The use of RL for optimal trading strategies that exploit latent information in the market is, to the best of our knowledge, not widely tackled. In this paper we study an optimal trading problem, where a trading signal follows an Ornstein-Uhlenbeck process with regime-switching dynamics. We employ a blend of RL and Recurrent Neural Networks (RNN) in order to make the most at extracting underlying information from the trading signal with latent parameters.\n  The latent parameters driving mean reversion, speed, and volatility are filtered from observations of the signal, and trading strategies are derived via RL. To address this problem, we propose three Deep Deterministic Policy Gradient (DDPG)-based algorithms that integrate Gated Recurrent Unit (GRU) networks to capture temporal dependencies in the signal. The first, a one -step approach (hid-DDPG), directly encodes hidden states from the GRU into the RL trader. The second and third are two-step methods: one (prob-DDPG) makes use of posterior regime probability estimates, while the other (reg-DDPG) relies on forecasts of the next signal value. Through extensive simulations with increasingly complex Markovian regime dynamics for the trading signal's parameters, as well as an empirical application to equity pair trading, we find that prob-DDPG achieves superior cumulative rewards and exhibits more interpretable strategies. By contrast, reg-DDPG provides limited benefits, while hid-DDPG offers intermediate performance with less interpretable strategies. Our results show that the quality and structure of the information supplied to the agent are crucial: embedding probabilistic insights into latent regimes substantially improves both profitability and robustness of reinforcement learning-based trading strategies.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究利用深度强化学习进行最优交易，针对部分信息（交易信号遵循具有状态切换动态的Ornstein-Uhlenbeck过程）的场景。作者提出了三种基于深度确定性策略梯度（DDPG）的算法，这些算法集成了门控循环单元（GRU）网络以捕捉信号中的时间依赖性，旨在从具有潜在参数的交易信号中提取底层信息并推导交易策略。",
    "fetch_date": "2026-01-19",
    "id": "20260119_2b0fdeed"
  },
  {
    "title": "Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from the 2008 Financial Crisis",
    "url": "https://arxiv.org/pdf/2511.08602v1",
    "source": "ArXiv",
    "date": "2025-11-02",
    "abstract": "The 2008 financial crisis exposed fundamental vulnerabilities in interconnected banking systems, yet existing frameworks fail to integrate spatial propagation with network contagion mechanisms. This paper develops a unified spatial-network framework to analyze systemic risk dynamics, revealing three critical findings that challenge conventional wisdom. First, banking consolidation paradoxically increased systemic fragility: while bank numbers declined 47.3% from 2007 to 2023, network fragility measured by algebraic connectivity rose 315.8%, demonstrating that interconnectedness intensity dominates institutional count. Second, financial contagion propagates globally with negligible spatial decay (boundary d* = 47,474 km), contrasting sharply with localized technology diffusion (d* = 69 km)--a scale difference of 688 times. Third, traditional difference-in-differences methods overestimate crisis impacts by 73.2% when ignoring network structure, producing severely biased policy assessments. Using bilateral exposure data from 156 institutions across 28 countries (2007-2023) and employing spectral analysis of network Laplacian operators combined with spatial difference-in-differences identification, we document that crisis effects amplified over time rather than dissipating, increasing fragility 68.4% above pre-crisis levels with persistent effects through 2023. The consolidation paradox exhibits near-perfect correlation (R = 0.97) between coupling strength and systemic vulnerability, validating theoretical predictions from continuous spatial dynamics. Policy simulations demonstrate network-targeted capital requirements achieve 11.3x amplification effects versus uniform regulations. These findings establish that accurate systemic risk assessment and macroprudential policy design require explicit incorporation of both spatial propagation and network topology.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过构建统一的空间-网络框架分析系统性风险动态，揭示了三个关键发现：1) 银行整合反而增加了系统脆弱性（2007-2023年间银行数量减少47.3%，但网络脆弱性指标代数连通性上升315.8%）；2) 金融传染具有全球性传播特征（边界衰减距离d*=47,474公里），与局部化的技术扩散（d*=69公里）形成688倍差异；3) 忽略网络结构的传统双重差分法会高估危机影响73.2%。研究采用28个国家156家机构的双边敞口数据（2007-2023），结合网络拉普拉斯算子的谱分析和空间双重差分法。对实战交易的价值在于：为系统性风险建模提供新框架，揭示网络结构对风险评估的关键影响，但需结合具体交易策略转化应用。",
    "fetch_date": "2026-01-19",
    "id": "20260119_91480d97"
  },
  {
    "title": "Exact Terminal Condition Neural Network for American Option Pricing Based on the Black-Scholes-Merton Equations",
    "url": "https://arxiv.org/pdf/2510.27132v1",
    "source": "ArXiv",
    "date": "2025-10-31",
    "abstract": "This paper proposes the Exact Terminal Condition Neural Network (ETCNN), a deep learning framework for accurately pricing American options by solving the Black-Scholes-Merton (BSM) equations. The ETCNN incorporates carefully designed functions that ensure the numerical solution not only exactly satisfies the terminal condition of the BSM equations but also matches the non-smooth and singular behavior of the option price near expiration. This method effectively addresses the challenges posed by the inequality constraints in the BSM equations and can be easily extended to high-dimensional scenarios. Additionally, input normalization is employed to maintain the homogeneity. Multiple experiments are conducted to demonstrate that the proposed method achieves high accuracy and exhibits robustness across various situations, outperforming both traditional numerical methods and other machine learning approaches.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种基于Black-Scholes-Merton方程的精确终端条件神经网络（ETCNN），用于精确求解美式期权定价问题。该方法通过精心设计的函数确保数值解不仅精确满足BSM方程的终端条件，还能匹配期权价格在到期日附近的不光滑和奇异行为，有效处理了BSM方程中的不等式约束，并可轻松扩展到高维场景。实验表明，该方法具有高精度和鲁棒性，优于传统数值方法和其他机器学习方法。",
    "fetch_date": "2026-01-19",
    "id": "20260119_12ea4ded"
  },
  {
    "title": "Cost-of-capital valuation with risky assets",
    "url": "https://arxiv.org/pdf/2511.00895v1",
    "source": "ArXiv",
    "date": "2025-11-02",
    "abstract": "Cost-of-capital valuation is a well-established approach to the valuation of liabilities and is one of the cornerstones of current regulatory frameworks for the insurance industry. Standard cost-of-capital considerations typically rely on the assumption that the required buffer capital is held in risk-less one-year bonds. The aim of this work is to analyze the effects of allowing investments of the buffer capital in risky assets, e.g.~in a combination of stocks and bonds. In particular, we make precise how the decomposition of the buffer capital into contributions from policyholders and investors varies as the degree of riskiness of the investment increases, and highlight the role of limited liability in the case of heavy-tailed insurance risks. We present a combination of general theoretical results, explicit results for certain stochastic models and numerical results that emphasize the key findings.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了资本成本估值中缓冲资本投资于风险资产（如股票和债券组合）的影响。作为保险业监管框架的基石，传统方法假设缓冲资本仅投资于无风险一年期债券。研究分析了投资风险增加时，缓冲资本在投保人和投资者之间分配的变化，并强调了在厚尾保险风险情况下有限责任的作用。结合理论分析、特定随机模型和数值结果，突出了关键发现。",
    "fetch_date": "2026-01-19",
    "id": "20260119_5fbf0f16"
  },
  {
    "title": "Robust Hedging of path-dependent options using a min-max algorithm",
    "url": "https://arxiv.org/pdf/2511.00781v1",
    "source": "ArXiv",
    "date": "2025-11-02",
    "abstract": "We consider an investor who wants to hedge a path-dependent option with maturity $T$ using a static hedging portfolio using cash, the underlying, and vanilla put/call options on the same underlying with maturity $ t_1$, where $0 < t_1 < T$. We propose a model-free approach to construct such a portfolio. The framework is inspired by the \\textit{primal-dual} Martingale Optimal Transport (MOT) problem, which was pioneered by \\cite{beiglbock2013model}. The optimization problem is to determine the portfolio composition that minimizes the expected worst-case hedging error at $t_1$ (that coincides with the maturity of the options that are used in the hedging portfolio). The worst-case scenario corresponds to the distribution that yields the worst possible hedging performance. This formulation leads to a \\textit{min-max} problem. We provide a numerical scheme for solving this problem when a finite number of vanilla option prices are available. Numerical results on the hedging performance of this model-free approach when the option prices are generated using a \\textit{Black-Scholes} and a \\textit{Merton Jump diffusion} model are presented. We also provide theoretical bounds on the hedging error at $T$, the maturity of the target option.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于原始-对偶鞅最优传输（MOT）理论的无模型方法，用于构建路径依赖期权的稳健对冲组合。该方法通过现金、标的资产及到期日为t₁的普通看跌/看涨期权构建静态对冲组合，以最小化在t₁时刻的最坏情况对冲误差（即最不利分布下的最大损失）。研究提供了数值求解方案，并在Black-Scholes和Merton跳跃扩散模型下验证了其对冲效果，同时给出了理论误差界。",
    "fetch_date": "2026-01-19",
    "id": "20260119_7d301669"
  },
  {
    "title": "Lambda Value-at-Risk under ambiguity and risk sharing",
    "url": "https://arxiv.org/pdf/2511.00717v1",
    "source": "ArXiv",
    "date": "2025-11-01",
    "abstract": "In this paper, we investigate the Lambda Value-at-Risk ($Λ$VaR) under ambiguity, where the ambiguity is represented by a family of probability measures. We establish that for increasing Lambda functions, the robust (i.e., worst-case) $Λ$VaR under such an ambiguity set is equivalent to $Λ$VaR computed with respect to a capacity, a novel extension in the literature. This framework unifies and extends both traditional $Λ$VaR and Choquet quantiles (Value-at-Risk under ambiguity). We analyze the fundamental properties of this extended risk measure and establish a novel equivalent representation for $Λ$VaR under capacities with monotone Lambda functions in terms of families of downsets. Moreover, explicit formulas are derived for robust $Λ$VaR when ambiguity sets are characterized by $φ$-divergence and the likelihood ratio constraints, respectively.\n  We further explore the applications in risk sharing among multiple agents. We demonstrate that the family of risk measures induced by families of downsets is closed under inf-convolution. In particular, we prove that the inf-convolution of $Λ$VaR with capacities and monotone Lambda functions is another$Λ$VaR under a capacity. The explicit forms of optimal allocations are also derived. Moreover, we obtain more explicit results for risk sharing under ambiguity sets characterized by $φ$-divergence and likelihood ratio constraints. Finally, we explore comonotonic risk-sharing for $Λ$VaR under ambiguity.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在模糊性（由一族概率测度表示）下的Lambda在险价值（ΛVaR）。主要贡献包括：证明了对于递增的Lambda函数，在此模糊集下的鲁棒（即最坏情况）ΛVaR等价于关于容度的ΛVaR，这是文献中的一个新扩展；该框架统一并扩展了传统的ΛVaR和Choquet分位数（模糊性下的在险价值）；分析了该扩展风险度量的基本性质，并为具有单调Lambda函数的容度下的ΛVaR建立了基于下集族的新的等价表示；分别在φ-散度和似然比约束刻画模糊集时，推导了鲁棒ΛVaR的显式公式。进一步探索了其在多主体间风险分担中的应用，证明了由下集族诱导的风险度量族在inf-卷积下是封闭的，并证明了具有容度和单调Lambda函数的ΛVaR的inf-卷积是另一个容度下的ΛVaR。",
    "fetch_date": "2026-01-19",
    "id": "20260119_8349d4b2"
  },
  {
    "title": "Asset Pricing in the Presence of Market Microstructure Noise",
    "url": "https://arxiv.org/pdf/2511.00308v1",
    "source": "ArXiv",
    "date": "2025-10-31",
    "abstract": "We present two models for incorporating the total effect of market microstructure noise into dynamic pricing of assets and European options. The first model is developed under a Black-Scholes-Merton, continuous-time framework. The second model is a discrete, binomial tree model developed as an extension of the static Grossman-Stiglitz model. Both models are market complete, providing a unique equivalent martingale measure that establishes a unique map between parameters governing the risk-neutral and real-world price dynamics. We provide empirical examples to extract the coefficients in the model, in particular those coefficients characterizing the influence of the microstructure noise on prices. In addition to isolating the impact of noise on the volatility, the discrete model enables us to extract the noise impact on the drift coefficient. We provide evidence for the primary microstructure noise we believe our empirical examples capture.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了两种模型，将市场微观结构噪声的整体效应纳入资产和欧式期权的动态定价中。第一种模型基于Black-Scholes-Merton连续时间框架；第二种是离散二项式树模型，作为静态Grossman-Stiglitz模型的扩展。两种模型均为市场完备模型，提供了唯一的等价鞅测度，建立了风险中性与现实世界价格动态参数之间的唯一映射。通过实证示例提取模型系数，特别是表征微观结构噪声对价格影响的系数。离散模型不仅能分离噪声对波动率的影响，还能提取噪声对漂移系数的影响。我们提供了证据支持实证示例捕捉到的主要微观结构噪声。",
    "fetch_date": "2026-01-19",
    "id": "20260119_bc9fc8bd"
  },
  {
    "title": "Risk-constrained stochastic scheduling of multi-market energy storage systems",
    "url": "https://arxiv.org/pdf/2510.27528v1",
    "source": "ArXiv",
    "date": "2025-10-31",
    "abstract": "Energy storage can promote the integration of renewables by operating with charge and discharge policies that balance an intermittent power supply. This study investigates the scheduling of energy storage assets under energy price uncertainty, with a focus on electricity markets. A two-stage stochastic risk-constrained approach is employed, whereby electricity price trajectories or specific power markets are observed, allowing for recourse in the schedule. Conditional value-at-risk is used to quantify tail risk in the optimization problems; this allows for the explicit specification of a probabilistic risk limit. The proposed approach is tested in an integrated hydrogen system (IHS) and a battery energy storage system (BESS). In the joint design and operation context for the IHS, the risk constraint results in larger installed unit capacities, increasing capital cost but enabling more energy inventory to buffer price uncertainty. As shown in both case studies, there is an operational trade-off between risk and expected reward; this is reflected in higher expected costs (or lower expected profits) with increasing levels of risk aversion. Despite the decrease in expected reward, both systems exhibit substantial benefits of increasing risk aversion. This work provides a general method to address uncertainties in energy storage scheduling, allowing operators to input their level of risk tolerance on asset decisions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究采用两阶段随机风险约束方法，在电价不确定性下优化储能系统调度，重点关注电力市场。通过条件风险价值量化尾部风险，允许在优化问题中明确设定概率风险限制。在集成氢系统和电池储能系统的案例测试中，风险约束导致更大的装机容量（增加资本成本但能缓冲价格波动），并揭示了风险与预期收益之间的运营权衡：风险规避程度越高，预期成本越高（或预期利润越低）。",
    "fetch_date": "2026-01-19",
    "id": "20260119_ef6c55a3"
  },
  {
    "title": "Real-DRL: Teach and Learn in Reality",
    "url": "https://arxiv.org/pdf/2511.00112v1",
    "source": "ArXiv",
    "date": "2025-10-30",
    "abstract": "This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了Real-DRL框架，一种用于安全关键自主系统的深度强化学习（DRL）运行时学习框架，旨在在真实物理系统中开发安全且高性能的动作策略。该框架包含三个交互组件：DRL-学生（负责双自学习和教学式学习的DRL智能体）、PHY-教师（基于物理模型的安全关键动作策略设计）和触发器（管理两者交互）。其创新点包括实时安全信息批量采样、实时安全补丁以及解决未知未知和仿真到现实（Sim2Real）差距的安全挑战。核心价值在于确保系统安全性和自动层次学习，但主要针对物理控制系统（如机器人、自动驾驶），与量化交易实战的直接关联较弱。",
    "fetch_date": "2026-01-19",
    "id": "20260119_4af02956"
  },
  {
    "title": "JaxMARL-HFT: GPU-Accelerated Large-Scale Multi-Agent Reinforcement Learning for High-Frequency Trading",
    "url": "https://arxiv.org/pdf/2511.02136v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "Agent-based modelling (ABM) approaches for high-frequency financial markets are difficult to calibrate and validate, partly due to the large parameter space created by defining fixed agent policies. Multi-agent reinforcement learning (MARL) enables more realistic agent behaviour and reduces the number of free parameters, but the heavy computational cost has so far limited research efforts. To address this, we introduce JaxMARL-HFT (JAX-based Multi-Agent Reinforcement Learning for High-Frequency Trading), the first GPU-accelerated open-source multi-agent reinforcement learning environment for high-frequency trading (HFT) on market-by-order (MBO) data. Extending the JaxMARL framework and building on the JAX-LOB implementation, JaxMARL-HFT is designed to handle a heterogeneous set of agents, enabling diverse observation/action spaces and reward functions. It is designed flexibly, so it can also be used for single-agent RL, or extended to act as an ABM with fixed-policy agents. Leveraging JAX enables up to a 240x reduction in end-to-end training time, compared with state-of-the-art reference implementations on the same hardware. This significant speed-up makes it feasible to exploit the large, granular datasets available in high-frequency trading, and to perform the extensive hyperparameter sweeps required for robust and efficient MARL research in trading. We demonstrate the use of JaxMARL-HFT with independent Proximal Policy Optimization (IPPO) for a two-player environment, with an order execution and a market making agent, using one year of LOB data (400 million orders), and show that these agents learn to outperform standard benchmarks. The code for the JaxMARL-HFT framework is available on GitHub.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "JaxMARL-HFT：首个基于JAX的GPU加速开源多智能体强化学习环境，专为高频交易（HFT）和市场逐单（MBO）数据设计。通过扩展JaxMARL框架并基于JAX-LOB实现，支持异构智能体、多样化观察/动作空间和奖励函数，灵活适用于单智能体RL或固定策略的基于智能体建模（ABM）。利用JAX实现端到端训练时间最高减少240倍，显著加速使大规模多智能体强化学习（MARL）在高频交易中的实际应用成为可能，解决了传统ABM校准困难、MARL计算成本高的问题，提升了交易策略的逼真度和参数效率。",
    "fetch_date": "2026-01-18",
    "id": "20260118_cbdff8d0"
  },
  {
    "title": "When Reasoning Fails: Evaluating 'Thinking' LLMs for Stock Prediction",
    "url": "https://arxiv.org/pdf/2511.08608v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "Problem. \"Thinking\" LLMs (TLLMs) expose explicit or hidden reasoning traces and are widely believed to generalize better on complex tasks than direct LLMs. Whether this promise carries to noisy, heavy-tailed and regime-switching financial data remains unclear.\n  Approach. Using Indian equities (NIFTY constituents), we run a rolling 48m/1m walk-forward evaluation at horizon k = 1 day and dial cross-sectional complexity via the universe size U in {5, 11, 21, 36} while keeping the reasoning budget fixed (B = 512 tokens) for the TLLM. We compare a direct LLM (gpt-4o-mini), a TLLM (gpt-5), and classical learners (ridge, random forest) on cross-sectional ranking loss 1 - IC, MSE, and long/short backtests with realistic costs. Statistical confidence is measured with Diebold-Mariano, Pesaran-Timmermann, and SPA tests.\n  Main findings.\n  (i) As U grows under a fixed budget B, the TLLM's ranking quality deteriorates, whereas the direct LLM remains flat and classical baselines are stable.\n  (ii) TLLM variance is higher, requiring ex-post calibration (winsorization and blending) for stability.\n  (iii) Portfolio results under transaction costs do not support a net advantage for the TLLM.\n  Hypotheses. Our results are consistent with the following testable hypotheses:\n  H1 (Capacity-Complexity Mismatch): for fixed B, TLLM accuracy degrades superlinearly in cross-sectional complexity.\n  H2 (Reasoning Variance): TLLM outputs exhibit higher dispersion date-by-date than direct LLMs, increasing error bars and turnover.\n  H3 (Domain Misfit): next-token prediction objectives and token-budgeted inference are poorly aligned with heavy-tailed, weakly predictable stock returns.\n  Implication. In our setting, \"thinking\" LLMs are not yet ready to replace classical or direct methods for short-horizon stock ranking; scaling the reasoning budget and/or re-aligning objectives appears necessary.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《当推理失败时：评估“思考型”大语言模型在股票预测中的应用》通过滚动窗口回测（48个月训练/1个月测试）评估了思考型大语言模型（TLLMs）在印度股市（NIFTY成分股）中的实战表现。研究发现：（1）在固定推理预算（B=512 tokens）下，随着股票池规模（U）增大，TLLM的排序能力显著下降，而直接LLM和传统模型（岭回归、随机森林）表现稳定；（2）TLLM预测方差较高，需事后校准（缩尾处理与混合策略）以提升稳定性；（3）考虑交易成本后，TLLM未显示出净优势。该研究对量化交易实战具有直接参考价值，揭示了TLLM在复杂金融数据中的局限性。",
    "fetch_date": "2026-01-18",
    "id": "20260118_a659e7d5"
  },
  {
    "title": "Data-driven Feynman-Kac Discovery with Applications to Prediction and Data Generation",
    "url": "https://arxiv.org/pdf/2511.08606v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "In this paper, we propose a novel data-driven framework for discovering probabilistic laws underlying the Feynman-Kac formula. Specifically, we introduce the first stochastic SINDy method formulated under the risk-neutral probability measure to recover the backward stochastic differential equation (BSDE) from a single pair of stock and option trajectories. Unlike existing approaches to identifying stochastic differential equations-which typically require ergodicity-our framework leverages the risk-neutral measure, thereby eliminating the ergodicity assumption and enabling BSDE recovery from limited financial time series data. Using this algorithm, we are able not only to make forward-looking predictions but also to generate new synthetic data paths consistent with the underlying probabilistic law.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种新颖的数据驱动框架，用于发现Feynman-Kac公式背后的概率定律。具体而言，我们引入了首个在风险中性概率测度下构建的随机SINDy方法，能够从单对股票和期权轨迹中恢复后向随机微分方程（BSDE）。与现有识别随机微分方程的方法（通常需要遍历性假设）不同，我们的框架利用风险中性测度，从而消除了遍历性假设，并能够从有限的金融时间序列数据中恢复BSDE。使用该算法，我们不仅能够进行前瞻性预测，还能生成与底层概率定律一致的新合成数据路径。",
    "fetch_date": "2026-01-18",
    "id": "20260118_8cb7bf81"
  },
  {
    "title": "Natural-gas storage modelling by deep reinforcement learning",
    "url": "https://arxiv.org/pdf/2511.02646v1",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们介绍了GasRL，这是一个模拟器，它将经过校准的天然气市场表示与通过深度强化学习（RL）训练的储气运营商策略模型相结合。我们用它来分析最优库存管理如何影响均衡价格以及供需动态。我们测试了多种RL算法，发现Soft Actor Critic（SAC）在GasRL环境中表现出卓越性能：储气运营商的多个目标——包括盈利能力、稳健的市场出清和价格稳定——均成功实现。此外，由SAC推导出的最优策略所引发的均衡价格动态具有波动性和季节性等特征，这些特征与现实世界价格的特征非常接近。值得注意的是，这种对价格历史分布的遵循是在没有明确将模型校准到价格数据的情况下获得的。我们展示了如何使用该模拟器来评估欧盟强制规定的最低储气阈值的影响。我们发现，此类阈值对市场抵御供应冲击分布意外变化的能力具有积极影响。例如，在异常大的冲击下，市场中断……",
    "fetch_date": "2026-01-18",
    "id": "20260118_b4ad04eb"
  },
  {
    "title": "Value of Information-Enhanced Exploration in Bootstrapped DQN",
    "url": "https://arxiv.org/pdf/2511.02969v2",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "Efficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $ε$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出在Bootstrapped DQN算法框架中集成期望信息价值（EVOI）概念，以增强深度强化学习在稀疏奖励环境中的探索能力。通过两种新算法，利用信息价值估计衡量不同网络头之间的意见差异，引导探索向最具潜力的区域。在复杂稀疏奖励的Atari游戏实验中，该方法在提升性能的同时更好地利用了网络初始化产生的不确定性。",
    "fetch_date": "2026-01-18",
    "id": "20260118_9812a06b"
  },
  {
    "title": "Option market making with hedging-induced market impact",
    "url": "https://arxiv.org/pdf/2511.02518v1",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "This paper develops a model for option market making in which the hedging activity of the market maker generates price impact on the underlying asset. The option order flow is modeled by Cox processes, with intensities depending on the state of the underlying and on the market maker's quoted prices. The resulting dynamics combine stochastic option demand with both permanent and transient impact on the underlying, leading to a coupled evolution of inventory and price. We first study market manipulation and arbitrage phenomena that may arise from the feedback between option trading and underlying impact. We then establish the well-posedness of the mixed control problem, which involves continuous quoting decisions and impulsive hedging actions. Finally, we implement a numerical method based on policy optimization to approximate optimal strategies and illustrate the interplay between option market liquidity, inventory risk, and underlying impact.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文开发了一个期权做市模型，其中做市商的对冲活动会对标的资产产生价格影响。期权订单流通过Cox过程建模，其强度取决于标的资产状态和做市商的报价。所得动力学结合了随机期权需求与对标的资产的永久性和暂时性影响，导致库存和价格的耦合演化。我们首先研究了期权交易与标的资产影响之间的反馈可能引发的市场操纵和套利现象。然后，我们建立了混合控制问题的适定性，该问题涉及连续报价决策和脉冲对冲操作。最后，我们实施了一种基于策略优化的数值方法来近似最优策略，并说明了期权市场流动性、库存风险和标的资产影响之间的相互作用。",
    "fetch_date": "2026-01-18",
    "id": "20260118_aa453643"
  },
  {
    "title": "ABIDES-MARL: A Multi-Agent Reinforcement Learning Environment for Endogenous Price Formation and Execution in a Limit Order Book",
    "url": "https://arxiv.org/pdf/2511.02016v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "We present ABIDES-MARL, a framework that combines a new multi-agent reinforcement learning (MARL) methodology with a new realistic limit-order-book (LOB) simulation system to study equilibrium behavior in complex financial market games. The system extends ABIDES-Gym by decoupling state collection from kernel interruption, enabling synchronized learning and decision-making for multiple adaptive agents while maintaining compatibility with standard RL libraries. It preserves key market features such as price-time priority and discrete tick sizes. Methodologically, we use MARL to approximate equilibrium-like behavior in multi-period trading games with a finite number of heterogeneous agents-an informed trader, a liquidity trader, noise traders, and competing market makers-all with individual price impacts. This setting bridges optimal execution and market microstructure by embedding the liquidity trader's optimization problem within a strategic trading environment. We validate the approach by solving an extended Kyle model within the simulation system, recovering the gradual price discovery phenomenon. We then extend the analysis to a liquidity trader's problem where market liquidity arises endogenously and show that, at equilibrium, execution strategies shape market-maker behavior and price dynamics. ABIDES-MARL provides a reproducible foundation for analyzing equilibrium and strategic adaptation in realistic markets and contributes toward building economically interpretable agentic AI systems for finance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文介绍了ABIDES-MARL框架，该框架将多智能体强化学习（MARL）方法与真实的限价订单簿（LOB）模拟系统相结合，用于研究复杂金融市场博弈中的均衡行为。该系统通过将状态收集与内核中断解耦，实现了多个自适应智能体的同步学习与决策，同时保持与标准强化学习库的兼容性。它保留了价格-时间优先级和离散报价单位等关键市场特征。方法上，作者使用MARL来近似具有有限数量异质智能体（包括知情交易者、流动性交易者、噪声交易者和竞争性做市商）的多期交易博弈中的类均衡行为，所有智能体均具有个体价格影响。该设置通过将流动性交易者的优化问题嵌入战略交易环境，桥接了最优执行与市场微观结构。作者通过在模拟系统中求解扩展的Kyle模型来验证该方法，重现了渐进价格发现现象，并进一步将分析扩展到流动性交易者的问题中。",
    "fetch_date": "2026-01-18",
    "id": "20260118_bda8ac26"
  },
  {
    "title": "Trade Execution Flow as the Underlying Source of Market Dynamics",
    "url": "https://arxiv.org/pdf/2511.01471v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "In this work, we demonstrate experimentally that the execution flow, $I = dV/dt$, is the fundamental driving force of market dynamics. We develop a numerical framework to calculate execution flow from sampled moments using the Radon-Nikodym derivative. A notable feature of this approach is its ability to automatically determine thresholds that can serve as actionable triggers. The technique also determines the characteristic time scale directly from the corresponding eigenproblem. The methodology has been validated on actual market data to support these findings. Additionally, we introduce a framework based on the Christoffel function spectrum, which is invariant under arbitrary non-degenerate linear transformations of input attributes and offers an alternative to traditional principal component analysis (PCA), which is limited to unitary invariance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文通过实验证明执行流（I = dV/dt）是市场动态的根本驱动力。作者开发了一个数值框架，利用Radon-Nikodym导数从采样矩计算执行流，该方法能自动确定可作为可操作触发器的阈值，并直接从相应特征问题确定特征时间尺度。该框架已在真实市场数据上验证。此外，作者引入了基于Christoffel函数谱的框架，该框架在输入属性的任意非退化线性变换下具有不变性，可作为传统主成分分析（PCA）的替代方案。",
    "fetch_date": "2026-01-18",
    "id": "20260118_f1cf4d80"
  },
  {
    "title": "Robust optimal consumption, investment and reinsurance for recursive preferences",
    "url": "https://arxiv.org/pdf/2511.03031v1",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "This paper investigates a robust optimal consumption, investment, and reinsurance problem for an insurer with Epstein-Zin recursive preferences operating under model uncertainty. The insurer's surplus follows the diffusion approximation of the Cramér-Lundberg model, and the insurer can purchase proportional reinsurance. Model ambiguity is characterised by a class of equivalent probability measures, and the insurer, being ambiguity-averse, aims to maximise utility under the worst-case scenario. By solving the associated coupled forward-backward stochastic differential equation (FBSDE), we derive closed-form solutions for the optimal strategies and the value function. Our analysis reveals how ambiguity aversion, risk aversion, and the elasticity of intertemporal substitution (EIS) influence the optimal policies. Numerical experiments illustrate the effects of key parameters, showing that optimal consumption decreases with higher risk aversion and EIS, while investment and reinsurance strategies are co-dependent on both financial and insurance market parameters, even without correlation. This study provides a comprehensive framework for insurers to manage capital allocation and risk transfer under deep uncertainty.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了具有Epstein-Zin递归偏好且面临模型不确定性的保险公司，其稳健最优消费、投资与再保险问题。保险公司盈余遵循Cramér-Lundberg模型的扩散近似，并可购买比例再保险。模型模糊性由一类等价概率测度刻画，厌恶模糊性的保险公司旨在最坏情形下最大化效用。通过求解关联的耦合正倒向随机微分方程（FBSDE），推导出最优策略与价值函数的闭式解。分析揭示了模糊厌恶、风险厌恶及跨期替代弹性（EIS）如何影响最优策略。数值实验展示了关键参数的影响：最优消费随风险厌恶和EIS增加而减少，投资与再保险策略则共同依赖于金融和保险市场参数（即使无相关性）。本研究为保险公司管理资本配置和风险提供了综合框架。",
    "fetch_date": "2026-01-18",
    "id": "20260118_d3524df9"
  },
  {
    "title": "Numerical valuation of European options under two-asset infinite-activity exponential Lévy models",
    "url": "https://arxiv.org/pdf/2511.02700v1",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "We propose a numerical method for the valuation of European-style options under two-asset infinite-activity exponential Lévy models. Our method extends the effective approach developed by Wang, Wan & Forsyth (2007) for the 1-dimensional case to the 2-dimensional setting and is applicable for general Lévy measures under mild assumptions. A tailored discretization of the non-local integral term is developed, which can be efficiently evaluated by means of the fast Fourier transform. For the temporal discretization, the semi-Lagrangian theta-method is employed in a convenient splitting fashion, where the diffusion term is treated implicitly and the integral term is handled explicitly by a fixed-point iteration. Numerical experiments for put-on-the-average options under Normal Tempered Stable dynamics reveal favourable second-order convergence of our method whenever the exponential Lévy process has finite-variation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种针对双资产无限活动指数Lévy模型下欧式期权定价的数值方法。该方法将Wang、Wan和Forsyth（2007）针对一维情况开发的有效方法扩展到二维设置，适用于满足温和假设的一般Lévy测度。开发了非局部积分项的定制离散化，可通过快速傅里叶变换高效计算。在时间离散化方面，采用半拉格朗日θ方法进行方便的分裂处理，其中扩散项隐式处理，积分项通过定点迭代显式处理。在正态调和稳定动态下对平均看跌期权的数值实验表明，当指数Lévy过程具有有限变差时，该方法具有良好的二阶收敛性。",
    "fetch_date": "2026-01-18",
    "id": "20260118_7c80c927"
  },
  {
    "title": "Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification",
    "url": "https://arxiv.org/pdf/2511.02469v1",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种新颖的框架，通过将多个大型语言模型（LLMs）建模为交互代理来结构性地模仿联邦公开市场委员会（FOMC）的集体决策过程。每个代理从不同的初始信念（如鹰派或鸽派）出发，基于定性政策文本和定量宏观经济指标生成预测，并通过多轮迭代观察他人输出以修订预测，模拟审议和共识形成。实证结果表明，这种基于辩论的方法在货币政策决策分类上显著优于标准LLM基准。",
    "fetch_date": "2026-01-18",
    "id": "20260118_3423e38f"
  },
  {
    "title": "Asset-liability management with Epstein-Zin utility under stochastic interest rate and unknown market price of risk",
    "url": "https://arxiv.org/pdf/2511.02158v2",
    "source": "ArXiv",
    "date": "2025-11-04",
    "abstract": "This paper solves a consumption-investment choice problem with Epstein-Zin recursive utility under partial information--unobservable market price of risk. The main novelty is the introduction of a terminal liability constraint, a feature directly motivated by practical portfolio management and insurance applications but absent from the recursive utility literature. Such constraint gives rise to a coupled forward-backward stochastic differential equation (FBSDE) whose well-posedness has not been addressed in earlier work. We provide an explicit solution to this FBSDE system--contrasting with the typical existence and uniqueness results with no closed-form expressions in the literature. Under mild additional assumptions, we also establish the Malliavin differentiability of the solution allowing the optimal investment strategy to be expressed as a conditional expectation of random variables that can be efficiently simulated. These results allows us to obtain the explicit expressions of the optimal controls and the value function. Finally, we quantify the utility loss from ignoring learning about the market price of risk, highlighting the economic significance of partial information.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在部分信息（不可观测的市场风险价格）下，使用Epstein-Zin递归效用函数解决了消费-投资选择问题。主要创新是引入了终端负债约束，这一特征直接源于实际投资组合管理和保险应用，但在递归效用文献中尚未涉及。该约束导致了一个耦合的前向-后向随机微分方程（FBSDE），其适定性在先前工作中未得到解决。我们提供了该FBSDE系统的显式解，与文献中通常仅存在性和唯一性而无闭式表达的结果形成对比。在温和的附加假设下，我们还建立了解的Malliavin可微性，使得最优投资策略可以表示为可高效模拟的随机变量的条件期望。这些结果使我们能够获得最优控制和价值函数的显式表达式。最后，我们量化了忽略学习市场风险价格带来的效用损失，突出了部分信息的经济意义。",
    "fetch_date": "2026-01-18",
    "id": "20260118_94fb3441"
  },
  {
    "title": "Numerical methods for solving PIDEs arising in swing option pricing under a two-factor mean-reverting model with jumps",
    "url": "https://arxiv.org/pdf/2511.01587v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "This paper concerns the numerical valuation of swing options with discrete action times under a linear two-factor mean-reverting model with jumps. The resulting sequence of two-dimensional partial integro-differential equations (PIDEs) are convection-dominated and possess a nonlocal integral term due to the presence of jumps. Further, the initial function is nonsmooth. We propose various second-order numerical methods that can adequately handle these challenging features. The stability and convergence of these numerical methods are analysed theoretically. By ample numerical experiments, we confirm their second-order convergence behaviour.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在具有跳跃的线性双因子均值回归模型下，离散执行时间的摆动期权的数值定价方法。所得到的二维偏积分微分方程序列具有对流主导特性，并因跳跃项而包含非局部积分项，且初始函数非光滑。作者提出了多种能有效处理这些挑战性特征的二阶数值方法，并进行了理论上的稳定性和收敛性分析。通过大量数值实验，验证了这些方法的二阶收敛行为。",
    "fetch_date": "2026-01-18",
    "id": "20260118_07d8099c"
  },
  {
    "title": "Differential Beliefs in Financial Markets Under Information Constraints: A Modeling Perspective",
    "url": "https://arxiv.org/pdf/2511.01486v2",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "We apply the theory of McKean-Vlasov-type SDEs to study several problems related to market efficiency in the context of partial information and partially observable financial markets: (i) convergence of reduced-information market price processes to the true price process under an increasing information flow; (ii) a specific mechanism of shrinking biases under increasing information flows; (iii) optimal aggregation of expert opinions by a trader seeking a positive alpha. All these problems are studied by means of (conditional) McKean-Vlasov-type SDEs, Wasserstein barycenters, KL divergence and relevant tools from convex optimization, optimal control and nonlinear filtering. We supply the theoretical results in (i)-(iii) with concrete simulations demonstrating how the proposed models can be applied in practice to model financial markets under information constraints and the arbitrage-seeking behavior of traders with differential beliefs.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文应用McKean-Vlasov型随机微分方程理论，研究部分信息和部分可观测金融市场中的市场效率问题：包括信息流增加时简化信息市场价格过程向真实价格过程的收敛、信息流增加下偏差缩小的具体机制，以及交易者寻求正阿尔法时专家意见的最优聚合。研究采用条件McKean-Vlasov型SDE、Wasserstein重心、KL散度及凸优化、最优控制和非线性滤波等工具，并通过具体模拟展示模型在信息约束下金融市场建模和差异化信念交易者套利行为中的应用。",
    "fetch_date": "2026-01-18",
    "id": "20260118_fa4e0185"
  },
  {
    "title": "High-Dimensional Spatial Arbitrage Pricing Theory with Heterogeneous Interactions",
    "url": "https://arxiv.org/pdf/2511.01271v1",
    "source": "ArXiv",
    "date": "2025-11-03",
    "abstract": "This paper investigates estimation and inference of a Spatial Arbitrage Pricing Theory (SAPT) model that integrates spatial interactions with multi-factor analysis, accommodating both observable and latent factors. Building on the classical mean-variance analysis, we introduce a class of Spatial Capital Asset Pricing Models (SCAPM) that account for spatial effects in high-dimensional assets, where we define {\\it spatial rho} as a counterpart to market beta in CAPM. We then extend SCAPM to a general SAPT framework under a {\\it complete} market setting by incorporating multiple factors. For SAPT with observable factors, we propose a generalized shrinkage Yule-Walker (SYW) estimation method that integrates ridge regression to estimate spatial and factor coefficients. When factors are latent, we first apply an autocovariance-based eigenanalysis to extract factors, then employ the SYW method using the estimated factors. We establish asymptotic properties for these estimators under high-dimensional settings where both the dimension and sample size diverge. Finally, we use simulated and real data examples to demonstrate the efficacy and usefulness of the proposed model and method.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一种结合空间交互与多因子分析的空间套利定价理论（SAPT）模型的估计与推断，该模型同时容纳可观测因子与潜在因子。基于经典的均值-方差分析，我们引入了一类空间资本资产定价模型（SCAPM），用于处理高维资产中的空间效应，其中定义了“空间rho”作为CAPM中市场beta的对应概念。随后，通过在“完全”市场设定下纳入多因子，我们将SCAPM扩展为通用的SAPT框架。对于含可观测因子的SAPT，我们提出了一种广义收缩Yule-Walker（SYW）估计方法，该方法结合岭回归来估计空间与因子系数。当因子为潜在时，我们首先应用基于自协方差的特征分析提取因子，然后使用估计的因子采用SYW方法。我们在高维设定下建立了这些估计量的渐近性质，其中维度与样本量均发散。最后，我们通过模拟与真实数据示例展示了所提模型与方法的有效性和实用性。",
    "fetch_date": "2026-01-18",
    "id": "20260118_9b921678"
  },
  {
    "title": "History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis",
    "url": "https://arxiv.org/pdf/2601.10143v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra \"History Is Not Enough\" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "在量化金融中，训练与实战表现之间的差距——由概念漂移和分布非平稳性驱动——仍是构建可靠数据驱动系统的关键障碍。基于静态历史数据训练的模型常过拟合，导致在动态市场中泛化能力差。“历史不足够”这一理念强调了需要能够随市场演化的自适应数据生成，而非仅依赖过往观测。本文提出一个漂移感知的数据流系统，将基于机器学习的自适应控制集成到数据管理流程中。该系统将参数化数据操作模块（包括单股变换、多股混合和管理操作）与采用基于梯度的双层优化的自适应规划调度器耦合。该设计将数据增强、课程学习和数据工作流管理统一于单一可微分框架下，支持溯源感知的回放和持续数据质量监控。在预测和强化学习交易任务上的大量实验表明，该框架提升了模型性能。",
    "fetch_date": "2026-01-17",
    "id": "20260117_cfe4d383"
  },
  {
    "title": "Reasoning on Time-Series for Financial Technical Analysis",
    "url": "https://arxiv.org/pdf/2511.08616v1",
    "source": "ArXiv",
    "date": "2025-11-06",
    "abstract": "While Large Language Models have been used to produce interpretable stock forecasts, they mainly focus on analyzing textual reports but not historical price data, also known as Technical Analysis. This task is challenging as it switches between domains: the stock price inputs and outputs lie in the time-series domain, while the reasoning step should be in natural language. In this work, we introduce Verbal Technical Analysis (VTA), a novel framework that combine verbal and latent reasoning to produce stock time-series forecasts that are both accurate and interpretable. To reason over time-series, we convert stock price data into textual annotations and optimize the reasoning trace using an inverse Mean Squared Error (MSE) reward objective. To produce time-series outputs from textual reasoning, we condition the outputs of a time-series backbone model on the reasoning-based attributes. Experiments on stock datasets across U.S., Chinese, and European markets show that VTA achieves state-of-the-art forecasting accuracy, while the reasoning traces also perform well on evaluation by industry experts.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种名为“言语技术分析”（VTA）的新框架，旨在结合言语推理与潜在推理，以生成既准确又可解释的股票时间序列预测。其核心创新在于：将股票价格数据转换为文本注释，通过逆均方误差（MSE）奖励目标优化推理轨迹，并基于推理属性调整时间序列骨干模型的输出。实验表明，VTA在美国、中国和欧洲市场股票数据集上实现了最先进的预测精度，且推理轨迹在行业专家评估中表现良好。该方法直接针对实战交易中的技术分析需求，通过提升预测准确性和可解释性，对量化交易策略开发具有明确的应用价值。",
    "fetch_date": "2026-01-17",
    "id": "20260117_00abbb00"
  },
  {
    "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
    "url": "https://arxiv.org/pdf/2511.03628v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "LiveTradeBench：利用大型语言模型寻求实战阿尔法收益。该论文提出了一个实时交易环境，用于在真实且动态变化的市场中评估LLM智能体。其设计遵循三个原则：（i）市场行情和新闻的实时数据流，消除了对离线回测的依赖，防止信息泄露，同时捕捉实时不确定性；（ii）投资组合管理抽象，将控制从单一资产操作扩展到多资产配置，整合了风险管理和跨资产推理；（iii）跨结构不同环境（美国股票和Polymarket预测市场）的多市场评估，这些环境在波动性、流动性和信息流方面存在差异。智能体在每个步骤观察价格、新闻及其投资组合，然后输出平衡风险与回报的百分比配置。该研究通过LiveTradeBench进行了为期50天的实时交易评估。",
    "fetch_date": "2026-01-17",
    "id": "20260117_66c487ad"
  },
  {
    "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
    "url": "https://arxiv.org/pdf/2601.10591v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "ProbFM：一种基于Transformer的概率时间序列基础模型，采用深度证据回归（DER）实现原则性不确定性量化，并提供明确的认知-随机不确定性分解。该模型通过高阶证据学习优化不确定性表示，保持单次计算效率，解决了现有方法在分布假设、不确定性来源混淆和校准机制方面的局限性，为零样本金融预测提供了理论支撑的不确定性分解框架。",
    "fetch_date": "2026-01-17",
    "id": "20260117_027a9635"
  },
  {
    "title": "Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models",
    "url": "https://arxiv.org/pdf/2511.04361v1",
    "source": "ArXiv",
    "date": "2025-11-06",
    "abstract": "Energy markets exhibit complex causal relationships between weather patterns, generation technologies, and price formation, with regime changes occurring continuously rather than at discrete break points. Current approaches model electricity prices without explicit causal interpretation or counterfactual reasoning capabilities. We introduce Augmented Time Series Causal Models (ATSCM) for energy markets, extending counterfactual reasoning frameworks to multivariate temporal data with learned causal structure. Our approach models energy systems through interpretable factors (weather, generation mix, demand patterns), rich grid dynamics, and observable market variables. We integrate neural causal discovery to learn time-varying causal graphs without requiring ground truth DAGs. Applied to real-world electricity price data, ATSCM enables novel counterfactual queries such as \"What would prices be under different renewable generation scenarios?\".",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "能源市场在天气模式、发电技术和价格形成之间存在复杂的因果关系，且制度变化是连续而非离散的。现有方法缺乏明确的因果解释和反事实推理能力。本文提出增强时间序列因果模型（ATSCM），将反事实推理框架扩展到具有学习因果结构的多变量时间序列数据。该方法通过可解释因素（天气、发电组合、需求模式）、丰富的电网动态和可观测市场变量对能源系统进行建模，并集成神经因果发现来学习时变因果图，无需真实有向无环图。应用于真实电价数据，ATSCM支持新型反事实查询，例如“在不同可再生能源发电情景下价格会如何变化？”。",
    "fetch_date": "2026-01-17",
    "id": "20260117_59e48323"
  },
  {
    "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.03724v2",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "AI研究人员长期将扑克类游戏作为测试多玩家动态、不完全信息和不确定性下推理能力的平台。本文介绍了Solly，首个在简化版Liar's Poker中达到精英人类水平的AI智能体，该游戏以广泛的多玩家互动为特征。Solly通过无模型、演员-评论家深度强化学习算法的自我对弈进行训练，在单挑和多玩家Liar's Poker中，以胜率（超过50%手牌获胜）和权益（赢得资金）衡量均达到精英人类水平，并优于包括具备推理能力的大型语言模型。Solly开发了新颖的叫牌策略，有效随机化游戏行为，且不易被世界级人类玩家利用。",
    "fetch_date": "2026-01-17",
    "id": "20260117_3287cbec"
  },
  {
    "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration",
    "url": "https://arxiv.org/pdf/2601.10148v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文《DecisionLLM：用于长序列决策探索的大语言模型》探讨了将大语言模型应用于离线决策任务。研究指出，传统强化学习方法在处理长序列决策时面临挑战，而基于Transformer架构的大语言模型在复杂推理和规划任务中表现出色。然而，LLMs天生无法直接理解连续数值，因为文本字符串表示缺乏对数值大小和顺序的本征理解。为解决这一问题，作者提出将轨迹数据视为一种独立模态，通过学习将轨迹数据与自然语言任务描述对齐，使模型能够自回归地预测未来决策。该研究为长序列决策问题提供了新的思路，但主要聚焦于离线场景和理论框架，尚未涉及实时交易环境的具体应用或性能验证。",
    "fetch_date": "2026-01-17",
    "id": "20260117_c5501908"
  },
  {
    "title": "Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition",
    "url": "https://arxiv.org/pdf/2601.10043v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "Particularly, financial named-entity recognition (NER) is one of the many important approaches to translate unformatted reports and news into structured knowledge graphs. However, free, easy-to-use large language models (LLMs) often fail to differentiate organisations as people, or disregard an actual monetary amount entirely. This paper takes Meta's Llama 3 8B and applies it to financial NER by combining instruction fine-tuning and Low-Rank Adaptation (LoRA). Each annotated sentence is converted into an instruction-input-output triple, enabling the model to learn task descriptions while fine-tuning with small low-rank matrices instead of updating all weights. Using a corpus of 1,693 sentences, our method obtains a micro-F1 score of 0.894 compared with Qwen3-8B, Baichuan2-7B, T5, and BERT-Base. We present dataset statistics, describe training hyperparameters, and perform visualizations of entity density, learning curves, and evaluation metrics. Our results show that instruction tuning combined with parameter-efficient fine-tuning enables state-of-the-art performance on domain-sensitive NER.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种结合指令微调与低秩适配（LoRA）的方法，将Meta的Llama 3 8B模型应用于金融命名实体识别（NER）。通过将标注句子转换为指令-输入-输出三元组，模型能在微调时学习任务描述，同时仅更新少量低秩矩阵而非全部权重。在1,693句语料上，该方法取得了0.894的微平均F1分数，优于Qwen3-8B、Baichuan2-7B、T5和BERT-Base等模型。结果表明，指令微调与参数高效微调相结合，能在领域敏感的NER任务上实现先进性能。",
    "fetch_date": "2026-01-17",
    "id": "20260117_4997e8dd"
  },
  {
    "title": "From rough to multifractal multidimensional volatility: A multidimensional Log S-fBM model",
    "url": "https://arxiv.org/pdf/2601.10517v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "We introduce the multivariate Log S-fBM model (mLog S-fBM), extending the univariate framework proposed by Wu \\textit{et al.} to the multidimensional setting. We define the multidimensional Stationary fractional Brownian motion (mS-fBM), characterized by marginals following S-fBM dynamics and a specific cross-covariance structure. It is parametrized by a correlation scale $T$, marginal-specific intermittency parameters and Hurst exponents, as well as their multidimensional counterparts: the co-intermittency matrix and the co-Hurst matrix. The mLog S-fBM is constructed by modeling volatility components as exponentials of the mS-fBM, preserving the dependence structure of the Gaussian core. We demonstrate that the model is well-defined for any co-Hurst matrix with entries in $[0, \\frac{1}{2}[$, supporting vanishing co-Hurst parameters to bridge rough volatility and multifractal regimes. We generalize the small intermittency approximation technique to the multivariate setting to develop an efficient Generalized Method of Moments calibration procedure, estimating cross-covariance parameters for pairs of marginals. We validate it on synthetic data and apply it to S\\&P 500 market data, modeling stock return fluctuations. Diagonal estimates of the stock Hurst matrix, corresponding to single-stock log-volatility Hurst exponents, are close to 0, indicating multifractal behavior, while co-Hurst off-diagonal entries are close to the Hurst exponent of the S\\&P 500 index ($H \\approx 0.12$), and co-intermittency off-diagonal entries align with univariate intermittency estimates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了多维Log S-fBM模型（mLog S-fBM），将单变量粗糙波动率框架扩展至多维场景。该模型基于多维平稳分数布朗运动（mS-fBM），其边缘遵循S-fBM动态并具有特定的交叉协方差结构，参数包括相关尺度T、边缘间歇参数、赫斯特指数以及多维对应项（共间歇矩阵和共赫斯特矩阵）。波动率分量被建模为mS-fBM的指数函数，保留了高斯核心的依赖结构。模型支持共赫斯特矩阵条目在[0, 1/2)范围内，允许消失的共赫斯特参数以桥接粗糙波动率和多重分形机制。作者将小间歇近似技术推广至多变量设置，开发了高效的广义矩估计校准程序，用于估计边缘对的交叉协方差参数，并在合成数据和标普500市场数据上进行了验证。",
    "fetch_date": "2026-01-17",
    "id": "20260117_540de3e5"
  },
  {
    "title": "Dynamic reinsurance via martingale transport",
    "url": "https://arxiv.org/pdf/2601.10375v1",
    "source": "ArXiv",
    "date": "2026-01-15",
    "abstract": "We formulate a dynamic reinsurance problem in which the insurer seeks to control the terminal distribution of its surplus while minimizing the L2-norm of the ceded risk. Using techniques from martingale optimal transport, we show that, under suitable assumptions, the problem admits a tractable solution analogous to the Bass martingale. We first consider the case where the insurer wants to match a given terminal distribution of the surplus process, and then relax this condition by only requiring certain moment or risk-based constraints.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种动态再保险问题，其中保险公司寻求控制其盈余的终端分布，同时最小化分出风险的L2范数。利用鞅最优传输技术，我们在适当假设下证明该问题存在类似于巴斯鞅的易处理解。首先考虑保险公司希望匹配盈余过程的给定终端分布的情况，然后通过仅要求某些矩或基于风险的约束来放宽这一条件。",
    "fetch_date": "2026-01-17",
    "id": "20260117_63b5f1bf"
  },
  {
    "title": "Efficiency versus Robustness under Tail Misspecification: Importance Sampling and Moment-Based VaR Bracketing",
    "url": "https://arxiv.org/pdf/2601.09927v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "Value-at-Risk (VaR) estimation at high confidence levels is inherently a rare-event problem and is particularly sensitive to tail behavior and model misspecification. This paper studies the performance of two simulation-based VaR estimation approaches, importance sampling and discrete moment matching, under controlled tail misspecification. The analysis separates the nominal model used for estimator construction from the true data-generating process used for evaluation, allowing the effects of heavy-tailed returns to be examined in a transparent and reproducible setting. Daily returns of a broad equity market proxy are used to calibrate a nominal Gaussian model, while true returns are generated from Student-t distributions with varying degrees of freedom to represent increasingly heavy tails. Importance sampling is implemented via exponential tilting of the Gaussian model, and VaR is estimated through likelihood-weighted root-finding. Discrete moment matching constructs deterministic lower and upper VaR bounds by enforcing a finite number of moment constraints on a discretized loss distribution. The results demonstrate a clear trade-off between efficiency and robustness. Importance sampling produces low-variance VaR estimates under the nominal model but systematically underestimates the true VaR under heavy-tailed returns, with bias increasing at higher confidence levels and for thicker tails. In contrast, discrete moment matching yields conservative VaR bracketing that remains robust under tail misspecification. These findings highlight that variance reduction alone is insufficient for reliable tail risk estimation when model uncertainty is significant.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在受控尾部误设条件下，研究了两种基于模拟的VaR估计方法（重要性采样和离散矩匹配）的性能，揭示了效率与鲁棒性之间的权衡。通过将用于估计器构建的名义模型与用于评估的真实数据生成过程分离，在透明可复现的环境中考察了厚尾收益的影响。研究使用广泛股票市场代理的日收益来校准名义高斯模型，而真实收益则来自不同自由度的Student-t分布以代表不同程度的厚尾。重要性采样通过高斯模型的指数倾斜实现，VaR通过似然加权求根法估计；离散矩匹配通过对离散化损失分布施加有限数量的矩约束来构建确定性的VaR上下界。结果表明，重要性采样在模型正确时效率高但在尾部误设时可能失效，而矩匹配方法更稳健但效率较低。",
    "fetch_date": "2026-01-17",
    "id": "20260117_aa283aa6"
  },
  {
    "title": "A continuous-time Kyle model with price-responsive traders",
    "url": "https://arxiv.org/pdf/2601.09872v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "Classical Kyle-type models of informed trading typically treat noise trader demand as purely exogenous. In reality, many market participants react to price movements and news, generating feedback effects that can significantly alter market dynamics. This paper develops a continuous-time Kyle framework in which two types of price-responsive traders (momentum and contrarian traders) adjust their demand in response to price signals. This extension yields a finite-dimensional Kalman filter for price discovery and leads to a forward-backward Riccati system characterizing equilibrium. We show that when feedback is weak, equilibrium exists and is unique as a smooth perturbation of the classical Kyle solution, allowing us to derive explicit comparative statics for insider profits and price informativeness. For stronger feedback, the model generates rich dynamics, including potential multiplicity of equilibria and amplification effects. Our framework thus bridges the gap between purely exogenous noise and more realistic, behaviorally motivated trading.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "经典Kyle模型通常将噪声交易者需求视为纯粹外生。现实中，许多市场参与者会对价格变动和新闻做出反应，产生可能显著改变市场动态的反馈效应。本文开发了一个连续时间Kyle框架，其中两种类型的价格响应型交易者（动量交易者和逆向交易者）根据价格信号调整其需求。这一扩展为价格发现提供了一个有限维卡尔曼滤波器，并导出了一个描述均衡的前向-后向Riccati系统。研究表明，当反馈较弱时，均衡存在且唯一，可作为经典Kyle解的光滑扰动，从而能够推导出内幕交易者利润和价格信息性的显式比较静态。对于更强的反馈，该模型产生了丰富的动态，包括潜在的多重均衡和放大效应。因此，该框架弥合了纯粹外生噪声与更现实、行为动机驱动的交易之间的差距。",
    "fetch_date": "2026-01-17",
    "id": "20260117_5c034d77"
  },
  {
    "title": "Defining the payback period for nonconventional cash flows: an axiomatic approach",
    "url": "https://arxiv.org/pdf/2511.03568v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "The payback period is unambiguously defined for conventional investment projects, projects in which a series of cash outflows is followed by a series of cash inflows. Its definition for nonconventional projects is more challenging, since their balances (cumulative cash flow streams) may have multiple break-even points. Academics and practitioners offer a few contradictory recipes to manage this issue, suggesting to use the first break-even point of the balance, the last break-even point of the balance, or the break-even point of the modified cumulative cash flow stream, representing the moment of time in which the cumulative cash inflow exceeds the total cash outflow. In this note, we show that the last break-even point of the project balance is the only definition of the payback period consistent with a set of economically meaningful axioms. An analogous result is established for the discounted payback period.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对非常规现金流项目的投资回收期定义问题，提出了基于经济公理的方法。作者指出，对于非常规项目（其累计现金流可能存在多个盈亏平衡点），现有定义存在矛盾（如使用首个、末个盈亏平衡点或修正累计现金流的平衡点）。通过建立一组经济上有意义的公理，证明了项目余额的最后一个盈亏平衡点是唯一符合公理的投资回收期定义，并对贴现投资回收期得出了类似结论。",
    "fetch_date": "2026-01-17",
    "id": "20260117_e134b5dd"
  },
  {
    "title": "PELVE from a regulatory perspective",
    "url": "https://arxiv.org/pdf/2511.03551v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "Under Solvency II, the Value-at-Risk (VaR) is applied, although there is broad consensus that the Expected Shortfall (ES) constitutes a more appropriate measure. Moving towards ES would necessitate specifying the corresponding ES level. The recently introduced Probability Equivalent Level of VaR and ES (PELVE) determines this by requiring that ES equals the prescribed VaR for a given future payoff, reflecting the situation of an individual insurer. We incorporate the regulator's perspective by proposing PELVE-inspired methods for multiple insurers. We analyze existence and uniqueness of the resulting ES levels, derive expressions for elliptically distributed payoffs and establish limit results for multivariate regularly distributed payoffs. A case study highlights that the choice of method is crucial when payoffs arise from different distribution families. Moreover, we recommend specific methods.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "从监管视角探讨PELVE（VaR与ES的概率等价水平）。在Solvency II框架下，尽管预期损失（ES）被广泛认为更合适，但仍采用风险价值（VaR）。转向ES需确定对应ES水平。PELVE通过要求ES等于给定未来收益的预设VaR来确定该水平，反映单个保险公司情况。本文纳入监管视角，提出适用于多家保险公司的PELVE启发方法，分析所得ES水平的存在性与唯一性，推导椭圆分布收益的表达式，建立多元正则分布收益的极限结果。案例研究表明，当收益来自不同分布族时，方法选择至关重要，并推荐了具体方法。",
    "fetch_date": "2026-01-17",
    "id": "20260117_a903ff77"
  },
  {
    "title": "Multifractality and sample size influence on Bitcoin volatility patterns",
    "url": "https://arxiv.org/pdf/2511.03314v1",
    "source": "ArXiv",
    "date": "2025-11-05",
    "abstract": "The finite sample effect on the Hurst exponent (HE) of realized volatility time series is examined using Bitcoin data. This study finds that the HE decreases as the sampling period $Δ$ increases and a simple finite sample ansatz closely fits the HE data. We obtain values of the HE as $Δ\\rightarrow 0$, which are smaller than 1/2, indicating rough volatility. The relative error is found to be $1\\%$ for the widely used five-minute realized volatility. Performing a multifractal analysis, we find the multifractality in the realized volatility time series, smaller than that of the price-return time series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究利用比特币数据，探讨了已实现波动率时间序列的Hurst指数（HE）受有限样本效应的影响。研究发现，HE随采样周期Δ增大而减小，且一个简单的有限样本模型能较好地拟合HE数据。当Δ趋近于0时，HE值小于1/2，表明存在粗糙波动率。对于广泛使用的五分钟已实现波动率，相对误差为1%。通过多重分形分析，发现已实现波动率时间序列存在多重分形性，但其程度小于价格收益率时间序列。",
    "fetch_date": "2026-01-17",
    "id": "20260117_560f77d0"
  },
  {
    "title": "Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings",
    "url": "https://arxiv.org/pdf/2511.10501v3",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文全面综述了图神经网络（GNN）、深度强化学习（DRL）和概率主题建模（PTM）方法，重点关注其在战略多智能体环境中的潜在应用。文章聚焦于：（i）当前用于揭示未知模型结构、适用于战略对手建模的机器学习方法；（ii）将这些方法与博弈论概念结合，避免依赖现实场景中常不成立的假设（如共同先验假设和自利假设）。文章分析了处理不确定性和异质性的能力（这两者在实际应用中非常常见）以及可扩展性。作为有效建模多智能体环境中关系和交互的潜在方案，作者倡导使用GNN，该方法专为处理图结构数据设计，已被证明在节点分类和链接预测等任务中非常强大。此外，文章回顾了强化学习领域，特别是多智能体深度强化学习，指出单智能体深度强化学习已在复杂游戏决策中广泛应用。",
    "fetch_date": "2026-01-16",
    "id": "20260116_271340db"
  },
  {
    "title": "Robo-Advising in Motion: A Model Predictive Control Approach",
    "url": "https://arxiv.org/pdf/2601.09127v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "Robo-advisors (RAs) are automated portfolio management systems that complement traditional financial advisors by offering lower fees and smaller initial investment requirements. While most existing RAs rely on static, one-period allocation methods, we propose a dynamic, multi-period asset-allocation framework that leverages Model Predictive Control (MPC) to generate suboptimal but practically effective strategies. Our approach combines a Hidden Markov Model with Black-Litterman (BL) methodology to forecast asset returns and covariances, and incorporates practically important constraints, including turnover limits, transaction costs, and target portfolio allocations. We study two predominant optimality criteria in wealth management: dynamic mean-variance (MV) and dynamic risk-budgeting (MRB). Numerical experiments demonstrate that MPC-based strategies consistently outperform myopic approaches, with MV providing flexible and diversified portfolios, while MRB delivers smoother allocations less sensitive to key parameters. These findings highlight the trade-offs between adaptability and stability in practical robo-advising design.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "论文《机器人顾问在行动：一种模型预测控制方法》提出了一种动态多周期资产配置框架，利用模型预测控制（MPC）生成次优但实际有效的策略。该方法结合隐马尔可夫模型与Black-Litterman（BL）方法来预测资产收益和协方差，并纳入实际重要约束，包括换手率限制、交易成本和目标投资组合配置。研究比较了动态均值-方差（MV）和动态风险预算（MRB）两种最优性标准。数值实验表明，基于MPC的策略持续优于短视方法，MV提供灵活多元的投资组合，而MRB则产生对关键参数不敏感的更平滑配置。这些发现突出了实际机器人顾问设计中适应性与稳定性之间的权衡。",
    "fetch_date": "2026-01-16",
    "id": "20260116_3bf2d2c5"
  },
  {
    "title": "XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation",
    "url": "https://arxiv.org/pdf/2601.08896v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究开发了一个稳健的机器学习框架，使用XGBoost回归器对尼泊尔证券交易所（NEPSE）指数的日对数收益率进行一步超前预测。通过工程化特征集（包括滞后对数收益率（最多30天）和成熟的技术指标，如短期和中期滚动波动率测量以及14周期相对强弱指数），并利用Optuna在初始训练段上进行时间序列交叉验证进行超参数优化。通过在前瞻验证（walk-forward validation）下采用扩展窗口和固定长度滚动窗口方案，在多个滞后配置中严格评估样本外性能，模拟真实世界部署并避免前瞻偏差（lookahead bias）。使用均方根误差、平均绝对误差、决定系数（R-squared）以及对数收益率和重构收盘价的方向准确性来评估预测准确性。实证结果表明，最优配置（具有20个滞后的扩展窗口）优于调优的ARIMA和岭回归基准，实现了最低的对数收益率RMSE（0.013450）和MAE（0.009814），并具有较高的方向准确性。",
    "fetch_date": "2026-01-16",
    "id": "20260116_62fa7036"
  },
  {
    "title": "Equilibrium Portfolio Selection under Utility-Variance Analysis of Log Returns in Incomplete Markets",
    "url": "https://arxiv.org/pdf/2511.05861v2",
    "source": "ArXiv",
    "date": "2025-11-08",
    "abstract": "This paper investigates a time-inconsistent portfolio selection problem in the incomplete mar ket model, integrating expected utility maximization with risk control. The objective functional\n  balances the expected utility and variance on log returns, giving rise to time inconsistency and\n  motivating the search of a time-consistent equilibrium strategy. We characterize the equilibrium\n  via a coupled quadratic backward stochastic differential equation (BSDE) system and establish\n  the existence theory in two special cases: (i)the two Brownian motions driven the price dynamics\n  and the factor process are independent with $ρ= 0$; (ii) the trading strategy is constrained to\n  be bounded. For the general case with correlation coefficient $ρ\\neq 0$, we introduce the notion\n  of an approximate time-consistent equilibrium. Employing the solution structure from the\n  equilibrium in the case $ρ= 0$, we can construct an approximate time-consistent equilibrium in\n  the general case with an error of order $O(ρ^2)$. Numerical examples and financial insights are\n  also presented based on deep learning algorithms.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文在不完全市场模型中研究了一个时间不一致的投资组合选择问题，将期望效用最大化与风险控制相结合。目标函数平衡了对数收益的期望效用和方差，导致了时间不一致性，并促使寻找时间一致的均衡策略。我们通过一个耦合的二次倒向随机微分方程（BSDE）系统来刻画均衡，并在两种特殊情况下建立了存在性理论：（i）驱动价格动态和因子过程的两个布朗运动相互独立（ρ=0）；（ii）交易策略被约束为有界的。对于相关系数ρ≠0的一般情况，我们引入了近似时间一致均衡的概念。利用ρ=0情况下均衡的解结构，我们可以在一般情况下构造一个误差为O(ρ²)的近似时间一致均衡。本文还基于深度学习算法提供了数值算例和金融见解。",
    "fetch_date": "2026-01-16",
    "id": "20260116_21d54c72"
  },
  {
    "title": "Economic uncertainty and exchange rates linkage revisited: modelling tail dependence with high frequency data",
    "url": "https://arxiv.org/pdf/2511.05315v1",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "The aim of this paper is to dig deeper into understanding the exchange rates and uncertainty dependence. Using the novel Baker et al. (2020)'s daily Twitter Uncertainty Index and BRICS exchange rates, we investigate their extreme tail dependence within an original time-varying copula framework. Our analysis makes several noteworthy results. Evidence for Indian, Russian and South African currencies indicates an elliptical copulas' dominance implying neither asymmetric features nor extreme movements in their dependence structure with the global economic uncertainty. Importantly, Brazilian and Chinese currencies tail dependence is upward trending suggesting a safe-haven role in times of high global economic uncertainty including the recent COVID-19 pandemic. In such circumstances, these markets offer opportunities to significant gains through portfolio diversification.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文旨在深入探究汇率与不确定性之间的依赖关系。利用Baker等人（2020）提出的每日Twitter不确定性指数和BRICS国家汇率，在原创的时变Copula框架内研究其极端尾部依赖。分析得出若干重要发现：印度、俄罗斯和南非货币的证据表明椭圆Copula占主导，意味着其与全球经济不确定性的依赖结构既无不对称特征也无极端波动。重要的是，巴西和中国货币的尾部依赖呈上升趋势，表明在全球经济高度不确定性（包括近期COVID-19大流行）时期具有避险作用。在此类情况下，这些市场通过投资组合多样化提供了显著收益的机会。",
    "fetch_date": "2026-01-16",
    "id": "20260116_5a9f4a23"
  },
  {
    "title": "Multi-period Learning for Financial Time Series Forecasting",
    "url": "https://arxiv.org/pdf/2511.08622v1",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (TSF). However, current TSF models either use only single-period input, or lack customized designs for addressing multi-period characteristics. In this paper, we propose a Multi-period Learning Framework (MLF) to enhance financial TSF performance. MLF considers both TSF's accuracy and efficiency requirements. Specifically, we design three new modules to better integrate the multi-period inputs for improving accuracy: (i) Inter-period Redundancy Filtering (IRF), that removes the information redundancy between periods for accurate self-attention modeling, (ii) Learnable Weighted-average Integration (LWI), that effectively integrates multi-period forecasts, (iii) Multi-period self-Adaptive Patching (MAP), that mitigates the bias towards certain periods by setting the same number of patches across all periods. Furthermore, we propose a Patch Squeeze module to reduce the number of patches in self-attention modeling for maximized efficiency. MLF incorporates multiple inputs with varying lengths (periods) to achieve better accuracy and reduces the costs of selecting input lengths during training. The codes and datasets are available at https://github.com/Meteor-Stars/MLF.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种多周期学习框架（MLF）用于提升金融时间序列预测（TSF）性能。该框架针对金融时间序列受短期舆论和中长期政策/市场趋势共同影响的特点，设计了三个新模块以更好地整合多周期输入：1) 周期间冗余过滤（IRF），用于消除周期间信息冗余以提升自注意力建模准确性；2) 可学习加权平均集成（LWI），用于有效整合多周期预测结果；3) 多周期自适应分块（MAP），通过设置跨周期统一分块数来减少对特定周期的偏差。此外还提出了补丁压缩模块以减少自注意力计算量。该研究兼顾了预测精度与效率需求，对量化交易中多时间尺度特征融合具有直接应用价值。",
    "fetch_date": "2026-01-16",
    "id": "20260116_5e774279"
  },
  {
    "title": "The LLM Pro Finance Suite: Multilingual Large Language Models for Financial Applications",
    "url": "https://arxiv.org/pdf/2511.08621v1",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "The financial industry's growing demand for advanced natural language processing (NLP) capabilities has highlighted the limitations of generalist large language models (LLMs) in handling domain-specific financial tasks. To address this gap, we introduce the LLM Pro Finance Suite, a collection of five instruction-tuned LLMs (ranging from 8B to 70B parameters) specifically designed for financial applications. Our approach focuses on enhancing generalist instruction-tuned models, leveraging their existing strengths in instruction following, reasoning, and toxicity control, while fine-tuning them on a curated, high-quality financial corpus comprising over 50% finance-related data in English, French, and German.\n  We evaluate the LLM Pro Finance Suite on a comprehensive financial benchmark suite, demonstrating consistent improvement over state-of-the-art baselines in finance-oriented tasks and financial translation. Notably, our models maintain the strong general-domain capabilities of their base models, ensuring reliable performance across non-specialized tasks. This dual proficiency, enhanced financial expertise without compromise on general abilities, makes the LLM Pro Finance Suite an ideal drop-in replacement for existing LLMs in financial workflows, offering improved domain-specific performance while preserving overall versatility. We publicly release two 8B-parameters models to foster future research and development in financial NLP applications: https://huggingface.co/collections/DragonLLM/llm-open-finance.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融行业对先进自然语言处理（NLP）能力的需求日益增长，突显了通用大型语言模型（LLMs）在处理特定领域金融任务方面的局限性。为填补这一空白，我们推出了LLM Pro Finance Suite，这是一套包含五个指令调优LLM（参数规模从8B到70B）的集合，专门为金融应用设计。我们的方法侧重于增强通用指令调优模型，利用其在指令遵循、推理和毒性控制方面的现有优势，同时在精心策划的高质量金融语料库上进行微调，该语料库包含超过50%的英语、法语和德语金融相关数据。我们在全面的金融基准套件上评估了LLM Pro Finance Suite，结果显示在面向金融的任务和金融翻译方面，相较于最先进的基线模型，我们的模型表现出一致的改进。值得注意的是，我们的模型保持了其基础模型的强大通用领域能力，确保了在非专业任务上的可靠性能。这种双重优势——在不损害通用能力的前提下增强金融专业知识——使得LLM Pro Finance Suite在实战交易中具有潜在价值，特别是在金融文本分析、多语言金融信息处理和自动化金融报告生成等应用场景中。",
    "fetch_date": "2026-01-16",
    "id": "20260116_a2cc92c7"
  },
  {
    "title": "Towards Causal Market Simulators",
    "url": "https://arxiv.org/pdf/2511.04469v3",
    "source": "ArXiv",
    "date": "2025-11-06",
    "abstract": "Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文《迈向因果市场模拟器》提出了一种结合变分自编码器与结构因果模型的时序神经因果模型VAE（TNCM-VAE），用于生成具有因果推理能力的合成金融时间序列数据。该方法通过解码器架构中的有向无环图施加因果约束，并采用因果Wasserstein距离进行训练，在保留时间依赖性和因果关系的同时生成反事实市场轨迹。在基于Ornstein-Uhlenbeck过程的合成自回归模型上验证显示，反事实概率估计的L1距离低至0.03-0.10，优于基准。该模型可用于金融压力测试、情景分析和增强回测，通过生成符合底层因果机制的反事实市场路径，提升实战交易中的风险管理和策略评估能力。",
    "fetch_date": "2026-01-16",
    "id": "20260116_b4ecf2e4"
  },
  {
    "title": "Martingale expansion for stochastic volatility",
    "url": "https://arxiv.org/pdf/2601.09324v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "The martingale expansion provides a refined approximation to the marginal distributions of martingales beyond the normal approximation implied by the martingale central limit theorem. We develop a martingale expansion framework specifically suited to continuous stochastic volatility models. Our approach accommodates both small volatility-of-volatility and fast mean-reversion models, yielding first-order perturbation expansions under essentially minimal conditions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "鞅展开为随机波动率模型提供了超越鞅中心极限定理所隐含的正态近似的边际分布精化近似。我们开发了一个专门适用于连续随机波动率模型的鞅展开框架。该方法同时适用于小波动率-波动率模型和快速均值回归模型，在基本最小条件下产生一阶扰动展开。",
    "fetch_date": "2026-01-16",
    "id": "20260116_3b372ea3"
  },
  {
    "title": "The Fourier estimator of spot volatility: Unbounded coefficients and jumps in the price process",
    "url": "https://arxiv.org/pdf/2601.09074v1",
    "source": "ArXiv",
    "date": "2026-01-14",
    "abstract": "In this paper we study the Fourier estimator of Malliavin and Mancino for the spot volatility. We establish the convergence of the trigonometric polynomial to the volatility's path in a setting that includes the following aspects. First, the volatility is required to satisfy a mild integrability condition, but otherwise allowed to be unbounded. Second, the price process is assumed to have cadlag paths, not necessarily continuous. We obtain convergence rates for the probability of a bad approximation in estimated coefficients, with a speed that allow to obtain an almost sure convergence and not just in probability in the estimated reconstruction of the volatility's path. This is a new result even in the setting of continuous paths. We prove that a rescaled trigonometric polynomial approximate the quadratic jump process.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了Malliavin和Mancino提出的傅里叶估计器在点波动率估计中的应用。在允许波动率无界且价格过程具有右连左极路径（可包含跳跃）的设定下，证明了三角多项式对波动率路径的收敛性，并获得了近似误差概率的收敛速度，实现了几乎必然收敛。即使在连续路径设定下，这也是新结果。同时证明了经缩放的三角多项式可近似二次跳跃过程。",
    "fetch_date": "2026-01-16",
    "id": "20260116_97607439"
  },
  {
    "title": "Competitive optimal portfolio selection under mean-variance criterion",
    "url": "https://arxiv.org/pdf/2511.05270v1",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "We investigate a portfolio selection problem involving multi competitive agents, each exhibiting mean-variance preferences. Unlike classical models, each agent's utility is determined by their relative wealth compared to the average wealth of all agents, introducing a competitive dynamic into the optimization framework. To address this game-theoretic problem, we first reformulate the mean-variance criterion as a constrained, non-homogeneous stochastic linear-quadratic control problem and derive the corresponding optimal feedback strategies. The existence of Nash equilibria is shown to depend on the well-posedness of a complex, coupled system of equations. Employing decoupling techniques, we reduce the well-posedness analysis to the solvability of a novel class of multi-dimensional linear backward stochastic differential equations (BSDEs). We solve a new type of nonlinear BSDEs (including the above linear one as a special case) using fixed-point theory. Depending on the interplay between market and competition parameters, three distinct scenarios arise: (i) the existence of a unique Nash equilibrium, (ii) the absence of any Nash equilibrium, and (iii) the existence of infinitely many Nash equilibria. These scenarios are rigorously characterized and discussed in detail.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个涉及多个竞争性代理人的投资组合选择问题，每个代理人都表现出均值-方差偏好。与经典模型不同，每个代理人的效用由其相对于所有代理人平均财富的相对财富决定，从而在优化框架中引入了竞争动态。为了解决这一博弈论问题，作者首先将均值-方差准则重新表述为一个受约束的非齐次随机线性二次控制问题，并推导出相应的最优反馈策略。纳什均衡的存在性被证明取决于一个复杂耦合方程组的适定性。通过使用解耦技术，作者将适定性分析简化为求解一类新颖的多维线性后向随机微分方程（BSDEs）的可解性。作者使用不动点理论求解了一类新型非线性BSDEs（包括上述线性BSDEs作为特例）。根据市场参数和竞争参数之间的相互作用，出现了三种不同的情景：（i）存在唯一的纳什均衡，（ii）不存在任何纳什均衡，以及（iii）存在无限多个纳什均衡。",
    "fetch_date": "2026-01-16",
    "id": "20260116_bdfe8a06"
  },
  {
    "title": "An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones",
    "url": "https://arxiv.org/pdf/2511.05265v1",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于解决带无人机旅行商问题（TSP-D）的端到端深度强化学习方法。该方法采用分层Actor-Critic框架，包含基于Transformer的编码器（采用优化的k近邻稀疏注意力机制）和高效的最小门控单元解码器，旨在通过自监督策略学习处理NP-hard组合优化问题。该研究主要聚焦于算法框架的理论设计与验证，未涉及金融市场数据、交易策略或风险管理等实战交易要素。",
    "fetch_date": "2026-01-16",
    "id": "20260116_62b9fd11"
  },
  {
    "title": "The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries",
    "url": "https://arxiv.org/pdf/2511.05030v3",
    "source": "ArXiv",
    "date": "2025-11-07",
    "abstract": "We introduce a Geometry Informed Model for financial forecasting by embedding high dimensional market data onto constant curvature 2manifolds. Guided by the uniformization theorem, we model market dynamics as Brownian motion on spherical S2, Euclidean R2, and hyperbolic H2 geometries. We further include the torus T, a compact, flat manifold admissible as a quotient space of the Euclidean plane anticipating its relevance for capturing cyclical dynamics. Manifold learning techniques infer the latent curvature from financial data, revealing the torus as the best performing geometry. We interpret this result through a macroeconomic lens, the torus circular dimensions align with endogenous cycles in output, interest rates, and inflation described by IS LM theory. Our findings demonstrate the value of integrating differential geometry with data-driven inference for financial modeling.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种几何信息模型用于金融预测，通过将高维市场数据嵌入到恒定曲率的二维流形上。基于一致化定理，将市场动态建模为球面S²、欧几里得R²和双曲H²几何上的布朗运动，并引入了环面T（欧几里得平面的紧致平坦商空间）以捕捉周期性动态。流形学习技术从金融数据推断潜在曲率，结果显示环面性能最佳。作者从宏观经济视角解读这一结果，认为环面的循环维度与IS-LM理论描述的输出、利率和通胀的内生周期一致。研究展示了微分几何与数据驱动推断结合在金融建模中的价值。",
    "fetch_date": "2026-01-16",
    "id": "20260116_6e5f37f1"
  },
  {
    "title": "Insights into Tail-Based and Order Statistics",
    "url": "https://arxiv.org/pdf/2511.04784v1",
    "source": "ArXiv",
    "date": "2025-11-06",
    "abstract": "Heavy-tailed phenomena appear across diverse domains --from wealth and firm sizes in economics to network traffic, biological systems, and physical processes-- characterized by the disproportionate influence of extreme values. These distributions challenge classical statistical models, as their tails decay too slowly for conventional approximations to hold. Among their key descriptive measures are quantile contributions, which quantify the proportion of a total quantity (such as income, energy, or risk) attributed to observations above a given quantile threshold. This paper presents a theoretical study of the quantile contribution statistic and its relationship with order statistics. We derive a closed-form expression for the joint cumulative distribution function (CDF) of order statistics and, based on it, obtain an explicit CDF for quantile contributions applicable to small samples. We then investigate the asymptotic behavior of these contributions as the sample size increases, establishing the asymptotic normality of the numerator and characterizing the limiting distribution of the quantile contribution. Finally, simulation studies illustrate the convergence properties and empirical accuracy of the theoretical results, providing a foundation for applying quantile contributions in the analysis of heavy-tailed data.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文对基于尾部的分位数贡献统计量及其与顺序统计量的关系进行了理论研究。作者推导了顺序统计量的联合累积分布函数闭式表达式，并基于此得到了适用于小样本的分位数贡献的显式CDF。研究还探讨了样本量增大时这些贡献的渐近行为，建立了分子的渐近正态性并描述了分位数贡献的极限分布。最后通过模拟研究说明了收敛性质。该论文主要提供理论框架和渐近分析，对实际交易策略的直接应用有限，但可为理解极端值分布和风险建模提供理论基础。",
    "fetch_date": "2026-01-16",
    "id": "20260116_e988f3a5"
  },
  {
    "title": "Enhancing Portfolio Optimization with Deep Learning Insights",
    "url": "https://arxiv.org/pdf/2601.07942v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "Our work focuses on deep learning (DL) portfolio optimization, tackling challenges in long-only, multi-asset strategies across market cycles. We propose training models with limited regime data using pre-training techniques and leveraging transformer architectures for state variable inclusion. Evaluating our approach against traditional methods shows promising results, demonstrating our models' resilience in volatile markets. These findings emphasize the evolving landscape of DL-driven portfolio optimization, stressing the need for adaptive strategies to navigate dynamic market conditions and improve predictive accuracy.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们的工作专注于深度学习（DL）投资组合优化，解决长期多头、多资产策略在市场周期中的挑战。我们提出使用预训练技术，在有限的市场状态数据下训练模型，并利用Transformer架构纳入状态变量。与传统方法相比，我们的方法评估结果显示出良好前景，证明了模型在波动市场中的韧性。这些发现强调了DL驱动的投资组合优化不断演变的格局，强调了需要适应性策略来应对动态市场条件并提高预测准确性。",
    "fetch_date": "2026-01-15",
    "id": "20260115_620e7883"
  },
  {
    "title": "Utility-Weighted Forecasting and Calibration for Risk-Adjusted Decisions under Trading Frictions",
    "url": "https://arxiv.org/pdf/2601.07852v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utility-weighted calibration reduces realised decision loss by over 30\\% relative to an uncalibrated baseline ($t$-stat -30.31) for loss differential and improves the Sharpe ratio from -3.62 to -2.29 during a drawdown regime. The mechanism is identified as a structural reduction in the frequency of binding constraints (from 16.0\\% to 5.1\\%), preventing the \"corner solution\" failures that characterize overconfident forecasts in high-friction environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种将预测作为约束决策问题计量经济学输入的方法。预测分布通过结合成本函数和可行集约束系统的效用目标诱导出决策规则。计量经济学目标变为最小化扣除成本后的预期决策损失，而非最小化预测误差。论文开发了与决策损失一致的效用加权校准标准，并建立了充分条件，证明校准后的预测分布弱支配未校准的替代方案。在流动性股票指数期货上使用预先承诺的嵌套前向滚动协议进行的实证研究证实了该理论：相对于未校准基线，所提出的效用加权校准将实际决策损失降低了30%以上（t统计量-30.31），并将夏普比率从-3.62提高到-2.29。",
    "fetch_date": "2026-01-15",
    "id": "20260115_ca50c669"
  },
  {
    "title": "A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures",
    "url": "https://arxiv.org/pdf/2511.06451v1",
    "source": "ArXiv",
    "date": "2025-11-09",
    "abstract": "We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX term structures under no-arbitrage constraints. ARBITER maps market states to an operator that outputs implied volatility and variance curves while enforcing static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and monotonicity. The model couples operator learning with constrained decoders and is trained with extragradient-style updates plus projection. We introduce evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap, Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and state-space sequence models on historical SPX and VIX data. Ablation studies indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI, Lipschitz projection stabilizes calibration, and selective state updates improve long-horizon generalization. We provide identifiability and approximation results and describe practical recipes for arbitrage-free interpolation and extrapolation across maturities and strikes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出ARBITER，一种风险中性神经算子，用于在无套利约束下学习SPX-VIX联合期限结构。该模型将市场状态映射到一个算子，输出隐含波动率和方差曲线，同时强制执行静态套利约束（日历、垂直、蝶式）、Lipschitz边界和单调性。ARBITER通过约束解码器与算子学习相结合，采用超梯度更新和投影进行训练。研究引入了衍生品期限结构的评估指标（NAS、CNAS、NI、Dual-Gap、稳定性率），并在历史SPX和VIX数据上展示了优于傅里叶神经算子、DeepONet和状态空间序列模型的性能。消融研究表明，绑定SPX和VIX分支可降低Dual-Gap并改善NI，Lipschitz投影稳定校准，选择性状态更新提升长期泛化能力。研究提供了可识别性和近似结果，并描述了跨期限和行权价进行无套利插值和外推的实用方法。",
    "fetch_date": "2026-01-15",
    "id": "20260115_2d2a05aa"
  },
  {
    "title": "Push-response anomalies in high-frequency S&P 500 price series",
    "url": "https://arxiv.org/pdf/2511.06177v1",
    "source": "ArXiv",
    "date": "2025-11-09",
    "abstract": "We test the hypothesis that consecutive intraday price changes in the most liquid U.S. equity ETF (SPY) are conditionally nonrandom. Using NBBO event-time data for about 1,500 regular trading days, we form for every lag L ordered pairs of a backward price increment (\"push\") and a forward price increment (\"response\"), standardize them, and estimate the expected responses on a fine grid of push magnitudes. The resulting lag-by-magnitude maps reveal a persistent structural shift: for short lags (1-5,000 ticks), expected responses cluster near zero across most push magnitudes, suggesting high short-term efficiency; beyond that range, pronounced tails emerge, indicating that larger historical pushes increasingly correlate with nonzero conditional responses. We also find that large negative pushes are followed by stronger positive responses than equally large positive pushes, consistent with asymmetric liquidity replenishment after sell-side shocks. Decomposition into symmetric and antisymmetric components and the associated dominance curves confirm that short-horizon efficiency is restored only partially. The evidence points to an intraday, lag-resolved anomaly that is invisible in unconditional returns and that can be used to define tradable pockets and risk controls.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们检验了美国最具流动性的股票ETF（SPY）中连续日内价格变化具有条件非随机性的假设。使用约1500个常规交易日的NBBO事件时间数据，我们为每个滞后L构建了后向价格增量（“推动”）和前向价格增量（“响应”）的有序对，将其标准化，并在推动幅度的精细网格上估计预期响应。得到的滞后-幅度图揭示了一个持续的结构性转变：对于短滞后（1-5000个tick），预期响应在大多数推动幅度下聚集在零附近，表明短期效率较高；超过该范围后，出现明显的尾部，表明较大的历史推动与非零条件响应的相关性逐渐增强。我们还发现，较大的负向推动之后会出现比同等大小的正向推动更强的正向响应，这与卖出方冲击后不对称的流动性补充一致。对称和反对称分量的分解以及相关的优势曲线证实，短期效率仅部分恢复。证据指向一种日内、滞后解析的异常现象，这在未协调的数据中不可见。",
    "fetch_date": "2026-01-15",
    "id": "20260115_d5a58cfd"
  },
  {
    "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
    "url": "https://arxiv.org/pdf/2601.08641v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.\n  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种可解释的多智能体系统，用于应对模因币跟单交易中的操纵性机器人问题。通过模拟资产管理团队结构，系统将复杂任务分解为子任务，并协调专业智能体协作解决。采用少样本思维链提示技术，使每个智能体获得专业模因币知识，以应对加密货币市场特有的挑战，如交易执行延迟和跟单钱包未来表现的不确定性。",
    "fetch_date": "2026-01-15",
    "id": "20260115_4ed8ec19"
  },
  {
    "title": "Regime Discovery and Intra-Regime Return Dynamics in Global Equity Markets",
    "url": "https://arxiv.org/pdf/2601.08571v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Financial markets alternate between tranquil periods and episodes of stress, and return dynamics can change substantially across these regimes. We study regime-dependent dynamics in developed and developing equity indices using a data-driven Hilbert--Huang-based regime identification and profiling pipeline, followed by variable-length Markov modeling of categorized returns. Market regimes are identified using an Empirical Mode Decomposition-based Hilbert--Huang Transform, where instantaneous energy from the Hilbert spectrum separates Normal, High, and Extreme regimes. We then profile each regime using Holo--Hilbert Spectral Analysis, which jointly resolves carrier frequencies, amplitude-modulation frequencies, and amplitude-modulation energy (AME). AME, interpreted as volatility intensity, declines monotonically from Extreme to High to Normal regimes. This decline is markedly sharper in developed markets, while developing markets retain higher baseline volatility intensity even in Normal regimes. Building on these regime-specific volatility signatures, we discretize daily returns into five quintile states $\\mathtt{R}_1$ to $\\mathtt{R}_5$ and estimate Variable-Length Markov Chains via context trees within each regime. Unconditional state probabilities show tail states dominate in Extreme regimes and recede as regimes stabilize, alongside persistent downside asymmetry. Entropy peaks in High regimes, indicating maximum unpredictability during moderate-volatility periods. Conditional transition dynamics, evaluated over contexts of length up to three days from the context-tree estimates, indicate that developed markets normalize more effectively as stress subsides, whereas developing markets retain residual tail dependence and downside persistence even in Normal regimes, consistent with a coexistence of continuation and burst-like shifts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于希尔伯特-黄变换的数据驱动市场状态识别与分析方法，用于研究全球股票市场的状态依赖收益动态。通过经验模态分解识别出正常、高波动和极端三种市场状态，并利用全息-希尔伯特谱分析量化各状态的波动强度特征。研究发现，发达市场在正常状态下波动强度显著下降，而新兴市场即使在正常状态下也保持较高的基础波动水平。基于这些状态特征，研究进一步将日收益离散化为五个分位数状态，并采用变长马尔可夫链建模收益动态。该方法为实战交易提供了状态识别、波动结构分析和状态转换建模的量化工具，有助于构建状态依赖的交易策略和风险管理框架。",
    "fetch_date": "2026-01-15",
    "id": "20260115_6dd59c3e"
  },
  {
    "title": "Feasibility-First Satellite Integration in Robust Portfolio Architectures",
    "url": "https://arxiv.org/pdf/2601.08721v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "The integration of thematic satellite allocations into core-satellite portfolio architectures is commonly approached using factor exposures, discretionary convictions, or backtested performance, with feasibility assessed primarily through liquidity screens or market-impact considerations. While such approaches may be appropriate at institutional scale, they are ill-suited to small portfolios and robustness-oriented allocation frameworks, where dominant constraints arise not from return predictability or trading capacity, but from fixed costs, irreversibility risk, and governance complexity. This paper develops a feasibility-first, non-predictive framework for satellite integration that is explicitly scale-aware. We formalize four nested feasibility layers (physical, economic, structural, and epistemic) that jointly determine whether a satellite allocation is admissible. Physical feasibility ensures implementability under concave market-impact laws; economic feasibility suppresses noise-dominated reallocations via cost-dominance threshold constraints; structural feasibility bounds satellite size through an explicit optionality budget defined by tolerable loss under thesis failure; and epistemic feasibility limits satellite breadth and dispersion through an entropy-based complexity budget. Within this hierarchy, structural optionality is identified as the primary design principle for thematic satellites, with the remaining layers acting as robustness lenses rather than optimization criteria. The framework yields closed-form feasibility bounds on satellite size, turnover, and breadth without reliance on return forecasts, factor premia, or backtested performance, providing a disciplined basis for integrating thematic satellites into small, robustness-oriented portfolios.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种“可行性优先”的非预测性框架，用于将主题卫星配置整合到核心-卫星投资组合架构中。它针对小型投资组合和注重稳健性的配置框架，强调了固定成本、不可逆风险和治理复杂性等主要约束，而非回报可预测性或交易能力。论文形式化了四个嵌套的可行性层级（物理、经济、结构和认知），共同决定卫星配置是否可采纳，旨在明确考虑规模因素，并抑制噪声主导的再配置。",
    "fetch_date": "2026-01-15",
    "id": "20260115_e0c107ca"
  },
  {
    "title": "Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks",
    "url": "https://arxiv.org/pdf/2601.08254v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种由大型语言模型引导的深度强化学习方法（LAM-DRL），用于非地面网络的资源分配。LLM作为高层协调器，在训练过程中生成文本指导来塑造DRL智能体的奖励函数。实验结果表明，在吞吐量、公平性和中断概率方面，相比启发式方法，LAM-DRL在正常天气场景下性能提升40%，在极端天气场景下提升64%。",
    "fetch_date": "2026-01-15",
    "id": "20260115_f7a12cbc"
  },
  {
    "title": "Reinforcement Learning Methods for Neighborhood Selection in Local Search",
    "url": "https://arxiv.org/pdf/2601.07948v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文研究了强化学习在局部搜索元启发式算法中邻域选择策略的应用，评估了多臂老虎机（UCB、ε-greedy）和深度强化学习方法（PPO、DDQN）在旅行商问题、带时间窗的取送货问题和车辆排序问题上的表现。研究发现ε-greedy方法表现稳定，而深度强化学习方法因计算开销较大，仅在较长运行时间下具有竞争力。论文强调了针对搜索特性（如约束违反惩罚导致的成本大幅波动）设计奖励函数的重要性。",
    "fetch_date": "2026-01-15",
    "id": "20260115_88def13e"
  },
  {
    "title": "On The Presence of Double-Descent in Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.06895v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "The double descent (DD) paradox, where over-parameterized models see generalization improve past the interpolation point, remains largely unexplored in the non-stationary domain of Deep Reinforcement Learning (DRL). We present preliminary evidence that DD exists in model-free DRL, investigating it systematically across varying model capacity using the Actor-Critic framework. We rely on an information-theoretic metric, Policy Entropy, to measure policy uncertainty throughout training. Preliminary results show a clear epoch-wise DD curve; the policy's entrance into the second descent region correlates with a sustained, significant reduction in Policy Entropy. This entropic decay suggests that over-parameterization acts as an implicit regularizer, guiding the policy towards robust, flatter minima in the loss landscape. These findings establish DD as a factor in DRL and provide an information-based mechanism for designing agents that are more general, transferable, and robust.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文探讨了深度强化学习（DRL）中存在的双下降（DD）现象，即在模型过参数化后泛化能力反而提升的悖论。研究通过Actor-Critic框架，使用策略熵作为信息论指标来测量训练过程中的策略不确定性。初步结果表明，策略进入第二下降区域与策略熵的持续显著降低相关，表明过参数化起到了隐式正则化的作用，引导策略趋向损失景观中更平坦、更稳健的极小值。这些发现为设计更具泛化性、可迁移性和鲁棒性的智能体提供了信息论机制。",
    "fetch_date": "2026-01-15",
    "id": "20260115_87e79967"
  },
  {
    "title": "Systemic Risk Surveillance",
    "url": "https://arxiv.org/pdf/2601.08598v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Following several episodes of financial market turmoil in recent decades, changes in systemic risk have drawn growing attention. Therefore, we propose surveillance schemes for systemic risk, which allow to detect misspecified systemic risk forecasts in an \"online\" fashion. This enables daily monitoring of the forecasts while controlling for the accumulation of false test rejections. Such online schemes are vital in taking timely countermeasures to avoid financial distress. Our monitoring procedures allow multiple series at once to be monitored, thus increasing the likelihood and the speed at which early signs of trouble may be picked up. The tests hold size by construction, such that the null of correct systemic risk assessments is only rejected during the monitoring period with (at most) a pre-specified probability. Monte Carlo simulations illustrate the good finite-sample properties of our procedures. An empirical application to US banks during multiple crises demonstrates the usefulness of our surveillance schemes for both regulators and financial institutions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种系统性风险的在线监测方案，旨在实时检测错误的风险预测，支持每日监控并控制误报累积。该方案可同时监测多个序列，提高发现早期风险迹象的可能性和速度。通过蒙特卡洛模拟验证了其有限样本性质，并以美国银行在多次危机中的实证应用展示了其对监管机构和金融机构的实用性。",
    "fetch_date": "2026-01-15",
    "id": "20260115_62fc2c28"
  },
  {
    "title": "Systemic Risk in DeFi: A Network-Based Fragility Analysis of TVL Dynamics",
    "url": "https://arxiv.org/pdf/2601.08540v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Systemic risk refers to the overall vulnerability arising from the high degree of interconnectedness and interdependence within the financial system. In the rapidly developing decentralized finance (DeFi) ecosystem, numerous studies have analyzed systemic risk through specific channels such as liquidity pressures, leverage mechanisms, smart contract risks, and historical risk events. However, these studies are mostly event-driven or focused on isolated risk channels, paying limited attention to the structural dimension of systemic risk. Overall, this study provides a unified quantitative framework for ecosystem-level analysis and continuous monitoring of systemic risk in DeFi. From a network-based perspective, this paper proposes the DeFi Correlation Fragility Indicator (CFI), constructed from time-varying correlation networks at the protocol category level. The CFI captures ecosystem-wide structural fragility associated with correlation concentration and increasing synchronicity. Furthermore, we define a Risk Contribution Score (RCS) to quantify the marginal contribution of different protocol types to overall systemic risk. By combining the CFI and RCS, the framework enables both the tracking of time-varying systemic risk and identification of structurally important functional modules in risk accumulation and amplification.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "系统性风险指金融系统因高度互联和相互依赖而产生的整体脆弱性。在快速发展的去中心化金融（DeFi）生态系统中，现有研究多通过流动性压力、杠杆机制、智能合约风险和历史风险事件等特定渠道分析系统性风险，但多为事件驱动或关注孤立风险渠道，对系统性风险的结构维度关注有限。本研究提供了一个统一的量化框架，用于生态系统层面的分析和持续监测DeFi中的系统性风险。从网络视角出发，本文提出了DeFi相关性脆弱性指标（CFI），该指标基于协议类别层面的时变相关性网络构建，捕捉了与相关性集中和同步性增加相关的生态系统范围的结构脆弱性。此外，我们定义了风险贡献分数（RCS）来量化不同协议类型对整体系统性风险的边际贡献。通过结合CFI和RCS，该框架能够实现系统性风险的量化评估和监控。",
    "fetch_date": "2026-01-15",
    "id": "20260115_aaeed36c"
  },
  {
    "title": "A Blessing in Disguise: How DeFi Hacks Trigger Unintended Liquidity Injections into US Money Markets",
    "url": "https://arxiv.org/pdf/2601.08263v1",
    "source": "ArXiv",
    "date": "2026-01-13",
    "abstract": "Do vulnerabilities in Decentralized Finance (DeFi) destabilize traditional short-term funding markets? While the prevailing \"Contagion Hypothesis\" posits that the liquidation of stablecoin reserves triggers fire-sale spirals that transmit distress to traditional markets , we document a robust \"Flight-to-Quality\" effect to the contrary. In the wake of major DeFi exploits, spreads on 3-month AA-rated commercial paper (CP) exhibit a paradoxical narrowing. We identify a \"liquidity recycling\" mechanism driving this outcome: capital fleeing DeFi protocols is re-intermediated into the traditional financial system via Prime Money Market Funds (MMFs) , where strict regulatory constraints (e.g., SEC Rule 2a-7) compel these funds to purchase high-quality paper. Our estimates indicate that this institutional demand shock quantitatively overwhelms the supply shock driven by stablecoin issuer redemptions. Rather than acting as vectors of financial contagion , these crypto native shocks serve as an inadvertent \"safety valve\" in segmented markets , providing transient liquidity support and effectively subsidizing borrowing costs for high-grade issuers in the real economy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文挑战了“传染假说”，提出DeFi黑客攻击意外地为传统货币市场注入流动性。研究发现，DeFi漏洞事件后，3个月期AA级商业票据利差反而收窄，原因是逃离DeFi的资金通过优质货币市场基金（受SEC Rule 2a-7等监管约束）流入传统金融系统，形成“流动性循环”机制。这种机构需求冲击压倒了稳定币赎回带来的供给冲击，为实体经济中的高评级发行人提供了暂时的流动性支持，降低了借贷成本。",
    "fetch_date": "2026-01-15",
    "id": "20260115_210d9387"
  },
  {
    "title": "Optimal Option Portfolios for Student t Returns",
    "url": "https://arxiv.org/pdf/2601.07991v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "We provide an explicit solution for optimal option portfolios under variance and Value at Risk (VaR) minimization when the underlying returns follow a Student t-distribution. The novelty of our paper is the departure from the traditional normal returns setting. Our main contribution is the methodology for obtaining optimal portfolios. Numerical experiments reveal that, as expected, the optimal variance and VaR portfolio compositions differ by a significant amount, suggesting that more realistic tail risk settings can lead to potentially more realistic portfolio allocations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在标的资产收益率服从学生t分布的条件下，为方差和风险价值（VaR）最小化的最优期权组合提供了显式解。其创新之处在于突破了传统正态收益率假设，主要贡献在于构建最优组合的方法论。数值实验表明，方差最优组合与VaR最优组合的构成存在显著差异，这提示更现实的尾部风险设定可能带来更合理的资产配置。",
    "fetch_date": "2026-01-15",
    "id": "20260115_d5d0cc22"
  },
  {
    "title": "Bitcoin Forecasting with Classical Time Series Models on Prices and Volatility",
    "url": "https://arxiv.org/pdf/2511.06224v1",
    "source": "ArXiv",
    "date": "2025-11-09",
    "abstract": "This paper evaluates the performance of classical time series models in forecasting Bitcoin prices, focusing on ARIMA, SARIMA, GARCH, and EGARCH. Daily price data from 2010 to 2020 were analyzed, with models trained on the first 90 percent and tested on the final 10 percent. Forecast accuracy was assessed using MAE, RMSE, AIC, and BIC. The results show that ARIMA provided the strongest forecasts for short-run log-price dynamics, while EGARCH offered the best fit for volatility by capturing asymmetry in responses to shocks. These findings suggest that despite Bitcoin's extreme volatility, classical time series models remain valuable for short-run forecasting. The study contributes to understanding cryptocurrency predictability and sets the stage for future work integrating machine learning and macroeconomic variables.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文评估了经典时间序列模型（ARIMA、SARIMA、GARCH、EGARCH）在预测比特币价格和波动率方面的表现，使用2010-2020年日度数据，通过MAE、RMSE、AIC、BIC等指标评估预测精度。结果表明，ARIMA在短期对数价格动态预测中表现最佳，而EGARCH在捕捉冲击响应不对称性方面对波动率拟合最好。研究发现，尽管比特币波动性极高，经典时间序列模型在短期预测中仍具价值，为理解加密货币可预测性及未来整合机器学习与宏观经济变量的研究奠定了基础。",
    "fetch_date": "2026-01-15",
    "id": "20260115_bf42cf74"
  },
  {
    "title": "The Limits of Complexity: Why Feature Engineering Beats Deep Learning in Investor Flow Prediction",
    "url": "https://arxiv.org/pdf/2601.07131v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "The application of machine learning to financial prediction has accelerated dramatically, yet the conditions under which complex models outperform simple alternatives remain poorly understood. This paper investigates whether advanced signal processing and deep learning techniques can extract predictive value from investor order flows beyond what simple feature engineering achieves. Using a comprehensive dataset of 2.79 million observations spanning 2,439 Korean equities from 2020--2024, we apply three methodologies: \\textit{Independent Component Analysis} (ICA) to recover latent market drivers, \\textit{Wavelet Coherence} analysis to characterize multi-scale correlation structure, and \\textit{Long Short-Term Memory} (LSTM) networks with attention mechanisms for non-linear prediction. Our results reveal a striking finding: a parsimonious linear model using market capitalization-normalized flows (``Matched Filter'' preprocessing) achieves a Sharpe ratio of 1.30 and cumulative return of 272.6\\%, while the full ICA-Wavelet-LSTM pipeline generates a Sharpe ratio of only 0.07 with a cumulative return of $-5.1\\%$. The raw LSTM model collapsed to predicting the unconditional mean, achieving a hit rate of 47.5\\% -- worse than random. We conclude that in low signal-to-noise financial environments, domain-specific feature engineering yields substantially higher marginal returns than algorithmic complexity. These findings establish important boundary conditions for the application of deep learning to financial prediction.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "论文《复杂性的局限：为何在投资者流量预测中特征工程优于深度学习》通过分析2020-2024年韩国2439只股票的279万条数据，对比了独立成分分析（ICA）、小波相干性分析和带注意力机制的LSTM网络等复杂方法，与简单的市值标准化流量（“匹配滤波器”预处理）线性模型。结果显示，简约线性模型的夏普比率达1.30，累计回报272.6%，而复杂深度学习流程的夏普比率仅0.07，累计回报-5.1%。该研究对实战交易具有重要价值，表明在特定金融预测任务中，精心设计的特征工程可能比过度复杂的深度学习模型更有效，为量化交易中的模型选择提供了实证依据。",
    "fetch_date": "2026-01-14",
    "id": "20260114_33e35d4c"
  },
  {
    "title": "Non-Convex Portfolio Optimization via Energy-Based Models: A Comparative Analysis Using the Thermodynamic HypergRaphical Model Library (THRML) for Index Tracking",
    "url": "https://arxiv.org/pdf/2601.07792v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "Portfolio optimization under cardinality constraints transforms the classical Markowitz mean-variance problem from a convex quadratic problem into an NP-hard combinatorial optimization problem. This paper introduces a novel approach using THRML (Thermodynamic HypergRaphical Model Library), a JAX-based library for building and sampling probabilistic graphical models that reformulates index tracking as probabilistic inference on an Ising Hamiltonian. Unlike traditional methods that seek a single optimal solution, THRML samples from the Boltzmann distribution of high-quality portfolios using GPU-accelerated block Gibbs sampling, providing natural regularization against overfitting.\n  We implement three key innovations: (1) dynamic coupling strength that scales inversely with market volatility (VIX), adapting diversification pressure to market regimes; (2) rebalanced bias weights prioritizing tracking quality over momentum for index replication; and (3) sector-aware post-processing ensuring institutional-grade diversification. Backtesting on a 100-stock S and P 500 universe from 2023 to 2025 demonstrates that THRML achieves 4.31 percent annualized tracking error versus 5.66 to 6.30 percent for baselines, while simultaneously generating 128.63 percent total return against the index total return of 79.61 percent. The Diebold-Mariano test confirms statistical significance with p less than 0.0001 across all comparisons. These results position energy-based models as a promising paradigm for portfolio construction, bridging statistical mechanics and quantitative finance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种基于THRML（热力学超图模型库）的非凸投资组合优化新方法，将指数跟踪问题转化为伊辛哈密顿量的概率推断问题。核心创新包括：1）动态耦合强度（与市场波动率VIX成反比），根据市场状况调整分散化压力；2）再平衡偏置权重，优先考虑跟踪质量而非动量；3）行业感知后处理，确保机构级分散化。在2023-2025年S&P 500的100只股票回测中，THRML实现了4.31%的年化跟踪误差，优于传统方法的5.66-6.30%。该方法通过GPU加速的块吉布斯采样从高质量投资组合的玻尔兹曼分布中采样，提供自然正则化以防止过拟合，对实战交易具有较高价值。",
    "fetch_date": "2026-01-14",
    "id": "20260114_fa0320b7"
  },
  {
    "title": "Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction",
    "url": "https://arxiv.org/pdf/2511.07014v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "Diffolio：一种用于多元概率金融时间序列预测和投资组合构建的扩散模型。该模型采用具有分层注意力架构的去噪网络，包含资产层面和市场层面。通过引入基于目标相关矩阵稳定估计的相关引导正则化器，更好地反映横截面相关性。实验表明，在12个行业投资组合的日超额收益上，Diffolio在多元预测准确性和投资组合表现上优于多种概率预测基准，构建的投资组合展现出持续稳健的性能。",
    "fetch_date": "2026-01-14",
    "id": "20260114_fcd6445c"
  },
  {
    "title": "Physics-Informed Singular-Value Learning for Cross-Covariances Forecasting in Financial Markets",
    "url": "https://arxiv.org/pdf/2601.07687v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "A new wave of work on covariance cleaning and nonlinear shrinkage has delivered asymptotically optimal analytical solutions for large covariance matrices. Building on this progress, these ideas have been generalized to empirical cross-covariance matrices, whose singular-value shrinkage characterizes comovements between one set of assets and another. Existing analytical cross-covariance cleaners are derived under strong stationarity and large-sample assumptions, and they typically rely on mesoscopic regularity conditions such as bounded spectra; macroscopic common modes (e.g., a global market factor) violate these conditions. When applied to real equity returns, where dependence structures drift over time and global modes are prominent, we find that these theoretically optimal formulas do not translate into robust out-of-sample performance. We address this gap by designing a random-matrix-inspired neural architecture that operates in the empirical singular-vector basis and learns a nonlinear mapping from empirical singular values to their corresponding cleaned values. By construction, the network can recover the analytical solution as a special case, yet it remains flexible enough to adapt to non-stationary dynamics and mode-driven distortions. Trained on a long history of equity returns, the proposed method achieves a more favorable bias-variance trade-off than purely analytical cleaners and delivers systematically lower out-of-sample cross-covariance prediction errors. Our results demonstrate that combining random-matrix theory with machine learning makes asymptotic theories practically effective in realistic time-varying markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于随机矩阵理论启发的神经网络架构，用于金融市场的交叉协方差矩阵预测。针对现有理论最优公式在真实股票收益数据（存在时变依赖结构和显著全局模式）中样本外表现不佳的问题，该方法在经验奇异向量基上操作，学习从经验奇异值到其清洗值的非线性映射。网络设计能恢复理论解作为特例，同时保持足够灵活性以适应实际市场条件，对实战交易中改进协方差估计和风险管理具有潜在应用价值。",
    "fetch_date": "2026-01-14",
    "id": "20260114_a78ab310"
  },
  {
    "title": "Emissions-Robust Portfolios",
    "url": "https://arxiv.org/pdf/2601.06507v1",
    "source": "ArXiv",
    "date": "2026-01-10",
    "abstract": "We study portfolio choice when firm-level emissions intensities are measured with error. We introduce a scope-specific penalty operator that rescales asset payoffs as a smooth function of revenue-normalized emissions intensity. Under payoff homogeneity, unit-scale invariance, mixture linearity, and a curvature semigroup axiom, the operator is unique and has the closed form $P^{(m)}_j(r,λ)=\\bigl(1-λ/λ_{\\max,j}\\bigr)^m r$. Combining this operator with norm- and moment-constrained ambiguity sets yields robust mean--variance and CVaR programs with exact linear and second-order cone reformulations and economically interpretable dual variables. In a U.S. large-cap equity universe with monthly rebalancing and uniform transaction costs, the resulting strategy reduces average Scope~1 emissions intensity by roughly 92\\% relative to equal weight while exhibiting no statistically detectable reduction in the Sharpe ratio under block-bootstrap inference and no statistically detectable change in average returns under HAC inference. We report the return--emissions Pareto frontier, sensitivity to robustness and turnover constraints, and uncertainty propagation from multiple imputation of emissions disclosures.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究当企业层面的排放强度存在测量误差时的投资组合选择问题。作者引入了一种特定范围的惩罚算子，将资产收益重新缩放为收入标准化排放强度的平滑函数。在收益同质性、单位尺度不变性、混合线性及曲率半群公理下，该算子具有唯一性，并给出闭式解。结合范数和矩约束的模糊集，该方法可转化为具有精确线性及二阶锥重构的稳健均值-方差和CVaR规划模型，其双重变量具有经济解释性。在美国大盘股月度再平衡及统一交易成本的实证中，该策略相对于等权重组合将平均Scope 1排放强度降低了约92%，同时基于块自助法推断的夏普比率无统计显著下降，基于HAC推断的平均收益也无统计显著变化。研究还报告了收益-排放的帕累托前沿、对稳健性和换手约束的敏感性，以及排放多重插补的不确定性传播。",
    "fetch_date": "2026-01-14",
    "id": "20260114_2c2dea9b"
  },
  {
    "title": "Cross-Market Alpha: Testing Short-Term Trading Factors in the U.S. Market via Double-Selection LASSO",
    "url": "https://arxiv.org/pdf/2601.06499v1",
    "source": "ArXiv",
    "date": "2026-01-10",
    "abstract": "Current asset pricing research exhibits a significant gap: a lack of sufficient cross-market validation regarding short-term trading-based factors. Against this backdrop, the development of the Chinese A-share market which is characterized by its retail-investor dominance, policy sensitivity, and high-frequency active trading -- has given rise to specific short-term trading-based factors. This study systematically examines the universality of factors from the Alpha191 library in the U.S. market, addressing the challenge of high-dimensional factor screening through the double-selection LASSO algorithm an established method for cross-market, high-dimensional research. After controlling for 151 fundamental factors from the U.S. equity factor zoo, 17 Alpha191 factors selected by this procedure exhibit significant incremental explanatory power for the cross-section of U.S. stock returns at the 5% level. Together these findings demonstrate that short-term trading-based factors, originating from the unique structure of the Chinese A-share market, provide incremental information not captured by existing mainstream pricing models, thereby enhancing the explanation of cross-sectional return differences.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "当前资产定价研究存在显著空白：缺乏关于短期交易因子的充分跨市场验证。在此背景下，以散户主导、政策敏感、高频活跃交易为特征的中国A股市场催生了特定的短期交易因子。本研究通过双选择LASSO算法（一种成熟的跨市场高维研究方法）系统检验了Alpha191因子库中因子在美国市场的普适性。在控制了美国股票因子动物园中的151个基本面因子后，该程序筛选出的17个Alpha191因子在5%显著性水平上对美国股票横截面收益具有显著的增量解释力。这些发现表明，源自中国A股市场独特结构的短期交易因子提供了现有主流定价模型未捕捉的增量信息，从而增强了对横截面收益的解释。",
    "fetch_date": "2026-01-14",
    "id": "20260114_b879dc2b"
  },
  {
    "title": "Deep Q-Network Based Resilient Drone Communication:Neutralizing First-Order Markov Jammers",
    "url": "https://arxiv.org/pdf/2601.06095v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Deep Reinforcement Learning based solution for jamming communications using Frequency Hopping Spread Spectrum technology in a 16 channel radio environment is presented. Deep Q Network based transmitter continuously selects the next frequency hopping channel while facing first order reactive jamming, which uses observed transition statistics to predict and interrupt transmissions. Through self training, the proposed agent learns a uniform random frequency hopping policy that effectively neutralizes the predictive advantage of the jamming. In the presence of Rayleigh fading and additive noise, the impact of forward error correction Bose Chaudhuri Hocquenghem type codes is systematically evaluated, demonstrating that even moderate redundancy significantly reduces packet loss. Extensive visualization of the learning dynamics, channel utilization distribution, epsilon greedy decay, cumulative reward, BER and SNR evolution, and detailed packet loss tables confirms convergence to a near optimal jamming strategy. The results provide a practical framework for autonomous resilient communications in modern electronic warfare scenarios.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于深度强化学习的抗干扰无人机通信方案，针对采用一阶马尔可夫模型的反应式干扰机。系统在16信道环境中使用跳频扩频技术，通过深度Q网络（DQN）发射机动态选择跳频信道，并利用自训练学习均匀随机跳频策略，有效抵消干扰机的预测优势。研究还系统评估了瑞利衰落和加性噪声下Bose-Chaudhuri-Hocquenghem（BCH）前向纠错码的影响，证明适度冗余能显著降低丢包率。通过学习动态、信道利用率分布、ε-贪婪衰减、累积奖励、误码率与信噪比演化等多维度可视化分析，验证了方案收敛至接近最优的抗干扰策略，为现代电子战场景下的自主弹性通信提供了实用框架。",
    "fetch_date": "2026-01-14",
    "id": "20260114_11f9766d"
  },
  {
    "title": "Correlation Structures and Regime Shifts in Nordic Stock Markets",
    "url": "https://arxiv.org/pdf/2601.06090v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Financial markets are complex adaptive systems characterized by collective behavior and abrupt regime shifts, particularly during crises. This paper studies time-varying dependencies in Nordic equity markets and examines whether correlation-eigenstructure dynamics can be exploited for regime-aware portfolio construction. Using two decades of daily data for the OMXS30, OMXC20, and OMXH25 universes, pronounced regime dependence in rolling correlation matrices is documented: crisis episodes are characterized by sharp increases in the leading eigenvalue and counter-cyclical behavior in the second eigenvalue. Eigenportfolio regressions further support a market-factor interpretation of the dominant eigenmode. Building on these findings, an adaptive portfolio allocation framework is proposed, combining correlation-matrix cleaning, an eigenvalue-ratio crisis indicator and long-only minimum-variance optimization with constraints that bound exposures to dominant eigenmodes. Backtesting results indicate improved downside protection and risk-adjusted performance during stress regimes, while remaining competitive with state-of-the-art benchmarks in tranquil periods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融市场作为复杂自适应系统，表现出集体行为和急剧的体制转换，尤其在危机期间。本文研究了北欧股票市场的时变依赖性，并探讨了能否利用相关性-特征结构动态进行体制感知的投资组合构建。基于OMXS30、OMXC20和OMXH25指数二十年的日度数据，研究发现滚动相关性矩阵具有显著的体制依赖性：危机时期表现为领先特征值急剧上升和第二特征值的逆周期行为。特征投资组合回归进一步支持了主导特征模式的市场因子解释。基于这些发现，本文提出了一个自适应投资组合配置框架，结合相关性矩阵清洗、特征值比率危机指标以及仅做多的最小方差优化，并约束对主导特征模式的敞口。回测结果表明，在压力体制下该框架能改善下行保护和风险调整后表现，同时在平静期与先进基准保持竞争力。",
    "fetch_date": "2026-01-14",
    "id": "20260114_217d6bf3"
  },
  {
    "title": "Who sets the range? Funding mechanics and 4h context in crypto markets",
    "url": "https://arxiv.org/pdf/2601.06084v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Financial markets often appear chaotic, yet ranges are rarely accidental. They emerge from structured interactions between market context and capital conditions. The four-hour timeframe provides a critical lens for observing this equilibrium zone where institutional positioning, leveraged exposure, and liquidity management converge. Funding mechanisms, especially in perpetual futures, act as disciplinary forces that regulate trader behavior, impose economic costs, and shape directional commitment. When funding aligns with the prevailing 4H context, price expansion becomes possible; when it diverges, compression and range-bound behavior dominate. Ranges therefore represent controlled balance rather than indecision, reflecting strategic positioning by informed participants. Understanding how 4H context and funding operate as market governors is essential for interpreting cryptocurrency price action as a rational, power-mediated process.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融市场常显混沌，但价格区间鲜为偶然。它们源于市场背景与资本条件的结构化互动。四小时时间框架为观察这一均衡区提供了关键视角，机构头寸、杠杆敞口和流动性管理在此交汇。资金机制（尤其是永续合约中的）作为纪律性力量，调节交易者行为、施加经济成本并塑造方向性承诺。当资金机制与主导的4H背景一致时，价格扩张成为可能；当两者背离时，压缩和区间震荡行为占主导。因此，区间代表受控平衡而非犹豫不决，反映了知情参与者的战略布局。理解4H背景和资金机制如何作为市场调节器运作，对于将加密货币价格行为解读为理性、权力中介的过程至关重要。",
    "fetch_date": "2026-01-14",
    "id": "20260114_c5b798d0"
  },
  {
    "title": "Crypto Pricing with Hidden Factors",
    "url": "https://arxiv.org/pdf/2601.07664v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "We estimate risk premia in the cross-section of cryptocurrency returns using the Giglio-Xiu (2021) three-pass approach, allowing for omitted latent factors alongside observed stock-market and crypto-market factors. Using weekly data on a broad universe of large cryptocurrencies, we find that crypto expected returns load on both crypto-specific factors and selected equity-industry factors associated with technology and profitability, consistent with increased integration between crypto and traditional markets. In addition, we study non-tradable state variables capturing investor sentiment (Fear and Greed), speculative rotation (Altcoin Season Index), and security shocks (hacked value scaled by market capitalization), which are new to the literature. Relative to conventional Fama-MacBeth estimates, the latent-factor approach yields materially different premia for key factors, highlighting the importance of controlling for unobserved risks in crypto asset pricing.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究采用Giglio-Xiu（2021）三通方法，在控制股票市场和加密货币市场观测因子的同时，允许存在未被观测的潜在因子，以估计加密货币横截面收益的风险溢价。基于广泛大型加密货币的周度数据，研究发现加密货币预期收益同时受加密特定因子以及与技术和盈利能力相关的选定股票行业因子影响，这符合加密与传统市场日益融合的趋势。此外，研究还引入了文献中首次探讨的非交易性状态变量，包括投资者情绪（恐惧与贪婪指数）、投机轮动（山寨币季节指数）和安全冲击（黑客攻击损失市值占比）。相较于传统的Fama-MacBeth估计，潜在因子方法对关键因子的风险溢价估计存在显著差异，凸显了在加密资产定价中控制未观测风险的重要性。",
    "fetch_date": "2026-01-14",
    "id": "20260114_f4ddbdff"
  },
  {
    "title": "Tab-TRM: Tiny Recursive Model for Insurance Pricing on Tabular Data",
    "url": "https://arxiv.org/pdf/2601.07675v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "We introduce Tab-TRM (Tabular-Tiny Recursive Model), a network architecture that adapts the recursive latent reasoning paradigm of Tiny Recursive Models (TRMs) to insurance modeling. Drawing inspiration from both the Hierarchical Reasoning Model (HRM) and its simplified successor TRM, the Tab-TRM model makes predictions by reasoning over the input features. It maintains two learnable latent tokens - an answer token and a reasoning state - that are iteratively refined by a compact, parameter-efficient recursive network. The recursive processing layer repeatedly updates the reasoning state given the full token sequence and then refines the answer token, in close analogy with iterative insurance pricing schemes. Conceptually, Tab-TRM bridges classical actuarial workflows - iterative generalized linear model fitting and minimum-bias calibration - on the one hand, and modern machine learning, in terms of Gradient Boosting Machines, on the other.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "Tab-TRM：一种针对表格数据的保险定价微小递归模型。该网络架构将微小递归模型（TRMs）的递归潜在推理范式应用于保险建模，灵感来源于分层推理模型（HRM）及其简化后继TRM。模型通过两个可学习的潜在标记（答案标记和推理状态）进行预测，这些标记由紧凑、参数高效的递归网络迭代优化。递归处理层在给定完整标记序列的情况下重复更新推理状态，然后优化答案标记，类似于迭代保险定价方案。概念上，Tab-TRM一方面连接了经典精算工作流程（迭代广义线性模型拟合和最小偏差校准），另一方面连接了现代机器学习（如梯度提升机）。",
    "fetch_date": "2026-01-14",
    "id": "20260114_c5945c37"
  },
  {
    "title": "Temporal-Aligned Meta-Learning for Risk Management: A Stacking Approach for Multi-Source Credit Scoring",
    "url": "https://arxiv.org/pdf/2601.07588v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "This paper presents a meta-learning framework for credit risk assessment of Italian Small and Medium Enterprises (SMEs) that explicitly addresses the temporal misalignment of credit scoring models.\n  The approach aligns financial statement reference dates with evaluation dates, mitigating bias arising from publication delays and asynchronous data sources. It is based on a two-step temporal decomposition that at first estimates annual probabilities of default (PDs) anchored to balance-sheet reference dates (December 31st) through a static model. Then it models the monthly evolution of PDs using higher-frequency behavioral data. Finally, we employ stacking-based architecture to aggregate multiple scoring systems, each capturing complementary aspects of default risk, into a unified predictive model. In this way, first level model outputs are treated as learned representations that encode non-linear relationships in financial and behavioral indicators, allowing integration of new expert-based features without retraining base models. This design provides a coherent and interpretable solution to challenges typical of low-default environments, including heterogeneous default definitions and reporting delays. Empirical validation shows that the framework effectively captures credit risk evolution over time, improving temporal consistency and predictive stability relative to standard ensemble methods.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于意大利中小企业信用风险评估的元学习框架，专门解决信用评分模型的时间错位问题。该方法通过两步时间分解：首先使用静态模型估计基于资产负债表参考日期（12月31日）的年度违约概率（PD），然后利用高频行为数据建模PD的月度演变。最后采用基于堆叠的架构整合多个评分系统，将一级模型输出作为学习表示，编码财务和行为指标的非线性关系，允许集成新的专家特征而无需重新训练基础模型。该设计为低违约环境中的典型挑战提供了连贯且可解释的解决方案。",
    "fetch_date": "2026-01-14",
    "id": "20260114_31d1358a"
  },
  {
    "title": "A Three--Dimensional Efficient Surface for Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2601.06271v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "The classical mean-variance framework characterizes portfolio risk solely through return variance and the covariance matrix, implicitly assuming that all relevant sources of risk are captured by second moments. In modern financial markets, however, shocks often propagate through complex networks of interconnections, giving rise to systemic and spillover risks that variance alone does not reflect.\n  This paper develops a unified portfolio optimization framework that incorporates connectedness risk alongside expected return and variance. Using a quadratic measure of network spillovers derived from a connectedness matrix, we formulate a three-objective optimization problem and characterize the resulting three-dimensional efficient surface. We establish existence, uniqueness, and continuity of optimal portfolios under mild regularity conditions and derive closed-form solutions when short-selling is allowed. The trade-off between variance and connectedness is shown to be strictly monotone except in degenerate cases, yielding a well-defined risk-risk frontier.\n  Under simultaneous diagonalizability of the covariance and connectedness matrices, we prove a three-fund separation theorem: all efficient portfolios can be expressed as affine combinations of a minimum-variance portfolio, a minimum-connectedness portfolio, and the tangency portfolio. The framework clarifies how network-based risk alters classical diversification results and provides a transparent theoretical foundation for incorporating systemic connectedness into portfolio choice.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种统一投资组合优化框架，在传统均值-方差框架基础上引入了连通性风险作为第三维度。通过从连通性矩阵导出的网络溢出二次度量，构建了三维目标优化问题，并刻画了三维有效前沿。论文证明了最优投资组合的存在性、唯一性和连续性，在允许卖空条件下给出了闭式解，并展示了方差与连通性之间的严格单调权衡关系。在协方差矩阵与连通性矩阵同时可对角化条件下，证明了三维基金分离定理。该研究主要贡献在于理论框架构建和数学性质证明，未涉及具体交易策略、回测验证或市场应用案例。",
    "fetch_date": "2026-01-14",
    "id": "20260114_1a604acf"
  },
  {
    "title": "PriceSeer: Evaluating Large Language Models in Real-Time Stock Prediction",
    "url": "https://arxiv.org/pdf/2601.06088v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Stock prediction, a subject closely related to people's investment activities in fully dynamic and live environments, has been widely studied. Current large language models (LLMs) have shown remarkable potential in various domains, exhibiting expert-level performance through advanced reasoning and contextual understanding. In this paper, we introduce PriceSeer, a live, dynamic, and data-uncontaminated benchmark specifically designed for LLMs performing stock prediction tasks. Specifically, PriceSeer includes 110 U.S. stocks from 11 industrial sectors, with each containing 249 historical data points. Our benchmark implements both internal and external information expansion, where LLMs receive extra financial indicators, news, and fake news to perform stock price prediction. We evaluate six cutting-edge LLMs under different prediction horizons, demonstrating their potential in generating investment strategies after obtaining accurate price predictions for different sectors. Additionally, we provide analyses of LLMs' suboptimal performance in long-term predictions, including the vulnerability to fake news and specific industries. The code and evaluation data will be open-sourced at https://github.com/BobLiang2113/PriceSeer.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了PriceSeer，一个专为大型语言模型（LLMs）执行股票预测任务设计的实时、动态且数据无污染的基准测试。该基准包含来自11个工业部门的110只美国股票，每只股票包含249个历史数据点，并通过内部和外部信息扩展（如额外金融指标、新闻和虚假新闻）来评估LLMs在不同预测周期下的表现。研究展示了LLMs在获取准确价格预测后生成投资策略的潜力，并分析了其在长期预测中的不足，包括对虚假新闻和特定行业的脆弱性。代码和评估数据将开源。",
    "fetch_date": "2026-01-14",
    "id": "20260114_0822b69a"
  },
  {
    "title": "A Clarifying Note on Long-Horizon Investment and Dollar-Cost Averaging: An Effective Investment Exposure Perspective",
    "url": "https://arxiv.org/pdf/2601.06074v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "It is widely claimed in investment education and practice that extending the investment horizon reduces risk, and that diversifying investment timing, for example through dollar-cost averaging (DCA), further mitigates investment risk. Although such claims are intuitively appealing, they are often stated without precise definitions of risk or a clear separation between risk and uncertainty.\n  This paper revisits these two beliefs within a unified probabilistic framework. We define risk at the expectation level as a property of the generating distribution of cumulative investment outcomes, and distinguish it from uncertainty, understood as the dispersion of realized outcomes across possible paths. To enable meaningful comparisons across horizons and investment schedules, we introduce the notion of effective investment exposure, defined as time-integrated invested capital.\n  Under stationary return processes with finite variance, we show that extending the investment horizon does not alter expected risk, expected return, or the risk-return ratio on a per-unit-exposure basis. In contrast, different investment timing strategies can induce distinct exposure profiles over time. As a result, lump-sum investment and dollar-cost averaging may differ not only in uncertainty but also in expected risk when compared at equal return exposure, although the resulting risk differences are of constant order and do not grow with the investment horizon.\n  These results clarify why common narratives surrounding long-horizon investment and dollar-cost averaging are conceptually misleading, while also explaining why adopting such strategies under budgetary or timing constraints need not be regarded as irrational.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在统一的概率框架下重新审视了“延长投资期限降低风险”和“分散投资时机（如定投）进一步降低风险”这两种常见观点。作者明确定义了风险（期望层面的累积投资结果分布特性）与不确定性（实现结果在不同路径上的离散度），并引入了“有效投资暴露”（时间积分投资资本）的概念进行比较分析。研究表明：在有限方差的平稳收益过程中，延长投资期限不会改变单位暴露的预期风险、预期收益或风险收益比；而不同的投资时机策略会产生随时间变化的暴露特征。",
    "fetch_date": "2026-01-14",
    "id": "20260114_a6e0d2ea"
  },
  {
    "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
    "url": "https://arxiv.org/pdf/2601.05975v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们提出DeePM（深度投资组合管理器），一种结构化深度学习宏观投资组合管理器，通过端到端训练以最大化稳健的风险调整后效用。DeePM解决了金融学习中的三个基本挑战：（1）通过定向延迟（因果筛选）机制解决异步“不规则过滤”问题，优先考虑因果脉冲响应学习而非信息新鲜度；（2）通过宏观经济图先验对抗低信噪比，根据经济第一原理正则化跨资产依赖性；（3）优化分布稳健目标，其中平滑的最差窗口惩罚作为熵风险价值（EVaR）的可微代理——一种窗口稳健的效用，鼓励在最不利历史子期间表现强劲。在2010-2025年对50种多元化期货进行的大规模回测中，DeePM仅使用每日收盘价就实现了净风险调整后收益，大约是经典趋势跟踪策略和被动基准的两倍。此外，DeePM相对于最先进的动量Transformer架构提升了约50%。",
    "fetch_date": "2026-01-13",
    "id": "20260113_f8fa8183"
  },
  {
    "title": "Forecast-to-Fill: Benchmark-Neutral Alpha and Billion-Dollar Capacity in Gold Futures (2015-2025)",
    "url": "https://arxiv.org/pdf/2511.08571v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We test whether simple, interpretable state variables-trend and momentum-can generate durable out-of-sample alpha in one of the world's most liquid assets, gold. Using a rolling 10-year training and 6-month testing walk-forward from 2015 to 2025 (2,793 trading days), we convert a smoothed trend-momentum regime signal into volatility-targeted, friction-aware positions through fractional, impact-adjusted Kelly sizing and ATR-based exits. Out of sample, the strategy delivers a Sharpe ratio of 2.88 and a maximum drawdown of 0.52 percent, net of 0.7 basis-point linear cost and a square-root impact term (gamma = 0.02). A regression on spot-gold returns yields a 43 percent annualized return (CAGR approximately 43 percent) and a 37 percent alpha (Sharpe = 2.88, IR = 2.09) at a 15 percent volatility target with beta approximately 0.03, confirming benchmark-neutral performance. Bootstrap confidence intervals ([2.49, 3.27]) and SPA tests (p = 0.000) confirm statistical significance and robustness to latency, reversal, and cost stress. We conclude that forecast-to-fill engineering-linking transparent signals to executable trades with explicit risk, cost, and impact control-can transform modest predictability into allocator-grade, billion-dollar-scalable alpha.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们测试了简单、可解释的状态变量——趋势和动量——能否在全球最具流动性的资产之一黄金中产生持久的样本外阿尔法。采用2015年至2025年滚动10年训练和6个月测试的前向验证（2,793个交易日），通过分数、影响调整的凯利头寸规模和基于ATR的退出，将平滑的趋势-动量机制信号转化为波动率目标、考虑摩擦的仓位。样本外，该策略在扣除0.7个基点的线性成本和平方根影响项（gamma = 0.02）后，实现了2.88的夏普比率和0.52%的最大回撤。对现货黄金收益的回归显示，在15%的波动率目标下，年化收益率为43%（复合年增长率约43%），阿尔法为37%（夏普比率=2.88，信息比率=2.09），贝塔约0.03，确认了基准中性表现。自助法置信区间（[2.49, 3.27]）和SPA检验（p = 0.000）证实了统计显著性以及对延迟、反转和成本压力的稳健性。我们得出结论，预测到执行工程——将透明信号与具有明确风险、成本和影响控制的可执行交易联系起来——可以转化适度的信号为实际交易价值。",
    "fetch_date": "2026-01-13",
    "id": "20260113_bf90557f"
  },
  {
    "title": "Multi-Period Martingale Optimal Transport: Classical Theory, Neural Acceleration, and Financial Applications",
    "url": "https://arxiv.org/pdf/2601.05290v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "This paper develops a computational framework for Multi-Period Martingale Optimal Transport (MMOT), addressing convergence rates, algorithmic efficiency, and financial calibration. Our contributions include: (1) Theoretical analysis: We establish discrete convergence rates of $O(\\sqrt{Δt} \\log(1/Δt))$ via Donsker's principle and linear algorithmic convergence of $(1-κ)^{2/3}$; (2) Algorithmic improvements: We introduce incremental updates ($O(M^2)$ complexity) and adaptive sparse grids; (3) Numerical implementation: A hybrid neural-projection solver is proposed, combining transformer-based warm-starting with Newton-Raphson projection. Once trained, the pure neural solver achieves a $1{,}597\\times$ online inference speedup ($4.7$s $\\to 2.9$ms) suitable for real-time applications, while the hybrid solver ensures martingale constraints to $10^{-6}$ precision. Validated on 12,000 synthetic instances (GBM, Merton, Heston) and 120 real market scenarios.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文开发了多周期鞅最优传输（MMOT）的计算框架，重点解决收敛速度、算法效率和金融校准问题。主要贡献包括：（1）理论分析：通过Donsker原理建立离散收敛速度O(√Δt log(1/Δt))和线性算法收敛速度(1-κ)^{2/3}；（2）算法改进：引入增量更新（O(M²)复杂度）和自适应稀疏网格；（3）数值实现：提出混合神经-投影求解器，结合基于Transformer的预热启动与牛顿-拉夫森投影。训练后，纯神经求解器实现1,597倍在线推理加速（4.7秒→2.9毫秒），适用于实时应用，而混合求解器确保鞅约束达到10^{-6}精度。在12,000个合成实例（GBM、Merton、Heston）和120个真实市场场景中验证。",
    "fetch_date": "2026-01-13",
    "id": "20260113_218c7ab8"
  },
  {
    "title": "Forecasting implied volatility surface with generative diffusion models",
    "url": "https://arxiv.org/pdf/2511.07571v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We introduce a conditional Denoising Diffusion Probabilistic Model (DDPM) for generating arbitrage-free implied volatility (IV) surfaces, offering a more stable and accurate alternative to existing GAN-based approaches. To capture the path-dependent nature of volatility dynamics, our model is conditioned on a rich set of market variables, including exponential weighted moving averages (EWMAs) of historical surfaces, returns and squared returns of underlying asset, and scalar risk indicators like VIX. Empirical results demonstrate our model significantly outperforms leading GAN-based models in capturing the stylized facts of IV dynamics. A key challenge is that historical data often contains small arbitrage opportunities in the earlier dataset for training, which conflicts with the goal of generating arbitrage-free surfaces. We address this by incorporating a standard arbitrage penalty into the loss function, but apply it using a novel, parameter-free weighting scheme based on the signal-to-noise ratio (SNR) that dynamically adjusts the penalty's strength across the diffusion process. We also show a formal analysis of this trade-off and provide a proof of convergence showing that the penalty introduces a small, controllable bias that steers the model toward the manifold of arbitrage-free surfaces while ensuring the generated distribution remains close to the real-world data.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于生成无套利隐含波动率曲面的条件去噪扩散概率模型，相比现有基于GAN的方法更稳定、准确。模型通过包含历史曲面指数加权移动平均、标的资产收益率及平方收益率、VIX等风险指标的市场变量来捕捉波动率的路径依赖特性。实证表明该模型在捕捉隐含波动率动态特征方面显著优于主流GAN模型。针对历史数据存在微小套利机会的问题，作者在损失函数中引入标准套利惩罚项，并采用基于信噪比的无参数加权方案动态调整惩罚强度。文中还对该权衡进行了形式化分析并提供了收敛性证明。",
    "fetch_date": "2026-01-13",
    "id": "20260113_2dc4728a"
  },
  {
    "title": "Deep Neural Operator Learning for Probabilistic Models",
    "url": "https://arxiv.org/pdf/2511.07235v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们提出了一种用于广义概率模型的深度神经算子框架。在算子满足全局Lipschitz条件且适用于广泛概率模型的情况下，我们建立了该架构的通用逼近定理，并给出了明确的网络规模边界。底层随机过程仅需满足可积性和一般尾部概率条件。我们在前向-后向随机微分方程（FBSDE）框架下验证了这些假设对欧式和美式期权定价问题的适用性，这进而涵盖了由抛物型偏微分方程（无论是否存在自由边界）产生的一类广泛算子。最后，我们提供了一个美式期权篮子的数值示例，证明学习到的模型无需重新训练即可为新执行价格生成最优停止边界。",
    "fetch_date": "2026-01-13",
    "id": "20260113_276bbc39"
  },
  {
    "title": "When the Rules Change: Adaptive Signal Extraction via Kalman Filtering and Markov-Switching Regimes",
    "url": "https://arxiv.org/pdf/2601.05716v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "Static linear models of order flow assume constant parameters, failing precisely when they are needed most: during periods of market stress and structural change. This paper proposes a dynamic, state-dependent framework for order flow signal extraction that adapts to shifting market conditions in the Korean stock market. Using daily transaction data from 2020--2024 covering 2,439 stocks and 2.79 million stock-day observations, we implement three complementary methodologies: (1) an Adaptive Kalman Filter where measurement noise variance is explicitly coupled to market volatility; (2) a three-state Markov-Switching model identifying Bull, Normal, and Crisis regimes; and (3) an Asymmetric Response Function capturing differential investor reactions to positive versus negative shocks. We find that foreign investor predictive power increases 8.9-fold during crisis periods relative to bull markets ($β_{crisis}=0.00204$ vs. $β_{bull}=0.00023$), while individual investors exhibit momentum-chasing behavior with 6.3 times stronger response to positive shocks. The integrated ``All-Weather'' strategy provides modest drawdown reduction during extreme market events, though challenges remain in the post-COVID high-rate environment.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种动态、状态依赖的订单流信号提取框架，旨在适应韩国股市的市场条件变化。通过结合自适应卡尔曼滤波（测量噪声方差与市场波动率耦合）、三状态马尔可夫转换模型（识别牛市、常态和危机状态）以及非对称响应函数（捕捉投资者对正负冲击的差异反应），该研究揭示了危机期间外国投资者预测能力显著增强（β值增长8.9倍），而个人投资者表现出更强的动量追逐行为。综合的“全天候”策略在极端市场事件中能适度减少回撤，但实际应用仍面临挑战。",
    "fetch_date": "2026-01-13",
    "id": "20260113_f6ee0520"
  },
  {
    "title": "Generative Pricing of Basket Options via Signature-Conditioned Mixture Density Networks",
    "url": "https://arxiv.org/pdf/2511.09061v1",
    "source": "ArXiv",
    "date": "2025-11-12",
    "abstract": "We present a generative framework for pricing European-style basket options by learning the conditional terminal distribution of the log arithmetic-weighted basket return. A Mixture Density Network (MDN) maps time-varying market inputs encoded via truncated path signatures to the full terminal density in a single forward pass. Traditional approaches either impose restrictive assumptions or require costly re-simulation whenever inputs change, limiting real-time use. Trained on Monte Carlo (MC) under GBM with time-varying volatility or local volatility, the MDN acts as a reusable surrogate distribution: once trained, it prices new scenarios by integrating the learned density. Across maturities, correlations, and basket weights, the learned densities closely match MC (low KL) and produce small pricing errors, while enabling \\emph{train-once, price-anywhere} reuse at inference-time latency.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于生成式框架的篮子期权定价方法，通过学习对数算术加权篮子收益的条件终端分布，利用混合密度网络（MDN）将经截断路径签名编码的时变市场输入映射至完整终端密度。该方法克服了传统方法在输入变化时需重新模拟的高成本限制，实现了“一次训练、随处定价”的实时应用。在GBM模型下通过蒙特卡洛模拟训练，该网络作为可重用的代理分布，在不同期限、相关性和篮子权重下均表现出与蒙特卡洛模拟相近的密度估计和较小的定价误差。",
    "fetch_date": "2026-01-13",
    "id": "20260113_4dd3024a"
  },
  {
    "title": "A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications",
    "url": "https://arxiv.org/pdf/2511.08735v2",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "In this work, we extend deep learning-based numerical methods to fully coupled forward-backward stochastic differential equations (FBSDEs) within a non-Markovian framework. Error estimates and convergence are provided. In contrast to the existing literature, our approach not only analyzes the non-Markovian framework but also addresses fully coupled settings, in which both the drift and diffusion coefficients of the forward process may be random and depend on the backward components $Y$ and $Z$. Furthermore, we illustrate the practical applicability of our framework by addressing utility maximization problems under rough volatility, which are solved numerically with the proposed deep learning-based methods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究将基于深度学习的数值方法扩展到非马尔可夫框架下的完全耦合前向-后向随机微分方程（FBSDEs），提供了误差估计和收敛性分析。与现有文献相比，该方法不仅分析了非马尔可夫框架，还处理了完全耦合设置，其中前向过程的漂移和扩散系数可以是随机的，并依赖于后向分量Y和Z。此外，通过解决粗糙波动率下的效用最大化问题，并采用所提出的基于深度学习的数值方法求解，展示了该框架的实际应用价值。",
    "fetch_date": "2026-01-13",
    "id": "20260113_a521f976"
  },
  {
    "title": "Levy-stable scaling of risk and performance functionals",
    "url": "https://arxiv.org/pdf/2511.07834v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We develop a finite-horizon model in which liquid-asset returns exhibit Levy-stable scaling on a data-driven window [tau_UV, tau_IR] and aggregate into a finite-variance regime outside. The window and the tail index alpha are identified from the log-log slope of the central body and a two-segment fit of scale versus horizon. With an anchor horizon tau_0, we derive horizon-correct formulas for Value-at-Risk, Expected Shortfall, Sharpe and Information ratios, Kelly under a Value-at-Risk constraint, and one-step drawdown, where each admits a closed-form Gaussian-bias term driven by the exponent gap (1/alpha - 1/2). The implementation is nonparametric up to alpha and fixed tail quantiles. The formulas are reproducible across horizons on the Levy window.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文开发了一个有限期限模型，其中流动性资产收益在数据驱动的窗口[tau_UV, tau_IR]内表现出Levy稳定标度，并在外部聚合成有限方差机制。通过中心体的对数-对数斜率和尺度与期限的两段拟合，识别出窗口和尾部指数alpha。基于锚定期限tau_0，推导了期限校正公式，包括风险价值、预期短缺、夏普和信息比率、风险价值约束下的凯利准则以及一步回撤，每个公式均包含由指数间隙(1/alpha - 1/2)驱动的高斯偏差项。实现过程在alpha和固定尾部分位数方面是非参数的。这些公式在Levy窗口内跨期限可重现。",
    "fetch_date": "2026-01-13",
    "id": "20260113_9550bf42"
  },
  {
    "title": "Geopolitical and Institutional Constraints on Adaptive Market Efficiency -- A Feasibility Diagnostic for Robust Portfolio Construction",
    "url": "https://arxiv.org/pdf/2601.05924v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "This paper develops a structural framework for characterizing the informational feasibility of financial markets under heterogeneous institutional and geopolitical conditions. Departing from the assumption of uniform and time-invariant market efficiency, adaptive efficiency is conceptualized as a localized and state-dependent property emerging from the interaction between economic scale, institutional enforcement, and geopolitical embedding. To operationalize this perspective, the paper introduces the Geopolitical-Adaptive Efficiency Ratio (GAER), a descriptive cross-sectional indicator measuring the concentration of adaptive-efficiency-supporting mass within institutionally and geopolitically central assets. GAER is not a return-predictive signal, factor, or regime classifier. Instead, it functions as a diagnostic boundary condition, delimiting the domain in which ranking-based and robustness-oriented portfolio construction methods are plausibly applicable. The framework integrates insights from adaptive market theory, institutional economics, and political economy, linking disclosure continuity, liquidity provision, and enforcement credibility to the persistence of informational signals in asset prices. GAER is formalized, its theoretical properties are discussed, and its interpretation is illustrated using a global equity snapshot based on publicly observable information. The contribution separates informational feasibility from portfolio construction and execution, providing a conceptual foundation for constraint-aware financial modeling without reliance on forecast-driven assumptions or parametric optimization.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个结构性框架，用于描述在异质性制度和地缘政治条件下金融市场的可行性。该框架摒弃了市场效率统一且时不变的假设，将适应性效率概念化为一种局部化、状态依赖的属性，源于经济规模、制度执行力和地缘政治嵌入之间的相互作用。为操作化这一视角，论文引入了地缘政治-适应性效率比率（GAER），这是一个描述性的横截面指标，用于衡量制度和地缘政治核心资产中支持适应性效率的集中度。GAER并非回报预测信号、因子或制度分类器，而是作为诊断性边界条件，界定基于排序和稳健性导向的投资组合构建方法可能适用的领域。该框架整合了适应性市场理论、制度经济学和政治经济学的见解，将信息披露连续性、流动性提供和执行可信度与信息持久性联系起来。",
    "fetch_date": "2026-01-13",
    "id": "20260113_3e7739f8"
  },
  {
    "title": "Equilibrium Strategies for Singular Dividend Control Problems under the Mean-Variance Criterion",
    "url": "https://arxiv.org/pdf/2511.08433v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We revisit the optimal dividend problem of de Finetti by adding a variance term to the usual criterion of maximizing the expected discounted dividends paid until ruin, in a singular control framework. Investors do not like variability in their dividend distribution, and the mean-variance (MV) criterion balances the desire for large expected dividend payments with small variability in those payments. The resulting MV singular dividend control problem is time-inconsistent, and we follow a game-theoretic approach to find a time-consistent equilibrium strategy. Our main contribution is a new verification theorem for the novel dividend problem, in which the MV criterion is applied to an integral of the control until ruin, a random time that is endogenous to the problem. We demonstrate the use of the verification theorem in two cases for which we obtain the equilibrium dividend strategy (semi-)explicitly, and we provide a numerical example to illustrate our results.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在de Finetti最优分红问题的基础上，在奇异控制框架下，将方差项加入最大化破产前预期折现分红的传统准则中，提出均值-方差（MV）奇异分红控制问题。投资者偏好分红分配的稳定性，MV准则平衡了大额预期分红支付与支付波动性之间的需求。该问题具有时间不一致性，作者采用博弈论方法寻找时间一致均衡策略。主要贡献是针对这一新型分红问题提出了新的验证定理，其中MV准则应用于控制积分直至破产（问题的内生随机时间）。作者通过两个案例（半）显式地获得均衡分红策略，并提供了数值示例来展示结果。",
    "fetch_date": "2026-01-13",
    "id": "20260113_5f20f5ab"
  },
  {
    "title": "Robust distortion risk metrics and portfolio optimization",
    "url": "https://arxiv.org/pdf/2511.08662v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We establish sharp upper and lower bounds for distortion risk metrics under distributional uncertainty. The uncertainty sets are characterized by four key features of the underlying distribution: mean, variance, unimodality, and Wasserstein distance to a reference distribution.\n  We first examine very general distortion risk metrics, assuming only finite variation for the underlying distortion function and without requiring continuity or monotonicity. This broad framework includes notable distortion risk metrics such as range value-at-risk, glue value-at-risk, Gini deviation, mean-median deviation and inter-quantile difference. In this setting, when the uncertainty set is characterized by a fixed mean, variance and a Wasserstein distance, we determine both the worst- and best-case values of a given distortion risk metric and identify the corresponding extremal distribution. When the uncertainty set is further constrained by unimodality with a fixed inflection point, we establish for the case of absolutely continuous distortion functions the extremal values along with their respective extremal distributions. We apply our results to robust portfolio optimization and model risk assessment offering improved decision-making under model uncertainty.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在分布不确定性条件下，为失真风险度量建立了尖锐的上界和下界。不确定性集由基础分布的四个关键特征刻画：均值、方差、单峰性以及与参考分布的Wasserstein距离。研究首先考察了非常一般的失真风险度量，仅假设基础失真函数具有有限变差，无需连续性或单调性。这一广泛框架包含了多种重要的失真风险度量，如范围风险价值、胶合风险价值、基尼偏差、均值-中位数偏差和分位数间差。在此设定下，当不确定性集由固定均值、方差和Wasserstein距离刻画时，确定了给定失真风险度量的最坏情况和最佳情况值，并识别了相应的极值分布。当不确定性集进一步受到具有固定拐点的单峰性约束时，针对绝对连续失真函数的情况，建立了极值及其相应的极值分布。研究结果应用于鲁棒投资组合优化和模型风险评估。",
    "fetch_date": "2026-01-13",
    "id": "20260113_0c20b0ff"
  },
  {
    "title": "\"It Looks All the Same to Me\": Cross-index Training for Long-term Financial Series Prediction",
    "url": "https://arxiv.org/pdf/2511.08658v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We investigate a number of Artificial Neural Network architectures (well-known and more ``exotic'') in application to the long-term financial time-series forecasts of indexes on different global markets. The particular area of interest of this research is to examine the correlation of these indexes' behaviour in terms of Machine Learning algorithms cross-training. Would training an algorithm on an index from one global market produce similar or even better accuracy when such a model is applied for predicting another index from a different market? The demonstrated predominately positive answer to this question is another argument in favour of the long-debated Efficient Market Hypothesis of Eugene Fama.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨了多种人工神经网络架构（包括经典与前沿模型）在全球不同市场指数长期金融时间序列预测中的应用。研究的核心关注点在于，通过机器学习算法的跨市场训练，检验这些指数行为之间的相关性。具体而言，研究试图回答：在一个全球市场的指数上训练的算法模型，当应用于预测另一个不同市场的指数时，是否会产生相似甚至更高的预测准确性？研究结果表明，答案总体上是肯定的。这一发现为长期备受争议的尤金·法玛有效市场假说提供了新的支持论据。",
    "fetch_date": "2026-01-13",
    "id": "20260113_0158cd7b"
  },
  {
    "title": "An extreme Gradient Boosting (XGBoost) Trees approach to Detect and Identify Unlawful Insider Trading (UIT) Transactions",
    "url": "https://arxiv.org/pdf/2511.08306v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "Corporate insiders have control of material non-public preferential information (MNPI). Occasionally, the insiders strategically bypass legal and regulatory safeguards to exploit MNPI in their execution of securities trading. Due to a large volume of transactions a detection of unlawful insider trading becomes an arduous task for humans to examine and identify underlying patterns from the insider's behavior. On the other hand, innovative machine learning architectures have shown promising results for analyzing large-scale and complex data with hidden patterns. One such popular technique is eXtreme Gradient Boosting (XGBoost), the state-of-the-arts supervised classifier. We, hence, resort to and apply XGBoost to alleviate challenges of identification and detection of unlawful activities. The results demonstrate that XGBoost can identify unlawful transactions with a high accuracy of 97 percent and can provide ranking of the features that play the most important role in detecting fraudulent activities.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种基于极端梯度提升（XGBoost）树的方法，用于检测和识别非法内幕交易（UIT）交易。公司内部人员掌握重要的非公开优先信息（MNPI），有时会绕过法律和监管保障措施，利用MNPI进行证券交易。由于交易量巨大，人工检测非法内幕交易并识别内部人员行为中的潜在模式变得困难。而创新的机器学习架构在分析具有隐藏模式的大规模和复杂数据方面显示出有希望的结果。XGBoost作为最先进的监督分类器，被应用于缓解识别和检测非法活动的挑战。结果表明，XGBoost能以97%的高准确率识别非法交易，并能提供在检测欺诈活动中起最重要作用的特征排名。",
    "fetch_date": "2026-01-13",
    "id": "20260113_a2cbd2ec"
  },
  {
    "title": "Machine-learning a family of solutions to an optimal pension investment problem",
    "url": "https://arxiv.org/pdf/2511.07045v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We use a neural network to identify the optimal solution to a family of optimal investment problems, where the parameters determining an investor's risk and consumption preferences are given as inputs to the neural network in addition to economic variables. This is used to develop a practical tool that can be used to explore how pension outcomes vary with preference parameters. We use a Black-Scholes economic model so that we may validate the accuracy of network using a classical and provably convergent numerical method developed using the duality approach.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究使用神经网络求解一类最优养老金投资问题，将投资者的风险偏好、消费偏好参数与经济变量一同作为网络输入，从而识别最优投资策略。研究基于Black-Scholes经济模型，并利用对偶方法构建的经典数值解法验证网络准确性，旨在开发可探索养老金结果随偏好参数变化的实用工具。",
    "fetch_date": "2026-01-13",
    "id": "20260113_6b0aba1f"
  },
  {
    "title": "Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy",
    "url": "https://arxiv.org/pdf/2511.12120v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于自动化股票交易的集成策略，采用深度强化学习方案，通过最大化投资回报来学习交易策略。该策略训练了一个深度强化学习智能体，并利用三种基于演员-评论家（actor-critic）的算法——近端策略优化（PPO）、优势演员-评论家（A2C）和深度确定性策略梯度（DDPG）——构建集成交易策略。该集成策略继承并整合了三种算法的最佳特性，从而能够稳健地适应不同的市场情况。为避免在连续动作空间中训练网络时消耗大量内存，采用了按需加载技术来处理超大规模数据。研究在30只具有充足流动性的道琼斯股票上测试了算法，评估了不同强化学习算法下交易智能体的表现，并与道琼斯工业平均指数及传统的最小方差投资组合配置策略进行了比较。",
    "fetch_date": "2026-01-12",
    "id": "20260112_0631a907"
  },
  {
    "title": "Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling",
    "url": "https://arxiv.org/pdf/2511.10501v2",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文全面综述了图神经网络（GNN）、深度强化学习（DRL）和概率主题建模（PTM）在战略多智能体环境中的应用潜力，重点关注其在战略对手建模中的价值。研究强调：（1）利用机器学习方法揭示未知模型结构，以适应对手建模任务；（2）结合博弈论概念，避免依赖现实场景中常无效的假设（如共同先验假设CPA和自利假设SIH）。论文分析了处理不确定性和异质性（现实应用中的常见特征）以及可扩展性的能力，并倡导使用图神经网络（GNN）作为有效建模多智能体环境中关系和交互的潜在解决方案。GNN专为处理图结构数据设计，在节点分类和链接预测等任务中表现出强大能力。此外，论文还回顾了强化学习（RL）领域，特别是多智能体强化学习（MARL）在动态决策中的应用。",
    "fetch_date": "2026-01-12",
    "id": "20260112_44f05882"
  },
  {
    "title": "Noise-proofing Universal Portfolio Shrinkage",
    "url": "https://arxiv.org/pdf/2511.10478v1",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "We enhance the Universal Portfolio Shrinkage Approximator (UPSA) of Kelly et al. (2023) by making it more robust with respect to estimation noise and covariate shift. UPSA optimizes the realized Sharpe ratio using a relatively small calibration window, leveraging ridge penalties and cross-validation to yield better portfolios. Yet, it still suffers from the staggering amount of noise in financial data. We propose two methods to make UPSA more robust and improve its efficiency: time-averaging of the optimal penalty weights and using the Average Oracle correlation eigenvalues to make covariance matrices less noisy and more robust to covariate shift. Combining these two long-term averages outperforms UPSA by a large margin in most specifications.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们通过增强对估计噪声和协变量漂移的鲁棒性，改进了Kelly等人（2023）提出的通用投资组合收缩近似器（UPSA）。UPSA利用相对较小的校准窗口优化已实现夏普比率，通过岭惩罚和交叉验证获得更好的投资组合。然而，它仍受金融数据中大量噪声的影响。我们提出两种方法使UPSA更鲁棒并提高其效率：对最优惩罚权重进行时间平均，以及使用平均Oracle相关特征值来降低协方差矩阵的噪声并增强对协变量漂移的鲁棒性。结合这两种长期平均方法，在大多数设定下显著优于UPSA。",
    "fetch_date": "2026-01-12",
    "id": "20260112_c57261ce"
  },
  {
    "title": "FCOC: A Fractal-Chaotic Co-driven Framework for Financial Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2511.10365v2",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "This paper introduces the Fractal-Chaotic Oscillation Co-driven (FCOC) framework, a novel paradigm for financial volatility forecasting that systematically resolves the dual challenges of feature fidelity and model responsiveness. FCOC synergizes two core innovations: our novel Fractal Feature Corrector (FFC), engineered to extract high-fidelity fractal signals, and a bio-inspired Chaotic Oscillation Component (COC) that replaces static activations with a dynamic processing system. Empirically validated on the S\\&P 500 and DJI, the FCOC framework demonstrates profound and generalizable impact. The framework fundamentally transforms the performance of previously underperforming architectures, such as the Transformer, while achieving substantial improvements in key risk-sensitive metrics for state-of-the-art models like Mamba. These results establish a powerful co-driven approach, where models are guided by superior theoretical features and powered by dynamic internal processors, setting a new benchmark for risk-aware forecasting.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为FCOC（分形-混沌协同驱动）的新框架，用于金融波动率预测。该框架通过结合分形特征校正器（FFC）提取高保真分形信号，以及采用受生物启发的混沌振荡组件（COC）替代静态激活函数，构建动态处理系统，旨在系统性地解决特征保真度与模型响应性的双重挑战。实证研究基于标普500和道琼斯工业平均指数，结果表明FCOC框架对Transformer等先前表现不佳的架构有根本性提升，同时对Mamba等前沿模型在关键风险敏感指标上也有显著改进。这确立了一种强大的协同驱动方法，为风险感知预测设立了新基准。",
    "fetch_date": "2026-01-12",
    "id": "20260112_25e39469"
  },
  {
    "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2511.12351v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种用于多元时间序列异常检测的深度强化学习框架，结合了变分自编码器（VAE）、基于LSTM的深度Q网络（DQN）、动态奖励塑形和主动学习模块。核心贡献是实现动态奖励缩放（DRSMT），VAE用于捕获紧凑的潜在表示并降噪，DQN实现自适应序列异常分类，动态奖励塑形通过调整重构和分类信号的重要性来平衡训练中的探索与利用，主动学习则识别最不确定的样本进行标注以减少人工监督需求。在SMD和Water Distribution Test两个基准数据集上的实验验证了该框架的有效性。",
    "fetch_date": "2026-01-12",
    "id": "20260112_055c5560"
  },
  {
    "title": "A Practical Machine Learning Approach for Dynamic Stock Recommendation",
    "url": "https://arxiv.org/pdf/2511.12129v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Dynamic-Stock-Recommendation-Machine_Learning-Published-Paper-IEEE}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种实用的机器学习方法用于动态股票推荐。核心方案是动态买入并持有排名前20%的S&P 500股票。首先筛选具有良好解释力的代表性股票指标；其次采用线性回归、岭回归、逐步回归、随机森林和广义提升回归五种常用机器学习方法，在滚动窗口中对股票指标与季度对数收益率进行建模；然后选择每期均方误差最低的模型对股票进行排序；最后通过等权重、均值-方差和最小方差等投资组合分配方法对所选股票进行测试。实证结果表明，该方案在夏普比率和累计收益方面均优于S&P 500指数的纯多头策略。",
    "fetch_date": "2026-01-12",
    "id": "20260112_6a992cf9"
  },
  {
    "title": "Risk-Aware Deep Reinforcement Learning for Dynamic Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.11481v1",
    "source": "ArXiv",
    "date": "2025-11-14",
    "abstract": "This paper presents a deep reinforcement learning (DRL) framework for dynamic portfolio optimization under market uncertainty and risk. The proposed model integrates a Sharpe ratio-based reward function with direct risk control mechanisms, including maximum drawdown and volatility constraints. Proximal Policy Optimization (PPO) is employed to learn adaptive asset allocation strategies over historical financial time series. Model performance is benchmarked against mean-variance and equal-weight portfolio strategies using backtesting on high-performing equities. Results indicate that the DRL agent stabilizes volatility successfully but suffers from degraded risk-adjusted returns due to over-conservative policy convergence, highlighting the challenge of balancing exploration, return maximization, and risk mitigation. The study underscores the need for improved reward shaping and hybrid risk-aware strategies to enhance the practical deployment of DRL-based portfolio allocation models.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于深度强化学习（DRL）的动态投资组合优化框架，用于应对市场不确定性和风险。该模型将基于夏普比率的奖励函数与直接风险控制机制（包括最大回撤和波动率约束）相结合，并采用近端策略优化（PPO）算法在历史金融时间序列上学习自适应资产配置策略。通过在高绩效股票上进行回测，模型表现与均值-方差和等权重投资组合策略进行了基准比较。结果表明，DRL代理能够有效稳定波动率，但由于策略收敛过于保守，风险调整后收益有所下降，凸显了在探索、收益最大化和风险缓解之间取得平衡的挑战。研究强调需要改进奖励塑造和采用混合风险感知策略，以增强基于DRL的投资组合分配模型的实际部署价值。",
    "fetch_date": "2026-01-12",
    "id": "20260112_23a85aee"
  },
  {
    "title": "On the utility problem in a market where price impact is transient",
    "url": "https://arxiv.org/pdf/2511.12093v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "We consider a discrete-time model of a financial market where a risky asset is bought and sold with transactions having a transient price impact. It is shown that the corresponding utility maximization problem admits a solution. We manage to remove some unnatural restrictions on the market depth and resilience processes that were present in earlier work. A non-standard feature of the problem is that the set of attainable portfolio values may fail the convexity property.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个离散时间金融市场模型，其中风险资产的买卖交易会产生暂时性价格冲击。研究表明，相应的效用最大化问题存在解。作者成功移除了先前研究中关于市场深度和恢复过程的一些非自然限制。该问题的一个非标准特征是，可实现的投资组合价值集合可能不满足凸性。",
    "fetch_date": "2026-01-12",
    "id": "20260112_3f0c1711"
  },
  {
    "title": "Deep Learning Based Hybrid Transformer Model for Stock Price Prediction",
    "url": "https://ieeexplore.ieee.org/abstract/document/10894439/",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… The backtesting of this study on global indices further displays that the Transformer proves to be even more superior to traditional methods and can offer investors with avenues to earn …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于深度学习的混合Transformer模型用于股价预测。该研究在全球指数上进行回测，结果显示Transformer模型显著优于传统方法，能为投资者提供盈利途径。",
    "fetch_date": "2026-01-11",
    "id": "20260111_1c77b35c"
  },
  {
    "title": "Automate strategy finding with llm in quant investment",
    "url": "https://arxiv.org/abs/2409.06289",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… based on LLM, to utilize the strong exploratory power of LLM to … We introduce the multi-Agent approach to the financial do… alpha factors and strategies in quantitative trading. It begins by …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于LLM（大语言模型）的量化投资策略自动化发现方法，利用LLM强大的探索能力，引入多智能体（multi-Agent）方法于金融领域，旨在自动生成alpha因子和量化交易策略。",
    "fetch_date": "2026-01-11",
    "id": "20260111_2da79e52"
  },
  {
    "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
    "url": "https://arxiv.org/abs/2508.02366",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Second, we propose a hybrid LLM+RL architecture in which the LLM provides guidance by augmenting the observation space of an RL agent. This design enables the …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种混合LLM+RL架构，其中大型语言模型通过增强强化学习智能体的观察空间来提供指导。这种设计使RL代理能够利用LLM的语义理解能力，在量化交易中实现更智能的决策。",
    "fetch_date": "2026-01-11",
    "id": "20260111_f9bf84fa"
  },
  {
    "title": "Language model guided reinforcement learning in quantitative trading: LLM-guided intelligence bridging strategy and safety in trading",
    "url": "https://www.um.edu.mt/library/oar/handle/123456789/141862",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Building on this capacity, this research investigates whether LLM agents can guide a RL agent to produce coherent, economically grounded strategies aligned with investor risk …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该研究探讨了大型语言模型（LLM）智能体能否指导强化学习（RL）智能体，在量化交易中生成连贯、基于经济原理的策略，并与投资者的风险偏好保持一致，旨在通过LLM引导的智能桥接策略与交易安全性。",
    "fetch_date": "2026-01-11",
    "id": "20260111_c491b345"
  },
  {
    "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
    "url": "https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2025.findings-emnlp.945.pdf",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Through this approach, we hope narrow the gap between LLM-based agents and human … Trademaster: A holistic quantitative trading platform empowered by reinforcement learning. In …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出QuantAgents多智能体金融系统，通过模拟交易缩小基于大语言模型的智能体与人类交易员之间的差距，并整合了Trademaster这一由强化学习驱动的综合量化交易平台，对实战交易具有较高的应用价值。",
    "fetch_date": "2026-01-11",
    "id": "20260111_3c84d91d"
  },
  {
    "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading",
    "url": "https://arxiv.org/abs/2509.05080",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)… expert agents. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "MM-DREX：一种多模态驱动的动态路由大语言模型专家系统用于金融交易。传统依赖固定结构和单模态数据的量化交易模型难以适应市场状态转换，而本文提出的方法利用多模态数据（如文本、时序、市场数据）动态路由至不同LLM专家代理，旨在提升交易策略的适应性和性能。",
    "fetch_date": "2026-01-11",
    "id": "20260111_dcfba4b6"
  },
  {
    "title": "Hedgeagents: A balanced-aware multi-agent financial trading system",
    "url": "https://dl.acm.org/doi/abs/10.1145/3701716.3715232",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… for a single agent. We have integrated the LLM-based intelligent investment agent into a … TradeMaster: A holistic quantitative trading platform empowered by reinforcement learning. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种平衡感知的多智能体金融交易系统（Hedgeagents），将基于大语言模型（LLM）的智能投资代理集成到TradeMaster平台中。TradeMaster是一个由强化学习（RL）驱动的整体量化交易平台。该系统通过多智能体协同和强化学习技术，旨在提升实战交易中的决策能力和适应性。",
    "fetch_date": "2026-01-11",
    "id": "20260111_0829c581"
  },
  {
    "title": "Quantagent: Price-driven multi-agent llms for high-frequency trading",
    "url": "https://arxiv.org/abs/2509.09995",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… To bridge the gap between traditional high-frequency quantitative trading and recent advances in multi-agent LLM systems, we introduce QuantAgent, a collaborative framework for low-…",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "为弥合传统高频量化交易与多智能体大语言模型系统最新进展之间的差距，本文提出QuantAgent——一个用于低频延迟交易场景的协作框架。该框架通过价格驱动机制协调多个LLM智能体，旨在提升交易决策的适应性和鲁棒性，属于将前沿AI技术应用于实际交易策略的探索性研究。",
    "fetch_date": "2026-01-11",
    "id": "20260111_aac51ed5"
  },
  {
    "title": "A Multi-agent System Based On LLM For Trading Financial Assets.",
    "url": "https://www.researchgate.net/profile/Simona-Oprea/publication/389806855_A_Multi-agent_System_Based_On_LLM_For_Trading_Financial_Assets/links/68218750bfbe974b23c7fc56/A-Multi-agent-System-Based-On-LLM-For-Trading-Financial-Assets.pdf",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Quantitative trading consists of using quantitative analysis and mathematical models to make trading decisions. It relies on data and algorithms, can be short-term or long-term, variable …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "基于大语言模型（LLM）的多智能体系统用于金融资产交易。量化交易利用定量分析和数学模型进行交易决策，依赖数据和算法，可应用于短期或长期策略。",
    "fetch_date": "2026-01-11",
    "id": "20260111_66e0ae60"
  },
  {
    "title": "TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis",
    "url": "https://arxiv.org/abs/2508.17565",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… LLM-based agents: FinMem integrates hierarchical memory and role prompts into the agent … We have introduced TradingGroup, an innovative multi-agent system for quantitative trading…",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "本文介绍TradingGroup：一种创新的多智能体量化交易系统，具备自我反思和数据合成能力。该系统基于LLM智能体，其中FinMem组件将分层记忆和角色提示集成到智能体中。该系统旨在通过多智能体协作提升量化交易性能。",
    "fetch_date": "2026-01-11",
    "id": "20260111_b0e7b1e4"
  },
  {
    "title": "Deep Reinforcement Learning for Optimum Order Execution: Mitigating Risk and Maximizing Returns",
    "url": "https://arxiv.org/pdf/2601.04896v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "Optimal Order Execution is a well-established problem in finance that pertains to the flawless execution of a trade (buy or sell) for a given volume within a specified time frame. This problem revolves around optimizing returns while minimizing risk, yet recent research predominantly focuses on addressing one aspect of this challenge. In this paper, we introduce an innovative approach to Optimal Order Execution within the US market, leveraging Deep Reinforcement Learning (DRL) to effectively address this optimization problem holistically. Our study assesses the performance of our model in comparison to two widely employed execution strategies: Volume Weighted Average Price (VWAP) and Time Weighted Average Price (TWAP). Our experimental findings clearly demonstrate that our DRL-based approach outperforms both VWAP and TWAP in terms of return on investment and risk management. The model's ability to adapt dynamically to market conditions, even during periods of market stress, underscores its promise as a robust solution.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文提出了一种基于深度强化学习（DRL）的创新方法，用于解决美国市场中的最优订单执行问题。该方法旨在整体优化回报并降低风险，通过动态适应市场条件（包括市场压力时期）来提升性能。实验结果表明，该DRL模型在投资回报和风险管理方面均优于两种广泛使用的执行策略：成交量加权平均价格（VWAP）和时间加权平均价格（TWAP）。这突显了其作为稳健解决方案的潜力，对实战交易具有较高价值。",
    "fetch_date": "2026-01-10",
    "id": "20260110_464a514c"
  },
  {
    "title": "Forecasting Equity Correlations with Hybrid Transformer Graph Neural Network",
    "url": "https://arxiv.org/pdf/2601.04602v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "This paper studies forward-looking stock-stock correlation forecasting for S\\&P 500 constituents and evaluates whether learned correlation forecasts can improve graph-based clustering used in basket trading strategies. We cast 10-day ahead correlation prediction in Fisher-z space and train a Temporal-Heterogeneous Graph Neural Network (THGNN) to predict residual deviations from a rolling historical baseline. The architecture combines a Transformer-based temporal encoder, which captures non-stationary, complex, temporal dependencies, with an edge-aware graph attention network that propagates cross-asset information over the equity network. Inputs span daily returns, technicals, sector structure, previous correlations, and macro signals, enabling regime-aware forecasts and attention-based feature and neighbor importance to provide interpretability. Out-of-sample results from 2019-2024 show that the proposed model meaningfully reduces correlation forecasting error relative to rolling-window estimates. When integrated into a graph-based clustering framework, forward-looking correlations produce adaptable and economically meaningfully baskets, particularly during periods of market stress. These findings suggest that improvements in correlation forecasts translate into meaningful gains during portfolio construction tasks.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文研究标普500成分股的股票间前瞻性相关性预测，并评估所学相关性预测是否能改进基于图的聚类在篮子交易策略中的应用。研究将10日超前相关性预测置于Fisher-z空间，训练一个时序异构图神经网络（THGNN）来预测相对于滚动历史基线的残差偏差。该架构结合了基于Transformer的时序编码器（捕捉非平稳、复杂的时序依赖）与边感知图注意力网络（在股票网络中传播跨资产信息）。输入涵盖日收益率、技术指标、行业结构、历史相关性和宏观信号，支持基于市场状态的预测，并通过注意力机制提供特征和邻居重要性的可解释性。2019-2024年样本外结果显示，该模型显著降低了相关性预测误差。当整合到基于图的聚类框架中时，前瞻性相关性能够生成适应性强且具有经济意义的篮子组合，尤其在市场压力时期表现突出。",
    "fetch_date": "2026-01-10",
    "id": "20260110_7bd13459"
  },
  {
    "title": "Discovery of a 13-Sharpe OOS Factor: Drift Regimes Unlock Hidden Cross-Sectional Predictability",
    "url": "https://arxiv.org/pdf/2511.12490v1",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "We document a high-performing cross-sectional equity factor that achieves out-of-sample Sharpe ratios above 13 through regime-conditional signal activation. The strategy combines value and short-term reversal signals only during stock-specific drift regimes, defined as periods when individual stocks show more than 60 percent positive days in trailing 63-day windows. Under these conditions, the factor delivers annualized returns of 158.6 percent with 12.0 percent volatility and a maximum drawdown of minus 11.9 percent. Using rigorous walk-forward validation across 20 years of S&P 500 data (2004 to 2024), we show performance roughly 13 times stronger than market benchmarks on a risk-adjusted basis, produced entirely out-of-sample with frozen parameters. The factor passes extensive robustness tests, including 1,000 randomization trials with p-values below 0.001, and maintains Sharpe ratios above 7 even under 30 percent parameter perturbations. Exposure to standard risk factors is negligible, with total R-squared values below 3 percent. We provide mechanistic evidence that drift regimes reshape market microstructure by amplifying behavioral biases, altering liquidity patterns, and creating conditions where cross-sectional price discovery becomes systematically exploitable. Conservative capacity estimates indicate deployable capital of 100 to 500 million dollars before noticeable performance degradation.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文发现了一个高绩效的横截面股票因子，通过制度条件信号激活，在样本外实现了超过13的夏普比率。该策略仅在股票特定的漂移制度（定义为个股在63天滚动窗口内正收益日占比超过60%的时期）下，结合价值和短期反转信号。在这些条件下，该因子年化收益率为158.6%，波动率为12.0%，最大回撤为-11.9%。通过对20年标普500数据（2004-2024年）进行严格的向前验证，该因子在风险调整后的表现比市场基准强约13倍，且完全在样本外使用固定参数实现。该因子通过了广泛的稳健性测试，包括1000次随机化试验（p值低于0.001），即使在30%的参数扰动下，夏普比率仍保持在7以上。对标准风险因子的暴露可忽略不计，总R平方值低于3%。机制证据表明，漂移制度通过放大行为偏差、改变流动性模式重塑市场微观结构。",
    "fetch_date": "2026-01-10",
    "id": "20260110_b932feb7"
  },
  {
    "title": "Trading Electrons: Predicting DART Spread Spikes in ISO Electricity Markets",
    "url": "https://arxiv.org/pdf/2601.05085v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "We study the problem of forecasting and optimally trading day-ahead versus real-time (DART) price spreads in U.S. wholesale electricity markets. Building on the framework of Galarneau-Vincent et al., we extend spike prediction from a single zone to a multi-zone setting and treat both positive and negative DART spikes within a unified statistical model. To translate directional signals into economically meaningful positions, we develop a structural and market-consistent price impact model based on day-ahead bid stacks. This yields closed-form expressions for the optimal vector of zonal INC/DEC quantities, capturing asymmetric buy/sell impacts and cross-zone congestion effects. When applied to NYISO, the resulting impact-aware strategy significantly improves the risk-return profile relative to unit-size trading and highlights substantial heterogeneity across markets and seasons.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们研究了在美国批发电力市场中预测和优化交易日前与实时（DART）价差的问题。基于Galarneau-Vincent等人的框架，我们将尖峰预测从单一区域扩展到多区域设置，并在统一的统计模型中处理正负DART尖峰。为了将方向性信号转化为具有经济意义的头寸，我们基于日前投标堆栈开发了一个结构化和市场一致的价格影响模型。这产生了区域增/减量的最优向量闭式表达式，捕捉了不对称的买卖影响和跨区域拥堵效应。当应用于纽约独立系统运营商（NYISO）时，所得的影响感知策略相对于单位规模交易显著改善了风险回报特征，并突显了市场和季节间的显著异质性。",
    "fetch_date": "2026-01-10",
    "id": "20260110_93896dc0"
  },
  {
    "title": "Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms",
    "url": "https://arxiv.org/pdf/2511.20678v1",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种基于强化学习的加密货币投资组合管理框架，采用Soft Actor-Critic（SAC）和Deep Deterministic Policy Gradient（DDPG）算法。针对加密货币市场的高波动性和非线性动态特性，传统投资组合优化方法往往难以适应。为此，研究设计了一个智能体，通过模拟交易环境与历史市场数据交互，直接学习连续交易动作。该智能体优化投资组合权重，旨在最大化累积收益，同时最小化下行风险和交易成本。在多种加密货币上的实验评估表明，SAC和DDPG智能体在表现上优于等权重和均值-方差等基准策略。其中，SAC算法凭借其熵正则化目标，在嘈杂市场条件下展现出比DDPG更高的稳定性和鲁棒性。这些结果凸显了深度强化学习在加密货币市场中实现自适应和数据驱动型投资组合管理的潜力。",
    "fetch_date": "2026-01-10",
    "id": "20260110_17518719"
  },
  {
    "title": "Intraday Limit Order Price Change Transition Dynamics Across Market Capitalizations Through Markov Analysis",
    "url": "https://arxiv.org/pdf/2601.04959v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "Quantitative understanding of stochastic dynamics in limit order price changes is essential for execution strategy design. We analyze intraday transition dynamics of ask and bid orders across market capitalization tiers using high-frequency NASDAQ100 tick data. Employing a discrete-time Markov chain framework, we categorize consecutive price changes into nine states and estimate transition probability matrices (TPMs) for six intraday intervals across High ($\\mathtt{HMC}$), Medium ($\\mathtt{MMC}$), and Low ($\\mathtt{LMC}$) market cap stocks. Element-wise TPM comparison reveals systematic patterns: price inertia peaks during opening and closing hours, stabilizing midday. A capitalization gradient is observed: $\\mathtt{HMC}$ stocks exhibit the strongest inertia, while $\\mathtt{LMC}$ stocks show lower stability and wider spreads. Markov metrics, including spectral gap, entropy rate, and mean recurrence times, quantify these dynamics. Clustering analysis identifies three distinct temporal phases on the bid side -- Opening, Midday, and Closing, and four phases on the ask side by distinguishing Opening, Midday, Pre-Close, and Close. This indicates that sellers initiate end-of-day positioning earlier than buyers. Stationary distributions show limit order dynamics are dominated by neutral and mild price changes. Jensen-Shannon divergence confirms the closing hour as the most distinct phase, with capitalization modulating temporal contrasts and bid-ask asymmetry. These findings support capitalization-aware and time-adaptive execution algorithms.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过马尔可夫分析研究纳斯达克100指数成分股日内限价单价格变动的转移动态。使用高频tick数据，将连续价格变动分为九个状态，并针对高、中、低市值股票在六个日内时段估计转移概率矩阵。研究发现：价格惯性在开盘和收盘时段最强，午间趋于稳定；高市值股票惯性最强，低市值股票稳定性较低且价差更宽。通过谱隙、熵率和平均递归时间等马尔可夫指标量化动态特征。聚类分析识别出买方三个时间阶段（开盘、午间、收盘）和卖方四个阶段（开盘、午间、预收盘、收盘），表明卖方在收盘前主动调整头寸。",
    "fetch_date": "2026-01-10",
    "id": "20260110_c17f32da"
  },
  {
    "title": "Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach",
    "url": "https://arxiv.org/pdf/2601.04608v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "We study U.S. Treasury yield curve forecasting under distributional uncertainty and recast forecasting as an operations research and managerial decision problem. Rather than minimizing average forecast error, the forecaster selects a decision rule that minimizes worst case expected loss over an ambiguity set of forecast error distributions. To this end, we propose a distributionally robust ensemble forecasting framework that integrates parametric factor models with high dimensional nonparametric machine learning models through adaptive forecast combinations. The framework consists of three machine learning components. First, a rolling window Factor Augmented Dynamic Nelson Siegel model captures level, slope, and curvature dynamics using principal components extracted from economic indicators. Second, Random Forest models capture nonlinear interactions among macro financial drivers and lagged Treasury yields. Third, distributionally robust forecast combination schemes aggregate heterogeneous forecasts under moment uncertainty, penalizing downside tail risk via expected shortfall and stabilizing second moment estimation through ridge regularized covariance matrices. The severity of the worst case criterion is adjustable, allowing the forecaster to regulate the trade off between robustness and statistical efficiency. Using monthly data, we evaluate out of sample forecasts across maturities and horizons from one to twelve months ahead. Adaptive combinations deliver superior performance at short horizons, while Random Forest forecasts dominate at longer horizons. Extensions to global sovereign bond yields confirm the stability and generalizability of the proposed framework.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究在分布不确定性下美国国债收益率曲线的预测问题，将其重新构建为运筹学和管理决策问题。预测者选择决策规则，以最小化预测误差分布模糊集上的最坏情况期望损失，而非最小化平均预测误差。为此，提出一个分布鲁棒的集成预测框架，通过自适应预测组合将参数化因子模型与高维非参数机器学习模型相结合。框架包含三个机器学习组件：1) 滚动窗口因子增强动态Nelson-Siegel模型，利用从经济指标中提取的主成分捕捉水平、斜率和曲率动态；2) 随机森林模型，捕捉宏观金融驱动因素与滞后国债收益率之间的非线性相互作用；3) 分布鲁棒的预测组合方案，在矩不确定性下聚合异质预测，通过预期短缺惩罚下行尾部风险，并通过岭正则化协方差矩阵稳定二阶矩估计。",
    "fetch_date": "2026-01-10",
    "id": "20260110_2eb95a5e"
  },
  {
    "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects",
    "url": "https://arxiv.org/pdf/2511.12865v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了一个具有随机活动持续时间和现金流的项目优化问题，项目活动需满足先后约束并产生现金流入和流出。目标是通过加速现金流入和推迟现金流出来最大化预期净现值（NPV）。作者将问题建模为离散时间马尔可夫决策过程（MDP），并提出了一种双深度Q网络（DDQN）方法。对比实验表明，DDQN在大型或高度不确定的环境中优于传统的刚性和动态策略，展现出卓越的计算能力、策略可靠性和适应性。消融研究进一步揭示，双网络架构减轻了动作值的高估，而目标网络显著提高了训练收敛性和鲁棒性。这些结果表明，DDQN不仅在复杂项目优化中实现了更高的预期NPV，还为稳定有效的策略实施提供了可靠框架。",
    "fetch_date": "2026-01-10",
    "id": "20260110_e12bdf0a"
  },
  {
    "title": "Basis Immunity: Isotropy as a Regularizer for Uncertainty",
    "url": "https://arxiv.org/pdf/2511.13334v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.\n  In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.\n  Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.\n  This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种将均值-方差优化与各向同性约束相结合的框架，作为对抗信号不确定性的几何正则化器。该方法通过可调的各向同性惩罚，在完全各向同性（闭式各向同性-均值分配）与纯均值-方差之间平滑插值，自然地分解为规范投资组合。论文回顾并澄清了各向同性、规范投资组合、主投资组合、原始与对偶表示以及收益、风险和各项同性的内在基不变度量之间的基本联系，并将其应用于行业趋势分析。",
    "fetch_date": "2026-01-10",
    "id": "20260110_eaf0fc71"
  },
  {
    "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
    "url": "https://arxiv.org/pdf/2511.13322v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新的模型无关方法，通过Voronoi状态划分将强化学习策略蒸馏为局部专用线性策略，以增强可解释性。该方法将状态空间划分为多个区域，在每个区域内使用可解释的线性模型近似原始深度强化学习控制器的性能，在网格世界环境和经典控制任务中验证了其有效性。",
    "fetch_date": "2026-01-10",
    "id": "20260110_96d20d08"
  },
  {
    "title": "Impact by design: translating Lead times in flux into an R handbook with code",
    "url": "https://arxiv.org/pdf/2511.12763v2",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "This commentary translates the central ideas in Lead times in flux into a practice ready handbook in R. The original article measures change in the full distribution of booking lead times with a normalized L1 distance and tracks that divergence across months relative to year over year and to a fixed 2018 reference. It also provides a bound that links divergence and remaining horizon to the relative error of pickup forecasts. We implement these ideas end to end in R, using a minimal data schema and providing runnable scripts, simulated examples, and a prespecified evaluation plan. All results use synthetic data so the exposition is fully reproducible without reference to proprietary sources.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文将《Lead times in flux》中的核心思想转化为R语言的实践手册。原文章通过归一化L1距离测量预订提前期的完整分布变化，并追踪该差异相对于同比和2018年固定基准的月度变化。同时提供了一个将差异与剩余时间范围关联到拾取预测相对误差的边界。我们在R中端到端实现了这些思想，使用最小数据模式，并提供可运行脚本、模拟示例和预设评估计划。所有结果均使用合成数据，因此无需参考专有来源即可完全复现。",
    "fetch_date": "2026-01-10",
    "id": "20260110_ffd51d03"
  },
  {
    "title": "Technology Adoption and Network Externalities in Financial Systems: A Spatial-Network Approach",
    "url": "https://arxiv.org/pdf/2601.04246v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "This paper develops a unified framework for analyzing technology adoption in financial networks that incorporates spatial spillovers, network externalities, and their interaction. The framework characterizes adoption dynamics through a master equation whose solution admits a Feynman-Kac representation as expected cumulative adoption pressure along stochastic paths through spatial-network space. From this representation, I derive the Adoption Amplification Factor -- a structural measure of technology leadership that captures the ratio of total system-wide adoption to initial adoption following a localized shock. A Levy jump-diffusion extension with state-dependent jump intensity captures critical mass dynamics: below threshold, adoption evolves through gradual diffusion; above threshold, cascade dynamics accelerate adoption through discrete jumps. Applying the framework to SWIFT gpi adoption among 17 Global Systemically Important Banks, I find strong support for the two-regime characterization. Network-central banks adopt significantly earlier ($ρ= -0.69$, $p = 0.002$), and pre-threshold adopters have significantly higher amplification factors than post-threshold adopters (11.81 versus 7.83, $p = 0.010$). Founding members, representing 29 percent of banks, account for 39 percent of total system amplification -- sufficient to trigger cascade dynamics. Controlling for firm size and network position, CEO age delays adoption by 11-15 days per year.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个统一框架，用于分析金融网络中的技术采用，该框架整合了空间溢出效应、网络外部性及其相互作用。通过主方程及其Feynman-Kac表示，框架描述了采用动态，并推导出“采用放大因子”——一种衡量技术领导力的结构指标，反映了局部冲击后系统总采用量与初始采用量的比率。通过Levy跳扩散扩展（具有状态依赖的跳跃强度）捕捉临界质量动态：阈值以下，采用通过渐进扩散演变；阈值以上，级联动态通过离散跳跃加速采用。将该框架应用于17家全球系统重要性银行的SWIFT gpi采用，发现强烈支持两阶段特征：网络中心银行显著更早采用（ρ = -0.69，p = 0.002），且阈值前采用者的放大因子显著高于阈值后采用者（11.81对比...）。",
    "fetch_date": "2026-01-10",
    "id": "20260110_65a2bfc9"
  },
  {
    "title": "CBDC Stress Test in a Dual-Currency Setting",
    "url": "https://arxiv.org/pdf/2511.13384v4",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "This study explores the potential impact of introducing a Central Bank Digital Currency (CBDC) on financial stability in an emerging dual-currency economy (Romania), where the domestic currency (RON) coexists with the euro. It develops an integrated analytical framework combining econometrics, machine learning, and behavioural modelling. CBDC adoption probabilities are estimated using XGBoost and logistic regression models trained on behavioural and macro-financial indicators rather than survey data. Liquidity stress simulations assess how banks would respond to deposit withdrawals resulting from CBDC adoption, while VAR, MSVAR, and SVAR models capture the macro-financial transmission of liquidity shocks into credit contraction and changes in monetary conditions. The findings indicate that CBDC uptake (co-circulating Digital RON and Digital EUR) would be moderate at issuance, amounting to around EUR 1 billion, primarily driven by digital readiness and trust in the central bank. The study concludes that a non-remunerated, capped CBDC, designed primarily as a means of payment rather than a store of value, can be introduced without compromising financial stability. In dual currency economies, differentiated holding limits for domestic and foreign digital currencies (e.g., Digital RON versus Digital Euro) are crucial to prevent uncontrolled euroisation and preserve monetary sovereignty. A prudent design with moderate caps, non remuneration, and macroprudential coordination can transform CBDC into a digital liquidity buffer and a complementary monetary policy instrument that enhances resilience and inclusion rather than destabilising the financial system.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨了在罗马尼亚这一新兴的双货币经济体（本国货币RON与欧元共存）中，引入央行数字货币（CBDC）对金融稳定的潜在影响。它构建了一个结合计量经济学、机器学习和行为建模的综合分析框架。CBDC的采用概率是通过使用XGBoost和逻辑回归模型，基于行为和宏观金融指标（而非调查数据）进行估计的。流动性压力模拟评估了银行如何应对因CBDC采用而导致的存款提取，而VAR、MSVAR和SVAR模型则捕捉了流动性冲击向信贷收缩和货币条件变化的宏观金融传导。研究结果表明，CBDC（数字RON和数字欧元共同流通）在发行初期的采用将是温和的，规模约为10亿欧元，主要由数字化准备程度和对央行的信任驱动。研究结论是，一种非付息、有上限的CBDC，主要设计为支付手段而非价值储存手段，可以在不损害金融稳定性的情况下引入。",
    "fetch_date": "2026-01-10",
    "id": "20260110_5001cae3"
  },
  {
    "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
    "url": "https://arxiv.org/pdf/2511.13214v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种利用图神经网络与深度强化学习解决资源受限项目调度问题（RCPSP）中任务工期不确定性的方法。该方法旨在通过已知概率建模不确定工期，最小化项目期望总工期，并生成可重复使用的基线调度方案。研究开发了名为Wheatley的框架，在标准基准测试中表现出优越性能与泛化能力。",
    "fetch_date": "2026-01-10",
    "id": "20260110_d52b1ee6"
  },
  {
    "title": "Sharpening Shapley Allocation: from Basel 2.5 to FRTB",
    "url": "https://arxiv.org/pdf/2511.12391v3",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Risk allocation, the decomposition of a portfolio-wide risk measure into component contributions, is a fundamental problem in financial risk management due to the non-additive nature of risk measures, the layered organizational structures of financial institutions, and the range of possible allocation strategies characterized by different rationales and properties.\n  In this work, we conduct a systematic review of the major risk allocation strategies typically used in finance, comparing their theoretical properties, practical advantages, and limitations. To this scope we set up a specific testing framework, including both simplified settings, designed to highlight basic intrinsic behaviours, and realistic financial portfolios under different risk regulations, i.e. Basel 2.5 and FRTB. Furthermore, we develop and test novel practical solutions to manage the issue of negative risk allocations and of multi-level risk allocation in the layered organizational structure of financial institutions, while preserving the additivity property. Finally, we devote particular attention to the computational aspects of risk allocation.\n  Our results show that, in this context, the Shapley allocation strategy offers the best compromise between simplicity, mathematical properties, risk representation and computational cost. The latter is still acceptable even in the challenging case of many business units, provided that an efficient Monte Carlo simulation is employed, which offers excellent scaling and convergence properties. While our empirical applications focus on market risk, our methodological framework is fully general and applicable to other financial context such as valuation risk, liquidity risk, credit risk, and counterparty credit risk.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "风险分配是将组合层面的风险度量分解为各组成部分贡献的基本问题，源于风险度量的非可加性、金融机构的层级组织结构以及不同分配策略的多样性。本文系统回顾了金融领域常用的主要风险分配策略，比较其理论特性、实践优势和局限性。通过建立测试框架，包括简化场景和基于不同风险监管（如Basel 2.5和FRTB）的现实金融组合，评估了Shapley分配方法。研究还提出了管理负风险分配和多层级风险分配的新解决方案，并重点关注了风险分配的计算方面。",
    "fetch_date": "2026-01-10",
    "id": "20260110_79273fc4"
  },
  {
    "title": "Uni-FinLLM: A Unified Multimodal Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment",
    "url": "https://arxiv.org/pdf/2601.02677v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified multimodal large language model that uses a shared Transformer backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified multimodal LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "Uni-FinLLM：一种统一的多模态大语言模型，采用模块化任务头，用于微观股票预测和宏观系统性风险评估。该模型通过共享Transformer骨干网络和跨模态注意力机制，联合处理金融文本、数值时间序列、基本面数据和视觉数据，实现微观、中观和宏观层面的统一预测。在股票方向预测准确率（67.4%）、信用风险评估准确率（84.1%）和宏观预警准确率（82.3%）上均显著超越基线模型，验证了统一多模态LLM在联合建模资产行为与系统性风险方面的有效性，为金融领域提供了可扩展的决策支持引擎。",
    "fetch_date": "2026-01-09",
    "id": "20260109_e295a081"
  },
  {
    "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
    "url": "https://arxiv.org/pdf/2601.02310v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文提出了一种用于高频限价订单簿（LOB）预测的时序Kolmogorov-Arnold网络（T-KAN），通过用可学习的B样条激活函数替代标准LSTM的固定线性权重，使模型能学习市场信号的“形状”而不仅仅是幅度。在FI-2010数据集上，T-KAN在k=100时间窗口的F1分数相对提升了19.1%，并在1.0基点交易成本下实现了132.48%的回报（对比DeepLOB的-82.76%回撤）。模型具有较好的可解释性（样条中的“死区”清晰可见），且架构针对低延迟FPGA实现进行了独特优化。这些特性直接针对实战交易中的噪声处理、Alpha衰减和延迟挑战，具有明确的实战价值。",
    "fetch_date": "2026-01-09",
    "id": "20260109_234a7923"
  },
  {
    "title": "Smart Predict--then--Optimize Paradigm for Portfolio Optimization in Real Markets",
    "url": "https://arxiv.org/pdf/2601.04062v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Improvements in return forecast accuracy do not always lead to proportional improvements in portfolio decision quality, especially under realistic trading frictions and constraints. This paper adopts the Smart Predict--then--Optimize (SPO) paradigm for portfolio optimization in real markets, which explicitly aligns the learning objective with downstream portfolio decision quality rather than pointwise prediction accuracy. Within this paradigm, predictive models are trained using an SPO-based surrogate loss that directly reflects the performance of the resulting investment decisions. To preserve interpretability and robustness, we employ linear predictors built on return-based and technical-indicator features and integrate them with portfolio optimization models that incorporate transaction costs, turnover control, and regularization. We evaluate the proposed approach on U.S. ETF data (2015--2025) using a rolling-window backtest with monthly rebalancing. Empirical results show that decision-focused training consistently improves risk-adjusted performance over predict--then--optimize baselines and classical optimization benchmarks, and yields strong robustness during adverse market regimes (e.g., the 2020 COVID-19). These findings highlight the practical value of the Smart Predict--then--Optimize paradigm for portfolio optimization in realistic and non-stationary financial environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文采用智能预测-优化（SPO）范式进行真实市场中的投资组合优化，该范式明确将学习目标与下游投资组合决策质量（而非逐点预测精度）对齐。在此范式中，预测模型使用基于SPO的代理损失进行训练，该损失直接反映最终投资决策的表现。为保持可解释性和鲁棒性，我们采用基于收益率和技术指标特征构建的线性预测器，并将其与包含交易成本、换手率控制和正则化的投资组合优化模型相结合。我们在美国ETF数据（2015-2025）上使用滚动窗口回测（每月再平衡）评估所提方法。实证结果表明，与预测-优化基线和经典优化基准相比，决策导向的训练能持续提升风险调整后表现，并在不利市场条件下展现出强鲁棒性。",
    "fetch_date": "2026-01-09",
    "id": "20260109_72f42092"
  },
  {
    "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
    "url": "https://arxiv.org/pdf/2601.03948v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "Trade-R1：通过过程级推理验证将可验证奖励桥接至随机环境。强化学习（RL）使大语言模型（LLM）在数学和编程等可验证奖励提供清晰信号的领域取得显著推理成就，但将其扩展至金融决策面临市场随机性的挑战：奖励可验证但本质嘈杂，导致标准RL退化为奖励黑客行为。为此，我们提出Trade-R1，一种通过过程级推理验证将可验证奖励桥接至随机环境的模型训练框架。核心创新是一种验证方法，将评估长篇金融文档推理的问题转化为结构化检索增强生成（RAG）任务。我们构建三角一致性度量，评估检索证据、推理链和决策之间的两两对齐，作为嘈杂市场回报的有效性过滤器。我们探索两种奖励整合策略：用于稳定对齐信号的固定效应语义奖励（FSR），以及用于耦合幅度优化的动态效应语义奖励（DSR）。不同国家资产选择实验表明...",
    "fetch_date": "2026-01-09",
    "id": "20260109_e30b157b"
  },
  {
    "title": "Market-Dependent Communication in Multi-Agent Alpha Generation",
    "url": "https://arxiv.org/pdf/2511.13614v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "多策略对冲基金面临一个根本性的组织选择：生成交易策略的分析师是否应该沟通，以及如何沟通？我们使用基于5个智能体的LLM交易系统，在21个月内进行了450次实验，比较了从孤立基线到协作和竞争对话的五种组织结构。研究表明，沟通能提高绩效，但最优的沟通设计取决于市场特征。竞争性对话在波动性大的科技股中表现出色，而协作性对话在稳定的普通股中占主导。金融股对所有沟通干预都表现出抵抗性。令人惊讶的是，所有结构（包括孤立智能体）都收敛到相似的策略对齐，这挑战了透明度会导致有害多样性损失的假设。绩效差异源于行为机制：竞争性智能体专注于股票层面的配置，而协作性智能体则发展技术框架。对话质量得分与回报零相关。这些发现表明，最优的沟通设计必须匹配市场波动特征，而复杂的讨论并不保证更好的交易结果。",
    "fetch_date": "2026-01-09",
    "id": "20260109_939fc4c3"
  },
  {
    "title": "Enhancing stock price prediction using GANs and transformer-based attention mechanisms",
    "url": "https://link.springer.com/article/10.1007/s00181-024-02644-6",
    "source": "Scholar",
    "date": "2026-01-09",
    "abstract": "… of Reinforcement Learning methodologies, fine-tuning of hyperparameters, and expansion to non-technology industries, we seek to develop more robust and accurate stock prediction …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种结合生成对抗网络（GANs）和基于Transformer的注意力机制来增强股票价格预测的方法。研究重点包括强化学习（RL）方法的应用、超参数微调，以及将模型扩展到非科技行业，旨在开发更稳健、准确的股票预测模型。",
    "fetch_date": "2026-01-09",
    "id": "20260109_abf31a5a"
  },
  {
    "title": "A comprehensive review and analysis of different modeling approaches for financial index tracking problem",
    "url": "https://arxiv.org/pdf/2601.03927v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Index tracking, also known as passive investing, has gained significant traction in financial markets due to its cost-effective and efficient approach to replicating the performance of a specific market index. This review paper provides a comprehensive overview of the various modeling approaches and strategies developed for index tracking, highlighting the strengths and limitations of each approach. We categorize the index tracking models into three broad frameworks: optimization-based models, statistical-based models and machine learning based data-driven approach. A comprehensive empirical study conducted on the S\\&P 500 dataset demonstrates that the tracking error volatility model under the optimization-based framework delivers the most precise index tracking, the convex co-integration model, under the statistical-based framework achieves the strongest return-risk balance, and the deep neural network with fixed noise model within the data-driven framework provides a competitive performance with notably low turnover and high computational efficiency. By combining a critical review of the existing literature with comparative empirical analysis, this paper aims to provide insights into the evolving landscape of index tracking and its practical implications for investors and fund managers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "这篇题为《金融指数跟踪问题的不同建模方法全面回顾与分析》的综述论文，系统梳理了指数跟踪（被动投资）的三大建模框架：基于优化的模型、基于统计的模型和基于机器学习的数据驱动方法。通过对标普500数据集的实证研究表明：优化框架下的跟踪误差波动率模型具有最高跟踪精度，统计框架下的凸协整模型实现了最优的风险收益平衡，而数据驱动框架下的带固定噪声深度神经网络模型在保持竞争力的同时，展现出显著的低换手率和高计算效率。该论文通过文献综述与实证对比，为实战交易中指数跟踪策略的选择与优化提供了有价值的参考。",
    "fetch_date": "2026-01-09",
    "id": "20260109_e69c36db"
  },
  {
    "title": "Breaking the Dimensional Barrier: Dynamic Portfolio Choice with Parameter Uncertainty via Pontryagin Projection",
    "url": "https://arxiv.org/pdf/2601.03175v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "We study continuous-time portfolio choice in diffusion markets with parameter $θ\\in Θ$ and uncertainty law $q(dθ)$. Nature draws latent $θ\\sim q$ at time 0; the investor cannot observe it and must deploy a single $θ$-blind feedback policy maximizing an ex-ante CRRA objective averaged over diffusion noise and $θ$. Our methods access $q$ only by sampling and assume no parametric form. We extend Pontryagin-Guided Direct Policy Optimization (PG-DPO) by sampling $θ$ inside the simulator and computing discrete-time gradients via backpropagation through time (BPTT), and we propose projected PG-DPO (P-PGDPO) that projects costate estimates to satisfy the $q$-aggregated Pontryagin first-order condition, yielding a deployable rule. We prove a BPTT-PMP correspondence uniform on compacts and a residual-based $θ$-blind policy-gap bound under local stability with explicit discretization/Monte Carlo errors; experiments show projection-driven stability and accurate decision-time benchmark recovery in high dimensions.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究在参数θ不确定的扩散市场中的连续时间投资组合选择问题。投资者无法观测潜在参数θ，只能采用单一θ盲反馈策略，最大化事前CRRA目标函数。方法通过采样访问不确定性分布q，无需参数形式假设。扩展了Pontryagin引导直接策略优化(PG-DPO)，在模拟器中采样θ并通过时间反向传播(BPTT)计算离散时间梯度，提出投影PG-DPO(P-PGDPO)，将协态估计投影以满足q聚合的Pontryagin一阶条件，得到可部署规则。证明了BPTT与Pontryagin极大值原理在紧集上的一致性，以及在局部稳定性下的θ盲策略间隙界限，包含显式离散化/蒙特卡洛误差。实验显示投影驱动稳定性和高维决策时基准的准确恢复。",
    "fetch_date": "2026-01-09",
    "id": "20260109_b823004b"
  },
  {
    "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2601.02850v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种神经符号深度强化学习方法，通过整合背景符号知识来提升样本效率和泛化能力。具体方法是将简单场景中定义的部分策略表示为逻辑规则，通过在线推理在训练过程中引导智能体：在探索阶段偏置动作分布，在利用阶段重新缩放Q值。这种神经符号集成增强了可解释性和可信度，同时加速了收敛，特别是在稀疏奖励环境和长规划视野的任务中。论文在挑战性任务上进行了实证验证，展示了该方法在避免从头调整DRL参数、加速复杂场景学习方面的潜力。",
    "fetch_date": "2026-01-09",
    "id": "20260109_db8b5b68"
  },
  {
    "title": "Class of topological portfolios: Are they better than classical portfolios?",
    "url": "https://arxiv.org/pdf/2601.03974v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Topological Data Analysis (TDA), an emerging field in investment sciences, harnesses mathematical methods to extract data features based on shape, offering a promising alternative to classical portfolio selection methodologies. We utilize persistence landscapes, a type of summary statistics for persistent homology, to capture the topological variation of returns, blossoming a novel concept of ``Topological Risk\". Our proposed topological risk then quantifies portfolio risk by tracking time-varying topological properties of assets through the $L_p$ norm of the persistence landscape. Through optimization, we derive an optimal portfolio that minimizes this topological risk. Numerical experiments conducted using nearly a decade long S\\&P 500 data demonstrate the superior performance of our TDA-based portfolios in comparison to the seven popular portfolio optimization models and two benchmark portfolio strategies, the naive $1/N$ portfolio and the S\\&P 500 market index, in terms of excess mean return, and several financial ratios. The outcome remains consistent through out the computational analysis conducted for the varying size of holding and investment time horizon. These results underscore the potential of our TDA-based topological risk metric in providing a more comprehensive understanding of portfolio dynamics than traditional statistical measures. As such, it holds significant relevance for modern portfolio management practices.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种基于拓扑数据分析（TDA）的投资组合优化方法，通过持久性景观（persistence landscapes）捕捉收益的拓扑变化，并定义了“拓扑风险”概念。利用L_p范数量化时变拓扑特性，通过优化最小化该风险构建投资组合。基于近十年标普500数据的数值实验表明，该方法在超额平均收益和多个财务比率上优于七种经典投资组合模型及两种基准策略（1/N组合和标普500指数），且结果在不同持仓规模和投资期限下保持稳健。",
    "fetch_date": "2026-01-09",
    "id": "20260109_4315c79c"
  },
  {
    "title": "Optimal execution on Uniswap v2/v3 under transient price impact",
    "url": "https://arxiv.org/pdf/2601.03799v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "We study the optimal liquidation of a large position on Uniswap v2 and Uniswap v3 in discrete time. The instantaneous price impact is derived from the AMM pricing rule. Transient impact is modeled to capture either exponential or approximately power-law decay, together with a permanent component. In the Uniswap v2 setting, we obtain optimal strategies in closed-form under general price dynamics. For Uniswap v3, we consider a two-layer liquidity framework, which naturally extends to multiple layers. We address the problem using dynamic programming under geometric Brownian motion dynamics and approximate the solution numerically using a discretization scheme. We obtain optimal strategies akin to classical ones in the LOB literature, with features specific to Uniswap. In particular, we show how the liquidity profile influences them.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究了在Uniswap v2/v3上大额头寸的最优清算问题。瞬时价格影响源自AMM定价规则，瞬态影响模型捕捉指数或近似幂律衰减，并包含永久性成分。在Uniswap v2中，我们获得了通用价格动态下的闭式最优策略；在Uniswap v3中，我们采用双层流动性框架（可扩展至多层），基于几何布朗运动动态规划，并通过离散化方案数值近似求解。所得最优策略类似于LOB文献中的经典方法，但具有Uniswap特有特征，特别是流动性分布如何影响策略。",
    "fetch_date": "2026-01-09",
    "id": "20260109_759461fb"
  },
  {
    "title": "Quantum computing for multidimensional option pricing: End-to-end pipeline",
    "url": "https://arxiv.org/pdf/2601.04049v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种用于多资产期权定价的端到端框架，该框架结合了市场一致的风险中性密度恢复与量子加速数值积分。首先使用正态逆高斯（NIG）模型从欧式期权报价中校准无套利边际分布，利用其解析易处理性及捕捉偏度和厚尾的能力。通过高斯Copula耦合边际分布以构建联合分布。为解决求解期权定价公式所需的高维积分计算瓶颈，采用了基于量子振幅估计（QAE）的量子加速蒙特卡洛（QAMC）技术，相比经典蒙特卡洛（CMC）方法实现了二次收敛改进。理论结果建立了边际密度估计（通过余弦级数展开）和多维定价的精度界限及查询复杂度。对流动性股票实体（法国农业信贷银行、安盛、米其林）的实证测试证实了高校准精度，并表明在可比精度下，QAMC所需的查询次数比经典方法少10-100倍。",
    "fetch_date": "2026-01-09",
    "id": "20260109_cad71305"
  },
  {
    "title": "Diversification Preferences and Risk Attitudes",
    "url": "https://arxiv.org/pdf/2601.04067v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Portfolio diversification is a cornerstone of modern finance, while risk aversion is central to decision theory; both concepts are long-standing and foundational. We investigate their connections by studying how different forms of diversification correspond to notions of risk aversion. We focus on the classical distinctions between weak and strong risk aversion, and consider diversification preferences for pairs of risks that are identically distributed, comonotonic, antimonotonic, independent, or exchangeable, as well as their intersections. Under a weak continuity condition and without assuming completeness of preferences, diversification for antimonotonic and identically distributed pairs implies weak risk aversion, and diversification for exchangeable pairs is equivalent to strong risk aversion. The implication from diversification for independent pairs to weak risk aversion requires a stronger continuity. We further provide results and examples that clarify the relationships between various diversification preferences and risk attitudes, in particular justifying the one-directional nature of many implications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了投资组合分散化与风险厌恶之间的理论联系，研究了在不同风险分布（如反单调、同分布、可交换、独立等）下，分散化偏好如何对应弱风险厌恶与强风险厌恶的概念。在弱连续性条件下，反单调和同分布风险对的分散化偏好意味着弱风险厌恶，而可交换风险对的分散化偏好则等价于强风险厌恶。独立风险对的分散化偏好推导出弱风险厌恶需要更强的连续性条件。论文通过结果和示例澄清了各种分散化偏好与风险态度之间的关系，特别是论证了多数蕴含关系的单向性。",
    "fetch_date": "2026-01-09",
    "id": "20260109_19a96837"
  },
  {
    "title": "Trading with market resistance and concave price impact",
    "url": "https://arxiv.org/pdf/2601.03215v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "We consider an optimal trading problem under a market impact model with endogenous market resistance generated by a sophisticated trader who (partially) detects metaorders and trades against them to exploit price overreactions induced by the order flow. The model features a concave transient impact driven by a power-law propagator with a resistance term responding to the trader's rate via a fixed-point equation involving a general resistance function. We derive a (non)linear stochastic Fredholm equation as the first-order optimality condition satisfied by optimal trading strategies. Existence and uniqueness of the optimal control are established when the resistance function is linear, and an existence result is obtained when it is strictly convex using coercivity and weak lower semicontinuity of the associated profit-and-loss functional. We also propose an iterative scheme to solve the nonlinear stochastic Fredholm equation and prove an exponential convergence rate. Numerical experiments confirm this behavior and illustrate optimal round-trip strategies under \"buy\" signals with various decay profiles and different market resistance specifications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一种具有内生市场阻力的市场冲击模型下的最优交易问题。该模型假设存在一个能够（部分）探测到大额订单（元订单）并与之反向交易以利用订单流引发的价格过度反应的复杂交易者，从而产生市场阻力。模型特征包括由幂律传播子驱动的凹型瞬时冲击，以及一个通过涉及一般阻力函数的定点方程对交易者速率做出响应的阻力项。作者推导出一个（非）线性随机Fredholm方程作为最优交易策略满足的一阶最优性条件。当阻力函数为线性时，建立了最优控制的存在性和唯一性；当阻力函数严格凸时，利用相关损益泛函的强制性和弱下半连续性得到了存在性结果。此外，作者提出了一个迭代方案来求解该非线性随机Fredholm方程，并证明了指数收敛速度。数值实验验证了该行为，并展示了在不同衰减曲线和市场阻力设定下，基于'买入'信号的最优往返交易策略。",
    "fetch_date": "2026-01-09",
    "id": "20260109_6dd1b64d"
  },
  {
    "title": "A Quantum Model for Constrained Markowitz Modern Portfolio Using Slack Variables to Process Mixed-Binary Optimization under QAOA",
    "url": "https://arxiv.org/pdf/2601.03278v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "Effectively encoding inequality constraints is a primary obstacle in applying quantum algorithms to financial optimization. A quantum model for Markowitz portfolio optimization is presented that resolves this by embedding slack variables directly into the problem Hamiltonian. The method maps each slack variable to a dedicated ancilla qubit, transforming the problem into a Quadratic Unconstrained Binary Optimization (QUBO) formulation suitable for the Quantum Approximate Optimization Algorithm (QAOA). This process internalizes the constraints within the quantum state, altering the problem's energy landscape to facilitate optimization. The model is empirically validated through simulation, showing it consistently finds the optimal portfolio where a standard penalty-based QAOA fails. This work demonstrates that modifying the Hamiltonian architecture via a slack-ancilla scheme provides a robust and effective pathway for solving constrained optimization problems on quantum computers. A fundamental quantum limit on the simultaneous precision of portfolio risk and return is also posited.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于约束型马科维茨现代投资组合优化的量子模型，通过引入松弛变量并将其直接嵌入问题哈密顿量中，解决了量子算法在金融优化中处理不等式约束的主要障碍。该方法将每个松弛变量映射到一个专用的辅助量子比特，将问题转化为适合量子近似优化算法（QAOA）的二次无约束二进制优化（QUBO）形式。该模型通过仿真进行了实证验证，表明其能持续找到最优投资组合，而标准的基于惩罚的QAOA则无法做到。这项工作表明，通过松弛-辅助方案修改哈密顿量架构，为在量子计算机上解决约束优化问题提供了一条稳健有效的途径。文中还提出了关于投资组合风险与回报同时精度的基本量子极限。",
    "fetch_date": "2026-01-09",
    "id": "20260109_ad68ae27"
  },
  {
    "title": "Algorithmic trading: winning strategies and their rationale",
    "url": "https://books.google.com/books?hl=en&lr=&id=CIwCTVqEj4oC&oi=fnd&pg=PR9&dq=%22algorithmic+trading%22+strategy+after:2024&ots=kVDIJtGBzE&sig=h_GIptC7VR5rAizz-bZt0STkCfI",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… to algorithmic trading strategies that can be readily implemented by both retail and institutional traders alike. More than an academic treatise on financial theory, Algorithmic Trading is …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "《算法交易：制胜策略及其原理》聚焦于可直接由零售和机构交易者实施的算法交易策略，超越金融理论的学术论述，强调实战应用。",
    "fetch_date": "2026-01-08",
    "id": "20260108_f8028324"
  },
  {
    "title": "Intelligent algorithmic trading strategy using reinforcement learning and directional change",
    "url": "https://ieeexplore.ieee.org/abstract/document/9514595/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… , employing an algorithmic trading strategy remains a challenge … algorithmic trading strategy using the proposed DCRL model, specifically, we present two algorithmic trading strategies …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于强化学习（RL）和方向变化（Directional Change）的智能算法交易策略，通过DCRL模型设计了两种具体的算法交易策略，旨在解决实际交易中算法策略应用的挑战。",
    "fetch_date": "2026-01-08",
    "id": "20260108_cefab63f"
  },
  {
    "title": "Optimal profit-making strategies in stock market with algorithmic trading",
    "url": "https://www.aimspress.com/aimspress-data/qfe/2024/3/PDF/QFE-08-03-021.pdf",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… former for strategy construction and use the machine learning model constructed in the previous chapter to realize strategy returns by … Our construction of the timing strategy is as follows: …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了在股票市场中通过算法交易实现最优盈利策略的方法。它结合了机器学习模型进行策略构建，并利用前一章节构建的模型来实现策略收益。其中，择时策略的构建方法被详细阐述，表明其具有实际应用潜力，适用于实战交易场景。",
    "fetch_date": "2026-01-08",
    "id": "20260108_672ad619"
  },
  {
    "title": "Deep Convolutional Transformer Network for Stock Movement Prediction",
    "url": "https://www.mdpi.com/2079-9292/13/21/4225",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… In [64], FDG-trans is built by integrating GRU, LSTM, and multi-head attention Transformers. Both TN and DT are stock prediction models based on the standard Transformer architecture…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度卷积Transformer网络的股票走势预测模型。通过整合GRU、LSTM和多头注意力Transformer等深度学习架构，构建了FDG-trans等股票预测模型，这些模型均基于标准Transformer架构。该研究对实战交易具有较高价值，因为它直接应用于股票市场预测这一量化交易核心领域，并采用了当前前沿的深度学习技术。",
    "fetch_date": "2026-01-08",
    "id": "20260108_ec739ba3"
  },
  {
    "title": "Hybrid LSTM-Transformer Model for Stock Market Prediction: A Deep Learning Approach",
    "url": "https://ieeexplore.ieee.org/abstract/document/11127241/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… This paper proposes a novel LSTM-Transformer hybrid model … LSTM, GRU, and Transformer baselines, achieving an MSE of … Below, we discuss key advancements in stock prediction …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种新颖的LSTM-Transformer混合模型用于股票市场预测。通过深度学习的方法，该模型在LSTM、GRU和Transformer基准模型上取得了较低的MSE（均方误差），展示了在股票预测领域的关键进展。",
    "fetch_date": "2026-01-08",
    "id": "20260108_15b45001"
  },
  {
    "title": "Global Stock Market Prediction Using Transformer-Based Deep Learning Techniques",
    "url": "https://ieeexplore.ieee.org/abstract/document/10724893/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… The Transformer framework, initially created for natural … The well-known Transformer architecture, which excels at … The outcomes demonstrate that the Transformer performs better than …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文研究基于Transformer的深度学习技术在全球股市预测中的应用。Transformer架构最初为自然语言处理设计，以其在序列建模中的卓越能力而闻名。结果表明，Transformer在预测性能上优于其他方法，显示出其在实战交易中捕捉市场时序模式和生成交易信号的潜力。",
    "fetch_date": "2026-01-08",
    "id": "20260108_d43c562e"
  },
  {
    "title": "Enhancing multi-factor stock selection with transformer networks: A comparative analysis against traditional machine learning models",
    "url": "https://www.sciencedirect.com/science/article/pii/S187705092502438X",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… Recent studies have begun to validate the Transformer's potential in finance. For instance, Transformer-based models have been shown to outperform LSTMs in stock prediction [5], and …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文《利用Transformer网络增强多因子选股：与传统机器学习模型的比较分析》探讨了Transformer架构在金融量化领域的应用潜力。研究表明，基于Transformer的模型在股票预测方面优于LSTM等传统时序模型，通过对比分析验证了其在多因子选股策略中的有效性，对实战交易中因子挖掘和预测模型优化具有直接参考价值。",
    "fetch_date": "2026-01-08",
    "id": "20260108_183ffad8"
  },
  {
    "title": "Llm-augmented linear transformer–cnn for enhanced stock price prediction",
    "url": "https://www.mdpi.com/2227-7390/13/3/487",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… from two additional branches: the Linear Transformer branch, which captures long-term … the proposed framework significantly improves stock prediction accuracy by effectively capturing …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "该论文提出了一种LLM增强的线性Transformer-CNN框架，通过线性Transformer分支捕捉长期依赖关系，并结合CNN处理局部模式，显著提升了股价预测的准确性。该方法融合了深度学习技术，对实战交易中的预测模型优化具有参考价值。",
    "fetch_date": "2026-01-08",
    "id": "20260108_a0e7c66b"
  },
  {
    "title": "Stock price prediction with LLM-guided market movement signals and transformer model",
    "url": "https://ojs.bonviewpress.com/index.php/FSI/article/view/5703",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… However, most Transformer-based stock prediction models … -generated sentiment features into stock prediction models [18]. … are not applied to directly output the final stock prediction. …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "该论文提出了一种结合LLM生成的市场情绪信号与Transformer模型的股价预测方法。虽然摘要指出现有Transformer模型通常不直接输出最终股价预测，但该方法通过整合LLM引导的市场动向信号，旨在提升预测性能。对实战交易的价值在于：可能提供更丰富的市场情绪特征，增强模型对市场动态的捕捉能力，但需验证其在实时交易环境中的稳定性和泛化性。",
    "fetch_date": "2026-01-08",
    "id": "20260108_90875abc"
  },
  {
    "title": "Algorithmic trading and quantitative strategies",
    "url": "https://www.taylorfrancis.com/books/mono/10.1201/9780429183942/algorithmic-trading-quantitative-strategies-raja-velu",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… Hardy is responsible for the development of the algorithmic trading strategies and models underpinning the agency electronic execution products for the Equities and Futures divisions …",
    "broker": "Google Scholar",
    "score": 6,
    "summary": "该论文聚焦于算法交易与量化策略的开发，由Hardy负责为股票和期货部门的机构电子执行产品构建算法交易策略和模型。内容涉及实际应用层面的策略与模型开发，但未明确提及强化学习、深度学习或Alpha生成等前沿实战技术，因此具有一定的实战参考价值，但可能偏向传统量化方法。",
    "fetch_date": "2026-01-08",
    "id": "20260108_8062999b"
  },
  {
    "title": "Stock price forecast based on improved Transformer",
    "url": "https://books.google.com/books?hl=en&lr=&id=XZEXEQAAQBAJ&oi=fnd&pg=PA170&dq=transformers+for+%22stock+prediction%22+after:2024&ots=A6yPxbhQ6b&sig=fEqjpsiwiEq0xwuCx805RMP8tHg",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… a stock prediction system based on five basic trading indicators of historical stock trading data, which provides investors with stock prediction … This paper use transformer model as the …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "本文基于历史股票交易数据的五个基本交易指标，构建了一个基于改进Transformer模型的股票价格预测系统，旨在为投资者提供股票预测。",
    "fetch_date": "2026-01-08",
    "id": "20260108_922c857b"
  },
  {
    "title": "Stock Price Prediction Using Transformers",
    "url": "https://ieeexplore.ieee.org/abstract/document/10911285/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… study focuses on the capabilities of Transformer architecture to handle intricate, non-linear trends effectively. Traditional approaches to stock prediction, including autoregressive and …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该研究聚焦于Transformer架构在有效处理复杂非线性趋势方面的能力。传统的股票预测方法包括自回归模型等。",
    "fetch_date": "2026-01-08",
    "id": "20260108_81c4d64f"
  },
  {
    "title": "Analyzing the impact of algorithmic trading on stock market behavior: A comprehensive review",
    "url": "https://pdfs.semanticscholar.org/5893/a486749185dfd0ac0453cdd89a0b4d0ef73e.pdf",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… interplay between algorithmic trading strategies and market … examination of algorithmic trading's principles, strategies, … The findings reveal that algorithmic trading, characterized by …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "该论文全面综述了算法交易对股市行为的影响，探讨了算法交易策略与市场之间的相互作用，并分析了算法交易的原则与策略。研究发现，算法交易以...为特征。",
    "fetch_date": "2026-01-08",
    "id": "20260108_20f72e0a"
  },
  {
    "title": "Systematic Review on Algorithmic Trading",
    "url": "https://www.ceeol.com/search/article-detail?id=1365022",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… for academics and practitioners to innovate and optimize algorithmic trading strategies. … interested in algorithmic trading, traders looking to optimize their strategies and developers …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "本文是一篇关于算法交易的系统性综述，主要面向学术界和从业者，旨在为算法交易策略的创新与优化提供参考。对算法交易感兴趣的研究人员、寻求策略优化的交易员以及相关开发者可能从中受益。",
    "fetch_date": "2026-01-08",
    "id": "20260108_548b7780"
  },
  {
    "title": "Algorithmic trading",
    "url": "https://ieeexplore.ieee.org/abstract/document/5696713/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… will shape the design of the algorithmic trading strategy; typically, broker AT systems seek to minimize the cost of trading by optimizing the execution strategy (ie minimize market impact …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "算法交易：论文探讨了算法交易策略的设计，通常经纪商的算法交易系统通过优化执行策略（即最小化市场冲击）来降低交易成本。",
    "fetch_date": "2026-01-08",
    "id": "20260108_a8706ded"
  },
  {
    "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
    "url": "https://arxiv.org/pdf/2601.02061v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "深度强化学习智能体常表现出不稳定、高频的控制行为，这在实际部署中会因过度能耗和机械磨损而受限。本文通过高阶导数惩罚系统研究动作平滑正则化，从连续控制基准的理论理解推进到建筑能源管理的实际验证。在四个连续控制环境中的综合评估表明，三阶导数惩罚（急动度最小化）能持续实现更优的平滑性，同时保持竞争力性能。我们将这些发现扩展到HVAC控制系统，其中平滑策略将设备开关次数减少60%，转化为显著的运营效益。我们的工作确立了高阶动作正则化作为RL优化与能源关键应用中操作约束之间的有效桥梁。",
    "fetch_date": "2026-01-07",
    "id": "20260107_8d61df9e"
  },
  {
    "title": "Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance",
    "url": "https://arxiv.org/pdf/2601.01709v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文扩展了Black-Scholes框架下的Q学习器（QLBS），通过引入风险厌恶和交易成本，并提出了新颖的期权定价复制学习（RLOP）方法。两种方法均与标准强化学习算法完全兼容，并在市场摩擦条件下运行。使用SPY和XOP期权数据进行评估，结果显示Adaptive-QLBS在隐含波动率空间实现了更高的静态定价精度，而RLOP通过降低短缺概率提供了更优的动态对冲表现。这些结果强调了评估期权定价模型时需超越静态拟合，重视实际对冲效果的重要性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_33758e27"
  },
  {
    "title": "On lead-lag estimation of non-synchronously observed point processes",
    "url": "https://arxiv.org/pdf/2601.01871v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "This paper introduces a new theoretical framework for analyzing lead-lag relationships between point processes, with a special focus on applications to high-frequency financial data. In particular, we are interested in lead-lag relationships between two sequences of order arrival timestamps. The seminal work of Dobrev and Schaumburg proposed model-free measures of cross-market trading activity based on cross-counts of timestamps. While their method is known to yield reliable results, it faces limitations because its original formulation inherently relies on discrete-time observations, an issue we address in this study. Specifically, we formulate the problem of estimating lead-lag relationships in two point processes as that of estimating the shape of the cross-pair correlation function (CPCF) of a bivariate stationary point process, a quantity well-studied in the neuroscience and spatial statistics literature. Within this framework, the prevailing lead-lag time is defined as the location of the CPCF's sharpest peak. Under this interpretation, the peak location in Dobrev and Schaumburg's cross-market activity measure can be viewed as an estimator of the lead-lag time in the aforementioned sense. We further propose an alternative lead-lag time estimator based on kernel density estimation and show that it possesses desirable theoretical properties and delivers superior numerical performance. Empirical evidence from high-frequency financial data demonstrates the effectiveness of our proposed method.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种新的理论框架，用于分析点过程之间的领先-滞后关系，特别关注高频金融数据的应用。研究重点在于两个订单到达时间戳序列之间的领先-滞后关系。Dobrev和Schaumburg的开创性工作提出了基于时间戳交叉计数的跨市场交易活动无模型度量方法。虽然他们的方法已知能产生可靠结果，但由于其原始公式本质上依赖于离散时间观测而面临局限性，本研究正是为了解决这一问题。具体而言，我们将估计两个点过程中领先-滞后关系的问题表述为估计二元平稳点过程的交叉配对相关函数形状的问题，这一量在神经科学和空间统计学文献中已有深入研究。在此框架下，主导的领先-滞后时间被定义为CPCF最尖锐峰值的位置。在这种解释下，Dobrev和Schaumburg的跨市场活动度量中的峰值位置可被视为领先-滞后时间的估计量。",
    "fetch_date": "2026-01-07",
    "id": "20260107_5419ed89"
  },
  {
    "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions",
    "url": "https://arxiv.org/pdf/2601.01803v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "论文《Moments Matter: Stabilizing Policy Optimization using Return Distributions》提出了一种通过利用高阶矩（偏度和峰度）来偏置PPO算法优势函数的方法，旨在解决深度强化学习（RL）中因环境和算法噪声导致的策略不稳定问题。该方法通过建模状态-动作回报分布来减少策略更新引起的变异性，从而提高策略优化的稳定性。对于实战交易，该技术有助于在复杂、高噪声的市场环境中训练更稳健的交易策略，减少策略性能的剧烈波动，提升算法在实盘中的可靠性和可迁移性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_62ba9edb"
  },
  {
    "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives",
    "url": "https://arxiv.org/pdf/2601.01665v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种面向多目标组合优化问题（MOCOPs）的偏好条件深度强化学习（DRL）求解器的统一鲁棒性框架。该框架包含：1）基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量退化量化攻击影响；2）防御策略，将难度感知偏好选择融入对抗训练，以减少对受限偏好区域的过拟合并提升分布外性能。实验在多目标旅行商问题（MOTSP）、多目标容量约束车辆路径问题（MOCVRP）和多目标背包问题（MOKP）上验证了攻击方法能有效针对不同求解器生成困难实例，且防御方法显著增强了鲁棒性和泛化能力。",
    "fetch_date": "2026-01-07",
    "id": "20260107_53571319"
  },
  {
    "title": "Can Large Language Models Improve Venture Capital Exit Timing After IPO?",
    "url": "https://arxiv.org/pdf/2601.00810v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "IPO后退出时机是风险投资（VC）最关键决策之一，现有研究主要描述VC何时退出而非评估其经济最优性。大型语言模型（LLMs）在整合复杂金融数据和文本信息方面展现潜力，但尚未应用于IPO后退出决策。本研究提出一个框架，利用LLMs分析IPO后月度财务表现、文件、新闻和市场信号，估算VC最优退出时间并建议卖出或继续持有。通过比较LLM生成建议与实际VC退出日期，计算两种策略的回报差异，量化遵循LLM指导的收益或损失，为AI驱动指导能否改善退出时机提供证据，并补充风险投资研究中的传统风险模型和实物期权模型。",
    "fetch_date": "2026-01-07",
    "id": "20260107_1799a059"
  },
  {
    "title": "Statistical and economic evaluation of forecasts in electricity markets: beyond RMSE and MAE",
    "url": "https://arxiv.org/pdf/2511.13616v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "In recent years, a rapid development of forecasting methods has led to an increase in the accuracy of predictions. In the literature, forecasts are typically evaluated using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). While appropriate for statistical assessment, these measures do not adequately reflect the economic value of forecasts. This study addresses the decision-making problem faced by a battery energy storage system, which must determine optimal charging and discharging times based on day-ahead electricity price forecasts. To explore the relationship between forecast accuracy and economic value, we generate a pool of 192 forecasts. These are evaluated using seven statistical metrics that go beyond RMSE and MAE, capturing various characteristics of the predictions and associated errors. We calculate the dynamic correlation between the statistical measures and gained profits to reveal that both RMSE and MAE are only weakly correlated with revenue. In contrast, measures that assess the alignment between predicted and actual daily price curves have a stronger relationship with profitability and are thus more effective for selecting optimal forecasts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "近年来，预测方法的快速发展提高了预测准确性。文献中通常使用均方根误差（RMSE）和平均绝对误差（MAE）等指标评估预测。虽然这些指标适用于统计评估，但不能充分反映预测的经济价值。本研究针对电池储能系统面临的决策问题，该系统必须基于日前电价预测确定最佳充放电时间。为探索预测准确性与经济价值之间的关系，我们生成了192个预测池，并使用七种超越RMSE和MAE的统计指标进行评估，这些指标捕捉了预测及相关误差的各种特征。通过计算统计指标与获得利润之间的动态相关性，我们发现RMSE和MAE与收入仅呈弱相关。相比之下，评估预测与实际日价格曲线一致性的指标与盈利能力有更强的关系，因此在选择最优预测方法时更有效。",
    "fetch_date": "2026-01-07",
    "id": "20260107_fdc6cf29"
  },
  {
    "title": "Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects",
    "url": "https://arxiv.org/pdf/2601.01783v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the \"too-similar-to-fail\" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过时变参数向量自回归（TVP-VAR）模型和30天滚动窗口，分析了2023年美国银行业危机期间的风险传染机制。研究发现，危机传播主要通过高频信息传染渠道而非直接金融关联，支持“太相似而不能倒”假说——在利率压力下，业务模式相似的银行间风险溢出显著。研究识别出SVB、FRC、WAL为主要风险净输出方，其相似同行成为净接收方，这种动态系统性脆弱指标无法通过逐资产分析捕获。市场情绪和政策冲击进一步放大了风险溢出效应。",
    "fetch_date": "2026-01-07",
    "id": "20260107_81f579ca"
  },
  {
    "title": "Almost-Exact Simulation Scheme for Heston-type Models: Bermudan and American Option Pricing",
    "url": "https://arxiv.org/pdf/2601.00815v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Recently, an Almost-Exact Simulation (AES) scheme was introduced for the Heston stochastic volatility model and tested for European option pricing. This paper extends this scheme for pricing Bermudan and American options under both Heston and double Heston models. The AES improves Monte Carlo simulation efficiency by using the non-central chi-square distribution for the variance process. We derive the AES scheme for the double Heston model and compare the performance of the AES schemes under both models with the Euler scheme. Our numerical experiments validate the effectiveness of the AES scheme in providing accurate option prices with reduced computational time, highlighting its robustness for both models. In particular, the AES achieves higher accuracy and computational efficiency when the number of simulation steps matches the exercise dates for Bermudan options.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文将Heston模型的几乎精确模拟（AES）方案扩展到双Heston模型，用于百慕大和美式期权定价。AES利用方差过程的非中心卡方分布提高蒙特卡洛模拟效率，相比欧拉方案在计算时间和精度上表现更优，尤其当模拟步数与行权日期匹配时。",
    "fetch_date": "2026-01-07",
    "id": "20260107_cbbee458"
  },
  {
    "title": "Object-Centric World Models for Causality-Aware Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.14262v2",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \\emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "世界模型旨在支持样本高效的深度强化学习智能体，但现有模型在准确模拟高维、非平稳且包含多个具有丰富交互的物体的环境方面仍面临挑战，因为它们通常学习所有环境组件的整体表示。相比之下，人类通过将环境分解为离散物体来感知环境，从而促进高效决策。受此启发，我们提出了STICA（Slot Transformer Imagination with CAusality-aware reinforcement learning），这是一个统一框架，其中以物体为中心的Transformer作为世界模型以及因果感知的策略和价值网络。STICA将每个观察表示为一组以物体为中心的token，以及表示智能体动作和所得奖励的token，使世界模型能够预测token级别的动态和交互。策略和价值网络随后估计token级别的因果关系，并在注意力层中使用它们，从而实现因果引导的决策。在物体丰富的基准测试中，STICA始终表现出色。",
    "fetch_date": "2026-01-07",
    "id": "20260107_38bd7ac8"
  },
  {
    "title": "Wasserstein Distributionally Robust Rare-Event Simulation",
    "url": "https://arxiv.org/pdf/2601.01642v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "标准罕见事件模拟技术需要精确的分布设定，这在存在分布不确定性时限制了其有效性。为此，我们开发了一个新颖的框架，用于估计受此类分布模型风险影响的罕见事件概率。具体而言，我们专注于计算最坏情况下的罕见事件概率，该概率被定义为针对以特定名义分布为中心的Wasserstein模糊集的一个分布鲁棒边界。通过利用该边界的对偶特性，我们提出了分布鲁棒重要性采样（DRIS），这是一种计算上易于处理的方法，旨在显著降低与估计对偶分量相关的方差。所提出的方法易于实现且采样成本低。最重要的是，它实现了消失的相对误差，这是罕见事件模拟中极难建立的最强效率保证。我们的数值研究证实了DRIS相对于现有基准的优越性能。",
    "fetch_date": "2026-01-07",
    "id": "20260107_6c6507be"
  },
  {
    "title": "Chaos and Synchronization in Financial Leverages Dynamics: Modeling Systemic Risk with Coupled Unimodal Maps",
    "url": "https://arxiv.org/pdf/2601.01505v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Systemic financial risk refers to the simultaneous failure or destabilization of multiple financial institutions, often triggered by contagion mechanisms or common exposures to shocks. In this paper, we present a dynamical model of bank leverage (the ratio of asset holdings to equity) a quantity that both reflects and drives risk dynamics. We model how banks, constrained by Value-at-Risk (VaR) regulations, adjust their leverage in response to changes in the price of a single asset, assumed to be held in fixed proportion across banks. This leverage-targeting behavior introduces a procyclical feedback loop between asset prices and leverage. In the dynamics, this can manifest as logistic-like behavior with a rich bifurcation structure across model parameters. By analyzing these coupled dynamics in both isolated and interconnected bank models, we outline a framework for understanding how systemic risk can emerge from seemingly rational micro-level behavior.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "系统性金融风险指多个金融机构同时失败或失稳，通常由传染机制或对冲击的共同敞口引发。本文提出一个银行杠杆（资产持有与权益之比）的动态模型，该指标既反映也驱动风险动态。我们模拟了受风险价值（VaR）监管约束的银行，如何根据单一资产价格变化调整其杠杆（假设各银行按固定比例持有该资产）。这种杠杆目标行为在资产价格与杠杆之间引入了顺周期反馈循环。在动态中，这可能表现为类似逻辑斯蒂的行为，具有丰富的分岔结构。通过分析孤立和互连银行模型中的这些耦合动态，我们概述了一个理解系统性风险如何从看似理性的微观行为中产生的框架。",
    "fetch_date": "2026-01-07",
    "id": "20260107_c9ba9ad2"
  },
  {
    "title": "Critical volatility threshold for log-normal to power-law transition",
    "url": "https://arxiv.org/pdf/2601.01269v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "Random walk models with log-normal outcomes fit local market observations remarkably well. Yet interconnected or recursive structures - layered derivatives, leveraged positions, iterative funding rounds - periodically produce power-law distributed events. We show that the transition from log-normal to power-law dynamics requires only three conditions: randomness in the underlying process, rectification of payouts, and iterative feed-forward of expected values. Using an infinite option-on-option chain as an illustrative model, we derive a critical volatility threshold at $σ^* = \\sqrt{2π} \\approx 250.66\\%$ for the unconditional case. With selective survival - where participants require minimum returns to continue - the critical threshold drops discontinuously to $σ_{\\text{th}}^{*} = \\sqrt{π/2} \\approx 125.3\\%$, and can decrease further with higher survival thresholds. The resulting outcomes follow what we term the Critical Volatility ($V^*$) Distribution - a power-law whose exponent admits closed-form expression in terms of survival pressure and conditional expected growth. The result suggests that fat tails may be an emergent property of iterative log-normal processes with selection rather than an exogenous feature.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了金融市场中从对数正态分布向幂律分布的动态转变。研究发现，这种转变仅需三个条件：基础过程的随机性、收益的整流以及期望值的迭代前馈。通过一个无限期权链模型，论文推导出无条件情况下的临界波动率阈值σ* ≈ 250.66%，而在存在选择性生存（参与者需要最低回报以继续参与）的情况下，该阈值会不连续地降至σ_th* ≈ 125.3%，并可能随着生存阈值的提高而进一步降低。由此产生的结果遵循所谓的“临界波动率分布”——一种幂律分布，其指数可通过生存压力和条件期望增长以闭式表达。研究结果表明，厚尾现象可能是具有选择机制的迭代对数正态过程的一种涌现特性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_ec011e94"
  },
  {
    "title": "European Options in Market Models with Multiple Defaults: the BSDE approach",
    "url": "https://arxiv.org/pdf/2601.01250v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "We study non-linear Backward Stochastic Differential Equations (BSDEs) driven by a Brownian motion and p default martingales. The driver of the BSDE with multiple default jumps can take a generalized form involving an optional finite variation process. We first show existence and uniqueness. We then establish comparison and strict comparison results for these BSDEs, under a suitable assumption on the driver. In the case of a linear driver, we derive an explicit formula for the first component of the BSDE using an adjoint exponential semimartingale. The representation depends on whether the finite variation process is predictable or only optional. We apply our results to the problem of pricing and hedging a European option in a linear complete market with two defaultable assets and in a non-linear complete market with p defaultable assets. Two examples of the latter market model are provided: an example where the seller of the option is a large investor influencing the probability of default of a single asset and an example where the large seller's strategy affects the default probabilities of all p assets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究由布朗运动和p个违约鞅驱动的非线性倒向随机微分方程（BSDE）。在驱动项包含可选有限变差过程的广义形式下，证明了存在唯一性、比较定理和严格比较定理。针对线性驱动情形，利用伴随指数半鞅给出了BSDE第一分量的显式公式，该表示取决于有限变差过程是可预测还是仅可选。应用结果研究了含两个违约资产的线性完全市场及含p个违约资产的非线性完全市场中欧式期权的定价与对冲问题，并提供了两个非线性市场实例：一是期权卖方作为大投资者影响单一资产违约概率；二是大卖方策略影响所有p个资产的违约概率。",
    "fetch_date": "2026-01-07",
    "id": "20260107_5b2b83ef"
  },
  {
    "title": "Order-Constrained Spectral Causality in Multivariate Time Series",
    "url": "https://arxiv.org/pdf/2601.01216v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于算子理论的多元时间序列因果分析框架，通过顺序约束谱非不变性定义方向性影响。该方法在线性高斯假设下等同于线性格兰杰因果性，但能捕捉超越该范围的集体和非线性方向依赖关系。论文建立了统计推断的理论基础，包括存在性、一致性和有效性，并通过模拟验证了其在非线性依赖检测上的优势。",
    "fetch_date": "2026-01-07",
    "id": "20260107_594d59c5"
  },
  {
    "title": "Uncertainty-Adjusted Sorting for Asset Pricing with Machine Learning",
    "url": "https://arxiv.org/pdf/2601.00593v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Machine learning is central to empirical asset pricing, but portfolio construction still relies on point predictions and largely ignores asset-specific estimation uncertainty. We propose a simple change: sort assets using uncertainty-adjusted prediction bounds instead of point predictions alone. Across a broad set of ML models and a U.S. equity panel, this approach improves portfolio performance relative to point-prediction sorting. These gains persist even when bounds are built from partial or misspecified uncertainty information. They arise mainly from reduced volatility and are strongest for flexible machine learning models. Identification and robustness exercises show that these improvements are driven by asset-level rather than time or aggregate predictive uncertainty.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "机器学习在实证资产定价中至关重要，但投资组合构建仍依赖点预测，并基本忽略了资产特定的估计不确定性。我们提出一个简单的改变：使用不确定性调整的预测边界而非仅点预测来对资产进行排序。在广泛的机器学习模型和美国股票面板数据中，该方法相对于点预测排序改善了投资组合表现。即使边界基于部分或错误指定的不确定性信息构建，这些收益依然存在。它们主要源于波动性的降低，并且对于灵活的机器学习模型效果最强。识别和稳健性检验表明，这些改进是由资产层面而非时间或总体预测不确定性驱动的。",
    "fetch_date": "2026-01-06",
    "id": "20260106_6f536e4c"
  },
  {
    "title": "Ultimate Forward Rate Prediction and its Application to Bond Yield Forecasting: A Machine Learning Perspective",
    "url": "https://arxiv.org/pdf/2601.00011v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "This study focuses on forecasting the ultimate forward rate (UFR) and developing a UFRbased bond yield prediction model using data from Chinese treasury bonds and macroeconomic variables spanning from December 2009 to December 2024. The de Kort-Vellekooptype methodology is applied to estimate the UFR, incorporating the optimal turning parameter determination technique proposed in this study, which helps mitigate anomalous fluctuations. In addition, both linear and nonlinear machine learning techniques are employed to forecast the UFR and ultra-long-term bond yields. The results indicate that nonlinear machine learning models outperform their linear counterparts in forecasting accuracy. Incorporating macroeconomic variables, particularly price index-related variables, significantly improves the accuracy of predictions. Finally, a novel UFR-based bond yield forecasting model is developed, demonstrating superior performance across different bond maturities.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究聚焦于预测终极远期利率（UFR）并开发基于UFR的债券收益率预测模型，使用中国国债和宏观经济变量数据（2009年12月至2024年12月）。应用de Kort-Vellekoop类型方法估计UFR，结合本研究提出的最优转折参数确定技术，有助于减轻异常波动。此外，采用线性和非线性机器学习技术预测UFR和超长期债券收益率。结果表明，非线性机器学习模型在预测准确性上优于线性模型。纳入宏观经济变量，特别是价格指数相关变量，显著提高了预测准确性。最后，开发了一种新颖的基于UFR的债券收益率预测模型，在不同债券期限上表现出优越性能。",
    "fetch_date": "2026-01-06",
    "id": "20260106_6f58b720"
  },
  {
    "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2601.00770v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "投资组合优化是各大金融机构的核心任务，其中基数约束均值-方差投资组合优化（CCPO）作为混合整数二次规划问题，传统精确求解器难以处理，通常依赖启发式算法获取近似解。本文提出一种新颖的智能体框架，针对CCPO问题探索多种具体架构，旨在自动化复杂工作流程并优化启发式算法开发，通过整合多算法解决方案提升有效前沿表现。研究表明智能体框架在组合优化领域展现出自动化工作流与算法开发方面的潜力，部分性能甚至超越人工水平。",
    "fetch_date": "2026-01-06",
    "id": "20260106_3f86608c"
  },
  {
    "title": "Second Thoughts: How 1-second subslots transform CEX-DEX Arbitrage on Ethereum",
    "url": "https://arxiv.org/pdf/2601.00738v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agent's DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereum's default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了以太坊区块时间缩短对去中心化交易所活动的影响，重点关注中心化交易所与去中心化交易所之间的套利行为。作者构建了一个交易模型，其中代理在DEX上的交易执行存在不确定性，代理在决定是否追求套利机会时会明确考虑这种执行风险。通过对比以太坊默认12秒区块时间环境与提供1秒子区块执行的更快机制下的代理行为，基于2025年7月至9月币安和Uniswap v3数据的模拟显示：更快的区块时间使套利交易数量平均增加535%，交易量平均增加203%。1秒子区块机制下CEX-DEX套利活动的增加，源于成功和失败交易结果方差的降低，这提高了风险调整后收益，使CEX-DEX套利更具吸引力。",
    "fetch_date": "2026-01-06",
    "id": "20260106_9f1da085"
  },
  {
    "title": "HODL Strategy or Fantasy? 480 Million Crypto Market Simulations and the Macro-Sentiment Effect",
    "url": "https://arxiv.org/pdf/2512.02029v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Crypto enthusiasts claim that buying and holding crypto assets yields high returns, often citing Bitcoin's past performance to promote other tokens and fuel fear of missing out. However, understanding the real risk-return trade-off and what factors affect future crypto returns is crucial as crypto becomes increasingly accessible to retail investors through major brokerages. We examine the HODL strategy through two independent analyses. First, we implement 480 million Monte Carlo simulations across 378 non-stablecoin crypto assets, net of trading fees and the opportunity cost of 1-month Treasury bills, and find strong evidence of survivorship bias and extreme downside concentration. At the 2-3 year horizon, the median excess return is -28.4 percent, the 1 percent conditional value at risk indicates that tail scenarios wipe out principal after all costs, and only the top quartile achieves very large gains, with a mean excess return of 1,326.7 percent. These results challenge the HODL narrative: across a broad set of assets, simple buy-and-hold loads extreme downside risk onto most investors, and the miracles mostly belong to the luckiest quarter. Second, using a Bayesian multi-horizon local projection framework, we find that endogenous predictors based on realized risk-return metrics have economically negligible and unstable effects, while macro-finance factors, especially the 24-week exponential moving average of the Fear and Greed Index, display persistent long-horizon impacts and high cross-basket stability. Where significant, a one-standard-deviation sentiment shock reduces forward top-quartile mean excess returns by 15-22 percentage points and median returns by 6-10 percentage points over 1-3 year horizons, suggesting that macro-sentiment conditions, rather than realized return histories, are the dominant indicators for future outcomes.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过两项独立分析评估HODL策略的实际价值。首先，对378种非稳定币加密资产进行了4.8亿次蒙特卡洛模拟（扣除交易费用和1个月国债机会成本），发现存在显著的生存偏差和极端下行风险集中：在2-3年持有期内，中位数超额收益为-28.4%，1%条件风险价值显示尾部情景会完全侵蚀本金，仅前四分之一资产获得平均1326.7%的超额收益。其次，采用贝叶斯多时间框架分析宏观情绪效应。研究结论挑战了HODL叙事，表明简单买入持有策略对多数投资者带来极端下行风险，超额收益主要集中于最幸运的少数资产。",
    "fetch_date": "2026-01-06",
    "id": "20260106_4a5f9385"
  },
  {
    "title": "Capital allocation and tail central moments for the multivariate normal mean-variance mixture distribution",
    "url": "https://arxiv.org/pdf/2601.00568v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Capital allocation is a procedure used to assess the risk contributions of individual risk components to the total risk of a portfolio. While the conditional tail expectation (CTE)-based capital allocation is arguably the most popular capital allocation method, its inability to reflect important tail behaviour of losses necessitates a more accurate approach. In this paper, we introduce a new capital allocation method based on the tail central moments (TCM), generalising the tail covariance allocation informed by the tail variance. We develop analytical expressions of the TCM as well as the TCM-based capital allocation for the class of normal mean-variance mixture distributions, which is widely used to model asymmetric and heavy-tailed data in finance and insurance. As demonstrated by a numerical analysis, the TCM-based capital allocation captures several significant patterns in the tail region of equity losses that remain undetected by the CTE, enhancing the understanding of the tail risk contributions of risk components.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "资本配置是评估投资组合中单个风险成分对总风险贡献的程序。虽然基于条件尾部期望（CTE）的资本配置方法最为流行，但其无法反映损失的重要尾部行为，因此需要更精确的方法。本文引入了一种基于尾部中心矩（TCM）的新资本配置方法，推广了由尾部方差驱动的尾部协方差配置。我们为广泛应用于金融和保险中建模非对称和厚尾数据的正态均值-方差混合分布类，开发了TCM以及基于TCM的资本配置的解析表达式。数值分析表明，基于TCM的资本配置捕捉了股权损失尾部区域的几个重要模式，这些模式未被CTE检测到，从而增强了对风险成分尾部风险贡献的理解。",
    "fetch_date": "2026-01-06",
    "id": "20260106_85310112"
  },
  {
    "title": "Causal Inference in Financial Event Studies",
    "url": "https://arxiv.org/pdf/2511.15123v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Financial event studies, ubiquitous in finance research, typically use linear factor models with known factors to estimate abnormal returns and identify causal effects of information events. This paper demonstrates that when factor models are misspecified -- an almost certain reality -- traditional event study estimators produce inconsistent estimates of treatment effects. The bias is particularly severe during volatile periods, over long horizons, and when event timing correlates with market conditions. We derive precise conditions for identification and expressions for asymptotic bias. As an alternative, we propose synthetic control methods that construct replicating portfolios from control securities without imposing specific factor structures. Revisiting four empirical applications, we show that some established findings may reflect model misspecification rather than true treatment effects. While traditional methods remain reliable for short-horizon studies with random event timing, our results suggest caution when interpreting long-horizon or volatile-period event studies and highlight the importance of quasi-experimental designs when available.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "金融事件研究通常使用已知因子的线性因子模型来估计异常回报并识别信息事件的因果效应。本文证明，当因子模型设定错误时（几乎是必然的现实），传统的事件研究估计量会产生不一致的处理效应估计。这种偏差在波动期、长期窗口以及事件时机与市场状况相关时尤为严重。我们推导了识别的精确条件和渐近偏差的表达式。作为替代方案，我们提出了合成控制方法，该方法从控制证券构建复制投资组合，而不强加特定的因子结构。通过重新审视四个实证应用，我们表明一些已确立的发现可能反映的是模型设定错误而非真实的处理效应。虽然传统方法对于事件时机随机的短期研究仍然可靠，但我们的结果表明在解释长期或波动期事件研究时应谨慎，并强调了准实验设计的重要性。",
    "fetch_date": "2026-01-06",
    "id": "20260106_be952b51"
  },
  {
    "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.15002v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "下一代网络采用开放式无线接入网络（O-RAN）架构，通过无线接入网络智能控制器（RIC）实现动态资源管理。尽管深度强化学习（DRL）模型在优化网络资源方面展现出潜力，但在动态环境中常面临鲁棒性和泛化性不足的问题。本文提出一种新颖的资源管理方法，在分布式多智能体强化学习（MARL）框架中，将锐度感知最小化（SAM）技术融入软演员-评论家（SAC）算法。该方法引入自适应选择性SAM机制，其正则化由时序差分（TD）误差方差驱动，确保仅对环境复杂度高的智能体进行正则化。这种针对性策略减少了不必要的开销，提高了训练稳定性，并在不牺牲学习效率的前提下增强了泛化能力。此外，还结合了动态ρ调度方案，以优化各智能体间的探索-利用权衡。实验结果表明，该方法显著优于传统DRL方法，在资源分配方面实现了高达22%的性能提升。",
    "fetch_date": "2026-01-06",
    "id": "20260106_708e569e"
  },
  {
    "title": "Multimodal Insights into Credit Risk Modelling: Integrating Climate and Text Data for Default Prediction",
    "url": "https://arxiv.org/pdf/2601.00478v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Credit risk assessment increasingly relies on diverse sources of information beyond traditional structured financial data, particularly for micro and small enterprises (mSEs) with limited financial histories. This study proposes a multimodal framework that integrates structured credit variables, climate panel data, and unstructured textual narratives within a unified learning architecture. Specifically, we use long short-term memory (LSTM), the gated recurrent unit (GRU), and transformer models to analyse the interplay between these data modalities. The empirical results demonstrate that unimodal models based on climate or text data outperform those relying solely on structured data, while the integration of multiple data modalities yields significant improvements in credit default prediction. Using SHAP-based explainability methods, we find that physical climate risks play an important role in default prediction, with water-logging by rain emerging as the most influential factor. Overall, this study demonstrates the potential of multimodal approaches in AI-enabled decision-making, which provides robust tools for credit risk assessment while contributing to the broader integration of environmental and textual insights into predictive analytics.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究提出了一种多模态信用风险建模框架，整合结构化信用变量、气候面板数据和非结构化文本叙述，采用LSTM、GRU和Transformer模型分析数据模态间的交互作用。实证结果表明，基于气候或文本数据的单模态模型优于仅依赖结构化数据的模型，而多模态数据整合能显著提升信用违约预测性能。通过SHAP可解释性方法发现，物理气候风险（特别是雨水内涝）在违约预测中起重要作用。该研究展示了多模态方法在AI决策中的潜力，为信用风险评估提供了更稳健的工具。",
    "fetch_date": "2026-01-06",
    "id": "20260106_25e03aa2"
  },
  {
    "title": "Core-Periphery Dynamics in Market-Conditioned Financial Networks: A Conditional P-Threshold Mutual Information Approach",
    "url": "https://arxiv.org/pdf/2601.00395v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "This study investigates how financial market structure reorganizes during the COVID-19 crash using a conditional p-threshold mutual information (MI) based Minimum Spanning Tree (MST) framework. We analyze nonlinear dependencies among the largest stocks from four diverse QUAD countries: the US, Japan, Australia, and India. Crashes are identified using the Hellinger distance and Hilbert spectrum; a crash occurs when HD = mu\\_H + 2*sigma\\_H, segmenting data into pre-crash, crash, and post-crash periods. Conditional p-threshold MI filters out common market effects and applies permutation-based significance testing. Resulting validated dependencies are used to construct MST networks for comparison across periods. Networks become more integrated during the crash, with shorter path lengths, higher centrality, and lower algebraic connectivity, indicating fragility. Core-periphery structure declines, with increased periphery vulnerability, and disassortative mixing facilitates shock transmission. Post-crash networks show only partial recovery. Aftershock analysis using the Gutenberg-Richter law indicates higher relative frequency of large volatility events following the crash. Results are consistent across all markets, highlighting the conditional p-threshold MI framework for capturing nonlinear interdependencies and systemic vulnerability.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究采用基于条件p阈值互信息的最小生成树框架，分析COVID-19崩盘期间金融市场的结构重组。通过Hellinger距离和Hilbert谱识别崩盘时段，将数据分为崩盘前、崩盘中、崩盘后三期。条件p阈值互信息方法过滤了共同市场效应，并应用基于排列的显著性检验。构建的MST网络显示：崩盘期间网络整合度提高（路径更短、中心性更高、代数连通性更低），核心-边缘结构减弱，边缘脆弱性增加，非同类混合促进了冲击传导。崩盘后网络仅部分恢复。基于古登堡-里克特定律的余震分析表明，崩盘后大型波动事件的相对频率更高。",
    "fetch_date": "2026-01-06",
    "id": "20260106_cba3ac3f"
  },
  {
    "title": "Option Pricing beyond Black-Scholes Model:Quantum Mechanics Approach",
    "url": "https://arxiv.org/pdf/2601.00293v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Based on the analog between the stochastic dynamics and quantum harmonic oscillator, we propose a market force driving model to generalize the Black-Scholes model in finance market. We give new schemes of option pricing, in which we can take various unexpected market behaviors into account to modify the option pricing. As examples, we present several market forces to analyze their effects on the option pricing. These results provide us two practical applications. One is to be used as a new scheme of option pricing when we can predict some hidden market forces or behaviors emerging. The other implies the existence of some risk premium when some unexpected forces emerge.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "基于随机动力学与量子谐振子的类比，提出市场驱动力模型以推广金融市场的Black-Scholes模型。给出新的期权定价方案，可纳入各种意外市场行为修正定价。通过示例分析多种市场力对期权定价的影响。结果提供两个实际应用：一是当预测到某些隐藏市场力或行为出现时，可作为新的期权定价方案；二是暗示意外市场力出现时存在风险溢价。",
    "fetch_date": "2026-01-06",
    "id": "20260106_f02bcbfc"
  },
  {
    "title": "A Global Optimal Theory of Portfolio beyond R-$σ$ Model",
    "url": "https://arxiv.org/pdf/2601.00281v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "The deviation of the efficient market hypothesis (EMH) for the practical economic system allows us gain the arbitrary or risk premium in finance markets. We propose the triplet $(R,H,σ)$ theory to give the local and global optimal portfolio, which eneralize from the $(R,σ)$ model. We present the formulation of the triplet $(R,H,σ)$ model and give the Pareto optimal solution as well as comparing it with the numerical investigations for the Chinese stock market. We define the local optimal weights of the triplet $(\\mathbf{w}_{R},\\mathbf{w}_{H},\\mathbf{w}_σ)$, which constructs the triangle of the quasi-optimal investing subspace such that we further define the centroid of the triangle or the incenter of the triangle as the optimal investing weights, which optimizes the mean return, the arbitrary or risk premium and the volatility risk. By investigating numerically the Chinese stock market as an example we demonstrate the validity of the formulation and obtain the global optimal strategy and quasi-optimal investing subspace. The theory provides an efficient way to design the portfolio for different style investors, conservative or aggressive investors, in finance market to maximize the mean return and arbitrary or risk premium with a small volatility risk.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种超越传统R-σ模型的全局最优投资组合理论，引入了(R,H,σ)三元组模型，其中H代表套利或风险溢价。通过定义局部最优权重(w_R,w_H,w_σ)构建准最优投资子空间三角形，并以三角形重心或内心作为全局最优投资权重，同时优化均值收益、套利/风险溢价和波动风险。论文以中国股市为例进行了数值验证，展示了该理论为保守型或激进型投资者设计投资组合的有效方法。",
    "fetch_date": "2026-01-06",
    "id": "20260106_0fe72c01"
  },
  {
    "title": "Full grid solution for multi-asset options pricing with tensor networks",
    "url": "https://arxiv.org/pdf/2601.00009v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出使用量化张量列车（QTT）方法解决多资产期权定价中的维度灾难问题。传统基于Black-Scholes PDE的全网格求解器受限于维度指数增长，通常只能处理3个标的资产，实践中多依赖蒙特卡洛方法。研究表明，QTT可将d资产Black-Scholes PDE转化为个人计算机可处理的高维问题，构建的算子、收益函数和边界条件表示其秩随d多项式增长、随网格大小多对数增长。论文开发了两种求解器：欧式和美式期权的时间步进算法，以及欧式期权的时空算法。在3-5维相关篮子期权和最大-最小期权上实现了高精度全网格价格和希腊值计算，该方法可扩展至10-15个标的资产的全网格求解。",
    "fetch_date": "2026-01-06",
    "id": "20260106_dd97f056"
  },
  {
    "title": "Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework",
    "url": "https://arxiv.org/pdf/2511.14980v1",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种用于期权定价模型校准的选择性遗忘（机器去学习）框架。当市场报价过时、损坏或需要删除时，现有校准流程通常需要完全重新训练模型。本文引入了一种基于算子理论的高斯-牛顿方法，允许仅移除部分数据而不必重建整个非线性最小二乘问题，并提供了稳定性保证和扰动边界。",
    "fetch_date": "2026-01-06",
    "id": "20260106_05ad2997"
  },
  {
    "title": "The Hidden Constant of Market Rhythms: How $1-1/e$ Defines Scaling in Intrinsic Time",
    "url": "https://arxiv.org/pdf/2511.14408v1",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "Directional-change Intrinsic Time analysis has long revealed scaling laws in market microstructure, but the origin of their stability remains elusive. This article presents evidence that Intrinsic Time can be modeled as a memoryless exponential hazard process. Empirically, the proportion of directional changes to total events stabilizes near $1 - 1/e = 0.632$, matching the probability that a Poisson process completes one mean interval. This constant provides a natural heuristic to identify scaling regimes across thresholds and supports an interpretation of market activity as a renewal process in intrinsic time.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文通过实证研究发现，市场内在时间中的方向性变化与总事件的比例稳定在$1-1/e≈0.632$附近，这与泊松过程完成一个平均区间的概率相匹配。该常数揭示了市场微观结构中的标度律稳定性，支持将市场活动解释为内在时间中的更新过程，为识别不同阈值下的标度机制提供了自然启发式方法。",
    "fetch_date": "2026-01-06",
    "id": "20260106_ccb77cc5"
  },
  {
    "title": "Scaling Conditional Autoencoders for Portfolio Optimization via Uncertainty-Aware Factor Selection",
    "url": "https://arxiv.org/pdf/2511.17462v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "Conditional Autoencoders (CAEs) offer a flexible, interpretable approach for estimating latent asset-pricing factors from firm characteristics. However, existing studies usually limit the latent factor dimension to around K=5 due to concerns that larger K can degrade performance. To overcome this challenge, we propose a scalable framework that couples a high-dimensional CAE with an uncertainty-aware factor selection procedure. We employ three models for quantile prediction: zero-shot Chronos, a pretrained time-series foundation model (ZS-Chronos), gradient-boosted quantile regression trees using XGBoost and RAPIDS (Q-Boost), and an I.I.D bootstrap-based sample mean model (IID-BS). For each model, we rank factors by forecast uncertainty and retain the top-k most predictable factors for portfolio construction, where k denotes the selected subset of factors. This pruning strategy delivers substantial gains in risk-adjusted performance across all forecasting models. Furthermore, due to each model's uncorrelated predictions, a performance-weighted ensemble consistently outperforms individual models with higher Sharpe, Sortino, and Omega ratios.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种可扩展的框架，将高维条件自编码器（CAE）与不确定性感知因子选择程序相结合，用于投资组合优化。通过使用三种分位数预测模型（零样本Chronos、基于XGBoost和RAPIDS的梯度提升分位数回归树、基于I.I.D自举的样本均值模型），根据预测不确定性对因子进行排序，并保留前k个最可预测的因子进行投资组合构建。这种剪枝策略在所有预测模型中均显著提升了风险调整后的绩效。此外，由于各模型的预测不相关，性能加权集成模型在夏普比率、索提诺比率和欧米茄比率方面持续优于单个模型。",
    "fetch_date": "2026-01-05",
    "id": "20260105_13b5c54e"
  },
  {
    "title": "Statistical Arbitrage in Polish Equities Market Using Deep Learning Techniques",
    "url": "https://arxiv.org/pdf/2512.02037v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "We study a systematic approach to a popular Statistical Arbitrage technique: Pairs Trading. Instead of relying on two highly correlated assets, we replace the second asset with a replication of the first using risk factor representations. These factors are obtained through Principal Components Analysis (PCA), exchange traded funds (ETFs), and, as our main contribution, Long Short Term Memory networks (LSTMs). Residuals between the main asset and its replication are examined for mean reversion properties, and trading signals are generated for sufficiently fast mean reverting portfolios.\n  Beyond introducing a deep learning based replication method, we adapt the framework of Avellaneda and Lee (2008) to the Polish market. Accordingly, components of WIG20, mWIG40, and selected sector indices replace the original S&P500 universe, and market parameters such as the risk free rate and transaction costs are updated to reflect local conditions.\n  We outline the full strategy pipeline: risk factor construction, residual modeling via the Ornstein Uhlenbeck process, and signal generation. Each replication technique is described together with its practical implementation. Strategy performance is evaluated over two periods: 2017-2019 and the recessive year 2020.\n  All methods yield profits in 2017-2019, with PCA achieving roughly 20 percent cumulative return and an annualized Sharpe ratio of up to 2.63. Despite multiple adaptations, our conclusions remain consistent with those of the original paper. During the COVID-19 recession, only the ETF based approach remains profitable (about 5 percent annual return), while PCA and LSTM methods underperform. LSTM results, although negative, are promising and indicate potential for future optimization.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于深度学习的统计套利策略，应用于波兰股市。核心创新在于：使用长短期记忆网络（LSTM）等风险因子（包括PCA和ETF）复制目标资产，替代传统配对交易中的高相关性资产；通过Ornstein-Uhlenbeck过程建模残差的均值回归特性，生成交易信号。研究将Avellaneda和Lee（2008）的框架适配至波兰市场（覆盖WIG20、mWIG40及行业指数），并更新了无风险利率和交易成本等本地参数。论文详细描述了从风险因子构建到信号生成的完整策略流程，强调了LSTM在资产复制中的实践应用，对实战交易具有较高的参考价值。",
    "fetch_date": "2026-01-05",
    "id": "20260105_cb8264be"
  },
  {
    "title": "Integration of LSTM Networks in Random Forest Algorithms for Stock Market Trading Predictions",
    "url": "https://arxiv.org/pdf/2512.02036v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "The aim of this paper is the analysis and selection of stock trading systems that combine different models with data of different nature, such as financial and microeconomic information. Specifically, based on previous work by the authors and applying advanced techniques of Machine Learning and Deep Learning, our objective is to formulate trading algorithms for the stock market with empirically tested statistical advantages, thus improving results published in the literature. Our approach integrates Long Short-Term Memory (LSTM) networks with algorithms based on decision trees, such as Random Forest and Gradient Boosting. While the former analyze price patterns of financial assets, the latter are fed with economic data of companies. Numerical simulations of algorithmic trading with data from international companies and 10-weekday predictions confirm that an approach based on both fundamental and technical variables can outperform the usual approaches, which do not combine those two types of variables. In doing so, Random Forest turned out to be the best performer among the decision trees. We also discuss how the prediction performance of such a hybrid approach can be boosted by selecting the technical variables.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文旨在通过整合不同性质的数据（如金融和微观经济信息）来分析和选择股票交易系统。具体方法是将长短期记忆（LSTM）网络与基于决策树的算法（如随机森林和梯度提升）相结合。LSTM用于分析金融资产的价格模式，而决策树算法则输入公司的经济数据。通过使用国际公司数据进行数值模拟和10个交易日的预测，结果表明，结合基本面和技术面变量的方法优于不结合这两类变量的常规方法，其中随机森林在决策树中表现最佳。论文还讨论了如何通过选择增强这种混合方法的预测性能。",
    "fetch_date": "2026-01-05",
    "id": "20260105_e66f400c"
  },
  {
    "title": "Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution",
    "url": "https://arxiv.org/pdf/2511.15262v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究探讨了强化学习在最优执行大宗订单中的应用，旨在逐步执行大额订单的同时，在较长时间内最小化执行差额和市场冲击。与传统参数化价格动态和冲击建模方法不同，本研究采用无模型、数据驱动的框架。由于策略优化需要历史数据无法提供的反事实反馈，我们采用队列反应模型来生成真实且可处理的限价订单簿模拟，涵盖瞬时价格冲击以及非线性、动态的订单流响应。在方法上，我们在包含时间、库存、价格和深度变量的状态空间上训练了一个双深度Q网络智能体，并评估其相对于现有基准的性能。数值模拟结果表明，该智能体学习到的策略兼具战略性和战术性，能有效适应订单簿条件，并在多种训练配置下优于标准方法。这些发现有力地证明，无模型强化学习能够为最优执行问题提供自适应且稳健的解决方案。",
    "fetch_date": "2026-01-05",
    "id": "20260105_8157885d"
  },
  {
    "title": "Financial Information Theory",
    "url": "https://arxiv.org/pdf/2511.16339v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This paper introduces a comprehensive framework for Financial Information Theory by applying information-theoretic concepts such as entropy, Kullback-Leibler divergence, mutual information, normalized mutual information, and transfer entropy to financial time series. We systematically derive these measures with complete mathematical proofs, establish their theoretical properties, and propose practical algorithms for estimation. Using S&P 500 data from 2000 to 2025, we demonstrate empirical usefulness for regime detection, market efficiency testing, and portfolio construction. We show that normalized mutual information (NMI) behaves as a powerful, bounded, and interpretable measure of temporal dependence, highlighting periods of structural change such as the 2008 financial crisis and the COVID-19 shock. Our entropy-adjusted Value at Risk, information-theoretic diversification criterion, and NMI-based market efficiency test provide actionable tools for risk management and asset allocation. We interpret NMI as a quantitative diagnostic of the Efficient Market Hypothesis and demonstrate that information-theoretic methods offer superior regime detection compared to traditional autocorrelation- or volatility-based approaches. All theoretical results include rigorous proofs, and empirical findings are validated across multiple market regimes spanning 25 years of daily returns.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出金融信息理论的综合框架，将信息论概念（如熵、Kullback-Leibler散度、互信息、归一化互信息、传递熵）应用于金融时间序列。通过系统推导这些度量（含完整数学证明），建立理论性质并提出实用估计算法。基于2000-2025年标普500数据的实证分析显示，该方法在机制检测、市场效率检验和投资组合构建中具有实用价值：归一化互信息（NMI）作为有界可解释的时序依赖性度量，能有效识别结构变化期（如2008年金融危机和COVID-19冲击）；熵调整VaR、信息论分散化准则及NMI市场效率检验为风险管理和资产配置提供可操作工具。研究将NMI解释为有效市场假说的定量诊断指标，并证明信息论方法在机制检测上优于传统自相关方法。",
    "fetch_date": "2026-01-05",
    "id": "20260105_36c1b600"
  },
  {
    "title": "Corporate Earnings Calls and Analyst Beliefs",
    "url": "https://arxiv.org/pdf/2511.15214v2",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Economic behavior is shaped not only by quantitative information but also by the narratives through which such information is communicated and interpreted (Shiller, 2017). I show that narratives extracted from earnings calls significantly improve the prediction of both realized earnings and analyst expectations. To uncover the underlying mechanisms, I introduce a novel text-morphing methodology in which large language models generate counterfactual transcripts that systematically vary topical emphasis (the prevailing narrative) while holding quantitative content fixed. This framework allows me to precisely measure how analysts under- and over-react to specific narrative dimensions. The results reveal systematic biases: analysts over-react to sentiment (optimism) and under-react to narratives of risk and uncertainty. Overall, the analysis offers a granular perspective on the mechanisms of expectation formation through the competing narratives embedded in corporate communication.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过文本变形方法，利用大型语言模型生成反事实收益电话会议记录，系统性地改变主题重点（主流叙事），同时保持定量内容不变。研究发现分析师对情绪（乐观）反应过度，而对风险和不确定性叙事反应不足。这些系统性偏差的识别为量化交易提供了可操作的Alpha信号，可通过文本分析预测分析师预期修正和实际收益，具有直接的实战应用价值。",
    "fetch_date": "2026-01-05",
    "id": "20260105_fc5bbd62"
  },
  {
    "title": "Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems",
    "url": "https://arxiv.org/pdf/2511.16657v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种针对欧元-美元货币对在高频外汇市场环境中实施的高级人工智能算法交易系统。其方法论核心在于整合一套全面的输入特征集：包括从欧元区和美国收集的关键宏观经济基本面变量（例如国内生产总值和失业率），以及一套全面的技术变量（包括指标、振荡器、斐波那契水平和价格背离）。通过标准机器学习指标量化预测准确性，并利用历史数据进行回测模拟以评估交易盈利能力和风险，从而评估所得算法的性能。研究最后通过比较分析来确定哪一类输入特征（基本面或技术面）能为生成盈利交易信号提供更强、更可靠的预测能力。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f79ba632"
  },
  {
    "title": "Probability Weighting Meets Heavy Tails: An Econometric Framework for Behavioral Asset Pricing",
    "url": "https://arxiv.org/pdf/2511.16563v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "We develop an econometric framework integrating heavy-tailed Student's $t$ distributions with behavioral probability weighting while preserving infinite divisibility. Using 432{,}752 observations across 86 assets (2004--2024), we demonstrate Student's $t$ specifications outperform Gaussian models in 88.4\\% of cases. Bounded probability-weighting transformations preserve mathematical properties required for dynamic pricing. Gaussian models underestimate 99\\% Value-at-Risk by 19.7\\% versus 3.2\\% for our specification. Joint estimation procedures identify tail and behavioral parameters with established asymptotic properties. Results provide robust inference for asset-pricing applications where heavy tails and behavioral distortions coexist.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "我们开发了一个计量经济学框架，将重尾的Student's t分布与行为概率加权相结合，同时保持无限可分性。使用86种资产（2004-2024年）的432,752个观测值，我们证明Student's t模型在88.4%的情况下优于高斯模型。有界概率加权变换保留了动态定价所需的数学性质。高斯模型低估了99%的风险价值（VaR）19.7%，而我们的模型仅低估3.2%。联合估计程序可识别尾部参数和行为参数，并具有已建立的渐近性质。结果为重尾和行为扭曲共存的资产定价应用提供了稳健的推断。",
    "fetch_date": "2026-01-05",
    "id": "20260105_bac290b9"
  },
  {
    "title": "Quantitative Geometric Market Structuralism: A Framework for Detecting Structural Endpoints in Financial Markets",
    "url": "https://arxiv.org/pdf/2511.16319v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This study introduces the Quantitative Geometric Market Structuralist (QGMS) framework a hybrid analytical methodology integrating geometric pattern recognition with quantitative mathematical modeling to identify terminal zones of large-scale market movements. Unlike conventional econometric or signal-based models, the QGMS framework conceptualizes market dynamics as evolving geometric structures governed by self-organizing principles of price formation.\n  To preserve the proprietary nature of its internal mathematical architecture, the methodology employs a blind-testing validation process, wherein price, symbol, and temporal identifiers are concealed during analysis. This design ensures objective verification without revealing the underlying algorithmic core. The frameworks predictive robustness has been empirically examined across multiple financial crises, including the 2008 Global Financial Collapse, the 2015 EUR CHF SNB event, the 2016 Brexit referendum, and the 2020 COVID-19 market crash. In each case, the system consistently identified structural endpoints preceding major market reversals.\n  The findings suggest that geometric quantitative market interpretation may offer a new class of predictive tools bridging the gap between mathematical formalism and empirical price behavior. By combining academic testability with intellectual property protection, the QGMS framework establishes a viable foundation for institutional evaluation and further research into nonlinear structural forecasting models.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究提出定量几何市场结构主义（QGMS）框架，这是一种混合分析方法，将几何模式识别与定量数学模型相结合，用于识别大规模市场运动的终端区域。与传统计量经济学或基于信号的模型不同，QGMS框架将市场动态概念化为由价格形成的自组织原则支配的演化几何结构。为保护其内部数学架构的专有性质，该方法采用盲测验证过程，在分析过程中隐藏价格、符号和时间标识符。该设计确保客观验证而不揭示底层算法核心。该框架的预测稳健性已在多次金融危机中进行了实证检验，包括2008年全球金融危机、2015年欧元/瑞郎瑞士央行事件、2016年英国脱欧公投和2020年COVID-19市场崩盘。在每种情况下，该系统始终识别出主要市场反转前的结构端点。研究结果表明，几何定量市场解释可能提供一种新的市场分析方法。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f8fb4b4e"
  },
  {
    "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining",
    "url": "https://arxiv.org/pdf/2511.15456v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "随着去中心化金融（DeFi）的发展，理解DeFi交易背后的用户意图至关重要，但由于复杂的智能合约交互、多方面的链上/链下因素以及不透明的十六进制日志，这变得极具挑战性。现有方法缺乏深入的语义洞察。为此，我们提出了交易意图挖掘（TIM）框架。TIM利用基于扎根理论构建的DeFi意图分类法和多智能体大型语言模型（LLM）系统，稳健地推断用户意图。元级规划器动态协调领域专家，将多视角特定的意图分析分解为可解决的子任务。问题求解器处理多模态链上/链下数据的任务。认知评估器则减轻LLM幻觉并确保可验证性。实验表明，TIM显著优于机器学习模型、单一LLM和单一智能体基线。我们还分析了意图推断中的核心挑战。这项工作有助于更可靠地理解DeFi中的用户动机，为复杂的区块链活动提供情境感知的解释。",
    "fetch_date": "2026-01-05",
    "id": "20260105_0a511ca3"
  },
  {
    "title": "Law-Strength Frontiers and a No-Free-Lunch Result for Law-Seeking Reinforcement Learning on Volatility Law Manifolds",
    "url": "https://arxiv.org/pdf/2511.17304v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "We study reinforcement learning (RL) on volatility surfaces through the lens of Scientific AI. We ask whether axiomatic no-arbitrage laws, imposed as soft penalties on a learned world model, can reliably align high-capacity RL agents, or mainly create Goodhart-style incentives to exploit model errors. From classical static no-arbitrage conditions we build a finite-dimensional convex volatility law manifold of admissible total-variance surfaces, together with a metric law-penalty functional and a Graceful Failure Index (GFI) that normalizes law degradation under shocks. A synthetic generator produces law-consistent trajectories, while a recurrent neural world model trained without law regularization exhibits structured off-manifold errors. On this testbed we define a Goodhart decomposition \\(r = r^{\\mathcal{M}} + r^\\perp\\), where \\(r^\\perp\\) is ghost arbitrage from off-manifold prediction error. We prove a ghost-arbitrage incentive theorem for PPO-type agents, a law-strength trade-off theorem showing that stronger penalties eventually worsen P\\&L, and a no-free-lunch theorem: under a law-consistent world model and law-aligned strategy class, unconstrained law-seeking RL cannot Pareto-dominate structural baselines on P\\&L, penalties, and GFI. In experiments on an SPX/VIX-like world model, simple structural strategies form the empirical law-strength frontier, while all law-seeking RL variants underperform and move into high-penalty, high-GFI regions. Volatility thus provides a concrete case where reward shaping with verifiable penalties is insufficient for robust law alignment.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文从科学AI的视角研究波动率曲面上的强化学习。核心探讨了将无套利公理作为软惩罚施加于学习的世界模型时，能否可靠地引导高容量RL智能体，还是主要产生Goodhart式激励以利用模型误差。作者构建了有限维凸波动率定律流形，定义了度量定律惩罚函数和优雅失效指数，并证明了三个定理：PPO类智能体的幽灵套利激励定理、定律强度权衡定理（更强惩罚最终会恶化盈亏），以及一个无免费午餐定理——在定律一致的世界模型和定律对齐的策略类下，无约束的定律寻求RL无法实现帕累托改进。论文主要贡献在于理论框架和定理证明，而非具体的实战交易策略或算法实现。",
    "fetch_date": "2026-01-05",
    "id": "20260105_48cdc0b0"
  },
  {
    "title": "Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building",
    "url": "https://arxiv.org/pdf/2511.17654v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Conflict resolution and consensus building represent critical challenges in multi-agent systems, negotiations, and collaborative decision-making processes. This paper introduces Dialogue Diplomats, a novel end-to-end multi-agent reinforcement learning (MARL) framework designed for automated conflict resolution and consensus building in complex, dynamic environments. The proposed system integrates advanced deep reinforcement learning architectures with dialogue-based negotiation protocols, enabling autonomous agents to engage in sophisticated conflict resolution through iterative communication and strategic adaptation. We present three primary contributions: first, a novel Hierarchical Consensus Network (HCN) architecture that combines attention mechanisms with graph neural networks to model inter-agent dependencies and conflict dynamics. second, a Progressive Negotiation Protocol (PNP) that structures multi-round dialogue interactions with adaptive concession strategies; and third, a Context-Aware Reward Shaping mechanism that balances individual agent objectives with collective consensus goals.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "《对话外交官：一种用于自动冲突解决与共识构建的端到端多智能体强化学习系统》提出了一种新颖的端到端多智能体强化学习框架，旨在解决复杂动态环境中的冲突与共识构建问题。该系统整合了先进的深度强化学习架构与基于对话的协商协议，使自主智能体能够通过迭代通信和策略适应进行复杂的冲突解决。主要贡献包括：1）结合注意力机制与图神经网络以建模智能体间依赖关系和冲突动态的层次化共识网络架构；2）采用自适应让步策略构建多轮对话交互的渐进式协商协议；3）平衡个体智能体目标与集体共识目标的上下文感知奖励塑造机制。",
    "fetch_date": "2026-01-05",
    "id": "20260105_d98d9b99"
  },
  {
    "title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense",
    "url": "https://arxiv.org/pdf/2511.16483v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在复杂动态环境中为自主网络攻防学习智能体设计奖励函数对领域专家而言具有挑战性。本研究提出基于大语言模型（LLM）的奖励设计方法，在深度强化学习（DRL）驱动的实验仿真环境中生成自主网络防御策略。通过构建反映智能体行为异质性的多类攻防角色，首先生成包含网络仿真环境上下文信息的LLM引导奖励设计方案，随后在DRL驱动的攻防仿真环境中利用这些奖励结构学习网络防御策略集合。结果表明，LLM引导的奖励设计能够针对多样化对抗行为生成有效防御策略。",
    "fetch_date": "2026-01-05",
    "id": "20260105_039df9ff"
  },
  {
    "title": "Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements",
    "url": "https://arxiv.org/pdf/2511.15960v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "二元期权交易常被宣传为预测模型可产生稳定利润的领域，但二元期权的内在随机性和随机性质使价格走势高度不可预测，给任何预测方法带来重大挑战。本研究证明，机器学习算法在预测二元期权走势方面难以超越简单基线。使用2021年至2023年EUR/USD货币对数据集，我们测试了多种模型，包括随机森林、逻辑回归、梯度提升和k-最近邻（kNN），均在超参数优化前后进行。此外，评估了多种神经网络架构，包括多层感知器（MLP）和长短期记忆（LSTM）网络在不同训练条件下。尽管这些努力详尽，但所有模型均未超越ZeroR基线准确率，突显了二元期权的内在随机性。这些发现强化了二元期权缺乏可预测模式的观点，使其不适合基于机器学习的预测。",
    "fetch_date": "2026-01-05",
    "id": "20260105_2fb789d3"
  },
  {
    "title": "Anonymization and Information Loss",
    "url": "https://arxiv.org/pdf/2511.15364v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "We show that while anonymization effectively obscures firm identity, it significantly reduces the power of textual understanding, thereby diminishing models' ability to extract meaningful economic signals from financial texts. This information loss is particularly severe when numerical and object entities are removed from texts and is amplified in texts characterized by high linguistic uncertainty and firm specificity. Importantly, in the setting of sentiment extraction from earnings call transcripts, we find that information loss induced by anonymization is more pervasive and severe than the effects of look-ahead bias, suggesting that the costs of anonymization may outweigh its benefits in certain financial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了金融文本匿名化处理导致的信息损失问题。研究表明，虽然匿名化能有效隐藏公司身份，但会显著削弱文本理解能力，降低模型从金融文本中提取有效经济信号的能力。这种信息损失在移除文本中的数值和实体对象时尤为严重，且在语言不确定性高、公司特异性强的文本中会被放大。特别在盈利电话会议记录的情感提取场景中，匿名化造成的信息损失比前瞻性偏差的影响更普遍且严重，表明在某些金融应用中，匿名化的成本可能超过其收益。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f31b728b"
  },
  {
    "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.17963v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文提出了一种融合长短期记忆网络预测与近端策略优化强化学习的混合框架，用于动态投资组合优化。该框架利用深度循环网络捕捉时序依赖性的预测能力，同时PPO智能体在连续动作空间中自适应调整资产配置，使系统既能预测趋势又能动态适应市场变化。基于2018年1月至2024年12月涵盖美国与印尼股票、美国国债及主要加密货币的多资产数据集，模型在考虑交易成本后，通过年化收益率、波动率、夏普比率和最大回撤等指标进行评估。结果显示，该混合架构在非平稳市场环境下实现了更高收益和更强韧性，展现了其作为稳健AI驱动交易策略的潜力。",
    "fetch_date": "2026-01-04",
    "id": "20260104_05180ddb"
  },
  {
    "title": "Re(Visiting) Time Series Foundation Models in Finance",
    "url": "https://arxiv.org/pdf/2511.18578v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Financial time series forecasting is central to trading, portfolio optimization, and risk management, yet it remains challenging due to noisy, non-stationary, and heterogeneous data. Recent advances in time series foundation models (TSFMs), inspired by large language models, offer a new paradigm for learning generalizable temporal representations from large and diverse datasets. This paper presents the first comprehensive empirical study of TSFMs in global financial markets. Using a large-scale dataset of daily excess returns across diverse markets, we evaluate zero-shot inference, fine-tuning, and pre-training from scratch against strong benchmark models. We find that off-the-shelf pre-trained TSFMs perform poorly in zero-shot and fine-tuning settings, whereas models pre-trained from scratch on financial data achieve substantial forecasting and economic improvements, underscoring the value of domain-specific adaptation. Increasing the dataset size, incorporating synthetic data augmentation, and applying hyperparameter tuning further enhance performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "金融时间序列预测是交易、投资组合优化和风险管理的核心，但由于数据噪声大、非平稳且异质性强，该任务极具挑战。受大语言模型启发，时间序列基础模型（TSFMs）的最新进展为从大规模多样化数据集中学习可泛化的时序表征提供了新范式。本文首次对TSFMs在全球金融市场中的应用进行了全面的实证研究。基于涵盖多个市场的日超额收益大规模数据集，我们评估了零样本推理、微调及从零开始预训练模型与强基准模型的性能。研究发现，现成的预训练TSFMs在零样本和微调设置下表现不佳，而在金融数据上从零开始预训练的模型则实现了显著的预测和经济收益提升，强调了领域特定适应的重要性。增加数据集规模、引入合成数据增强以及应用超参数调优可进一步提升模型性能。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d10039fd"
  },
  {
    "title": "Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons",
    "url": "https://arxiv.org/pdf/2511.18076v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究提出了一种增强的G-Learning算法，结合GIRL算法进行参数优化，用于投资组合优化。目标是在目标日期前最大化投资组合价值，同时最小化投资者的定期贡献。模型在高度波动的市场中运行，通过多元化投资组合确保低风险水平，并利用强化学习动态调整持仓。结果显示，夏普比率从0.42提升至0.483，在高度波动的多元化投资组合市场中表现显著。GIRL与G-Learning的对比表明，虽然GIRL优化了奖励函数参数，但对投资组合性能的影响有限，说明强化学习方法如G-Learning已能实现稳健优化。",
    "fetch_date": "2026-01-04",
    "id": "20260104_429bc0cd"
  },
  {
    "title": "Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints",
    "url": "https://arxiv.org/pdf/2511.17892v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们开发了一个基于Heath-Jarrow-Morton（HJM）期限结构模型和动态Nelson-Siegel参数化远期利率的无套利深度学习框架，用于收益率曲线和债券价格预测。该方法通过将卡尔曼滤波器、扩展卡尔曼滤波器、粒子滤波器与循环神经网络（LSTM/CLSTM）相结合，将无套利漂移限制嵌入神经状态空间架构，并在训练中引入显式套利误差正则化（AER）项。模型应用于美国国债和公司债券数据，评估其在1天和5天期限的收益率空间和价格空间预测性能。实证表明，套利正则化在短期期限（尤其是5天预测）中带来最显著的改进，提高了以买卖价差命中率衡量的市场一致性，并降低了美元计价的预测误差。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d5fc23a3"
  },
  {
    "title": "Partial multivariate transformer as a tool for cryptocurrencies time series prediction",
    "url": "https://arxiv.org/pdf/2512.04099v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "Forecasting cryptocurrency prices is hindered by extreme volatility and a methodological dilemma between information-scarce univariate models and noise-prone full-multivariate models. This paper investigates a partial-multivariate approach to balance this trade-off, hypothesizing that a strategic subset of features offers superior predictive power. We apply the Partial-Multivariate Transformer (PMformer) to forecast daily returns for BTCUSDT and ETHUSDT, benchmarking it against eleven classical and deep learning models. Our empirical results yield two primary contributions. First, we demonstrate that the partial-multivariate strategy achieves significant statistical accuracy, effectively balancing informative signals with noise. Second, we experiment and discuss an observable disconnect between this statistical performance and practical trading utility; lower prediction error did not consistently translate to higher financial returns in simulations. This finding challenges the reliance on traditional error metrics and highlights the need to develop evaluation criteria more aligned with real-world financial objectives.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "预测加密货币价格面临极端波动性和方法论困境：信息稀缺的单变量模型与噪声充斥的全多变量模型之间的权衡。本文研究了一种部分多变量方法以平衡这一权衡，假设策略性特征子集能提供更优的预测能力。我们应用部分多变量Transformer（PMformer）预测BTCUSDT和ETHUSDT的日收益率，并以十一种经典和深度学习模型为基准。实证结果有两个主要贡献：首先，我们证明部分多变量策略实现了显著的统计准确性，有效平衡了信息信号与噪声；其次，我们实验并讨论了这种统计性能与实际交易效用之间的可观察脱节——较低的预测误差在模拟中并未一致转化为更高的财务回报。这一发现挑战了对传统误差指标的依赖，并强调需要开发更符合现实世界金融目标的评估标准。",
    "fetch_date": "2026-01-04",
    "id": "20260104_210cbaf1"
  },
  {
    "title": "Optimal dividend and capital injection under self-exciting claims",
    "url": "https://arxiv.org/pdf/2511.19701v1",
    "source": "ArXiv",
    "date": "2025-11-24",
    "abstract": "In this paper, we study an optimal dividend and capital-injection problem in a Cramér--Lundberg model where claim arrivals follow a Hawkes process, capturing clustering effects often observed in insurance portfolios. We establish key analytical properties of the value function and characterise the optimal capital-injection strategy through an explicit threshold. We also show that the value function is the unique viscosity solution of the associated HJB variational inequality. For numerical purposes, we first compute a benchmark solution via a monotone finite-difference scheme with Howard's policy iteration. We then develop a reinforcement learning approach based on policy-gradient and actor-critic methods. The learned strategies closely match the PDE benchmark and remain stable across initial conditions. The results highlight the relevance of policy-gradient techniques for dividend optimisation under self-exciting claim dynamics and point toward scalable methods for higher-dimensional extensions.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究了一个最优分红与资本注入问题，采用Cramér–Lundberg模型，其中索赔到达遵循Hawkes过程，以捕捉保险组合中常见的聚集效应。论文建立了价值函数的关键解析性质，并通过显式阈值刻画了最优资本注入策略，证明价值函数是相关HJB变分不等式的唯一粘性解。数值计算方面，首先通过带Howard策略迭代的单调有限差分法获得基准解，随后开发了基于策略梯度和演员-评论家方法的强化学习算法。学习得到的策略与PDE基准高度吻合，且在不同初始条件下保持稳定。结果表明策略梯度技术在处理自激励索赔动态下的分红优化问题中具有适用性，并为高维扩展提供了可扩展的方法。",
    "fetch_date": "2026-01-04",
    "id": "20260104_b78c6b9f"
  },
  {
    "title": "A calibrated model of debt recycling with interest costs and tax shields: viability under different fiscal regimes and jurisdictions",
    "url": "https://arxiv.org/pdf/2511.18614v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Debt recycling is a leveraged equity management strategy in which homeowners use accumulated home equity to finance investments, applying the resulting returns to accelerate mortgage repayment. We propose a novel framework to model equity and mortgage dynamics in presence of mortgage interest rates, borrowing costs on equity-backed credit lines, and tax shields arising from interest deductibility. The model is calibrated on three jurisdictions -- Australia, Germany, and Switzerland -- representing diverse interest rate environments and fiscal regimes. Results demonstrate that introducing positive interest rates without tax shields contracts success regions and lengthens repayment times, while tax shields partially reverse these effects by reducing effective borrowing costs and adding equity boosts from mortgage interest deductibility. Country-specific outcomes vary systematically, and rental properties consistently outperform owner-occupied housing due to mortgage interest deductibility provisions.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "债务回收是一种杠杆股权管理策略，房主利用累积的房屋净值融资投资，并将所得回报用于加速抵押贷款还款。本文提出了一个新颖框架，在抵押贷款利率、股权支持信贷额度的借款成本以及利息可抵扣性产生的税收盾的背景下，模拟股权和抵押贷款的动态变化。该模型在澳大利亚、德国和瑞士三个司法管辖区进行了校准，代表了不同的利率环境和财政制度。结果表明，在没有税收盾的情况下引入正利率会缩小成功区域并延长还款时间，而税收盾通过降低有效借款成本和增加抵押贷款利息可抵扣性带来的股权提升，部分逆转了这些影响。特定国家的结果存在系统性差异，由于抵押贷款利息可抵扣性条款，租赁房产的表现始终优于自住住房。",
    "fetch_date": "2026-01-04",
    "id": "20260104_38111ddb"
  },
  {
    "title": "Limit Order Book Dynamics in Matching Markets: Microstructure, Spread, and Execution Slippage",
    "url": "https://arxiv.org/pdf/2511.20606v2",
    "source": "ArXiv",
    "date": "2025-11-25",
    "abstract": "Conventional models of matching markets assume that monetary transfers can clear markets by compensating for utility differentials. However, empirical patterns show that such transfers often fail to close structural preference gaps. This paper introduces a market microstructure framework that models matching decisions as a limit order book system with rigid bid ask spreads. Individual preferences are represented by a latent preference state matrix, where the spread between an agent's internal ask price (the unconditional maximum) and the market's best bid (the reachable maximum) creates a structural liquidity constraint. We establish a Threshold Impossibility Theorem showing that linear compensation cannot close these spreads unless it induces a categorical identity shift. A dynamic discrete choice execution model further demonstrates that matches occur only when the market to book ratio crosses a time decaying liquidity threshold, analogous to order execution under inventory pressure. Numerical experiments validate persistent slippage, regional invariance of preference orderings, and high tier zero spread executions. The model provides a unified microstructure explanation for matching failures, compensation inefficiency, and post match regret in illiquid order driven environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文《匹配市场中的限价订单簿动态：微观结构、价差与执行滑点》提出了一种市场微观结构框架，将匹配决策建模为具有刚性买卖价差的限价订单簿系统。个体偏好由潜在偏好状态矩阵表示，其中代理内部要价（无条件最大值）与市场最佳出价（可达最大值）之间的价差构成了结构性流动性约束。阈值不可能定理表明，线性补偿无法消除这些价差，除非引发分类身份转变。动态离散选择执行模型进一步证明，只有当市场与订单簿比率超过随时间衰减的流动性阈值时，匹配才会发生，类似于库存压力下的订单执行。数值实验验证了持续的滑点、偏好排序的区域不变性以及高层级零价差执行。该模型为匹配市场的微观结构提供了统一解释。",
    "fetch_date": "2026-01-04",
    "id": "20260104_190db185"
  },
  {
    "title": "Carbon-Penalised Portfolio Insurance Strategies in a Stochastic Factor Model with Partial Information",
    "url": "https://arxiv.org/pdf/2511.19186v1",
    "source": "ArXiv",
    "date": "2025-11-24",
    "abstract": "Given the increasing importance of environmental, social and governance (ESG) factors, particularly carbon emissions, we investigate optimal proportional portfolio insurance (PPI) strategies accounting for carbon footprint reduction. PPI strategies enable investors to mitigate downside risk while retaining the potential for upside gains. This paper aims to determine the multiplier of the PPI strategy to maximise the expected utility of the terminal cushion, where the terminal cushion is penalised proportionally to the realised volatility of stocks issued by firms operating in carbon-intensive sectors. We model the risky assets' dynamics using geometric Brownian motions whose drift rates are modulated by an unobservable common stochastic factor to capture market-specific or economy-wide state variables that are typically not directly observable. Using classical stochastic filtering theory, we formulate a suitable optimization problem and solve it for CRRA utility function. We characterise optimal carbon penalised PPI strategies and optimal value functions under full and partial information and quantify the loss of utility due incomplete information. Finally, we carry a numerical analysis showing that the proposed strategy reduces carbon emission intensity without compromising financial performance.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "鉴于环境、社会和治理（ESG）因素，特别是碳排放的重要性日益增加，本研究探讨了考虑碳足迹减少的最优比例投资组合保险（PPI）策略。PPI策略使投资者能够减轻下行风险，同时保留上行收益的潜力。本文旨在确定PPI策略的乘数，以最大化终端缓冲的期望效用，其中终端缓冲根据碳密集型行业公司发行的股票的实际波动率按比例惩罚。我们使用几何布朗运动对风险资产的动态进行建模，其漂移率由不可观测的公共随机因子调节，以捕捉通常无法直接观测的市场特定或经济范围内的状态变量。利用经典随机滤波理论，我们构建了一个合适的优化问题，并针对CRRA效用函数进行求解。我们刻画了在完全信息和部分信息下的最优碳惩罚PPI策略和最优价值函数，并量化了由于信息不完全导致的效用损失。最后，我们进行了数值分析。",
    "fetch_date": "2026-01-04",
    "id": "20260104_746f0249"
  },
  {
    "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization",
    "url": "https://arxiv.org/pdf/2512.20623v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出BitRL-Light框架，将1-bit量化大型语言模型（LLMs）与深度Q网络（DQN）强化学习结合，用于边缘设备上的实时智能家居照明控制。该系统在Raspberry Pi硬件上部署1-bit量化的Llama-3.2-1B模型，相比全精度模型实现71.4倍能耗降低，并通过多目标强化学习从用户反馈中学习最优照明策略，平衡能耗、舒适度和昼夜节律对齐。实验结果显示，相比基于规则的系统节能32%，在Raspberry Pi 4上推理延迟低于200ms，用户满意度达95%。系统通过Google Home/IFTTT集成处理自然语言命令，并通过手动覆盖学习隐式反馈。比较分析表明，1-bit模型在ARM处理器上比2-bit方案提速5.07倍，同时保持92%任务准确率。",
    "fetch_date": "2026-01-04",
    "id": "20260104_a67c750a"
  },
  {
    "title": "Superhedging under Proportional Transaction Costs in Continuous Time",
    "url": "https://arxiv.org/pdf/2511.18169v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "We revisit the well-studied superhedging problem under proportional transaction costs in continuous time using the recently developed tools of set-valued stochastic analysis. By relying on a simple Black-Scholes-type market model for mid-prices and using continuous trading schemes, we define a dynamic family of superhedging sets in continuous time and express them in terms of set-valued integrals. We show that these sets, defined as subsets of Lebesgue spaces at different times, form a dynamic set-valued risk measure with multi-portfolio time-consistency. Finally, we transfer the problem formulation to a path-space setting and introduce approximate versions of superhedging sets that will involve relaxing the superhedging inequality, the superhedging probability, and the solvency requirement for the superhedging strategy with a predetermined error level. In this more technical framework, we are able to relate the approximate superhedging sets at different times by means of a set-valued Bellman's principle, which we believe will pave the way for a set-valued differential structure that characterizes the superhedging sets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在连续时间下，利用集合值随机分析工具重新研究了比例交易成本下的超对冲问题。基于简单的Black-Scholes型中间价市场模型和连续交易策略，定义了连续时间下的动态超对冲集合，并用集合值积分表示。研究表明，这些在不同时间作为Lebesgue空间子集定义的集合构成了具有多投资组合时间一致性的动态集合值风险度量。最后，将问题表述转移到路径空间设置，引入了超对冲集合的近似版本，通过预设误差水平放宽超对冲不等式、超对冲概率和超对冲策略的偿付要求。在此技术框架下，通过集合值Bellman原理关联不同时间的近似超对冲集合，为表征超对冲集合的集合值微分结构奠定了基础。",
    "fetch_date": "2026-01-04",
    "id": "20260104_367cade6"
  },
  {
    "title": "Random processes for long-term market simulations",
    "url": "https://arxiv.org/pdf/2511.18125v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "For long term investments, model portfolios are defined at the level of indexes, a setup known as Strategic Asset Allocation (SAA). The possible outcomes at a scale of a few decades can be obtained by Monte Carlo simulations, resulting in a probability density for the possible portfolio values at the investment horizon. Such studies are critical for long term wealth plannings, for example in the financial component of social insurances or in accumulated capital for retirement. The quality of the results depends on two inputs: the process used for the simulations and its parameters. The base model is a constant drift, a constant covariance and normal innovations, as pioneered by Bachelier. Beyond this model, this document presents in details a multivariate process that incorporate the most recent advances in the models for financial time series. This includes the negative correlations of the returns at a scale of a few years, the heteroskedasticity (i.e. the volatility' dynamics), and the fat tails and asymmetry for the distributions of returns. For the parameters, the quantitative outcomes depend critically on the estimate for the drift, because this is a non random contribution acting at each time step. Replacing the point forecast by a probabilistic forecast allows us to analyze the impact of the drift values, and then to incorporate this uncertainty in the Monte Carlo simulations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了用于长期市场模拟的随机过程，核心应用于战略资产配置（SAA）层面的长期投资组合建模。通过蒙特卡洛模拟生成数十年投资期限的可能结果概率分布，适用于养老金、退休储蓄等长期财富规划。论文详细介绍了超越基础模型（恒定漂移、协方差及正态创新）的多变量过程，纳入了金融时间序列的最新进展，包括数年尺度的收益负相关性、异方差性（波动率动态）以及收益分布的厚尾和不对称性。论文强调，模拟结果的定量输出关键取决于漂移参数的估计，因为这是非随机贡献。",
    "fetch_date": "2026-01-04",
    "id": "20260104_9fb61e40"
  },
  {
    "title": "Diffusive Limit of Hawkes Driven Order Book Dynamics With Liquidity Migration",
    "url": "https://arxiv.org/pdf/2511.18117v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This paper develops a theoretical mesoscopic model of the limit order book driven by multivariate Hawkes processes, designed to capture temporal self-excitation and the spatial propagation of order flow across price levels. In contrast to classical zero-intelligence or Poisson based queueing models, the proposed framework introduces mathematically defined migration events between neighbouring price levels, whose intensities are themselves governed by the underlying Hawkes structure. This provides a principled stochastic mechanism for modeling interactions between order arrivals, cancellations, and liquidity movement across adjacent queues.\n  Starting from a microscopic specification of Hawkes driven order flow, we derive a diffusion approximation which yields a reflected mesoscopic stochastic differential equation (SDE) system for queue volumes. The limiting generator is obtained through a Taylor expansion of the microscopic generator, demonstrating how temporal excitation together with spatial migration determine the drift and diffusion structure of the limit order book in the mesoscopic regime. The resulting model extends existing diffusion limits by incorporating correlated excitations and price level to price level liquidity movement within a unified Hawkes based formulation.\n  By establishing this diffusive limit, the paper provides a mathematically consistent bridge between high frequency event based models and macroscopic stochastic descriptions of market microstructure. The work is entirely theoretical and lays a foundation for future analytical and numerical developments without relying on empirical calibration.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种由多元霍克斯过程驱动的限价订单簿理论介观模型，旨在捕捉订单流的时间自激效应及其在价格层级间的空间传播。与经典的零智能或基于泊松的排队模型不同，该框架引入了相邻价格层级间数学定义的迁移事件，其强度由底层霍克斯结构控制，为建模订单到达、取消及相邻队列间流动性移动的相互作用提供了原则性随机机制。从霍克斯驱动订单流的微观设定出发，推导出扩散近似，得到队列量的反射介观随机微分方程系统。通过微观生成元的泰勒展开获得极限生成元，展示了时间激励与空间迁移如何共同决定介观状态下限价订单簿的漂移和扩散结构。该模型通过纳入相关激励和流动性迁移，扩展了现有的扩散极限。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d92a3607"
  },
  {
    "title": "A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization",
    "url": "https://arxiv.org/pdf/2511.18093v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于深度强化学习（DRL）的新型误差时序差分（ETD）算法，旨在解决微电网优化中预测模型不确定性导致控制策略次优的问题。首先，建立了集成可再生能源（RES）和储能系统（ESS）的微电网系统及其马尔可夫决策过程（MDP）模型；其次，提出了一种基于深度Q网络（DQN）的预测控制方法，其中设计了加权平均算法和ETD算法分别量化和处理预测不确定性；最后，基于美国真实数据集的仿真表明，ETD算法有效提升了DRL在优化微电网运行中的性能。",
    "fetch_date": "2026-01-04",
    "id": "20260104_e4bba900"
  },
  {
    "title": "Emergence of Randomness in Temporally Aggregated Financial Tick Sequences",
    "url": "https://arxiv.org/pdf/2511.17479v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "Markets efficiency implies that the stock returns are intrinsically unpredictable, a property that makes markets comparable to random number generators. We present a novel methodology to investigate ultra-high frequency financial data and to evaluate the extent to which tick by tick returns resemble random sequences. We extend the analysis of ultra high-frequency stock market data by applying comprehensive sets of randomness tests, beyond the usual reliance on serial correlation or entropy measures. Our purpose is to extensively analyze the randomness of these data using statistical tests from standard batteries that evaluate different aspects of randomness.\n  We illustrate the effect of time aggregation in transforming highly correlated high-frequency trade data to random streams. More specifically, we use many of the tests in the NIST Statistical Test Suite and in the TestU01 battery (in particular the Rabbit and Alphabit sub-batteries), to prove that the degree of randomness of financial tick data increases together with the increase of the aggregation level in transaction time. Additionally, the comprehensive nature of our tests also uncovers novel patterns, such as non-monotonic behaviors in predictability for certain assets. This study demonstrates a model-free approach for both assessing randomness in financial time series and generating pseudo-random sequences from them, with potential relevance in several applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "市场效率意味着股票回报本质上是不可预测的，这一特性使市场类似于随机数生成器。我们提出了一种新颖的方法来研究超高频金融数据，并评估逐笔回报与随机序列的相似程度。通过应用全面的随机性测试集（超越通常依赖的序列相关性或熵度量），我们扩展了对超高频股票市场数据的分析。我们的目的是使用标准测试套件中的统计测试来广泛分析这些数据的随机性，这些测试评估随机性的不同方面。我们说明了时间聚合在将高度相关的高频交易数据转换为随机流中的作用。具体而言，我们使用NIST统计测试套件和TestU01测试套件（特别是Rabbit和Alphabit子套件）中的许多测试，证明金融逐笔数据的随机性程度随着交易时间聚合水平的增加而增加。此外，我们测试的全面性还揭示了新的模式，例如非单调性。",
    "fetch_date": "2026-01-04",
    "id": "20260104_75507072"
  },
  {
    "title": "A3T-GCN for FTSE100 Components Price Forecasting",
    "url": "https://arxiv.org/pdf/2511.21873v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We examine the predictive power of a novel hybrid A3T-GCN architecture for forecasting closing stock prices of FTSE100 constituents. The dataset comprises 79 companies and 375,329 daily observations from 2007 to 2024, with node features including technical indicators (RSI, MACD), normalized and log returns, and annualized log returns over multiple windows (ALR1W, ALR2W, ALR1M, ALR2M). Graphs are constructed based on sector classifications and correlations of returns or financial ratios. Our results show that the A3T-GCN model using annualized log-returns and shorter sequence lengths improves prediction accuracy while reducing computational requirements. Additionally, longer historical sequences yield only modest improvements, highlighting their importance for longer-term forecasts.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了一种新型混合A3T-GCN架构在预测富时100指数成分股收盘价方面的预测能力。数据集包含2007年至2024年79家公司的375,329个日度观测值，节点特征包括技术指标（RSI、MACD）、归一化和对数收益率，以及多个时间窗口的年化对数收益率（ALR1W、ALR2W、ALR1M、ALR2M）。图结构基于行业分类和收益率或财务比率的相关系数构建。结果表明，使用年化对数收益率和较短序列长度的A3T-GCN模型提高了预测精度，同时降低了计算需求。此外，较长的历史序列仅带来有限的改进，突显了其对长期预测的重要性。",
    "fetch_date": "2026-01-03",
    "id": "20260103_2b03ff62"
  },
  {
    "title": "Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba",
    "url": "https://arxiv.org/pdf/2511.22101v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "The report goes through the main steps of replicating and improving the article \"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.\" The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过复制并改进《在Uniswap V3中使用深度强化学习的自适应流动性提供》一文，提出了一种结合Mamba与DDQN（Dueling Double Deep Q-networks）及新奖励函数的新结构。论文涵盖了从Uniswap Subgraph获取数据、实现细节、结果分析，并引入了两个新基准进行比较。尽管尚未在所有数据集上应用，但新模型在部分测试中表现更优，且具有更强的理论支持。",
    "fetch_date": "2026-01-03",
    "id": "20260103_cb6973a7"
  },
  {
    "title": "LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline",
    "url": "https://arxiv.org/pdf/2512.07867v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文开发了一种基于LLM的透明、可审计的宏观金融压力测试管道，结合结构化提示与可选的国家基本面及新闻检索。该系统为G7国家生成机器可读的宏观经济情景（涵盖GDP增长、通胀及政策利率），并通过基于因子的映射将其转化为投资组合损失，从而支持相对于传统计量经济学基线的在险价值（VaR）和预期短缺（ES）评估。研究表明，LLM能生成连贯且针对特定国家的压力叙事，产生稳定的尾部风险放大效应，且对检索选择的敏感性有限。全面的合理性检查、情景诊断和基于ANOVA的方差分解表明，风险变异主要由投资组合构成和提示设计驱动，而非检索机制。该管道包含快照、确定性模式和哈希验证工件，以确保可重复性和可审计性。总体而言，结果表明，当LLM生成的宏观情景与透明结构和严格验证相结合时，可为实战交易提供有价值的风险模拟工具。",
    "fetch_date": "2026-01-03",
    "id": "20260103_0b660ac8"
  },
  {
    "title": "Black-Litterman and ESG Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.21850v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We introduce a simple portfolio optimization strategy using ESG data with the Black-Litterman allocation framework. ESG scores are used as a bias for Stein shrinkage estimation of equilibrium risk premiums used in assigning Black-Litterman asset weights. Assets are modeled as multivariate affine normal-inverse Gaussian variables using CVaR as a risk measure. This strategy, though very simple, when employed with a soft turnover constraint is exceptionally successful. Portfolios are reallocated daily over a 4.7 year period, each with a different set of hyperparameters used for optimization. The most successful strategies have returns of approximately 40-45% annually.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种结合ESG数据的简单投资组合优化策略，采用Black-Litterman资产配置框架。ESG评分被用作Stein收缩估计的偏差，用于计算Black-Litterman资产权重中的均衡风险溢价。资产被建模为多元仿射正态逆高斯变量，并使用CVaR作为风险度量。该策略虽然简单，但在采用软换手率约束时表现异常出色。投资组合在4.7年期间每日重新配置，每次使用不同的超参数进行优化。最成功的策略年化回报率约为40-45%。",
    "fetch_date": "2026-01-03",
    "id": "20260103_44a40e03"
  },
  {
    "title": "Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting",
    "url": "https://arxiv.org/pdf/2512.07860v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "This paper investigates an optimal integration of deep learning with financial models for robust asset price forecasting. Specifically, we developed a hybrid framework combining a Long Short-Term Memory (LSTM) network with the Merton-Lévy jump-diffusion model. To optimise this framework, we employed the Grey Wolf Optimizer (GWO) for the LSTM hyperparameter tuning, and we explored three calibration methods for the Merton-Levy model parameters: Artificial Neural Networks (ANNs), the Marine Predators Algorithm (MPA), and the PyTorch-based TorchSDE library. To evaluate the predictive performance of our hybrid model, we compared it against several benchmark models, including a standard LSTM and an LSTM combined with the Fractional Heston model. This evaluation used three real-world financial datasets: Brent oil prices, the STOXX 600 index, and the IT40 index. Performance was assessed using standard metrics, including Mean Squared Error (MSE), Mean Absolute Error(MAE), Mean Squared Percentage Error (MSPE), and the coefficient of determination (R2). Our experimental results demonstrate that the hybrid model, combining a GWO-optimized LSTM network with the Levy-Merton Jump-Diffusion model calibrated using an ANN, outperformed the base LSTM model and all other models developed in this study.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文研究了一种将深度学习与金融模型相结合进行资产价格稳健预测的混合框架。具体而言，作者开发了一个结合长短期记忆网络与Merton-Lévy跳跃扩散模型的混合模型。为优化该框架，采用灰狼优化器进行LSTM超参数调优，并探索了三种Merton-Levy模型参数校准方法：人工神经网络、海洋捕食者算法和基于PyTorch的TorchSDE库。通过在布伦特原油价格、STOXX 600指数和IT40指数三个真实金融数据集上的实验评估，使用均方误差、平均绝对误差、均方百分比误差和决定系数等标准指标，与标准LSTM及LSTM结合分数Heston模型等基准模型进行比较。实验结果表明，结合GWO优化LSTM与Levy-Merton跳跃扩散模型的混合框架在预测性能上表现优异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_b3fb4ca0"
  },
  {
    "title": "Portfolio Optimization via Transfer Learning",
    "url": "https://arxiv.org/pdf/2511.21221v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "认识到资产市场通常表现出共享的信息特征，我们开发了一种基于迁移学习的投资组合策略，该策略利用跨市场信息，通过前向验证来提升目标市场的投资表现。我们的策略渐近地识别并利用信息丰富的数据集，选择性地纳入有效信息，同时摒弃误导性信息。这使得我们的策略能够渐近地实现最大夏普比率。通过数值研究以及两个投资组合的案例研究（一个由A股和H股双重上市的股票组成，另一个包含美国各行业的股票），展示了该策略的优异表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_d33d2a08"
  },
  {
    "title": "Constrained deep learning for pricing and hedging european options in incomplete markets",
    "url": "https://arxiv.org/pdf/2511.20837v1",
    "source": "ArXiv",
    "date": "2025-11-25",
    "abstract": "In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在不完全金融市场中，欧式期权的定价和对冲因不可对冲风险而缺乏唯一无套利解。本文提出一种约束深度学习方法来最小化损益分布，使用单一神经网络表示期权价格函数，其梯度作为对冲策略，通过强制自融资组合条件的损失函数进行优化。针对期权收益非光滑特性（如香草看涨期权在平价点不可微，数字期权不连续）与神经网络固有平滑性的冲突，比较了无约束网络与显式嵌入终端收益条件的约束架构，借鉴偏微分方程求解技术。框架假设两种可交易资产：标的资产和捕捉波动率动态的流动性看涨期权。数值实验评估了该方法在具有不同非光滑性的简单期权、奇异Equinox期权及市场跳跃场景下的表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_061536e2"
  },
  {
    "title": "Evolutionary Discovery of Heuristic Policies for Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2511.23122v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\\textbf{\\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \\method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《Evolutionary Discovery of Heuristic Policies for Traffic Signal Control》提出了一种名为Temporal Policy Evolution for Traffic的方法，旨在解决交通信号控制（TSC）中经典启发式方法过于简化、深度强化学习（DRL）泛化能力差且策略不透明，以及在线大型语言模型（LLMs）延迟高且缺乏环境特定优化的问题。该方法利用LLMs作为进化引擎，通过结构化状态抽象（SSA）将高维交通数据转换为时序逻辑事实，并结合信用分配反馈（CAF）追踪微观决策错误与宏观结果的关系，从而在无需训练的情况下生成轻量级、鲁棒的启发式策略，针对特定交通环境进行优化。",
    "fetch_date": "2026-01-03",
    "id": "20260103_4318e585"
  },
  {
    "title": "Factors Influencing Cryptocurrency Prices: Evidence from Bitcoin, Ethereum, Dash, Litecoin, and Monero",
    "url": "https://arxiv.org/pdf/2511.22782v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "This paper examines factors that influence prices of most common five cryptocurrencies such as Bitcoin, Ethereum, Dash, Litecoin, and Monero over 2010-2018 using weekly data. The study employs ARDL technique and documents several findings. First, cryptomarket-related factors such as market beta, trading volume, and volatility appear to be significant determinant for all five cryptocurrencies both in short- and long-run. Second, attractiveness of cryptocurrencies also matters in terms of their price determination, but only in long-run. This indicates that formation (recognition) of the attractiveness of cryptocurrencies are subjected to time factor. In other words, it travels slowly within the market. Third, SP500 index seems to have weak positive long-run impact on Bitcoin, Ethereum, and Litcoin, while its sign turns to negative losing significance in short-run, except Bitcoin that generates an estimate of -0.20 at 10% significance level. Lastly, error-correction models for Bitcoin, Etherem, Dash, Litcoin, and Monero show that cointegrated series cannot drift too far apart, and converge to a long-run equilibrium at a speed of 23.68%, 12.76%, 10.20%, 22.91%, and 14.27% respectively.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文使用ARDL技术分析了2010-2018年比特币、以太坊、达世币、莱特币和门罗币这五种主要加密货币的周度数据，探讨了影响其价格的因素。研究发现：1）加密货币市场相关因素（如市场贝塔、交易量、波动性）在短期和长期对所有五种货币均有显著影响；2）加密货币的吸引力（如认可度）仅在长期影响价格，表明市场认知形成缓慢；3）标普500指数对比特币、以太坊和莱特币有微弱的长期正向影响，但短期影响不显著（除比特币在10%显著性水平下为负向）；4）误差修正模型显示各币种均存在长期均衡关系，收敛速度在10.20%-23.68%之间。",
    "fetch_date": "2026-01-03",
    "id": "20260103_72eb24b5"
  },
  {
    "title": "Beta-Dependent Gamma Feedback and Endogenous Volatility Amplification in Option Markets",
    "url": "https://arxiv.org/pdf/2511.22766v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "We develop a theoretical framework that aims to link micro-level option hedging and stock-specific factor exposure with macro-level market turbulence and explain endogenous volatility amplification during gamma-squeeze events. By explicitly modeling market-maker delta-neutral hedging and incorporating beta-dependent volatility normalization, we derive a stability condition that characterizes the onset of a gamma-squeeze event. The model captures a nonlinear recursive feedback loop between market-maker hedging and price movements and the resulting self-reinforcing dynamics. From a complex-systems perspective, the dynamics represent a bounded nonlinear response in which effective gain depends jointly on beta-normalized shock perception and gamma-scaled sensitivity. Our analysis highlights that low-beta stocks exhibit disproportionately strong feedback even for modest absolute price movements.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个理论框架，旨在将微观层面的期权对冲和股票特定因子暴露与宏观层面的市场动荡联系起来，解释伽玛挤压事件中的内生波动率放大。通过显式建模做市商的Delta中性对冲并纳入Beta依赖的波动率归一化，推导出表征伽玛挤压事件发生的稳定性条件。模型捕捉了做市商对冲与价格变动之间的非线性递归反馈循环及其产生的自我强化动态。从复杂系统视角看，该动态代表了一种有界非线性响应，其中有效增益同时取决于Beta归一化的冲击感知和伽玛缩放敏感性。分析强调，低Beta股票即使面对温和的绝对价格变动，也会表现出不成比例的强烈反馈效应。",
    "fetch_date": "2026-01-03",
    "id": "20260103_40682348"
  },
  {
    "title": "The Risk-Adjusted Intelligence Dividend: A Quantitative Framework for Measuring AI Return on Investment Integrating ISO 42001 and Regulatory Exposure",
    "url": "https://arxiv.org/pdf/2511.21975v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Organizations investing in artificial intelligence face a fundamental challenge: traditional return on investment calculations fail to capture the dual nature of AI implementations, which simultaneously reduce certain operational risks while introducing novel exposures related to algorithmic malfunction, adversarial attacks, and regulatory liability. This research presents a comprehensive financial framework for quantifying AI project returns that explicitly integrates changes in organizational risk profiles. The methodology addresses a critical gap in current practice where investment decisions rely on optimistic benefit projections without accounting for the probabilistic costs of AI-specific threats including model drift, bias-related litigation, and compliance failures under emerging regulations such as the European Union Artificial Intelligence Act and ISO/IEC 42001. Drawing on established risk quantification methods, including annual loss expectancy calculations and Monte Carlo simulation techniques, this framework enables practitioners to compute net benefits that incorporate both productivity gains and the delta between pre-implementation and post-implementation risk exposures. The analysis demonstrates that accurate AI investment evaluation requires explicit modeling of control effectiveness, reserve requirements for algorithmic failures, and the ongoing operational costs of maintaining model performance. Practical implications include specific guidance for establishing governance structures, conducting phased validations, and integrating risk-adjusted metrics into capital allocation decisions, ultimately enabling evidence-based AI portfolio management that satisfies both fiduciary responsibilities and regulatory mandates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "组织投资人工智能面临一个根本性挑战：传统的投资回报率计算未能捕捉AI实施的双重性——在降低某些运营风险的同时，引入了与算法故障、对抗性攻击和监管责任相关的新型风险敞口。本研究提出了一个全面的财务框架，用于量化AI项目回报，明确整合组织风险状况的变化。该方法解决了当前实践中的一个关键空白，即投资决策依赖于乐观的效益预测，而未考虑AI特定威胁（包括模型漂移、偏见相关诉讼以及欧盟人工智能法案和ISO/IEC 42001等新兴法规下的合规失败）的概率成本。借鉴已建立的风险量化方法，包括年度损失预期计算和蒙特卡洛模拟技术，该框架使从业者能够计算净效益，既包含生产力收益，也包含实施前和实施后风险敞口之间的差异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_dbd49bab"
  },
  {
    "title": "Extended Convolution Bounds on the Fréchet Problem: Robust Risk Aggregation and Risk Sharing",
    "url": "https://arxiv.org/pdf/2511.21929v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "In this paper, we provide extended convolution bounds for the Fréchet problem and discuss related implications in quantitative risk management. First, we establish a new form of inequality for the Range-Value-at-Risk (RVaR). Based on this inequality, we obtain bounds for robust risk aggregation with dependence uncertainty for (i) RVaR, (ii) inter-RVaR difference and (iii) inter-quantile difference, and provide sharpness conditions. These bounds are called extended convolution bounds, which not only complement the results in the literature (convolution bounds in Blanchet et al. (2025)) but also offer results for some variability measures. Next, applying the above inequality, we study the risk sharing for the averaged quantiles (corresponding to risk sharing for distortion risk measures with special inverse S-shaped distortion functions), which is a non-convex optimization problem. We obtain the expression of the minimal value of the risk sharing and the explicit expression for the corresponding optimal allocation, which is comonotonic risk sharing for large losses and counter-comonotonic risk sharing for small losses or large gains. Finally, we explore the dependence structure for the optimal allocations, showing that the optimal allocation does not exist if the risk is not bounded from above.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对Fréchet问题提出了扩展卷积界，并探讨了其在量化风险管理中的应用。首先，建立了Range-Value-at-Risk（RVaR）的新不等式，并基于此获得了在依赖不确定性下关于（i）RVaR、（ii）RVaR间差值及（iii）分位数间差值的稳健风险聚合界，同时给出了锐度条件。这些扩展卷积界不仅补充了现有文献结果，还为一些变异性度量提供了结论。其次，应用上述不等式研究了平均分位数的风险分担问题（对应于具有特殊逆S形扭曲函数的扭曲风险度量的风险分担），这是一个非凸优化问题。我们获得了风险分担的最小值表达式及相应最优分配的显式表达式，即对大损失采用共单调风险分担，对小损失或大收益采用反共单调风险分担。最后，探讨了相关依赖结构。",
    "fetch_date": "2026-01-03",
    "id": "20260103_a2552928"
  },
  {
    "title": "Informative Risk Measures in the Banking Industry: A Proposal based on the Magnitude-Propensity Approach",
    "url": "https://arxiv.org/pdf/2511.21556v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Despite decades of research in risk management, most of the literature has focused on scalar risk measures (like e.g. Value-at-Risk and Expected Shortfall). While such scalar measures provide compact and tractable summaries, they provide a poor informative value as they miss the intrinsic multivariate nature of risk.To contribute to a paradigmatic enhancement, and building on recent theoretical work by Faugeras and Pagés (2024), we propose a novel multivariate representation of risk that better reflects the structure of potential portfolio losses, while maintaining desirable properties of interpretability and analytical coherence. The proposed framework extends the classical frequency-severity approach and provides a more comprehensive characterization of extreme events. Several empirical applications based on real-world data demonstrate the feasibility, robustness and practical relevance of the methodology, suggesting its potential for both regulatory and managerial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "尽管风险管理研究已有数十年，但多数文献聚焦于标量风险度量（如风险价值和预期缺口）。虽然这些标量度量提供了简洁易处理的总结，但其信息价值有限，因为它们忽略了风险固有的多元性。为促进范式改进，并基于Faugeras和Pagés（2024）的最新理论工作，本文提出了一种新颖的多元风险表示方法，能更好地反映潜在投资组合损失的结构，同时保持可解释性和分析一致性的理想特性。该框架扩展了经典的频率-严重性方法，提供了对极端事件的更全面刻画。基于真实数据的多个实证应用证明了该方法的可行性、稳健性和实际相关性，表明其在监管和管理应用中的潜力。",
    "fetch_date": "2026-01-03",
    "id": "20260103_200d1274"
  },
  {
    "title": "The Quantum Network of Assets: A Non-Classical Framework for Market Correlation and Structural Risk",
    "url": "https://arxiv.org/pdf/2511.21515v2",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Classical correlation matrices capture only linear and pairwise co-movements, leaving higher-order, nonlinear, and state-dependent interactions of financial markets unrepresented. This paper introduces the Quantum Network of Assets (QNA), a density-matrix based framework that embeds cross-asset dependencies into a quantum-information representation. The approach does not assume physical quantum effects but uses the mathematical structure of density operators, entropy, and mutual information to describe market organisation at a structural level.\n  Within this framework we define two structural measures: the Entanglement Risk Index (ERI), which summarises global non-separability and the compression of effective market degrees of freedom, and the Quantum Early-Warning Signal (QEWS), which tracks changes in entropy to detect latent information build-up. These measures reveal dependency geometry that classical covariance-based tools cannot capture.\n  Using NASDAQ-100 data from 2024-2025, we show that quantum entropy displays smoother evolution and clearer regime distinctions than classical entropy, and that ERI rises during periods of structural tightening even when volatility remains low. Around the 2025 US tariff announcement, QEWS shows a marked pre-event increase in structural tension followed by a sharp collapse after the announcement, indicating that structural transitions can precede price movements without implying predictive modelling.\n  QNA therefore provides a structural diagnostic of market fragility, regime shifts, and latent information flow. The framework suggests new directions for systemic risk research by linking empirical asset networks with tools from quantum information theory.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "经典相关性矩阵仅捕捉线性和成对的共动，未能体现金融市场的高阶、非线性和状态依赖的相互作用。本文引入资产量子网络（QNA），一种基于密度矩阵的框架，将跨资产依赖性嵌入量子信息表示中。该方法不假设物理量子效应，而是利用密度算子、熵和互信息的数学结构来描述市场在结构层面的组织。在此框架内，我们定义了两个结构性度量：纠缠风险指数（ERI），用于总结全局不可分性和有效市场自由度的压缩；以及量子早期预警信号（QEWS），通过追踪熵的变化来检测潜在信息的积累。这些度量揭示了基于经典协方差的工具无法捕捉的依赖性几何结构。使用2024-2025年的纳斯达克100指数数据，我们表明量子熵比经典熵展现出更平滑的演化和更清晰的制度区分，并且ERI在结构紧缩期间上升，即使波动率保持不变。",
    "fetch_date": "2026-01-03",
    "id": "20260103_ee106c42"
  },
  {
    "title": "Algorithmic trading and ai: A review of strategies and market impact",
    "url": "https://www.researchgate.net/profile/Titilola-Falaiye/publication/378548435_Algorithmic_Trading_and_AI_A_Review_of_Strategies_and_Market_Impact/links/65e60893e7670d36abfd1738/Algorithmic-Trading-and-AI-A-Review-of-Strategies-and-Market-Impact.pdf",
    "source": "Scholar",
    "date": "2026-01-03",
    "abstract": "… This review seeks to delve into the intricate strategies employed in algorithmic trading, … redefined these strategies. Furthermore, it aims to unravel the impact of algorithmic trading on …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "该论文综述了算法交易中采用的复杂策略，并探讨了人工智能（特别是机器学习）如何重新定义这些策略。同时，文章旨在揭示算法交易对市场的影响，包括市场效率、流动性和波动性等方面。",
    "fetch_date": "2026-01-03",
    "id": "20260103_fc1d08db"
  },
  {
    "title": "Retail Investor Horizon and Earnings Announcements",
    "url": "https://arxiv.org/pdf/2512.00280v2",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "This paper moves beyond aggregate measures of retail intensity to explore investment horizon as a distinguishing feature of earnings-related return patterns. Using self-reported holding periods from StockTwits (2010-2021), we observe that separating retail activity into \"long-horizon\" and \"short-horizon\" cohorts reveals divergent price anomalies. Long-horizon composition is associated with underreaction, characterized by larger initial reactions and pronounced Post-Earnings Announcement Drift (PEAD), suggesting a slow but persistent convergence toward fundamental value. In contrast, short-horizon activity parallels sentiment-driven overreaction, where elevated pre-event sentiment precedes weaker subsequent performance and price reversals. A zero-cost strategy exploiting this heterogeneity, going long on long-horizon stocks and short on short-horizon stocks, yields risk-adjusted alphas of 0.43% per month. These findings suggest that accounting for investment horizon helps disentangles the fundamental signal in retail flow from speculative noise.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文超越零售投资者整体参与度的衡量，通过投资期限（基于StockTwits 2010-2021年自报持有期）区分“长期”与“短期”零售投资者活动，揭示了财报公告前后不同的价格异常模式。长期投资者活动与反应不足相关，表现为较大的初始反应和显著的财报公告后漂移（PEAD），暗示向基本面价值的缓慢持续收敛；而短期投资者活动则与情绪驱动的过度反应相关，表现为事件前情绪高涨、后续表现疲软及价格反转。利用这种异质性构建的零成本策略（做多长期股票、做空短期股票）每月可获得0.43%的风险调整后阿尔法收益。这表明，考虑投资期限有助于从零售资金流中分离基本面信号与投机噪音。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b70a95e0"
  },
  {
    "title": "Forward-Oriented Causal Observables for Non-Stationary Financial Markets",
    "url": "https://arxiv.org/pdf/2512.24621v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We study short-horizon forecasting in financial time series under strict causal constraints, treating the market as a non-stationary stochastic system in which any predictive observable must be computable online from information available up to the decision time. Rather than proposing a machine-learning predictor or a direct price-forecast model, we focus on \\emph{constructing} an interpretable causal signal from heterogeneous micro-features that encode complementary aspects of the dynamics (momentum, volume pressure, trend acceleration, and volatility-normalized price location). The construction combines (i) causal centering, (ii) linear aggregation into a composite observable, (iii) causal stabilization via a one-dimensional Kalman filter, and (iv) an adaptive ``forward-like'' operator that mixes the composite signal with a smoothed causal derivative term. The resulting observable is mapped into a transparent decision functional and evaluated through realized cumulative returns and turnover. An application to high-frequency EURUSDT (1-minute) illustrates that causally constructed observables can exhibit substantial economic relevance in specific regimes, while degrading under subsequent regime shifts, highlighting both the potential and the limitations of causal signal design in non-stationary markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们研究在严格因果约束下的金融时间序列短期预测，将市场视为一个非平稳随机系统，其中任何预测性可观测值必须能够在线从决策时间前可获得的信息中计算得出。我们并非提出一个机器学习预测器或直接的价格预测模型，而是专注于从编码动态互补方面（动量、成交量压力、趋势加速和波动率归一化价格位置）的异构微观特征中构建一个可解释的因果信号。该构建结合了（i）因果中心化，（ii）线性聚合为复合可观测值，（iii）通过一维卡尔曼滤波器进行因果稳定化，以及（iv）一个将复合信号与平滑因果导数项混合的自适应“前向”算子。所得可观测值被映射到一个透明的决策函数中，并通过实现的累积收益和换手率进行评估。对高频EURUSDT（1分钟）的应用表明，因果构建的可观测值在特定制度下可展现出显著的经济相关性，而在次优条件下则会退化。",
    "fetch_date": "2026-01-02",
    "id": "20260102_fb4c17f1"
  },
  {
    "title": "Generative AI-enhanced Sector-based Investment Portfolio Construction",
    "url": "https://arxiv.org/pdf/2512.24526v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025).\n  Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency.\n  This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了如何将领先提供商（OpenAI、Google、Anthropic、DeepSeek和xAI）的大型语言模型（LLMs）应用于量化行业投资组合构建。研究使用LLMs识别标普500行业指数中的可投资股票范围，并评估其选股结果与经典投资组合优化方法结合后的表现。每个模型被提示为每个行业选择和加权20只股票，结果投资组合在两个不同的样本外时期（稳定市场阶段：2025年1月至3月，波动阶段：2025年4月至6月）与其相应的行业指数进行比较。结果显示，LLM投资组合表现具有强烈的时间依赖性：在稳定市场条件下，LLM加权投资组合在累计回报和风险调整（夏普比率）指标上经常优于行业指数；但在波动时期，许多LLM投资组合表现不佳，表明当前模型可能难以适应其训练数据中代表性不足的制度转换或高波动环境。重要的是，当基于LLM的选股与传统优化方法结合时，表现有所改善。",
    "fetch_date": "2026-01-02",
    "id": "20260102_388c3be1"
  },
  {
    "title": "Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem",
    "url": "https://arxiv.org/pdf/2512.24251v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于深度强化学习（DRL）的方法，用于解决车队规模与混合车辆路径问题（FSMVRP）。该方法将问题建模为马尔可夫决策过程（MDP），并开发了一种名为FRIPN的新型策略网络，能够同时处理车队组合和路径规划决策。论文通过随机生成实例和基准数据集进行了全面实验，结果表明该方法能在几秒内生成接近最优的解决方案，适用于短期车辆租赁和按需物流等实际场景。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6fbbf264"
  },
  {
    "title": "Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2512.00243v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Our work investigates the economic efficiency of the prevailing \"ladder-step\" investment strategy in oil and gas exploration, which advocates for the incremental acquisition of geological information throughout the project lifecycle. By employing a multi-agent Deep Reinforcement Learning (DRL) framework, we model an alternative strategy that prioritizes the early acquisition of high-quality information assets. We simulate the entire upstream value chain-comprising competitive bidding, exploration, and development phases-to evaluate the economic impact of this approach relative to traditional methods. Our results demonstrate that front-loading information investment significantly reduces the costs associated with redundant data acquisition and enhances the precision of reserve valuation. Specifically, we find that the alternative strategy outperforms traditional methods in highly competitive environments by mitigating the \"winner's curse\" through more accurate bidding. Furthermore, the economic benefits are most pronounced during the development phase, where superior data quality minimizes capital misallocation. These findings suggest that optimal investment timing is structurally dependent on market competition rather than solely on price volatility, offering a new paradigm for capital allocation in extractive industries.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究通过多智能体深度强化学习（DRL）框架，评估了石油天然气勘探领域传统“阶梯式”信息资产投资策略的经济效率，并提出了一种优先早期获取高质量信息资产的替代策略。模拟上游价值链（包括竞争性投标、勘探和开发阶段）显示，该替代策略能显著降低冗余数据采集成本，提高储量评估精度，在高度竞争环境中通过更精准的投标缓解“赢家诅咒”，并在开发阶段因数据质量优化而减少资本错配，表明信息投资时机优化具有结构性经济价值。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b7002262"
  },
  {
    "title": "Stochastic factors can matter: improving robust growth under ergodicity",
    "url": "https://arxiv.org/pdf/2512.24906v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strategies obtained from portfolio optimization are very sensitive to them. To mitigate this well-known phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price process $X$, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of admissible models allows $X$ to depend on a multivariate stochastic factor $Y$ and fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor process $Y$. A principal motivation of this framework comes from pairs trading, where $X$ is the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certain partial differential equation (PDE). We demonstrate that utilizing the stochastic factor leads to improvement in robust growth complementing the conclusions of the previous study by Itkin et. al. (arXiv:2211.15628 [q-fin.MF], forthcoming in $\\textit{Finance and Stochastics}$), which additionally robustified the dynamics of the stochastic factor leading to $Y$-independent optimal strategies. Our analysis leads to new financial insights, quantifying the improvement in growth the investor can achieve by optimally incorporating stochastic factors into their trading decisions. We illustrate our theoretical results on several numerical examples including an application to pairs trading.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文针对资产收益率漂移难以准确建模且投资组合优化策略对其高度敏感的问题，研究了在高维不完全市场中、资产价格过程X存在漂移不确定性下的稳健增长优化，并引入了遍历性假设。该框架允许X依赖于多元随机因子Y，并固定了(a)联合波动结构、(b)长期联合遍历密度和(c)随机因子过程Y的动力学。主要动机来自配对交易，其中X为价差过程。研究确定了稳健最优增长率，构建了最坏情况下的可接受模型，并通过求解特定偏微分方程(PDE)来表征稳健增长最优策略。结果表明，利用随机因子可提升稳健增长，补充了先前研究的结论。",
    "fetch_date": "2026-01-02",
    "id": "20260102_7acb0abd"
  },
  {
    "title": "Signature approach for pricing and hedging path-dependent options with frictions",
    "url": "https://arxiv.org/pdf/2511.23295v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "We introduce a novel signature approach for pricing and hedging path-dependent options with instantaneous and permanent market impact under a mean-quadratic variation criterion. Leveraging the expressive power of signatures, we recast an inherently nonlinear and non-Markovian stochastic control problem into a tractable form, yielding hedging strategies in (possibly infinite) linear feedback form in the time-augmented signature of the control variables, with coefficients characterized by non-standard infinite-dimensional Riccati equations on the extended tensor algebra. Numerical experiments demonstrate the effectiveness of these signature-based strategies for pricing and hedging general path-dependent payoffs in the presence of frictions. In particular, market impact naturally smooths optimal trading strategies, making low-truncated signature approximations highly accurate and robust in frictional markets, contrary to the frictionless case.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新颖的签名方法，用于在均值-二次变分准则下，对具有瞬时和永久市场摩擦影响的路径依赖期权进行定价和对冲。利用签名的表达能力，我们将一个本质非线性且非马尔可夫性的随机控制问题转化为可处理的形式，得到了以控制变量的时间增强签名的（可能无限）线性反馈形式表示的对冲策略，其系数由扩展张量代数上的非标准无限维Riccati方程表征。数值实验证明了这些基于签名的策略在存在摩擦的情况下，对一般路径依赖收益进行定价和对冲的有效性。特别是，市场摩擦自然地平滑了最优交易策略，使得低阶截断签名近似在高摩擦市场中具有高精度和鲁棒性，这与无摩擦情况相反。",
    "fetch_date": "2026-01-02",
    "id": "20260102_647600a8"
  },
  {
    "title": "Boundary error control for numerical solution of BSDEs by the convolution-FFT method",
    "url": "https://arxiv.org/pdf/2512.24714v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文首先回顾了基于卷积快速傅里叶变换（CFFT）的倒向随机微分方程（BSDE）数值解法，随后提出了一种改进期权定价中边界误差的方法。通过优化原方法中的阻尼与平移方案，将目标函数转化为有界周期函数以适用傅里叶变换，其中时变平移方案显著降低了边界误差。数值实验与误差分析验证了改进后方法在精度和收敛性上的提升。",
    "fetch_date": "2026-01-02",
    "id": "20260102_1ef00a6e"
  },
  {
    "title": "Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.24580v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于风险敏感强化学习（RSRL）的新框架，该框架结合了对状态转移不确定性的鲁棒性。作者定义了两个不同但耦合的风险度量：一个内部风险度量处理状态和成本的随机性，一个外部风险度量捕捉状态转移动态的不确定性。该框架通过允许对内部和外部风险度量使用一般的相干风险度量，统一并推广了大多数现有的强化学习框架。在此框架内，作者构建了一个风险敏感鲁棒马尔可夫决策过程（RSRMDP），推导了其贝尔曼方程，并在给定后验分布下提供了误差分析。作者进一步开发了一种贝叶斯动态规划（Bayesian DP）算法，该算法在后验更新和价值迭代之间交替进行。该方法采用了一个基于风险的贝尔曼算子估计器，该估计器结合了蒙特卡洛采样和凸优化，作者为此证明了强一致性保证。此外，作者证明了该算法在训练环境中收敛到接近最优的策略，并在狄利克雷后验和条件风险价值（CVaR）下分析了样本复杂度和计算复杂度。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9e6c8c0d"
  },
  {
    "title": "Minimal Solutions to the Skorokhod Reflection Problem Driven by Jump Processes and an Application to Reinsurance",
    "url": "https://arxiv.org/pdf/2512.24491v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider a reflected process in the positive orthant driven by an exogenous jump process. For a given input process, we show that there exists a unique minimal strong solution to the given particle system up until a certain maximal stopping time, which is stated explicitly in terms of the dual formulation of a linear programming problem associated with the state of the system. We apply this model to study the ruin time of interconnected insurance firms, where the stopping time can be interpreted as the failure time of a reinsurance agreement between the firms. Our work extends the analysis of the particle system in Baker, Hambly, and Jettkant (2025) to the case of jump driving processes, and the existence result of Reiman (1984) beyond the case of sub-stochastic reflection matrices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了由外生跳跃过程驱动的正象限反射过程。对于给定的输入过程，我们证明了在某个最大停时之前，该粒子系统存在唯一的最小强解，该停时通过系统状态相关的线性规划问题的对偶形式明确给出。我们将该模型应用于研究互联保险公司的破产时间，其中停时可解释为公司间再保险协议的失效时间。我们的工作将Baker、Hambly和Jettkant（2025）对粒子系统的分析扩展到跳跃驱动过程的情况，并将Reiman（1984）的存在性结果推广到非次随机反射矩阵的情形。",
    "fetch_date": "2026-01-02",
    "id": "20260102_230af948"
  },
  {
    "title": "Utility Maximisation with Model-independent Constraints",
    "url": "https://arxiv.org/pdf/2512.24371v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider an agent who has access to a financial market, including derivative contracts, who looks to maximise her utility. Whilst the agent looks to maximise utility over one probability measure, or class of probability measures, she must also ensure that the mark-to-market value of her portfolio remains above a given threshold. When the mark-to-market value is based on a more pessimistic valuation method, such as model-independent bounds, we recover a novel optimisation problem for the agent where the agents investment problem must satisfy a pathwise constraint.\n  For complete markets, the expression of the optimal terminal wealth is given, using the max-plus decomposition for supermartingales. Moreover, for the Black-Scholes-Merton model the explicit form of the process involved in such decomposition is obtained, and we are able to investigate numerically optimal portfolios in the presence of options which are mispriced according to the agent's beliefs.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了一个在金融市场（包括衍生品合约）中寻求效用最大化的代理人问题。代理人虽然在一个概率测度或一类概率测度下最大化效用，但必须同时确保其投资组合的盯市价值维持在给定阈值之上。当盯市价值基于更悲观的估值方法（如模型无关边界）时，我们为代理人推导出一个新颖的优化问题，其中投资问题必须满足路径约束。对于完全市场，利用超鞅的最大加分解给出了最优终端财富的表达式。此外，对于Black-Scholes-Merton模型，我们获得了该分解中涉及过程的显式形式，并能够数值研究在代理人认为期权被错误定价情况下的最优投资组合。",
    "fetch_date": "2026-01-02",
    "id": "20260102_de031d92"
  },
  {
    "title": "A Test of Lookahead Bias in LLM Forecasts",
    "url": "https://arxiv.org/pdf/2512.23847v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state-of-the-art pre-training data detection techniques, we estimate the likelihood that a given prompt appeared in an LLM's training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们开发了一种统计检验方法，用于检测大型语言模型（LLMs）生成的经济预测中的前瞻性偏差。利用最先进的预训练数据检测技术，我们估计给定提示出现在LLM训练语料库中的可能性，这一统计量我们称为前瞻倾向（LAP）。我们正式证明，LAP与预测准确性之间的正相关表明前瞻性偏差的存在及其程度，并将该检验应用于两个预测任务：新闻标题预测股票收益和财报电话会议记录预测资本支出。我们的检验为评估LLM生成预测的有效性和可靠性提供了一种成本效益高的诊断工具。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9a4a2330"
  },
  {
    "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations",
    "url": "https://arxiv.org/pdf/2512.00249v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在支持兵棋推演的作战模拟领域，智能体开发长期以基于规则的脚本方法为主，深度强化学习（RL）方法近期才被引入。脚本智能体在受控环境中提供可预测性和一致性，但在动态复杂场景中因其固有僵化性而表现不足。相反，RL智能体在适应性和学习方面表现优异，在处理意外情况方面具有改进潜力，但面临重大挑战，如黑盒决策过程和在大型模拟环境中的可扩展性问题。本文提出了一种新颖的分层混合人工智能（AI）方法，将脚本智能体的可靠性和可预测性与RL的动态自适应学习能力相结合。通过分层构建AI系统，该方法旨在利用脚本智能体处理常规战术级决策，利用RL智能体处理更高级别的战略决策，从而在利用各自优势的同时解决每种方法的局限性。",
    "fetch_date": "2026-01-02",
    "id": "20260102_f8dabb31"
  },
  {
    "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions",
    "url": "https://arxiv.org/pdf/2512.00142v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究介绍了DeFi TrustBoost框架，该框架结合区块链技术和可解释AI，旨在解决贷款机构在审批来自低收入家庭的小企业贷款申请时面临的挑战。该框架强调满足区块链和AI系统的四个关键要求：机密性、数据保护法规合规性、对抗攻击抵抗力和监管审计合规性。它提出了一种用于自动化AI决策防篡改审计的技术，以及一种链上（区块链内）和链下数据存储策略，以促进金融组织内部及跨组织的协作。",
    "fetch_date": "2026-01-02",
    "id": "20260102_82b264a1"
  },
  {
    "title": "Responsible LLM Deployment for High-Stake Decisions by Decentralized Technologies and Human-AI Interactions",
    "url": "https://arxiv.org/pdf/2512.04108v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "High-stakes decision domains are increasingly exploring the potential of Large Language Models (LLMs) for complex decision-making tasks. However, LLM deployment in real-world settings presents challenges in data security, evaluation of its capabilities outside controlled environments, and accountability attribution in the event of adversarial decisions. This paper proposes a framework for responsible deployment of LLM-based decision-support systems through active human involvement. It integrates interactive collaboration between human experts and developers through multiple iterations at the pre-deployment stage to assess the uncertain samples and judge the stability of the explanation provided by post-hoc XAI techniques. Local LLM deployment within organizations and decentralized technologies, such as Blockchain and IPFS, are proposed to create immutable records of LLM activities for automated auditing to enhance security and trace back accountability. It was tested on Bert-large-uncased, Mistral, and LLaMA 2 and 3 models to assess the capability to support responsible financial decisions on business lending.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《通过去中心化技术和人机交互实现高风险决策的负责任大语言模型部署》探讨了在金融等高风险决策领域部署大语言模型（LLMs）的挑战，包括数据安全、模型能力评估和决策问责。提出一个负责任部署框架，强调人类专家在部署前阶段的主动参与，通过多轮迭代评估不确定样本和事后可解释人工智能（XAI）技术的稳定性。建议在组织内部署本地LLMs，并利用区块链和IPFS等去中心化技术创建不可篡改的记录，以增强安全性和可追溯性。该框架在Bert-large-uncased、Mistral和LLaMA 2/3模型上进行了测试，用于评估其在商业贷款等金融决策中的支持能力。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6751ad03"
  },
  {
    "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
    "url": "https://arxiv.org/pdf/2512.01565v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为Deep FlexQP的加速非线性规划方法，通过深度展开技术改进QP优化器。该方法基于对QP约束的精确松弛，无论原始约束是否可行，都能找到最优解或最小化约束违反的稀疏解。Deep FlexQP具有良好的可扩展性、鲁棒性和热启动能力，通过数据驱动技术学习维度无关的反馈策略，能够泛化到更大维度的问题，并在投资组合优化等基准测试中优于现有方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2e864486"
  },
  {
    "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets",
    "url": "https://arxiv.org/pdf/2512.10971v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AI-Trader：实时金融市场中自主智能体的基准测试》提出首个完全自动化、实时且数据无污染的LLM智能体金融决策评估基准。该基准覆盖美股、A股和加密货币三大市场，采用多粒度交易模拟实时金融环境，并引入革命性的全自主最小信息范式——智能体仅接收基本上下文，需独立搜索、验证和整合实时市场信息。研究评估了六种主流LLM，旨在解决动态实时环境中决策制定的挑战，对量化交易实战具有直接应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23a72d9d"
  },
  {
    "title": "Autodeleveraging: Impossibilities and Optimization",
    "url": "https://arxiv.org/pdf/2512.01112v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by $\\approx 28\\times$ relative to our optimal policy, imposing roughly \\$653 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "自动去杠杆化（ADL）是永续期货交易所的最后损失社会化机制，在维持偿付能力的清算失败时触发。本文首次对ADL进行严格建模，证明了ADL机制面临一个根本性的“三难困境”：没有任何政策能同时满足交易所的偿付能力、收入和交易者的公平性。这一不可能定理意味着，随着参与规模扩大，一种新型的“道德风险”会渐近增长，使得“零损失”社会化变得不可能。建设性地，本文展示了三类ADL机制可以最优地应对这一三难困境，提供公平性、对价格冲击的鲁棒性以及交易所收入最大化。通过分析2025年10月10日Hyperliquid数据集（当时ADL在12分钟内反复用于平仓21亿美元头寸），本文实证比较了这些ADL机制与实践中使用的标准方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_dd83332c"
  },
  {
    "title": "Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model",
    "url": "https://arxiv.org/pdf/2512.00630v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文评估了基于Qwen3-8B模型通过rLoRA微调进行金融文本分类的性能。该方法采用噪声嵌入指令微调增强鲁棒性，结合秩稳定低秩优化与FlashAttention提升训练效率，适用于金融情感分析和新闻分类等量化交易相关任务。研究对比了传统Transformer模型及多个大模型，展示了该技术在实战交易系统中的潜在应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_f74585e2"
  },
  {
    "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts",
    "url": "https://arxiv.org/pdf/2512.00916v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "评估罕见事件预测具有挑战性，因为标准指标在事件发生率下降时会失效。F1分数、AUPRC、MCC和准确率等指标会导致退化阈值（收敛于0或1），其值被类别不平衡而非尾部判别能力主导。我们开发了一族罕见事件稳定（RES）指标，其最优阈值在事件概率趋近于零时仍保持严格内部性，确保在极端罕见情况下的连贯决策规则。模拟显示RES指标保持稳定阈值、一致的模型排名和近乎完全的流行度不变性，而传统指标则表现出统计显著的阈值漂移和结构崩溃。信用违约应用证实了这些结果：RES指标产生可解释的违约概率阈值（4-9%），并在子采样下保持稳健，而经典指标在操作上失效。RES框架为评估极端风险预测提供了原则性的、流行度不变的基础。",
    "fetch_date": "2026-01-01",
    "id": "20260101_70ccf6da"
  },
  {
    "title": "Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election",
    "url": "https://arxiv.org/pdf/2512.00893v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "We study how the 2024 U.S. presidential election, viewed as a major political risk event, affected cryptocurrency markets by distinguishing human-driven peer-to-peer stablecoin transactions from automated algorithmic activity. Using structural break analysis, we find that human-driven Ethereum Request for Comment 20 (ERC-20) transactions shifted on November 3, two days before the election, while exchange trading volumes reacted only on Election Day. Automated smart-contract activity adjusted much later, with structural breaks appearing in January 2025. We validate these shifts using surrogate-based robustness tests. Complementary energy-spectrum analysis of Bitcoin and Ethereum identifies pronounced post-election turbulence, and a structural vector autoregression confirms a regime shift in stablecoin dynamics. Overall, human-driven stablecoin flows act as early-warning indicators of political stress, preceding both exchange behavior and algorithmic responses.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究探讨了2024年美国总统大选这一重大政治风险事件对加密货币市场的影响，通过区分人类驱动的点对点稳定币交易与自动化算法活动。利用结构断点分析发现，人类驱动的ERC-20交易在选举前两天（11月3日）已出现变化，而交易所交易量仅在选举日才反应。自动化智能合约活动调整更晚，结构断点出现在2025年1月。通过替代性稳健性检验验证了这些变化。对比特币和以太坊的互补能量谱分析识别出显著的选举后市场波动，结构向量自回归模型确认了稳定币动态中的制度转变。总体而言，人类驱动的稳定币流动可作为政治压力的早期预警指标，领先于交易所行为和算法响应。",
    "fetch_date": "2026-01-01",
    "id": "20260101_1eadb97f"
  },
  {
    "title": "Efficient Calibration in the rough Bergomi model by Wasserstein distance",
    "url": "https://arxiv.org/pdf/2512.00448v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Despite the empirical success in modeling volatility of the rough Bergomi (rBergomi) model, it suffers from pricing and calibration difficulties stemming from its non-Markovian structure. To address this, we propose a comprehensive computational framework that enhances both simulation and calibration. First, we develop a modified Sum-of-Exponentials (mSOE) Monte Carlo scheme which hybridizes an exact simulation of the singular kernel near the origin with a multi-factor approximation for the remainder. This method achieves high accuracy, particularly for out-of-the-money options, with an $\\mathcal{O}(n)$ computational cost. Second, based on this efficient pricing engine, we then propose a distribution-matching calibration scheme by using Wasserstein distance as the optimization objective. This leverages a minimax formulation against Lipschitz payoffs, which effectively distributes pricing errors and improving robustness. Our numerical results confirm the mSOE scheme's convergence and demonstrate that the calibration algorithm reliably identifies model parameters and generalizes well to path-dependent options, which offers a powerful and generic tool for practical model fitting.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "针对粗糙Bergomi模型在定价和校准上的困难，本文提出了一个综合计算框架。首先，开发了一种改进的指数和蒙特卡洛方案，通过混合原点附近奇异核的精确模拟与剩余部分的多因子近似，实现了高精度（尤其对价外期权）和O(n)计算成本。其次，基于此高效定价引擎，提出了一种使用Wasserstein距离作为优化目标的分布匹配校准方案，通过极小极大公式对抗Lipschitz收益，有效分配定价误差并提高鲁棒性。数值结果验证了方案的收敛性，并表明校准算法能可靠识别模型参数并良好推广至路径依赖期权，为实际模型拟合提供了强大通用工具。",
    "fetch_date": "2026-01-01",
    "id": "20260101_5a90d3c8"
  },
  {
    "title": "Does it take two to tango: Interaction between Credit Default Swaps and National Stock Indices",
    "url": "https://arxiv.org/pdf/2512.07887v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper investigates both short and long-run interaction between BIST-100 index and CDS prices over January 2008 to May 2015 using ARDL technique. The paper documents several findings. First, ARDL analysis shows that 1 TL increase in CDS shrinks BIST-100 index by 22.5 TL in short-run and 85.5 TL in long-run. Second, 1000 TL increase in BIST index price causes 25 TL and 44 TL reducation in Turkey's CDS prices in short- and long-run respectively. Third, a percentage increase in interest rate shrinks BIST index by 359 TL and a percentage increase in inflation rate scales CDS prices up to 13.34 TL both in long-run. In case of short-run, these impacts are limited with 231 TL and 5.73 TL respectively. Fourth, a kurush increase in TL/USD exchange rate leads 24.5 TL (short-run) and 78 TL (long-run) reductions in BIST, while it augments CDS prices by 2.5 TL (short-run) and 3 TL (long-run) respectively. Fifth, each negative political events decreases BIST by 237 TL in short-run and 538 TL in long-run, while it increases CDS prices by 33 TL in short-run and 89 TL in long-run. These findings imply the highly dollar indebted capital structure of Turkish firms, and overly sensitivity of financial markets to the uncertainties in political sphere. Finally, the paper provides evidence for that BIST and CDS with control variables drift too far apart, and converge to a long-run equilibrium at a moderate monthly speed.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文使用ARDL技术研究了2008年1月至2015年5月期间土耳其BIST-100指数与CDS价格之间的短期和长期互动关系。主要发现包括：1）CDS价格每上涨1土耳其里拉，BIST-100指数在短期和长期分别下跌22.5里拉和85.5里拉；2）BIST指数每上涨1000里拉，土耳其CDS价格在短期和长期分别下降25里拉和44里拉；3）利率每上升1个百分点，BIST指数在长期下跌359里拉；通胀率每上升1个百分点，CDS价格在长期上涨13.34里拉；4）土耳其里拉兑美元汇率每上涨1库鲁什，BIST指数在短期和长期分别下跌24.5里拉和78里拉，同时CDS价格分别上涨2.5里拉和3里拉；5）每个负面政治事件使BIST指数在短期和长期分别下跌237里拉和538里拉，同时使CDS价格分别上涨33里拉和89里拉。这些发现揭示了土耳其企业高度美元化的资本结构及其对金融市场的过度敏感性。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2689c421"
  },
  {
    "title": "A Hybrid Architecture for Options Wheel Strategy Decisions: LLM-Generated Bayesian Networks for Transparent Trading",
    "url": "https://arxiv.org/pdf/2512.01123v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Large Language Models (LLMs) excel at understanding context and qualitative nuances but struggle with the rigorous and transparent reasoning required in high-stakes quantitative domains such as financial trading. We propose a model-first hybrid architecture for the options \"wheel\" strategy that combines the strengths of LLMs with the robustness of a Bayesian Network. Rather than using the LLM as a black-box decision-maker, we employ it as an intelligent model builder. For each trade decision, the LLM constructs a context-specific Bayesian network by interpreting current market conditions, including prices, volatility, trends, and news, and hypothesizing relationships among key variables. The LLM also selects relevant historical data from an 18.75-year, 8,919-trade dataset to populate the network's conditional probability tables. This selection focuses on scenarios analogous to the present context. The instantiated Bayesian network then performs transparent probabilistic inference, producing explicit probability distributions and risk metrics to support decision-making. A feedback loop enables the LLM to analyze trade outcomes and iteratively refine subsequent network structures and data selection, learning from both successes and failures. Empirically, our hybrid system demonstrates effective performance on the wheel strategy. Over nearly 19 years of out-of-sample testing, it achieves a 15.3% annualized return with significantly superior risk-adjusted performance (Sharpe ratio 1.08 versus 0.62 for market benchmarks) and dramatically lower drawdown (-8.2% versus -60%) while maintaining a 0% assignment rate through strategic option rolling. Crucially, each trade decision is fully explainable, involving on average 27 recorded decision factors (e.g., volatility level, option premium, risk indicators, market context).",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于期权“轮动”策略决策的混合架构，结合了大型语言模型（LLM）与贝叶斯网络的优势。LLM不直接作为黑盒决策器，而是作为智能模型构建器，根据当前市场条件（如价格、波动率、趋势、新闻）构建情境特定的贝叶斯网络，并假设关键变量间的关系。LLM还从18.75年、8,919笔交易的历史数据集中选取类似情境的数据，填充网络的条件概率表。实例化的贝叶斯网络执行透明的概率推理，生成明确的概率分布和风险指标以支持决策。反馈循环使LLM能分析交易结果并迭代优化后续网络结构。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2daebce1"
  },
  {
    "title": "Arbitrage-Free Option Price Surfaces via Chebyshev Tensor Bases and a Hamiltonian Fog Post-Fit",
    "url": "https://arxiv.org/pdf/2512.01967v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We study the construction of arbitrage-free option price surfaces from noisy bid-ask quotes across strike and maturity. Our starting point is a Chebyshev representation of the call price surface on a warped log-moneyness/maturity rectangle, together with linear sampling and no-arbitrage operators acting on a collocation grid. Static no-arbitrage requirements are enforced as linear inequalities, while the surface is fitted directly to prices via a coverage-seeking quadratic objective that trades off squared band misfit against spectral and transport-inspired regularisation of the Chebyshev coefficients. This yields a strictly convex quadratic program in the modal coefficients, solvable at practical scales with off-the-shelf solvers (OSQP).\n  On top of the global backbone, we introduce a local post-fit layer based on a discrete fog of risk-neutral densities on a three-dimensional lattice (m,t,u) and an associated Hamiltonian-type energy. On each patch of the (m,t) plane, the fog variables are coupled to a nodal price field obtained from the baseline surface, yielding a joint convex optimisation problem that reweights noisy quotes and applies noise-aware local corrections while preserving global static no-arbitrage and locality.\n  The method is designed such that for equity options panels, the combined procedure achieves high inside-spread coverage in stable regimes (in calm years, 98-99% of quotes are priced inside the bid-ask intervals) and low rates of static no-arbitrage violations (below 1%). In stressed periods, the fog layer provides a mechanism for controlled leakage outside the band: when local quotes are mutually inconsistent or unusually noisy, the optimiser allocates fog mass outside the bid-ask tube and justifies small out-of-band deviations of the post-fit surface, while preserving a globally arbitrage-free and well-regularised description of the option surface.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "该论文提出了一种构建无套利期权价格曲面的方法，通过切比雪夫张量基和哈密顿雾后拟合技术。核心包括：使用切比雪夫基在扭曲的对数货币性/期限矩形上表示看涨期权价格曲面，结合线性采样和无套利算子；通过线性不等式强制静态无套利约束，采用覆盖性二次目标函数直接拟合价格，平衡带状误差平方与谱正则化；最终转化为严格凸二次规划问题。在全局基础上，引入基于三维格点风险中性密度雾和哈密顿型能量的局部后拟合层，通过联合凸优化对噪声报价进行重加权和局部修正。该方法理论性强，但未涉及实际交易策略或市场动态，对实战交易的价值主要体现在模型构建层面。",
    "fetch_date": "2026-01-01",
    "id": "20260101_9c7bbf8b"
  },
  {
    "title": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections",
    "url": "https://arxiv.org/pdf/2512.01408v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao \\cite{KZ98}-type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文通过贝叶斯分布鲁棒视角重新审视默顿连续时间投资组合选择问题。核心创新在于：在贝叶斯默顿模型中，将单一模糊集置于先验漂移项上（而非传统的时间矩形分布鲁棒控制），从而保留学习能力与可处理性。通过极小极大交换将鲁棒控制简化为优化先验的非线性泛函，并利用Wasserstein不确定性下的显式渐近最优推前映射来表征小半径最坏情况先验。该方法通过针对默顿泛函定制的非线性Wasserstein投影校准模糊半径，实验表明相较于分布鲁棒控制（DRC）减少了悲观性并提升了性能。",
    "fetch_date": "2026-01-01",
    "id": "20260101_582de585"
  },
  {
    "title": "The Endogenous Constraint: Hysteresis, Stagflation, and the Structural Inhibition of Monetary Velocity in the Bitcoin Network (2016-2025)",
    "url": "https://arxiv.org/pdf/2512.07886v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Bitcoin operates as a macroeconomic paradox: it combines a strictly predetermined, inelastic monetary issuance schedule with a stochastic, highly elastic demand for scarce block space. This paper empirically validates the Endogenous Constraint Hypothesis, positing that protocol-level throughput limits generate a non-linear negative feedback loop between network friction and base-layer monetary velocity. Using a verified Transaction Cost Index (TCI) derived from Blockchain.com on-chain data and Hansen's (2000) threshold regression, we identify a definitive structural break at the 90th percentile of friction (TCI ~ 1.63). The analysis reveals a bifurcation in network utility: while the network exhibits robust velocity growth of +15.44% during normal regimes, this collapses to +6.06% during shock regimes, yielding a statistically significant Net Utility Contraction of -9.39% (p = 0.012). Crucially, Instrumental Variable (IV) tests utilizing Hashrate Variation as a supply-side instrument fail to detect a significant relationship in a linear specification (p=0.196), confirming that the velocity constraint is strictly a regime-switching phenomenon rather than a continuous linear function. Furthermore, we document a \"Crypto Multiplier\" inversion: high friction correlates with a +8.03% increase in capital concentration per entity, suggesting that congestion forces a substitution from active velocity to speculative hoarding.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "比特币网络的内生约束：滞后效应、滞胀与货币流通速度的结构性抑制（2016-2025）。本文实证验证了内生约束假说，认为协议层面的吞吐量限制在网络摩擦与基础层货币流通速度之间形成了非线性负反馈循环。通过使用基于Blockchain.com链上数据验证的交易成本指数（TCI）和Hansen（2000）的门槛回归，我们在摩擦的90百分位数（TCI约1.63）处识别出明确的结构性断点。分析揭示了网络效用的分叉：在正常状态下，网络表现出+15.44%的强劲流通速度增长，而在冲击状态下则崩溃至+6.06%，产生了统计显著的净效用收缩-9.39%（p=0.012）。关键的是，利用哈希率变化作为供给侧工具变量的工具变量（IV）测试在线性规范中未检测到显著关系（p=0.196），证实了流通速度约束严格是一种状态转换现象，而非连续线性函数。",
    "fetch_date": "2026-01-01",
    "id": "20260101_95084b35"
  },
  {
    "title": "Equilibrium Investment with Random Risk Aversion: (Non-)uniqueness, Optimality, and Comparative Statics",
    "url": "https://arxiv.org/pdf/2512.00830v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "This paper investigates infinite-dimensional portfolio selection problem under a general distribution of the risk aversion parameter. We provide a complete characterization of all deterministic equilibrium investment strategies. Our results reveal that the solution structure depends critically on the distribution of risk aversion: the equilibrium is unique whenever it exists in the case of finite expected risk aversion, whereas an infinite expectation can lead to infinitely many equilibria or to a unique trivial one (pi equals 0). To address this multiplicity, we introduce three optimality criteria-optimal, uniformly optimal, and uniformly strictly optimal-and explicitly characterize the existence and uniqueness of the corresponding equilibria. Under the same necessary and sufficient condition, the optimal and uniformly optimal equilibria exist uniquely and coincide. Furthermore, by additionally assuming that the market price of risk is non-zero near the terminal time, we show that the optimal (and hence uniformly optimal) equilibrium is also uniformly strictly optimal. Finally, we perform comparative statics to demonstrate that a risk aversion distribution dominating another in the reverse hazard rate order leads to a less aggressive equilibrium strategy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在风险厌恶参数服从一般分布下的无限维投资组合选择问题。我们完整刻画了所有确定性均衡投资策略。结果表明，解的结构关键取决于风险厌恶的分布：当期望风险厌恶有限时，均衡存在则唯一；而无限期望可能导致无穷多均衡或唯一平凡均衡（π=0）。针对多重性，我们引入了三种最优性准则——最优、一致最优和一致严格最优——并明确刻画了相应均衡的存在性与唯一性。在相同充要条件下，最优与一致最优均衡存在唯一且重合。进一步假设市场风险价格在终端时刻附近非零，我们证明最优（从而一致最优）均衡也是一致严格最优的。最后，通过比较静态分析表明，若一个风险厌恶分布随机占优于另一个，则其均衡投资策略在随机占优意义下更大。",
    "fetch_date": "2026-01-01",
    "id": "20260101_33a484bf"
  },
  {
    "title": "Convergence Rates of Turnpike Theorems for Portfolio Choice in Stochastic Factor Models",
    "url": "https://arxiv.org/pdf/2512.00346v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Turnpike theorems state that if an investor's utility is asymptotically equivalent to a power utility, then the optimal investment strategy converges to the CRRA strategy as the investment horizon tends to infinity. This paper aims to derive the convergence rates of the turnpike theorem for optimal feedback functions in stochastic factor models. In these models, optimal feedback functions can be decomposed into two terms: myopic portfolios and excess hedging demands. We obtain convergence rates for myopic portfolios in nonlinear stochastic factor models and for excess hedging demands in quadratic term structure models, where the interest rate is a quadratic function of a multivariate Ornstein-Uhlenbeck process. We show that the convergence rates are determined by (i) the decay speed of the price of a zero-coupon bond and (ii) how quickly the investor's utility becomes power-like at high levels of wealth. As an application, we consider optimal collective investment problems and show that sharing rules for terminal wealth affect convergence rates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了随机因子模型中投资组合选择的“大道定理”收敛速率。大道定理指出，若投资者效用渐近等价于幂效用，则最优投资策略在投资期限趋于无穷时收敛于CRRA策略。论文旨在推导随机因子模型中最优反馈函数收敛速率的理论结果，将最优反馈分解为短视组合与超额对冲需求两部分，并在非线性随机因子模型和二次期限结构模型中分别获得其收敛速率。收敛速率取决于（i）零息债券价格的衰减速度，以及（ii）投资者效用在高财富水平下趋近幂效用的速度。作为应用，论文探讨了最优集体投资问题，表明终端财富的分享规则会影响收敛速率。",
    "fetch_date": "2026-01-01",
    "id": "20260101_fd006166"
  },
  {
    "title": "Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network",
    "url": "https://arxiv.org/pdf/2512.00299v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sion's maxmin theorem. For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在随机占优约束下的S型非凹效用最大化静态投资组合选择问题。主要贡献包括：1）用一阶随机占优约束替代清算边界要求，为风险管理提供替代方案；2）提出数值算法检测二阶随机占优约束下的表现不佳区域，通过修改该区域分布获得（次）优解。核心金融洞见是：决策者应在表现不佳情景中遵循随机占优约束，在表现良好情景中追求效用最大化。该研究属于理论优化方法，未涉及强化学习/深度学习/Alpha生成等实战交易技术。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23e46ee2"
  },
  {
    "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.23515v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "Alpha-R1：通过强化学习实现基于LLM推理的Alpha因子筛选。信号衰减和市场机制转换是数据驱动投资策略在非平稳市场中面临的持续挑战。传统时间序列和机器学习方法主要依赖历史相关性，当经济环境变化时往往难以泛化。虽然大语言模型在处理非结构化信息方面具有强大能力，但其通过明确的经济推理支持量化因子筛选的潜力尚未充分开发。现有基于因子的方法通常将Alpha简化为数值时间序列，忽略了决定因子何时具有经济相关性的语义逻辑。我们提出Alpha-R1，一个通过强化学习训练的80亿参数推理模型，用于上下文感知的Alpha筛选。Alpha-R1基于因子逻辑和实时新闻进行推理，评估不断变化的市场条件下的Alpha相关性，根据上下文一致性选择性地激活或停用因子。在多个资产池中的实证结果表明，Alpha-R1持续优于基准策略，并表现出对Alpha衰减的改进鲁棒性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_7845ee87"
  },
  {
    "title": "Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading",
    "url": "https://arxiv.org/pdf/2512.02227v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, while the S&P 500 index yielded a return of $15.97\\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, whereas the BTC price increased by $3.80\\%$. Our code is available on \\href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场因其时间动态性和低信噪比特性，成为AI智能体的关键应用领域。传统算法交易系统通常需要专业团队多年开发和测试。本文提出一个金融智能体编排框架，旨在将金融智能民主化给公众。该框架将传统算法交易系统的各个组件映射为智能体，包括规划器、编排器、阿尔法智能体、风险智能体、投资组合智能体、回测智能体、执行智能体、审计智能体和记忆智能体。论文展示了两个内部交易实例：在股票交易任务（2024年4月至12月每小时数据）中，该方法实现了20.42%的收益率、2.63的夏普比率和-3.59%的最大回撤，而同期标普500指数收益率为15.97%；在BTC交易任务（2025年7月27日至8月13日每分钟数据）中，实现了8.39%的收益率、0.38的夏普比率和-2.80%的最大回撤，而同期BTC价格上涨3.80%。代码已在GitHub开源。",
    "fetch_date": "2025-12-31",
    "id": "20251231_c175bb6a"
  },
  {
    "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction",
    "url": "https://arxiv.org/pdf/2512.23596v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究非平稳环境下股票收益预测的机器学习模型，揭示了一个根本性的非平稳性-复杂度权衡：复杂模型能减少设定误差，但需要更长的训练窗口，这会引入更强的非平稳性。作者通过一种新颖的模型选择方法解决了这一矛盾，该方法使用锦标赛程序联合优化模型类别和训练窗口大小，在非平稳验证数据上自适应评估候选模型。理论分析表明，该方法能平衡设定误差、估计方差和非平稳性，表现接近事后最佳模型。将该方法应用于17个行业投资组合收益，持续优于标准的滚动窗口基准，样本外R²平均提高14-23%。在NBER指定的衰退期间，改进尤为显著：例如，在海湾战争衰退期间，该方法实现了正的R²，而基准为负；在2001年衰退期间，R²绝对值至少提高80个基点；在2008年金融危机期间也表现优异。从经济角度看，基于该方法的交易策略具有实际应用价值。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbb43c5"
  },
  {
    "title": "Squeezed Covariance Matrix Estimation: Analytic Eigenvalue Control",
    "url": "https://arxiv.org/pdf/2512.23021v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "We revisit Gerber's Informational Quality (IQ) framework, a data-driven approach for constructing correlation matrices from co-movement evidence, and address two obstacles that limit its use in portfolio optimization: guaranteeing positive semidefinite ness (PSD) and controlling spectral conditioning. We introduce a squeezing identity that represents IQ estimators as a convex-like combination of structured channel matrices, and propose an atomic-IQ parameterization in which each channel-class matrix is built from PSD atoms with a single class-level normalization. This yields constructive PSD guarantees over an explicit feasibility region, avoiding reliance on ex-post projection. To regulate conditioning, we develop an analytic eigen floor that targets either a minimum eigenvalue or a desired condition number and, when necessary, repairs PSD violations in closed form while remaining compatible with the squeezing identity. In long-only tangency back tests with transaction costs, atomic-IQ improves out-of-sample Sharpe ratios and delivers a more stable risk profile relative to a broad set of standard covariance estimators.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视Gerber的信息质量（IQ）框架——一种基于数据驱动方法从共动证据构建相关矩阵的框架，并解决了限制其在投资组合优化中应用的两个障碍：保证正半定性（PSD）和控制谱条件数。作者引入了一种压缩恒等式，将IQ估计量表示为结构化通道矩阵的类凸组合，并提出了一种原子IQ参数化方法，其中每个通道类矩阵由具有单一类级别归一化的PSD原子构建。这在一个明确的可行区域内提供了构造性的PSD保证，避免依赖事后投影。为了调节条件数，作者开发了一种解析特征值下限方法，旨在实现最小特征值或期望的条件数，并在必要时以闭式形式修复PSD违规，同时保持与压缩恒等式的兼容性。在考虑交易成本的仅多头切线投资组合回测中，原子IQ方法相对于一系列标准协方差估计器，提高了样本外夏普比率，并提供了更稳定的风险特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_cefa3cff"
  },
  {
    "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.22895v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\\% higher Return, 5\\% higher Sharpe ratio, 5\\% higher Sortino ratio, and 2\\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "SAMP-HDRL：通过分层深度强化学习实现多智能体投资组合管理的分段分配与动量调整效用。该框架针对非平稳市场中的投资组合优化挑战（如制度转换、动态相关性和DRL策略可解释性有限），提出动态资产分组方法，将市场划分为高质量和普通子集。上层智能体提取全局市场信号，下层智能体在掩码约束下执行组内分配。基于效用的资本分配机制整合风险资产与无风险资产，确保全局与局部决策的协调一致。在三种市场制度（2019-2021）的回测中，SAMP-HDRL在波动和震荡条件下持续优于9个传统基准和9个DRL基准，相比最强基准至少实现5%的更高回报、夏普比率、索提诺比率和2%的更高欧米茄比率，在动荡市场中收益尤为显著。消融研究证实了各模块的有效性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_b5de9d1a"
  },
  {
    "title": "AutoQuant: An Auditable Expert-System Framework for Execution-Constrained Auto-Tuning in Cryptocurrency Perpetual Futures",
    "url": "https://arxiv.org/pdf/2512.22476v1",
    "source": "ArXiv",
    "date": "2025-12-27",
    "abstract": "Backtests of cryptocurrency perpetual futures are fragile when they ignore microstructure frictions and reuse evaluation windows during parameter search. We study four liquid perpetuals (BTC/USDT, ETH/USDT, SOL/USDT, AVAX/USDT) and quantify how execution delay, funding, fees, and slippage can inflate reported performance. We introduce AutoQuant, an execution-centric, alpha-agnostic framework for auditable strategy configuration selection. AutoQuant encodes strict T+1 execution semantics and no-look-ahead funding alignment, runs Bayesian optimization under realistic costs, and applies a two-stage double-screening protocol across held-out rolling windows and a cost-sensitivity grid. We show that fee-only and zero-cost backtests can materially overestimate annualized returns relative to a fully costed configuration, and that double screening tends to reduce drawdowns under the same strict semantics even when returns are not higher. A CSCV/PBO diagnostic indicates substantial residual overfitting risk, motivating AutoQuant as validation and governance infrastructure rather than a claim of persistent alpha. Returns are reported for small-account simulations with linear trading costs and without market impact or capacity modeling.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AutoQuant：一种用于加密货币永续期货执行约束自动调参的可审计专家系统框架》针对加密货币永续期货回测中忽略微观结构摩擦（如执行延迟、资金费、手续费、滑点）导致性能虚高的问题，提出以执行为中心、与阿尔法无关的可审计策略配置选择框架。该框架采用严格的T+1执行语义和无前瞻性资金费对齐，在真实成本下进行贝叶斯优化，并通过保留滚动窗口和成本敏感性网格的双阶段双重筛选协议。研究表明，仅考虑手续费和零成本的回测会显著高估年化收益，而双重筛选在相同严格语义下即使收益未提高也能降低回撤。CSCV/PBO诊断显示存在显著的残余过拟合风险，因此AutoQuant更适合作为验证和治理基础设施，而非声称持续阿尔法的工具。收益报告基于小账户模拟和线性交易成本。",
    "fetch_date": "2025-12-31",
    "id": "20251231_1842d9c9"
  },
  {
    "title": "Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling",
    "url": "https://arxiv.org/pdf/2512.21798v2",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance. This paper investigates the use of deep generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) to generate realistic synthetic financial return series for portfolio construction and risk modeling applications. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean--variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据为解决量化金融实证研究中的隐私、可访问性和可复现性挑战提供了实用方案。本文研究了深度生成模型，特别是时间序列生成对抗网络（TimeGAN）和变分自编码器（VAE），在生成用于投资组合构建和风险建模应用的现实合成金融收益序列方面的应用。以标普500的历史日收益率为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。研究表明，TimeGAN生成的合成数据在分布形状、波动模式和自相关行为方面接近真实收益。当应用于均值-方差投资组合优化时，所得合成数据集产生的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE提供更稳定的训练，但倾向于平滑极端市场波动，从而影响风险估计。",
    "fetch_date": "2025-12-31",
    "id": "20251231_3fd18e9d"
  },
  {
    "title": "Beyond Binary Screens: A Continuous Shariah Compliance Index for Asset Pricing and Portfolio Design",
    "url": "https://arxiv.org/pdf/2512.22858v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Binary Shariah screens vary across standards and apply hard thresholds that create discontinuous classifications. We construct a Continuous Shariah Compliance Index (CSCI) in $[0,1]$ by mapping standard screening ratios to smooth scores between conservative ``comfort'' bounds and permissive outer bounds, and aggregating them conservatively with a sectoral activity factor. Using CRSP/Compustat U.S. equities (1999-2024) with lagged accounting inputs and monthly rebalancing, we find that CSCI-based long-only portfolios have historical risk-adjusted performance similar to an emulated binary Islamic benchmark. Tightening the minimum compliance threshold reduces the investable universe and diversification and is associated with lower Sharpe ratios. The framework yields a practical compliance gradient that supports portfolio construction, constraint design, and cross-standard comparisons without reliance on pass/fail screening.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种连续伊斯兰教法合规指数（CSCI），用于替代传统的二元筛选方法。该指数在[0,1]区间内对标准筛选比率进行平滑评分，并通过行业活动因子进行保守聚合。基于美国股票数据（1999-2024年）的回测显示，CSCI构建的纯多头投资组合在历史风险调整后表现与模拟的二元伊斯兰基准相似。提高最低合规阈值会缩小可投资范围、降低分散化程度，并与较低的夏普比率相关。该框架提供了一种实用的合规梯度，支持投资组合构建、约束设计及跨标准比较，无需依赖通过/失败筛选。",
    "fetch_date": "2025-12-31",
    "id": "20251231_a943828e"
  },
  {
    "title": "Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%",
    "url": "https://arxiv.org/pdf/2512.03107v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出ECLIPSE框架，将AI幻觉视为模型语义熵与可用证据容量之间的不匹配。该框架结合多样本聚类的熵估计和一种新颖的困惑度分解方法，以衡量模型如何使用检索到的证据。在受控的金融问答数据集上，ECLIPSE实现了0.89的ROC AUC和0.90的平均精度，显著优于仅基于语义熵的基线（AUC 0.50）。然而，该方法的有效性依赖于校准的令牌级不确定性，在缺乏令牌级对数概率的模型上性能会显著下降。",
    "fetch_date": "2025-12-31",
    "id": "20251231_36769bcf"
  },
  {
    "title": "Broken Symmetry of Stock Returns -- a Modified Jones-Faddy Skew t-Distribution",
    "url": "https://arxiv.org/pdf/2512.23640v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We argue that negative skew and positive mean of the distribution of stock returns are largely due to the broken symmetry of stochastic volatility governing gains and losses. Starting with stochastic differential equations for stock returns and for stochastic volatility we argue that the distribution of stock returns can be effectively split in two -- for gains and losses -- assuming difference in parameters of their respective stochastic volatilities. A modified Jones-Faddy skew t-distribution utilized here allows to reflect this in a single organic distribution which tends to meaningfully capture this asymmetry. We illustrate its application on distribution of daily S&P500 returns, including analysis of its tails.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出股票收益率分布的负偏和正均值主要源于控制收益与损失的随机波动率对称性破缺。通过建立股票收益率和随机波动率的随机微分方程，作者认为可将收益率分布有效拆分为收益和损失两部分，并假设其各自随机波动率参数存在差异。采用的修正Jones-Faddy偏斜t分布能够以单一有机分布反映这种不对称性，并有效捕捉该特征。论文以标普500日收益率分布为例进行了应用展示，包括尾部分析。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbf4bea"
  },
  {
    "title": "Impact of Volatility on Time-Based Transaction Ordering Policies",
    "url": "https://arxiv.org/pdf/2512.23386v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了Arbitrum的快速通道拍卖（ELA），这是一种提前进行的次高价拍卖，获胜者将获得一分钟的独家延迟优势。基于风险厌恶投标人的单轮模型，我们提出一个假设：由于预测短期波动性的困难以及投标人的风险厌恶，优先访问的价值相对于风险中性估值有所折价。我们使用与高频ETH价格匹配的ELA投标记录来测试这些预测，发现结果与模型一致。",
    "fetch_date": "2025-12-31",
    "id": "20251231_688d9f03"
  },
  {
    "title": "Lambda Expected Shortfall",
    "url": "https://arxiv.org/pdf/2512.23139v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "The Lambda Value-at-Risk (Lambda$-VaR) is a generalization of the Value-at-Risk (VaR), which has been actively studied in quantitative finance. Over the past two decades, the Expected Shortfall (ES) has become one of the most important risk measures alongside VaR because of its various desirable properties in the practice of optimization, risk management, and financial regulation. Analogously to the intimate relation between ES and VaR, we introduce the Lambda Expected Shortfall (Lambda-ES), as a generalization of ES and a counterpart to Lambda-VaR. Our definition of Lambda-ES has an explicit formula and many convenient properties, and we show that it is the smallest quasi-convex and law-invariant risk measure dominating Lambda-VaR under mild assumptions. We examine further properties of Lambda-ES, its dual representation, and related optimization problems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "Lambda期望损失（Lambda-ES）是期望损失（ES）的推广，作为Lambda风险价值（Lambda-VaR）的对应概念。该定义具有显式公式和多种便利性质，在温和假设下是最小化Lambda-VaR的拟凸且律不变风险度量。论文探讨了Lambda-ES的进一步性质、对偶表示及相关优化问题。",
    "fetch_date": "2025-12-31",
    "id": "20251231_6138efd6"
  },
  {
    "title": "A Note on the Conditions for COS Convergence",
    "url": "https://arxiv.org/pdf/2512.02745v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We study the truncation error of the COS method and give simple, verifiable conditions that guarantee convergence. In one dimension, COS is admissible when the density belongs to both L1 and L2 and has a finite weighted L2 moment of order strictly greater than one. We extend the result to multiple dimensions by requiring the moment order to exceed the dimension. These conditions enlarge the class of densities covered by previous analyses and include heavy-tailed distributions such as Student t with small degrees of freedom.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了COS方法的截断误差，并给出了保证收敛的简单可验证条件。在一维情况下，当密度函数同时属于L1和L2空间且具有严格大于1阶的有限加权L2矩时，COS方法是可接受的。通过要求矩的阶数超过维度，我们将结果扩展到多维情况。这些条件扩大了先前分析所涵盖的密度函数类别，包括具有小自由度的Student t等重尾分布。",
    "fetch_date": "2025-12-31",
    "id": "20251231_8fd2e7eb"
  },
  {
    "title": "Visibility-Graph Asymmetry as a Structural Indicator of Volatility Clustering",
    "url": "https://arxiv.org/pdf/2512.02352v2",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Volatility clustering is one of the most robust stylized facts of financial markets, yet it is typically detected using moment-based diagnostics or parametric models such as GARCH. This paper shows that clustered volatility also leaves a clear imprint on the time-reversal symmetry of horizontal visibility graphs (HVGs) constructed on absolute returns in physical time. For each time point, we compute the maximal forward and backward visibility distances, $L^{+}(t)$ and $L^{-}(t)$, and use their empirical distributions to build a visibility-asymmetry fingerprint comprising the Kolmogorov--Smirnov distance, variance difference, entropy difference, and a ratio of extreme visibility spans. In a Monte Carlo study, these HVG asymmetry features sharply separate volatility-clustered GARCH(1,1) dynamics from i.i.d.\\ Gaussian noise and from randomly shuffled GARCH series that preserve the marginal distribution but destroy temporal dependence; a simple linear classifier based on the fingerprint achieves about 90\\% in-sample accuracy. Applying the method to daily S\\&P500 data reveals a pronounced forward--backward imbalance, including a variance difference $Δ\\mathrm{Var}$ that exceeds the simulated GARCH values by two orders of magnitude and vanishes after shuffling. Overall, the visibility-graph asymmetry fingerprint emerges as a simple, model-free, and geometrically interpretable indicator of volatility clustering and time irreversibility in financial time series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种基于水平可见性图（HVG）非对称性的新方法来检测金融市场中的波动率聚集现象。通过计算绝对收益序列在物理时间上的前向和后向最大可见距离，并构建包含Kolmogorov-Smirnov距离、方差差、熵差和极端可见跨度比率的非对称性指纹，该方法能够有效区分具有波动率聚集的GARCH(1,1)动态与独立同分布的高斯噪声。在蒙特卡洛研究中，基于该指纹的简单线性分类器实现了约90%的样本内准确率。应用于标普500日度数据时，该方法揭示了显著的前后向不平衡特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_0d518006"
  },
  {
    "title": "The Three-Dimensional Decomposition of Volatility Memory",
    "url": "https://arxiv.org/pdf/2512.02166v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper develops a three-dimensional decomposition of volatility memory into orthogonal components of level, shape, and tempo. The framework unifies regime-switching, fractional-integration, and business-time approaches within a single canonical representation that identifies how each dimension governs persistence strength, long-memory form, and temporal speed. We establish conditions for existence, uniqueness, and ergodicity of this decomposition and show that all GARCH-type processes arise as special cases. Empirically, applications to SPY and EURUSD (2005--2024) reveal that volatility memory is state-dependent: regime and tempo gates dominate in equities, while fractional-memory gates prevail in foreign exchange. The unified tri-gate model jointly captures these effects. By formalizing volatility dynamics through a level--shape--tempo structure, the paper provides a coherent link between information flow, market activity, and the evolving memory of financial volatility.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种将波动率记忆分解为水平、形态和节奏三个正交维度的三维分解框架。该框架在单一规范表示中统一了机制转换、分数积分和业务时间方法，识别了每个维度如何控制持续性强度、长记忆形式和时间速度。我们建立了该分解的存在性、唯一性和遍历性条件，并证明所有GARCH类过程都是其特例。对SPY和EURUSD（2005-2024）的实证应用表明，波动率记忆具有状态依赖性：机制和节奏维度在股票市场中占主导，而分数记忆维度在外汇市场中更为显著。统一的“三闸门”模型共同捕捉了这些效应。通过将波动率动态形式化为水平-形态-节奏结构，本文为信息流、市场活动和金融波动率演化记忆之间提供了连贯的联系。",
    "fetch_date": "2025-12-31",
    "id": "20251231_587625c2"
  },
  {
    "title": "Hidden Order in Trades Predicts the Size of Price Moves",
    "url": "https://arxiv.org/pdf/2512.15720v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p < 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场呈现一个明显悖论：虽然方向性价格变动基本不可预测（符合弱式有效市场假说），但价格变动幅度却显示出系统性结构。本文证明，基于15状态马尔可夫转移矩阵以秒级分辨率计算的实时订单流熵，能够预测日内收益的幅度而不提供方向性信息。对36个交易日内3850万笔SPY交易的分析显示，在熵低于第5百分位的条件下，后续5分钟绝对收益增加2.89倍（t=12.41，p<0.0001），而方向性准确率保持在45.0%——与随机水平无统计学差异（p=0.12）。这种解耦源于基本对称性：熵在符号置换下保持不变，可检测知情交易的存在而不揭示其方向。五个非重叠测试期的前向验证确认了样本外可预测性，标签置换安慰剂检验得出z=14.4。这些发现表明，信息论度量可作为市场微观结构中的波动率状态变量。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a096c37e"
  },
  {
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
    "url": "https://arxiv.org/pdf/2512.21878v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "MASFIN是一个模块化多智能体框架，将大型语言模型（LLMs）与结构化金融指标和非结构化新闻相结合，同时嵌入明确的偏差缓解协议。该系统利用GPT-4.1-nano实现可重复性和成本效益推理，生成包含15-30只股票的周度投资组合，其配置权重针对短期表现进行了优化。在为期八周的评估中，MASFIN实现了7.33%的累计回报，在八周中的六周内表现优于标普500、纳斯达克100和道琼斯基准，尽管波动性较高。这些发现展示了具有偏差意识的生成式AI框架在金融预测中的潜力，并凸显了模块化多智能体设计在推进实用、透明和可重复方法方面的机遇。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0237ad76"
  },
  {
    "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling",
    "url": "https://arxiv.org/pdf/2512.21791v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种统一的合成金融数据多标准评估框架，并应用于三种代表性生成范式：统计ARIMA-GARCH基线、变分自编码器（VAEs）和时间序列生成对抗网络（TimeGAN）。基于标普500历史日数据，评估了保真度（最大均值差异MMD）、时间结构（自相关和波动率聚集）以及在下游任务中的实际效用，特别是均值-方差投资组合优化和波动率预测。实证结果表明：ARIMA-GARCH能捕捉线性趋势和条件波动率但无法复现非线性动态；VAEs生成平滑轨迹但低估极端事件；TimeGAN在真实性和时间连贯性之间取得了最佳平衡（如TimeGAN获得最低MMD：1.84e-3，5次种子平均）。最后，论文根据应用需求和计算约束提出了生成模型选择的实用指南。统一的评估协议和可复现性为实战交易中解决数据稀缺和保密性问题提供了直接价值，特别是对需要大量数据进行模型训练和压力测试的量化策略开发。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a7fa7b3b"
  },
  {
    "title": "Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification",
    "url": "https://arxiv.org/pdf/2512.22109v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We study the construction and rebalancing of sparse index-tracking portfolios from an operational research perspective, with explicit emphasis on uncertainty quantification and implementability. The decision variables are portfolio weights constrained to sum to one; the aims are to track a reference index closely while controlling the number of names and the turnover induced by rebalancing. We cast index tracking as a high-dimensional linear regression of index returns on constituent returns, and employ a sparsity-inducing Laplace prior on the weights. A single global shrinkage parameter controls the trade-off between tracking error and sparsity, and is calibrated by an empirical-Bayes stochastic approximation scheme. Conditional on this calibration, we approximate the posterior distribution of the portfolio weights using proximal Langevin-type Markov chain Monte Carlo algorithms tailored to the budget constraint. This yields posterior uncertainty on tracking error, portfolio composition and prospective rebalancing moves. Building on these posterior samples, we propose rules for rebalancing that gate trades through magnitude-based thresholds and posterior activation probabilities, thereby trading off expected tracking error against turnover and portfolio size. A case study on tracking the S&P~500 index is carried out to showcase how our tools shape the decision process from portfolio construction to rebalancing.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文从运筹学视角研究稀疏指数跟踪投资组合的构建与再平衡，重点强调不确定性量化与可实施性。决策变量为总和为1的投资组合权重；目标是在控制持仓数量与再平衡引起的换手率的同时，紧密跟踪基准指数。作者将指数跟踪建模为指数收益对成分股收益的高维线性回归，并对权重施加诱导稀疏性的拉普拉斯先验。单一全局收缩参数控制跟踪误差与稀疏性之间的权衡，并通过经验贝叶斯随机逼近方案进行校准。基于此校准，作者采用针对预算约束设计的近似近端朗之万型马尔可夫链蒙特卡洛算法，近似投资组合权重的后验分布，从而获得关于跟踪误差、投资组合构成及预期再平衡操作的后验不确定性。基于这些后验样本，作者提出了通过基于幅度的阈值和后验激活概率来门控交易的再平衡规则。",
    "fetch_date": "2025-12-30",
    "id": "20251230_7977417e"
  },
  {
    "title": "Applications of synthetic financial data in portfolio and risk modeling",
    "url": "https://arxiv.org/pdf/2512.21798v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data offers a practical way to address the privacy and accessibility challenges that limit research in quantitative finance. This paper examines the use of generative models, in particular TimeGAN and Variational Autoencoders (VAEs), for creating synthetic return series that support portfolio construction, trading analysis, and risk modeling. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean-variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据在投资组合和风险建模中的应用。该论文探讨了使用生成模型（特别是TimeGAN和变分自编码器VAE）创建合成收益率序列，以支持投资组合构建、交易分析和风险建模。研究以标普500历史日收益率作为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。结果表明，TimeGAN生成的合成数据在分布形态、波动率模式和自相关行为方面接近真实收益率。应用于均值-方差投资组合优化时，合成数据集得出的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE训练更稳定但倾向于平滑极端市场波动，从而影响风险估计。分析支持将合成数据集作为真实金融数据的替代品，用于解决隐私和可访问性限制。",
    "fetch_date": "2025-12-30",
    "id": "20251230_c2ad24df"
  },
  {
    "title": "FX Market Making with Internal Liquidity",
    "url": "https://arxiv.org/pdf/2512.04603v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "As the FX markets continue to evolve, many institutions have started offering passive access to their internal liquidity pools. Market makers act as principal and have the opportunity to fill those orders as part of their risk management, or they may choose to adjust pricing to their external OTC franchise to facilitate the matching flow. It is, a priori, unclear how the strategies managing internal liquidity should depend on market condions, the market maker's risk appetite, and the placement algorithms deployed by participating clients. The market maker's actions in the presence of passive orders are relevant not only for their own objectives, but also for those liquidity providers who have certain expectations of the execution speed. In this work, we investigate the optimal multi-objective strategy of a market maker with an option to take liquidity on an internal exchange, and draw important qualitative insights for real-world trading.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "随着外汇市场不断发展，许多机构开始提供对其内部流动性池的被动访问。做市商作为主体，有机会将这些订单作为其风险管理的一部分进行成交，或者他们可以选择调整其外部场外交易特许经营的价格以促进匹配流动。从先验角度看，管理内部流动性的策略应如何依赖市场条件、做市商的风险偏好以及参与客户部署的配置算法尚不明确。做市商在被动订单存在下的行为不仅与其自身目标相关，也与对执行速度有特定预期的流动性提供者相关。在本研究中，我们探讨了做市商在可选择从内部交易所获取流动性时的最优多目标策略，并为实际交易提供了重要的定性见解。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4668657e"
  },
  {
    "title": "Investigating Conditional Restricted Boltzmann Machines in Regime Detection",
    "url": "https://arxiv.org/pdf/2512.21823v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "This study investigates the efficacy of Conditional Restricted Boltzmann Machines (CRBMs) for modeling high-dimensional financial time series and detecting systemic risk regimes. We extend the classical application of static Restricted Boltzmann Machines (RBMs) by incorporating autoregressive conditioning and utilizing Persistent Contrastive Divergence (PCD) to incorporate complex temporal dependency structures. Comparing a discrete Bernoulli-Bernoulli architecture against a continuous Gaussian-Bernoulli variant across a multi-asset dataset spanning 2013-2025, we observe a dichotomy between generative fidelity and regime detection. While the Gaussian CRBM successfully preserves static asset correlations, it exhibits limitations in generating long-range volatility clustering. Thus, we analyze the free energy as a relative negative log-likelihood (surprisal) under a fixed, trained model. We demonstrate that the model's free energy serves as a robust, regime stability metric. By decomposing the free energy into quadratic (magnitude) and structural (correlation) components, we show that the model can distinguish between pure magnitude shocks and market regimes. Our findings suggest that the CRBM offers a valuable, interpretable diagnostic tool for monitoring systemic risk, providing a supplemental metric to implied volatility metrics like the VIX.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了条件受限玻尔兹曼机（CRBMs）在建模高维金融时间序列和检测系统性风险状态方面的有效性。通过引入自回归条件和使用持续对比散度（PCD），扩展了静态受限玻尔兹曼机（RBMs）的经典应用，以纳入复杂的时间依赖结构。在2013-2025年跨资产数据集中，比较离散伯努利-伯努利架构与连续高斯-伯努利变体，观察到生成保真度与状态检测之间的二分法。虽然高斯CRBM成功保留了静态资产相关性，但在生成长程波动率聚类方面存在局限性。因此，我们在固定训练模型下分析自由能作为相对负对数似然（惊奇度）。研究表明，模型的自由能可作为稳健的状态稳定性度量。通过将自由能分解为二次（幅度）和结构（相关性）分量，模型能够区分纯幅度冲击和市场状态。",
    "fetch_date": "2025-12-30",
    "id": "20251230_137a0b37"
  },
  {
    "title": "S&P 500 Stock's Movement Prediction using CNN",
    "url": "https://arxiv.org/pdf/2512.21804v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].\n  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文使用卷积神经网络（CNN）预测标普500指数成分股的股价走势，创新之处在于直接利用包含股票拆分/股息事件的多维原始市场数据，而非传统工程化金融数据。虽然CNN在图像分类领域表现优异，但论文未深入探讨金融数据的复杂性，且主要基于单维度数据，对实战交易的价值有限，更多是为未来研究奠定基础。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4893f447"
  },
  {
    "title": "A High-Level Framework for Practically Model-Independent Pricing",
    "url": "https://arxiv.org/pdf/2512.15718v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We present a high-level framework that explains why, in practice, different pricing models calibrated to the same vanilla surface tend to produce similar valuations for exotic derivatives. Our approach acts as an overlay on the Monte Carlo infrastructure already used in banks, combining path reweighting with a conic optimisation layer without requiring any changes to existing code. This construction delivers narrow, practically model-independent price bands for exotics, reconciling front-office practice with the robust, model-independent ideas developed in the academic literature.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种高层框架，解释了为何在实践中，校准至相同普通期权曲面的不同定价模型往往对奇异衍生品给出相近的估值。该方法作为银行现有蒙特卡洛基础设施的覆盖层，结合路径重加权与锥优化层，无需修改现有代码。该框架为奇异品生成狭窄、近乎模型独立的价格区间，将前台实践与学术文献中发展的稳健、模型独立理念相统一。",
    "fetch_date": "2025-12-30",
    "id": "20251230_877a57bf"
  },
  {
    "title": "Enhancing trading strategies: a multi-indicator analysis for profitable algorithmic trading",
    "url": "https://link.springer.com/article/10.1007/s10614-024-10669-3",
    "source": "Scholar",
    "date": "2025-12-30",
    "abstract": "… backtesting to compare the strategy's historical performance to benchmarks. The … algorithmic trading models. This research contributes to understanding of algorithmic trading strategies …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该论文通过多指标分析增强交易策略，采用回测方法比较策略历史表现与基准，为理解算法交易模型提供了理论贡献。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0b3c59aa"
  },
  {
    "title": "Variational Quantum Eigensolver for Real-World Finance: Scalable Solutions for Dynamic Portfolio Optimization Problems",
    "url": "https://arxiv.org/pdf/2512.22001v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We present a scalable, hardware-aware methodology for extending the Variational Quantum Eigensolver (VQE) to large, realistic Dynamic Portfolio Optimization (DPO) problems. Building on the scaling strategy from our previous work, where we tailored a VQE workflow to both the DPO formulation and the target QPU, we now put forward two significant advances. The first is the implementation of the Ising Sample-based Quantum Configuration Recovery (ISQR) routine, which improves solution quality in Quadratic Unconstrained Binary Optimization problems. The second is the use of the VQE Constrained method to decompose the optimization task, enabling us to handle DPO instances with more variables than the available qubits on current hardware. These advances, which are broadly applicable to other optimization problems, allow us to address a portfolio with a size relevant to the financial industry, consisting of up to 38 assets and covering the full Spanish stock index (IBEX 35). Our results, obtained on a real Quantum Processing Unit (IBM Fez), show that this tailored workflow achieves financial performance on par with classical methods while delivering a broader set of high-quality investment strategies, demonstrating a viable path towards obtaining practical advantage from quantum optimization in real financial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种可扩展、硬件感知的方法，将变分量子本征求解器（VQE）应用于大规模、现实的动态投资组合优化（DPO）问题。通过引入伊辛样本量子配置恢复（ISQR）例程和改进的VQE约束方法，该研究成功处理了包含多达38种资产（覆盖西班牙IBEX 35指数）的投资组合，并在真实量子处理单元（IBM Fez）上实现了与经典方法相当的财务绩效。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4c468636"
  },
  {
    "title": "Mean-Field Price Formation on Trees with a Network of Relative Performance Concerns",
    "url": "https://arxiv.org/pdf/2512.21621v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Financial firms and institutional investors are routinely evaluated based on their performance relative to their peers. These relative performance concerns significantly influence risk-taking behavior and market dynamics. While the literature studying Nash equilibrium under such relative performance competitions is extensive, its effect on asset price formation remains largely unexplored. This paper investigates mean-field equilibrium price formation of a single risky stock in a discrete-time market where agents exhibit exponential utility and relative performance concerns. Unlike existing literature that typically treats asset prices as exogenous, we impose a market-clearing condition to determine the price dynamics endogenously within a relative performance equilibrium. Using a binomial tree framework, we establish the existence and uniqueness of the market-clearing mean-field equilibrium in both single- and multi-population settings. Finally, we provide illustrative numerical examples demonstrating the equilibrium price distributions and agents' optimal position sizes.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "金融机构和机构投资者通常根据其相对于同行的表现进行评估。这些相对绩效关注显著影响风险承担行为和市场动态。虽然研究此类相对绩效竞争下纳什均衡的文献广泛，但其对资产价格形成的影响在很大程度上仍未得到探索。本文研究了在离散时间市场中，具有指数效用和相对绩效关注的单一风险股票的平均场均衡价格形成。与通常将资产价格视为外生变量的现有文献不同，我们施加了市场出清条件，以在相对绩效均衡内内生地确定价格动态。使用二叉树框架，我们建立了单群体和多群体设置下市场出清平均场均衡的存在性和唯一性。最后，我们提供了说明性的数值示例，展示了均衡价格分布和代理人的最优头寸规模。",
    "fetch_date": "2025-12-30",
    "id": "20251230_f73cd03a"
  },
  {
    "title": "A Co-evolutionary Approach for Heston Calibration",
    "url": "https://arxiv.org/pdf/2512.03922v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "We evaluate a co-evolutionary calibration framework for the Heston model in which a genetic algorithm (GA) over parameters is coupled to an evolving neural inverse map from option surfaces to parameters. While GA-history sampling can reduce training loss quickly and yields strong in-sample fits to the target surface, learning-curve diagnostics show a widening train--validation gap across generations, indicating substantial overfitting induced by the concentrated and less diverse dataset. In contrast, a broad, space-filling dataset generated via Latin hypercube sampling (LHS) achieves nearly comparable calibration accuracy while delivering markedly better out-of-sample stability across held-out surfaces. These results suggest that apparent improvements from co-evolutionary data generation largely reflect target-specific specialization rather than a more reliable global inverse mapping, and that maintaining dataset diversity is critical for robust amortized calibration.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文评估了一种用于Heston模型校准的协同进化框架，其中参数遗传算法（GA）与从期权曲面到参数的进化神经逆映射相结合。研究发现，虽然GA历史采样能快速降低训练损失并在样本内对目标曲面实现强拟合，但学习曲线诊断显示代际间训练-验证差距扩大，表明由集中且多样性不足的数据集引发了显著过拟合。相比之下，通过拉丁超立方采样（LHS）生成的广泛、空间填充数据集实现了几乎相当的校准精度，同时在保留曲面上表现出明显更好的样本外稳定性。这些结果表明，协同进化数据生成带来的明显改进主要反映了针对特定目标的专门化，而非更可靠的全局逆映射，且保持数据集多样性对于稳健的摊销校准至关重要。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8a431d9a"
  },
  {
    "title": "The Effect of High-Speed Rail Connectivity on Capital Market Earnings Forecast Error: Evidence from the Chinese Stock Market",
    "url": "https://arxiv.org/pdf/2512.03709v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "This study examines how China's high-speed rail (HSR) expansion affects analyst earnings forecast errors from an economic information friction perspective. Using firm-year panel data from 2008-2019, a period that covers HSR's early introduction and rapid nationwide rollout, the findings show that analysts' relative earnings forecast errors (RFE) decline significantly only after firms' cities become connected by high-speed rail. The placebo test, which artificially shifts HSR connectivity 3 years earlier than the actual opening year, yields an insignificant DID coefficient, rejecting the possibility that forecast errors were improving before the infrastructure shock. This supports the conclusion that forecast error reduction is linked to real geographic accessibility improvements rather than coincidence, pre-existing trends, or analyst anticipation. Economically, the study highlights that HSR reduces analysts' costs of gathering private, incremental information, particularly soft information obtained via plant or management visits. The rail network does not directly alter firms' internal capital allocation or earnings generation paths, but it lowers spatial barriers to information collection, enabling analysts to update EPS expectations under reduced travel friction. This work provides intuitive evidence that geography and mobility improvements contribute to forecasting accuracy in China's emerging, decentralized capital market corridors, and it encourages future research to consider transport accessibility as an exogenous information cost shock rather than an internal firm-capital shock.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究从经济信息摩擦视角，探讨中国高速铁路（HSR）扩张如何影响分析师盈利预测误差。利用2008-2019年企业年度面板数据（覆盖HSR早期引入和全国快速推广期），研究发现仅在企业所在城市通过高铁连接后，分析师的相对盈利预测误差（RFE）显著下降。安慰剂测试（人为将HSR连通时间提前3年）显示DID系数不显著，排除了预测误差在基础设施冲击前已改善的可能性，支持预测误差减少与真实地理可达性改善相关，而非巧合、既有趋势或分析师预期。经济意义上，研究强调HSR降低了分析师收集私有增量信息（特别是通过工厂或管理层访问获取的软信息）的成本。铁路网络并未直接改变企业内部资本配置或盈利生成路径，但降低了信息获取的空间壁垒。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8749e3da"
  },
  {
    "title": "A Stochastic Thermodynamics Approach to Price Impact and Round-Trip Arbitrage: Theory and Empirical Implications",
    "url": "https://arxiv.org/pdf/2512.03123v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "This paper develops a comprehensive theoretical framework that imports concepts from stochastic thermodynamics to model price impact and characterize the feasibility of round-trip arbitrage in financial markets. A trading cycle is treated as a non-equilibrium thermodynamic process, where price impact represents dissipative work and market noise plays the role of thermal fluctuations. The paper proves a Financial Second Law: under general convex impact functionals, any round-trip trading strategy yields non-positive expected profit. This structural constraint is complemented by a fluctuation theorem that bounds the probability of profitable cycles in terms of dissipated work and market volatility. The framework introduces a statistical ensemble of trading strategies governed by a Gibbs measure, leading to a free energy decomposition that connects expected cost, strategy entropy, and a market temperature parameter. The framework provides rigorous, testable inequalities linking microstructural impact to macroscopic no-arbitrage conditions, offering a novel physics-inspired perspective on market efficiency. The paper derives explicit analytical results for prototypical trading strategies and discusses empirical validation protocols.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个将随机热力学概念引入金融市场的理论框架，用于建模价格影响并分析往返套利的可行性。交易周期被视为非平衡热力学过程，其中价格影响代表耗散功，市场噪声类比热涨落。论文证明了“金融第二定律”：在一般凸影响泛函下，任何往返交易策略的期望利润均为非正。该结构约束辅以一个涨落定理，将盈利周期的概率与耗散功和市场波动率联系起来。框架引入了由吉布斯测度支配的交易策略统计系综，导出了连接期望成本、策略熵和市场温度参数的自由能分解。该框架提供了连接微观结构影响与宏观无套利条件的严格可检验不等式，为市场效率提供了新颖的物理学视角。论文针对典型交易策略推导了显式解析结果。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a03b3233"
  },
  {
    "title": "The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading",
    "url": "https://arxiv.org/pdf/2512.15732v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the \"Holy Grail\" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of \"Galaxy Empire,\" a hybrid framework coupling LSTM/Transformer-based perception with a genetic \"Time-is-Life\" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\\%$) and live performance (Capital Decay $>70\\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \\textit{Aleatoric Uncertainty} in low-entropy time-series, the \\textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《红皇后的陷阱：高频交易中深度进化的局限》对实战交易具有重要警示价值。该论文对名为“银河帝国”的混合框架（结合LSTM/Transformer感知与遗传“时间即生命”生存机制）进行了严谨的事后分析。在加密货币高频环境中部署500个自主代理后，观察到训练指标（验证年化收益率>300%）与实际表现（资本衰减>70%）之间的灾难性背离。研究通过多学科视角解构了失败原因，识别出三个关键失效模式：低熵时间序列中偶然不确定性的过拟合、高方差下进化选择固有的幸存者偏差，以及缺乏订单流数据时无法克服微观结构摩擦的数学不可能性。研究结果为模型复杂性增加在缺乏信息不对称时加剧系统脆弱性提供了实证证据。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f1a0b406"
  },
  {
    "title": "Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks",
    "url": "https://arxiv.org/pdf/2512.05868v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文《利用脉冲神经网络预测高频金融数据中的价格变动》针对高频交易环境中传统模型难以捕捉的毫秒级价格尖峰问题，提出了一种生物启发的解决方案。研究将高频股票数据转换为脉冲序列，并评估了三种架构：基于无监督STDP训练的经典脉冲神经网络、具有显式抑制竞争的新型脉冲神经网络，以及监督式反向传播网络。通过贝叶斯优化结合新颖的惩罚性脉冲准确率目标函数进行超参数调优，确保模型预测的价格尖峰率与实证事件率一致。模拟交易显示，经PSA优化的模型在性能上持续优于仅使用脉冲准确率调优的对比模型及基线方法。",
    "fetch_date": "2025-12-29",
    "id": "20251229_8824f09c"
  },
  {
    "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.05753v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "针对现代战争中认知雷达快速部署以对抗干扰的关键挑战，本文提出了一种基于深度强化学习的快速抗干扰雷达部署算法（FARDA）。现有方法主要基于进化算法，耗时且易陷入局部最优。FARDA通过神经网络高效推理，将雷达部署建模为端到端任务，设计了集成神经模块感知热图信息和新奖励格式的深度强化学习算法。实证结果表明，该方法在达到与进化算法相当覆盖范围的同时，部署速度提升约7000倍。消融实验验证了FARDA各组件的必要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5a33edc9"
  },
  {
    "title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming",
    "url": "https://arxiv.org/pdf/2512.15735v3",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种将强化学习（RL）控制器与抗扰扩展状态观测器（ESO）相结合的统一控制架构，并辅以事件触发机制（ETM）以减少不必要的计算。ESO用于实时估计系统状态和集总扰动，为有效扰动补偿奠定基础。为在缺乏精确系统描述的情况下获得近似最优行为，采用基于值迭代的自适应动态规划（ADP）方法进行策略逼近。ETM的引入确保学习模块的参数更新仅在状态偏差超过预设界限时执行，从而避免过度学习活动并显著降低计算负荷。通过李雅普诺夫导向分析表征了闭环系统的稳定性。数值实验进一步证实，与标准时间触发ADP方案相比，该方法在保持强大控制性能和扰动容忍度的同时，显著减少了采样和处理工作量。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f5507cc4"
  },
  {
    "title": "A Unified AI System For Data Quality Control and DataOps Management in Regulated Environments",
    "url": "https://arxiv.org/pdf/2512.05559v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "In regulated domains such as finance, the integrity and governance of data pipelines are critical - yet existing systems treat data quality control (QC) as an isolated preprocessing step rather than a first-class system component. We present a unified AI-driven Data QC and DataOps Management framework that embeds rule-based, statistical, and AI-based QC methods into a continuous, governed layer spanning ingestion, model pipelines, and downstream applications. Our architecture integrates open-source tools with custom modules for profiling, audit logging, breach handling, configuration-driven policies, and dynamic remediation. We demonstrate deployment in a production-grade financial setup: handling streaming and tabular data across multiple asset classes and transaction streams, with configurable thresholds, cloud-native storage interfaces, and automated alerts. We show empirical gains in anomaly detection recall, reduction of manual remediation effort, and improved auditability and traceability in high-throughput data workflows. By treating QC as a system concern rather than an afterthought, our framework provides a foundation for trustworthy, scalable, and compliant AI pipelines in regulated environments.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在金融等受监管领域，数据管道的完整性和治理至关重要。本文提出了一种统一的AI驱动数据质量控制和DataOps管理框架，将基于规则、统计和AI的质量控制方法嵌入到一个跨越数据摄取、模型管道和下游应用的连续治理层中。该架构整合了开源工具与定制模块，用于数据剖析、审计日志记录、违规处理、配置驱动策略和动态修复。在金融生产环境中部署的演示表明，该框架能够处理多资产类别和交易流中的流数据和表格数据，具有可配置阈值、云原生存储接口和自动警报功能。实证结果显示，在高吞吐量数据工作流中，异常检测召回率得到提升，手动修复工作量减少，审计性和可追溯性得到改善。通过将质量控制视为系统核心而非事后补救，该框架为可信、可扩展且合规的AI管道提供了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_144f113c"
  },
  {
    "title": "Differential ML with a Difference",
    "url": "https://arxiv.org/pdf/2512.05301v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Differential ML (Huge and Savine 2020) is a technique for training neural networks to provide fast approximations to complex simulation-based models for derivatives pricing and risk management. It uses price sensitivities calculated through pathwise adjoint differentiation to reduce pricing and hedging errors. However, for options with discontinuous payoffs, such as digital or barrier options, the pathwise sensitivities are biased, and incorporating them into the loss function can magnify errors. We consider alternative methods for estimating sensitivities and find that they can substantially reduce test errors in prices and in their sensitivities. Using differential labels calculated through the likelihood ratio method expands the scope of Differential ML to discontinuous payoffs. A hybrid method incorporates gamma estimates as well as delta estimates, providing further regularization.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "差分机器学习（Differential ML）是一种训练神经网络快速近似复杂模拟模型的技术，用于衍生品定价和风险管理。该方法利用通过路径伴随微分计算的价格敏感性来减少定价和对冲误差。然而，对于具有不连续收益的期权（如数字或障碍期权），路径敏感性存在偏差，将其纳入损失函数可能放大误差。研究探讨了替代敏感性估计方法，发现这些方法能显著降低价格及其敏感性的测试误差。使用似然比法计算的差分标签将差分机器学习扩展到不连续收益领域。混合方法结合了伽马估计和德尔塔估计，提供了进一步的规范化。",
    "fetch_date": "2025-12-29",
    "id": "20251229_c043a012"
  },
  {
    "title": "Continuous-time reinforcement learning for optimal switching over multiple regimes",
    "url": "https://arxiv.org/pdf/2512.04697v2",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了多体制下最优切换问题的连续时间强化学习。在熵正则化的探索性框架下，代理通过关联连续时间有限状态马尔可夫链的生成矩阵，随机化切换时机和体制选择。我们建立了相关哈密顿-雅可比-贝尔曼方程组的适定性，并给出了最优策略的表征。通过分析方程组，严格证明了策略改进和策略迭代的收敛性。当温度参数趋近于零时，我们还展示了探索性框架下的价值函数向经典框架价值函数的收敛。最后，基于鞅表征的策略评估，设计并实现了一种强化学习算法。借助神经网络的数值算例说明了所提RL算法的有效性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_126a24ad"
  },
  {
    "title": "Convolution-FFT for option pricing in the Heston model",
    "url": "https://arxiv.org/pdf/2512.05326v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "We propose a convolution-FFT method for pricing European options under the Heston model that leverages a continuously differentiable representation of the joint characteristic function. Unlike existing Fourier-based methods that rely on branch-cut adjustments or empirically tuned damping parameters, our approach yields a stable integrand even under large frequency oscillations. Crucially, we derive fully analytical error bounds that quantify both truncation error and discretization error in terms of model parameters and grid settings. To the best of our knowledge, this is the first work to provide such explicit, closed-form error estimates for an FFT-based convolution method specialized to the Heston model. Numerical experiments confirm the theoretical rates and illustrate robust, high-accuracy option pricing at modest computational cost.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于Heston模型下欧式期权定价的卷积-FFT方法，该方法利用联合特征函数的连续可微表示。与现有依赖分支切割调整或经验调优阻尼参数的傅里叶方法不同，本方法即使在大频率振荡下也能产生稳定的被积函数。关键贡献在于推导了完全解析的误差界，以模型参数和网格设置量化截断误差和离散化误差。据我们所知，这是首个为专门针对Heston模型的基于FFT的卷积方法提供此类显式闭式误差估计的工作。数值实验验证了理论收敛速度，并展示了以适中计算成本实现稳健、高精度的期权定价。",
    "fetch_date": "2025-12-29",
    "id": "20251229_111d7440"
  },
  {
    "title": "Market Reactions to Material Cybersecurity Incident Disclosures",
    "url": "https://arxiv.org/pdf/2512.06144v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study examines short-term market responses to material cybersecurity incidents disclosed under Item 1.05 of Form 8-K. Drawing on a sample of disclosures made between 2023 and 2025, daily stock price movements were evaluated over a standardized event window surrounding each filing. On average, companies experienced negative price reactions following the disclosure of a material cybersecurity incident. Comparisons across company characteristics indicate that smaller companies tended to incur more pronounced declines, while differences by sector and beta were not evident. These findings offer empirical insight into how public markets interpret cybersecurity risks when they are formally reported and suggest that firm size may influence the degree of sensitivity to such events.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本研究考察了根据8-K表格第1.05项披露的重大网络安全事件引发的短期市场反应。基于2023年至2025年间的披露样本，评估了每份申报文件周围标准化事件窗口内的每日股价变动。平均而言，公司在披露重大网络安全事件后经历了负面价格反应。跨公司特征的比较表明，规模较小的公司往往遭受更明显的股价下跌，而按行业和贝塔值的差异并不明显。这些发现为公开市场如何解释正式报告的网络安全风险提供了实证见解，并表明公司规模可能影响对此类事件的敏感程度。",
    "fetch_date": "2025-12-29",
    "id": "20251229_da2b1b43"
  },
  {
    "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem",
    "url": "https://arxiv.org/pdf/2512.05946v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "资源分配问题因其组合复杂性而保持NP难。虽然深度强化学习方法（如Rainbow DQN）通过优先回放和分布头提高了可扩展性，但经典函数逼近器限制了其表示能力。本文提出变分量子Rainbow DQN（VQR-DQN），将环形拓扑变分量子电路与Rainbow DQN集成，以利用量子叠加和纠缠。我们将人力资源分配问题（HRAP）建模为基于官员能力、事件时间表和转移时间的组合动作空间的马尔可夫决策过程（MDP）。在四个HRAP基准测试中，VQR-DQN相比随机基线实现了26.8%的归一化完工时间减少，并优于Double DQN和经典Rainbow DQN 4.9-13.4%。这些收益与电路表达能力、纠缠和策略质量之间的理论联系一致，展示了量子增强DRL在大规模资源分配中的潜力。",
    "fetch_date": "2025-12-29",
    "id": "20251229_4b5ad711"
  },
  {
    "title": "FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction",
    "url": "https://arxiv.org/pdf/2512.15728v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \\emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\\% and stability of 93.33\\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文介绍了FedSight AI，这是一个利用大型语言模型（LLMs）模拟联邦公开市场委员会（FOMC）审议过程以预测联邦基金目标利率的多智能体系统架构。该系统通过智能体分析结构化指标（如经济数据）和非结构化输入（如褐皮书），进行辩论和投票，模拟委员会决策逻辑。其Chain-of-Draft（CoD）扩展通过强制简洁的多阶段推理，进一步提升了效率和准确性。在2023-2024年会议评估中，FedSight CoD实现了93.75%的准确率和93.33%的稳定性，优于包括MiniFed和Ordinal Random Forest（RF）在内的基线模型，同时提供了与真实FOMC沟通一致的透明推理。",
    "fetch_date": "2025-12-29",
    "id": "20251229_dc3f93f2"
  },
  {
    "title": "Standard and stressed value at risk forecasting using dynamic Bayesian networks",
    "url": "https://arxiv.org/pdf/2512.05661v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究引入动态贝叶斯网络（DBN）框架来预测风险价值（VaR）和压力风险价值（SVaR），并与几种常用模型进行性能比较。使用1991年至2020年标普500指数的日收益率数据，通过滚动周期和历史收益率生成10天99%的VaR和SVaR预测，而三个DBN模型同时使用历史收益率和预测收益率。通过标准回测和预测误差指标评估模型预测准确性。结果显示，自回归模型提供最准确的VaR预测，而DBN模型尽管包含前瞻性收益率预测，其表现与历史模拟模型相当。对于SVaR，所有模型均产生高度保守的预测，违约次数极少且准确性差异有限。虽然DBN未超越传统模型，但其作为前瞻性方法的可行性为未来将因果推断整合到金融风险预测的研究奠定了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f41d7e01"
  },
  {
    "title": "Risk aversion of insider and dynamic asymmetric information",
    "url": "https://arxiv.org/pdf/2512.05011v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies a Kyle-Back model with a risk-averse insider possessing exponential utility and a dynamic stochastic signal about the asset's terminal fundamental value. While the existing literature considers either risk-neutral insiders with dynamic signals or risk-averse insiders with static signals, we establish equilibrium when both features are present. Our approach imposes no restrictions on the magnitude of the risk aversion parameter, extending beyond previous work that requires sufficiently small risk aversion. We employ a weak conditioning methodology to construct a Schrödinger bridge between the insider's signal and the asset price process, an approach that naturally accommodates stochastic signal evolution and removes risk aversion constraints.\n  We derive necessary conditions for equilibrium, showing that the optimal insider strategy must be continuous with bounded variation. Under these conditions, we characterize the market-maker pricing rule and insider strategy that achieve equilibrium. We obtain explicit closed-form solutions for important cases including deterministic and quadratic signal volatilities, demonstrating the tractability of our framework.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个Kyle-Back模型，其中内幕交易者具有风险厌恶特征（采用指数效用函数）并拥有关于资产最终基础价值的动态随机信号。现有文献通常只考虑风险中性内幕交易者配动态信号，或风险厌恶内幕交易者配静态信号，而本文在两者同时存在的情况下建立了均衡。我们的方法不对风险厌恶参数的大小施加限制，超越了先前要求风险厌恶足够小的工作。我们采用弱条件方法在内幕交易者信号与资产价格过程之间构建了一个薛定谔桥，这种方法自然地适应了随机信号的演化并消除了风险厌恶约束。我们推导了均衡的必要条件，表明最优内幕交易策略必须是连续且有界变差的。在这些条件下，我们描述了实现均衡的做市商定价规则和内幕交易策略。我们针对包括确定性和二次信号波动率在内的重要案例获得了显式闭式解，证明了我们框架的可处理性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5800e438"
  },
  {
    "title": "Coordinated Mean-Field Control for Systemic Risk",
    "url": "https://arxiv.org/pdf/2512.04704v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "We develop a robust linear-quadratic mean-field control framework for systemic risk under model uncertainty, in which a central bank jointly optimizes interest rate policy and supervisory monitoring intensity against adversarial distortions. Our model features multiple policy instruments with interactive dynamics, implemented via a variance weight that depends on the policy rate, generating coupling effects absent in single-instrument models. We establish viscosity solutions for the associated HJB--Isaacs equation, prove uniqueness via comparison principles, and provide verification theorems. The linear-quadratic structure yields explicit feedback controls derived from a coupled Riccati system, preserving analytical tractability despite adversarial uncertainty. Simulations reveal distinct loss-of-control regimes driven by robustness-breakdown and control saturation, alongside a pronounced asymmetry in sensitivity between the mean and variance channels. These findings demonstrate the importance of instrument complementarity in systemic risk modeling and control.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文开发了一个稳健的线性二次平均场控制框架，用于在模型不确定性下处理系统性风险。中央银行通过联合优化利率政策和监管监控强度来对抗对抗性扭曲。模型具有多个政策工具的动态交互，通过依赖于政策利率的方差权重实现，产生了单工具模型中不存在的耦合效应。作者建立了相关HJB-Isaacs方程的粘性解，通过比较原理证明了唯一性，并提供了验证定理。线性二次结构产生了从耦合Riccati系统导出的显式反馈控制，尽管存在对抗性不确定性，但仍保持了分析的可处理性。模拟揭示了由稳健性崩溃和控制饱和驱动的不同失控机制，以及均值通道和方差通道之间显著的敏感性不对称。这些发现证明了在系统性风险建模和控制中工具互补性的重要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_05510e0b"
  },
  {
    "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2512.04653v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种用于多交叉口自适应交通信号控制的半集中训练去中心化执行架构，通过区域划分、参数共享和复合状态奖励设计来解决完全集中式方法的维度灾难和完全分布式方法的局部可观测性问题。虽然架构具有可迁移性并实现了两个具体模型，但其针对交通信号控制这一特定领域，与量化交易的实战应用关联较弱。",
    "fetch_date": "2025-12-29",
    "id": "20251229_6d6beb51"
  },
  {
    "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks",
    "url": "https://arxiv.org/pdf/2512.07162v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "DeepSVM：一种基于物理信息深度算子网络学习随机波动率模型的方法。实时校准随机波动率模型（SVMs）的计算瓶颈在于需要反复求解耦合偏微分方程（PDEs）。本文提出的DeepSVM无需标记训练数据，通过硬约束假设强制满足终端收益和静态无套利条件，并采用残差自适应细化（RAR）稳定高梯度区域的训练。该模型在典型市场动态范围内实现了高度准确的期权定价，但发现其衍生品（Greeks）在平价（ATM）区域存在噪声，凸显了物理信息算子学习中高阶正则化的需求。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b22309f0"
  },
  {
    "title": "Learning to Hedge Swaptions",
    "url": "https://arxiv.org/pdf/2512.06639v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了基于强化学习的深度对冲框架在掉期期权动态对冲中的应用，并与传统的基于敏感性的Rho对冲方法进行了性能对比。研究设计了三种不同目标函数（均方误差、下行风险、条件风险价值）的智能体以捕捉不同的风险偏好，并评估这些目标如何塑造对冲风格。基于三因子无套利动态Nelson-Siegel模型进行模拟实验，结果表明使用两种互换作为对冲工具时能实现接近最优的对冲效果。深度对冲策略能根据市场状态动态调整对冲组合对风险因子的敞口。实验显示，即使在模型存在一定误设的情况下，其表现仍持续优于Rho对冲策略。这些结果突显了强化学习在提供更高效、更具韧性的掉期期权对冲策略方面的潜力。",
    "fetch_date": "2025-12-28",
    "id": "20251228_80dea847"
  },
  {
    "title": "Unveiling Hedge Funds: Topic Modeling and Sentiment Correlation with Fund Performance",
    "url": "https://arxiv.org/pdf/2512.06620v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "The hedge fund industry presents significant challenges for investors due to its opacity and limited disclosure requirements. This pioneering study introduces two major innovations in financial text analysis. First, we apply topic modeling to hedge fund documents-an unexplored domain for automated text analysis-using a unique dataset of over 35,000 documents from 1,125 hedge fund managers. We compared three state-of-the-art methods: Latent Dirichlet Allocation (LDA), Top2Vec, and BERTopic. Our findings reveal that LDA with 20 topics produces the most interpretable results for human users and demonstrates higher robustness in topic assignments when the number of topics varies, while Top2Vec shows superior classification performance. Second, we establish a novel quantitative framework linking document sentiment to fund performance, transforming qualitative information traditionally requiring expert interpretation into systematic investment signals. In sentiment analysis, contrary to expectations, the general-purpose DistilBERT outperforms the finance-specific FinBERT in generating sentiment scores, demonstrating superior adaptability to diverse linguistic patterns found in hedge fund documents that extend beyond specialized financial news text. Furthermore, sentiment scores derived using DistilBERT in combination with Top2Vec show stronger correlations with subsequent fund performance compared to other model combinations. These results demonstrate that automated topic modeling and sentiment analysis can effectively process hedge fund documents, providing investors with new data-driven decision support tools.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究在金融文本分析领域引入两大创新：首先，将主题建模应用于对冲基金文档这一自动化文本分析未涉足领域，使用来自1,125家对冲基金管理人的超过35,000份文档数据集，比较了潜在狄利克雷分配（LDA）、Top2Vec和BERTopic三种前沿方法。研究发现，20个主题的LDA对人类用户最具可解释性，且在主题数量变化时表现出更高鲁棒性，而Top2Vec则展现更优的分类性能。其次，建立了一个将文档情感与基金表现相关联的量化框架，将传统需要专家解读的定性信息转化为系统性投资信号。在情感分析中，通用模型DistilBERT意外地优于金融专用模型FinBERT，显示出对对冲基金多样化语言模式的更强适应性。",
    "fetch_date": "2025-12-28",
    "id": "20251228_3d108839"
  },
  {
    "title": "Hybrid Quantum-Classical Ensemble Learning for S\\&P 500 Directional Prediction",
    "url": "https://arxiv.org/pdf/2512.15738v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\\% directional accuracy on S\\&P 500 prediction, a 3.10\\% improvement over individual models.\n  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\\% vs.\\ 52.80\\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\\% to +1.5\\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\\%$), improving ensemble performance (Top-7 models: 60.14\\% vs.\\ all 35 models: 51.2\\%).\n  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种混合量子-经典集成学习框架，用于标普500指数的方向性预测。核心创新包括：1）架构多样性优于数据集多样性——在相同数据上结合不同算法（LSTM、决策Transformer、XGBoost等）比单一架构多数据集训练效果更佳（60.14% vs. 52.80%）；2）4量子比特变分量子电路增强情感分析，为各模型带来0.8%-1.5%的准确率提升；3）智能筛选排除弱预测器（准确率<52%），优化集成性能。最终实现60.14%的方向预测准确率，较单一模型提升3.10%，对量化交易实战具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_fab16cfe"
  },
  {
    "title": "Deep learning approaches in Finance: Forecasting volatility and enhancing Quantitative trading strategies",
    "url": "https://etheses.whiterose.ac.uk/id/eprint/27202/",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… services industry using Deep Learning architectures. The main focus is on advancing current approaches in the areas of Risk Management and Quantitative Trading. The former is …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文聚焦于深度学习在金融领域的应用，主要致力于推动风险管理与量化交易领域的现有方法。核心内容包括利用深度学习架构预测波动率，并以此增强量化交易策略。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6403d992"
  },
  {
    "title": "Intraday and Post-Market Investor Sentiment for Stock Price Prediction: A Deep Learning Framework with Explainability and Quantitative Trading Strategy",
    "url": "https://www.mdpi.com/2079-8954/13/5/390",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… In contrast to traditional deep learning models, which are often … Quantitative trading backtesting under the T+1 trading … Most academic studies neglect real-world trading constraints…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种用于股价预测的深度学习框架，结合了盘中与盘后投资者情绪分析，并具备模型可解释性。研究特别设计了考虑T+1交易制度的量化交易策略进行回测，强调了传统学术研究常忽略的实际交易约束，对实战交易具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ce0d31fc"
  },
  {
    "title": "Research on quantitative investment strategies based on deep learning",
    "url": "https://www.mdpi.com/1999-4893/12/2/35",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… with four trading strategies (Long Call, Short Call, Long Put, Short Put) where deep learning … mirror the impact of its accuracy on quantitative trading strategies in a straightforward way. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于深度学习的量化投资策略研究，涉及四种交易策略（买入看涨期权、卖出看涨期权、买入看跌期权、卖出看跌期权），通过深度学习模型直接反映其预测准确性对量化交易策略的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d4332f98"
  },
  {
    "title": "Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing",
    "url": "https://arxiv.org/pdf/2512.17923v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们引入了一种新颖的混淆测试方法，用于验证大型语言模型是否通过因果推理而非时间关联来检测结构性市场模式。在标普500期权数据的242个交易日（覆盖95.6%）上测试三种做市商对冲约束模式（伽马持仓、股票钉住、0DTE对冲），我们发现LLM使用无偏提示（仅提供原始伽马暴露值，无制度标签或时间背景）实现了71.5%的检测率。WHO-WHOM-WHAT因果框架迫使模型识别观察到的市场动态背后的经济参与者（做市商）、受影响方（方向性交易者）和结构性机制（强制对冲）。关键的是，即使季度经济盈利能力变化，检测准确率（91.2%）保持稳定，表明模型识别的是结构性约束而非盈利模式。当提供制度标签时，检测率提升至100%，但71.5%的无偏率验证了真正的模式识别。我们的发现表明，LLM通过纯结构性推理检测复杂金融机制具有新兴能力，对系统交易有潜在影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_19f9e1af"
  },
  {
    "title": "Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance",
    "url": "https://arxiv.org/pdf/2512.15739v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种贝叶斯分析框架，用于金融风险预测与合规中的不确定性管理。该方法在波动率预测、欺诈检测和合规监控方面提升了风险处理能力。通过评估标普500指数日收益率的单日95%风险价值预测（训练期2000-2019，测试期2020-2024），发现LSTM基线接近名义校准，而带学生t分布的GARCH(1,1)模型低估尾部风险。提出的折扣因子DLM模型产生略宽松的VaR估计，存在聚集性违规证据。贝叶斯逻辑回归提高了欺诈检测的召回率和AUC-ROC，分层Beta状态空间模型提供了透明自适应的合规风险评估。该框架以精确的不确定性量化、可解释性和GPU加速分析为特点。",
    "fetch_date": "2025-12-28",
    "id": "20251228_29b631c7"
  },
  {
    "title": "Analysis of Contagion in China's Stock Market: A Hawkes Process Perspective",
    "url": "https://arxiv.org/pdf/2512.08000v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "This study explores contagion in the Chinese stock market using Hawkes processes to analyze autocorrelation and cross-correlation in multivariate time series data. We examine whether market indices exhibit trending behavior and whether sector indices influence one another. By fitting self-exciting and inhibitory Hawkes processes to daily returns of indices like the Shanghai Composite, Shenzhen Component, and ChiNext, as well as sector indices (CSI Consumer, Healthcare, and Financial), we identify long-term dependencies and trending patterns, including upward, downward, and oversold rebound trends. Results show that during high trading activity, sector indices tend to sustain their trends, while low activity periods exhibit strong sector rotation. This research models stock price movements using spatiotemporal Hawkes processes, leveraging conditional intensity functions to explain sector rotation, advancing the understanding of financial contagion.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究使用Hawkes过程分析中国股市的传染效应，通过拟合自激励和抑制型Hawkes过程到上证综指、深证成指、创业板指及消费、医疗、金融等行业指数的日收益率数据，识别长期依赖性和趋势模式（包括上升、下降和超跌反弹趋势）。研究发现高交易活跃度期间行业指数倾向于维持趋势，低活跃度期间则呈现显著的行业轮动现象。该研究通过时空Hawkes过程建模股价变动，利用条件强度函数解释行业轮动，深化了对金融传染机制的理解。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d0ecab01"
  },
  {
    "title": "Asian option valuation under price impact",
    "url": "https://arxiv.org/pdf/2512.07154v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We study the valuation of Asian options in a binomial market with permanent price impact, extending the Cox-Ross-Rubinstein framework under a modified risk-neutral probability. We obtain an exact pathwise representation for geometric Asian options and derive two-sided bounds for arithmetic Asian options. Our analysis identifies the no-arbitrage region in terms of hedging volumes and shows that permanent price impact systematically raises Asian option prices. Numerical examples illustrate the effect of the impact parameter and hedging volumes on the resulting prices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在带有永久价格影响的二项式市场中研究亚式期权的定价，扩展了修正风险中性概率下的Cox-Ross-Rubinstein框架。研究获得了几何亚式期权的精确路径表示，并推导了算术亚式期权的双边边界。分析从对冲头寸角度识别了无套利区间，表明永久价格影响会系统性提高亚式期权价格。数值示例展示了影响参数和对冲头寸对最终价格的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_a5b016a2"
  },
  {
    "title": "Market Reactions and Information Spillovers in Bank Mergers: A Multi-Method Analysis of the Japanese Banking Sector",
    "url": "https://arxiv.org/pdf/2512.06550v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Major bank mergers and acquisitions (M&A) transform the financial market structure, but their valuation and spillover effects remain open to question. This study examines the market reaction to two M&A events: the 2005 creation of Mitsubishi UFJ Financial Group following the Financial Big Bang in Japan, and the 2018 merger involving Resona Holdings after the global financial crisis. The multi-method analysis in this research combines several distinct methods to explore these M&A events. An event study using the market model, the capital asset pricing model (CAPM), and the Fama-French three-factor model is implemented to estimate cumulative abnormal returns (CAR) for valuation purposes. Vector autoregression (VAR) models are used to test for Granger causality and map dynamic effects using impulse response functions (IRFs) to investigate spillovers. Propensity score matching (PSM) helps provide a causal estimate of the average treatment effect on the treated (ATT). The analysis detected a significant positive market reaction to the mergers. The findings also suggest the presence of prolonged positive spillovers to other banks, which may indicate a synergistic effect among Japanese banks. Combining these methods provides a unique perspective on M&A events in the Japanese banking sector, offering valuable insights for investors, managers, and regulators concerned with market efficiency and systemic stability",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究通过多方法分析评估日本银行业两次重大并购事件（2005年三菱UFJ金融集团成立与2018年Resona控股合并）的市场反应与信息溢出效应。采用事件研究法（市场模型、CAPM、Fama-French三因子模型）计算累积异常收益（CAR），运用向量自回归（VAR）模型检验格兰杰因果关系并通过脉冲响应函数（IRFs）分析动态溢出效应，辅以倾向得分匹配（PSM）估计平均处理效应（ATT）。研究发现并购产生显著正向市场反应，并存在对其他银行的持续正向溢出效应，暗示日本银行业可能存在协同效应。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b46875b4"
  },
  {
    "title": "Amortizing Perpetual Options",
    "url": "https://arxiv.org/pdf/2512.06505v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "In this work, we introduce amortizing perpetual options (AmPOs), a fungible variant of continuous-installment options suitable for exchange-based trading. Traditional installment options lapse when holders cease their payments, destroying fungibility across units of notional. AmPOs replace explicit installment payments and the need for lapsing logic with an implicit payment scheme via a deterministic decay in the claimable notional. This amortization ensures all units evolve identically, preserving fungibility. Under the Black-Scholes framework, AmPO valuation can be reduced to an equivalent vanilla perpetual American option on a dividend-paying asset. In this way, analytical expressions are possible for the exercise boundaries and risk-neutral valuations for calls and puts. These formulas and relations allow us to derive the Greeks and study comparative statics with respect to the amortization rate. Illustrative numerical case studies demonstrate how the amortization rate shapes option behavior and reveal the resulting tradeoffs in the effective volatility sensitivity.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了摊销型永续期权（AmPOs），这是一种适用于交易所交易的可替代连续分期期权变体。传统分期期权在持有人停止支付时会失效，破坏了名义金额单位间的可替代性。AmPOs通过可索赔名义金额的确定性衰减隐含支付方案，取代了显性分期付款和失效逻辑。这种摊销确保所有单位以相同方式演变，保持了可替代性。在Black-Scholes框架下，AmPO估值可简化为等价于带股息资产上的普通永续美式期权。通过这种方式，可以得出看涨和看跌期权的行权边界和风险中性估值的解析表达式。这些公式和关系使我们能够推导希腊字母，并研究相对于摊销率的比较静态。示例数值案例研究展示了摊销率如何塑造期权行为，并揭示了有效波动率敏感性中的权衡取舍。",
    "fetch_date": "2025-12-28",
    "id": "20251228_7b969bb3"
  },
  {
    "title": "Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market",
    "url": "https://arxiv.org/pdf/2512.06473v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $ρ_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "复杂系统中的相关性常被非平稳性、长程记忆和重尾波动所掩盖，限制了传统协方差分析的有效性。为应对这些挑战，本文使用多分形去趋势互相关系数ρ_r构建了尺度与波动依赖的相关矩阵，该系数能选择性地强调不同幅度的波动。作者检验了这些去趋势相关矩阵的谱特性，并与从合成高斯及q高斯信号以相同方式计算得到的矩阵谱特性进行比较。结果表明，即使在时间序列不存在互相关的情况下，去趋势处理、重尾分布和波动阶参数r共同产生的谱也与随机情况显著偏离。将该框架应用于2021-2024年间140种主要加密货币的一分钟收益率数据，揭示了稳健的集体模式，包括一个主导的市场因子和几个行业成分，其强度取决于分析的尺度和波动阶。在滤除市场模式后，经验特征值的主体部分与随机矩阵理论预测的极限分布紧密对齐。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ae9ae498"
  },
  {
    "title": "Wealth or Stealth? The Camouflage Effect in Insider Trading",
    "url": "https://arxiv.org/pdf/2512.06309v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "We consider a Kyle-type model where insider trading takes place among a potentially large population of liquidity traders and is subject to legal penalties. Insiders exploit the liquidity provided by the trading masses to \"camouflage\" their actions and balance expected wealth with the necessary stealth to avoid detection. Under a diverse spectrum of prosecution schemes, we establish the existence of equilibria for arbitrary population sizes and a unique limiting equilibrium. A convergence analysis determines the scale of insider trading by a stealth index $γ$, revealing that the equilibrium can be closely approximated by a simple limit due to diminished price informativeness. Empirical aspects are derived from two calibration experiments using non-overlapping data sets spanning from 1980 to 2018, which underline the indispensable role of a large population in insider trading models with legal risk, along with important implications for the incidence of stealth trading and the deterrent effect of legal enforcement.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个Kyle型模型，研究内幕交易者在面临法律处罚风险时，如何利用大量流动性交易者提供的流动性来“伪装”其交易行为，在预期财富与避免被发现的必要隐蔽性之间取得平衡。通过分析多种起诉方案，证明了任意市场规模下均衡的存在性及唯一的极限均衡。收敛分析通过隐蔽指数γ确定内幕交易规模，并揭示由于价格信息性减弱，均衡可被一个简单极限近似逼近。实证部分基于1980年至2018年的两个非重叠数据集进行校准实验，强调了在考虑法律风险的内幕交易模型中大规模交易群体的不可或缺作用，并对隐蔽交易的发生频率及法律执行的威慑效应提出了重要启示。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6e05cc4e"
  },
  {
    "title": "Not All Factors Crowd Equally: Modeling, Measuring, and Trading on Alpha Decay",
    "url": "https://arxiv.org/pdf/2512.11913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We derive a specific functional form for factor alpha decay -- hyperbolic decay alpha(t) = K/(1+lambda*t) -- from a game-theoretic equilibrium model, and test it against linear and exponential alternatives. Using eight Fama-French factors (1963--2024), we find: (1) Hyperbolic decay fits mechanical factors. Momentum exhibits clear hyperbolic decay (R^2 = 0.65), outperforming linear (0.51) and exponential (0.61) baselines -- validating the equilibrium foundation. (2) Not all factors crowd equally. Mechanical factors (momentum, reversal) fit the model; judgment-based factors (value, quality) do not -- consistent with a signal-ambiguity taxonomy paralleling Hua and Sun's \"barriers to entry.\" (3) Crowding accelerated post-2015. Out-of-sample, the model over-predicts remaining alpha (0.30 vs. 0.15), correlating with factor ETF growth (rho = -0.63). (4) Average returns are efficiently priced. Crowding-based factor selection fails to generate alpha (Sharpe: 0.22 vs. 0.39 factor momentum benchmark). (5) Crowding predicts tail risk. Out-of-sample (2001--2024), crowded reversal factors show 1.7--1.8x higher crash probability (bottom decile returns), while crowded momentum shows lower crash risk (0.38x, p = 0.006). Our findings extend equilibrium crowding models (DeMiguel et al.) to temporal dynamics and show that crowding predicts crashes, not means -- useful for risk management, not alpha generation.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文从博弈论均衡模型推导出因子阿尔法衰减的特定函数形式——双曲衰减alpha(t)=K/(1+lambda*t)，并在线性和指数衰减基准上进行检验。基于八个Fama-French因子（1963-2024）的研究发现：（1）机械因子（如动量）呈现清晰的双曲衰减（R²=0.65），验证了均衡基础；（2）因子拥挤存在异质性——机械因子（动量、反转）符合模型，而基于判断的因子（价值、质量）则不符合，这与Hua和Sun的“进入壁垒”信号模糊度分类一致；（3）2015年后拥挤加速，样本外模型高估剩余阿尔法（0.30 vs. 0.15），且与因子ETF增长负相关（ρ=-0.63）；（4）平均收益已被有效定价，基于拥挤的因子选择未能产生阿尔法（夏普比率：0.22 vs. 因子动量基准0.39）；（5）拥挤可预测尾部风险——样本外（2001-2024）拥挤的反转因子崩盘概率高1.7-1.8倍，而拥挤的动量因子崩盘风险较低。",
    "fetch_date": "2025-12-27",
    "id": "20251227_0101e588"
  },
  {
    "title": "Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making",
    "url": "https://arxiv.org/pdf/2512.17936v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in Türkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "面对日益增长的金融不确定性和市场复杂性，本研究提出了一种新颖的风险感知金融预测框架，该框架将先进的机器学习技术与直觉模糊多准则决策（MCDM）相结合。该框架专为BIST 100指数设计，并通过土耳其一家主要国防公司的案例研究进行验证，融合了结构化金融数据、非结构化文本数据和宏观经济指标，以提高预测准确性和鲁棒性。它包含一套混合模型，包括极端梯度提升（XGBoost）、长短期记忆（LSTM）网络和图神经网络（GNN），以提供具有量化不确定性的概率预测。实证结果显示高预测准确性，净利润平均绝对百分比误差（MAPE）为3.03%，关键财务指标的95%置信区间较窄。风险感知分析显示有利的风险回报特征，夏普比率为1.25，索提诺比率更高为1.80，表明在市场波动下相对较低的下行波动性和稳健表现。敏感性分析表明关键财务指标具有稳定性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_96d3f766"
  },
  {
    "title": "Exploratory Mean-Variance with Jumps: An Equilibrium Approach",
    "url": "https://arxiv.org/pdf/2512.09224v1",
    "source": "ArXiv",
    "date": "2025-12-10",
    "abstract": "Revisiting the continuous-time Mean-Variance (MV) Portfolio Optimization problem, we model the market dynamics with a jump-diffusion process and apply Reinforcement Learning (RL) techniques to facilitate informed exploration within the control space. We recognize the time-inconsistency of the MV problem and adopt the time-inconsistent control (TIC) approach to analytically solve for an exploratory equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem. Our approach accounts for time-inconsistent preferences and actions, and our equilibrium policy is the best option an investor can take at any given time during the investment period. Moreover, we leverage the martingale properties of the equilibrium policy, design a RL model, and propose an Actor-Critic RL algorithm. All of our RL model parameters converge to the corresponding true values in a simulation study. Our numerical study on 24 years of real market data shows that the proposed RL model is profitable in 13 out of 14 tests, demonstrating its practical applicability in real world investment.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视连续时间均值-方差（MV）投资组合优化问题，采用跳跃扩散过程建模市场动态，并应用强化学习（RL）技术在控制空间内进行知情探索。针对MV问题的时间不一致性，采用时间不一致控制（TIC）方法解析求解探索性均衡投资策略——一个以经典MV问题均衡控制为中心的高斯分布。该策略考虑了时间不一致的偏好与行动，是投资期内任一时刻的最佳选择。此外，利用均衡策略的鞅性质设计RL模型，提出Actor-Critic RL算法。模拟研究中所有RL模型参数均收敛至真实值，基于24年真实市场数据的数值研究表明，所提RL模型在14次测试中13次盈利，证明了其在实际投资中的适用性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_b4ce1604"
  },
  {
    "title": "Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market",
    "url": "https://arxiv.org/abs/2506.06356",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… -day turnover quantitative trading algorithm that integrates advanced deep learning techniques … Index Terms—quantitative trading, deep learning, crosssectional prediction, multi-day …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度学习增强的多日换手率量化交易算法应用于中国A股市场。该算法整合了先进的深度学习技术，专注于横截面预测和多日交易策略。",
    "fetch_date": "2025-12-27",
    "id": "20251227_c3728bb7"
  },
  {
    "title": "Intelligent optimization based multi-factor deep learning stock selection model and quantitative trading strategy",
    "url": "https://www.mdpi.com/2227-7390/10/4/566",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… Thirdly, this paper designs and implements a quantitative trading strategy. Based on the CS-GRU stock selection model, this paper designs and implements a quantitative trading …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了一种基于智能优化的多因子深度学习选股模型与量化交易策略。首先，构建了CS-GRU选股模型（结合了Cuckoo Search优化算法与门控循环单元网络），用于预测股票收益。其次，基于该模型设计并实施了量化交易策略，包括信号生成、仓位管理和风险控制等实战环节。最后，通过回测验证了策略的有效性，表明该模型在实战交易中具有潜在应用价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_887f4f01"
  },
  {
    "title": "Research on Deep Learning-Based Quantitative Trading Models",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-99477-7_12",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… This paper examines the role of quantitative trading in finance and the potential applications of deep learning. Quantitative trading automates investment strategies using mathematical …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文探讨了量化交易在金融领域的作用及深度学习的潜在应用。量化交易利用数学模型自动化投资策略...",
    "fetch_date": "2025-12-27",
    "id": "20251227_ed097bf0"
  },
  {
    "title": "Sustainability, accuracy, fairness, and explainability (safe) machine learning in quantitative trading",
    "url": "https://www.mdpi.com/2227-7390/13/3/442",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… based signal strategies: those based on deep learning models and those grounded in classical machine learning techniques. The deep learning models employed in this research were …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了可持续性、准确性、公平性和可解释性（SAFE）机器学习在量化交易中的应用，比较了基于深度学习模型和经典机器学习技术的信号策略。研究采用的深度学习模型包括...，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_020ded1a"
  },
  {
    "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
    "url": "https://arxiv.org/pdf/2512.10913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "强化学习（RL）作为一种金融决策的创新方法，为传统方法难以解决的复杂投资问题提供了专门解决方案。该综述分析了2017-2025年的167篇文章，重点关注做市、投资组合优化和算法交易。研究发现RL在金融应用中存在关键性能问题和挑战，但在做市等领域相比传统方法具有优势。研究提出了一个统一框架来解决可解释性、鲁棒性和部署可行性等常见问题。基于合成数据的实证证据表明，实施质量和领域知识通常比算法复杂性更重要。研究强调需要可解释的RL架构以满足监管要求、增强非平稳环境下的鲁棒性，并建立标准化基准测试协议。建议机构应减少对算法复杂性的关注，更多关注市场微观结构、监管约束和风险管理。",
    "fetch_date": "2025-12-27",
    "id": "20251227_801c29bb"
  },
  {
    "title": "Local and Global Balance in Financial Correlation Networks: an Application to Investment Decisions",
    "url": "https://arxiv.org/pdf/2512.10606v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "The global balance is a well-known indicator of the behavior of a signed network. Recent literature has introduced the concept of local balance as a measure of the contribution of a single node to the overall balance of the network. In the present research, we investigate the potential of using deviations of local balance from global balance as a criterion for selecting outperforming assets. The underlying idea is that, during financial crises, most assets in the investment universe behave similarly: losses are severe and widespread, and the global balance of the correlation-based signed network reaches its maximum value. Under such circumstances, standard diversification (mainly related to portfolio size) is unable to reduce risk or limit losses. Therefore, it may be useful to concentrate portfolio exposures on the few assets - if such assets exist-that behave differently from the rest of the market. We argue that these assets are those for which the local balance strongly departs from the global balance of the underlying signed network. The paper supports this hypothesis through an application using real financial data. The results, in both descriptive and predictive contexts, confirm the proposed intuition.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于金融相关网络局部平衡与全局平衡偏离度的资产选择方法。核心观点是：在金融危机期间，大多数资产表现趋同，传统分散化策略失效；此时，应集中投资于那些局部平衡与网络全局平衡显著偏离的资产，因为这些资产的市场行为与整体市场不同。研究通过真实金融数据验证了这一假设，表明该方法在描述性和预测性场景下均能识别出表现优异的资产。",
    "fetch_date": "2025-12-27",
    "id": "20251227_9fed57fe"
  },
  {
    "title": "A New Application of Hoeffding's Inequality Can Give Traders Early Warning of Financial Regime Change",
    "url": "https://arxiv.org/pdf/2512.08851v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "Hoeffding's Inequality provides the maximum probability that a series of n draws from a bounded random variable differ from the variable's true expectation u by more than given tolerance t. The random variable is typically the error rate of a classifier in machine learning applications. Here, a trading strategy is premised on the assumption of an underlying distribution of causal factors, in other words, a market regime, and the random variable is the performance of that trading strategy. A larger deviation of observed performance from the trader's expectation u can be characterized as a lower probability that the financial regime supporting that strategy remains in force, and a higher probability of financial regime change. The changing Hoeffding probabilities can be used as an early warning indicator of this change.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种将霍夫丁不等式应用于量化交易的新方法。该方法将交易策略的绩效视为有界随机变量，通过计算观察到的绩效与预期绩效之间的偏差概率，来评估当前市场状态（即金融制度）是否发生变化。当偏差增大时，霍夫丁概率降低，表明支持该策略的市场制度可能不再有效，从而为金融制度变更提供早期预警。该方法为基于市场制度假设的交易策略提供了一种理论上的风险监控工具。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d6fbced2"
  },
  {
    "title": "Pareto-optimal reinsurance under dependence uncertainty",
    "url": "https://arxiv.org/pdf/2512.11430v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper studies Pareto-optimal reinsurance design in a monopolistic market with multiple primary insurers and a single reinsurer, all with heterogeneous risk preferences. The risk preferences are characterized by a family of risk measures, called Range Value-at-Risk (RVaR), which includes both Value-at-Risk (VaR) and Expected Shortfall (ES) as special cases. Recognizing the practical difficulty of accurately estimating the dependence structure among the insurers' losses, we adopt a robust optimization approach that assumes the marginal distributions are known while leaving the dependence structure unspecified. We provide a complete characterization of optimal indemnity schedules under the worst-case scenario, showing that the infinite-dimensional optimization problem can be reduced to a tractable finite-dimensional problem involving only two or three parameters for each indemnity function. Additionally, for independent and identically distributed risks, we exploit the argument of asymptotic normality to derive optimal two-parameter layer contracts. Finally, numerical applications are considered in a two-insurer setting to illustrate the influence of the dependence structures and heterogeneous risk tolerances on optimal strategies and the corresponding risk evaluation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究垄断市场中多个原保险公司与单一再保险公司之间的帕累托最优再保险设计，各方具有异质性风险偏好。风险偏好由一系列风险度量（称为范围风险价值，RVaR）表征，该度量包含风险价值（VaR）和预期损失（ES）作为特例。考虑到准确估计保险公司损失间依赖结构的实际困难，作者采用鲁棒优化方法，假设边际分布已知而依赖结构未指定。在最坏情况下，作者完整刻画了最优赔偿方案，表明无限维优化问题可简化为仅涉及每个赔偿函数两到三个参数的可处理有限维问题。此外，对于独立同分布风险，作者利用渐近正态性论证推导出最优两参数分层合约。最后，通过双保险公司设置的数值应用说明了依赖结构的影响。",
    "fetch_date": "2025-12-27",
    "id": "20251227_80cceaa5"
  },
  {
    "title": "Generative AI for Analysts",
    "url": "https://arxiv.org/pdf/2512.19705v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨生成式人工智能（AI）如何改变金融分析师的工作。以2023年FactSet AI平台上线为自然实验，发现采用AI后报告内容显著丰富全面——信息源增加40%，主题覆盖扩大34%，高级分析方法使用提升25%，同时时效性改善。然而，预测误差上升59%，因为AI辅助报告呈现更平衡的正负面信息，尤其对认知负荷较重的分析师更难整合。使用其他数据供应商的安慰剂测试证实这些效应是FactSet AI集成独有的。总体而言，研究揭示了生成式AI在金融信息生产中既带来生产力提升，也存在认知局限。",
    "fetch_date": "2025-12-27",
    "id": "20251227_778da003"
  },
  {
    "title": "Option-Implied Zero-Coupon Yields: Unifying Bond and Equity Markets",
    "url": "https://arxiv.org/pdf/2512.10823v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "This paper addresses a critical inconsistency in models of the term structure of interest rates (TSIR), where zero-coupon bonds are priced under risk-neutral measures distinct from those used in equity markets. We propose a unified TSIR framework that treats zero-coupon bonds as European options with deterministic payoffs ensuring that they are priced under the same risk-neutral measure that governs equity derivatives. Using put-call parity, we extract zero-coupon bond implied yield curves from S&P 500 index options and compare them with the US daily treasury par yield curves. As the implied yield curves contain maturity time T and strike price K as independent variables, we investigate the K-dependence of the implied yield curve. Our findings, that at-the-money, option-implied yield curves provide the closest match to treasury par yield curves, support the view that the equity options market contains information that is highly relevant for the TSIR. By insisting that the risk-neutral measure used for bond valuation is the same as that revealed by equity derivatives, we offer a new organizing principle for future TSIR research.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对利率期限结构模型中的关键不一致性提出解决方案，即零息债券与权益衍生品使用不同的风险中性测度定价。作者提出统一框架，将零息债券视为具有确定性收益的欧式期权，确保其与权益衍生品在同一风险中性测度下定价。利用看跌-看涨平价关系，从标普500指数期权中提取零息债券隐含收益率曲线，并与美国国债平价收益率曲线比较。研究发现，平值期权隐含收益率曲线与国债曲线最接近，表明权益期权市场包含对利率期限结构高度相关的信息。该框架为未来TSIR研究提供了新的组织原则。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d45f5bf0"
  },
  {
    "title": "Volatility time series modeling by single-qubit quantum circuit learning",
    "url": "https://arxiv.org/pdf/2512.10584v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We employ single-qubit quantum circuit learning (QCL) to model the dynamics of volatility time series. To assess its effectiveness, we generate synthetic data using the Rational GARCH model, which is specifically designed to capture volatility asymmetry. Our results show that QCL-based volatility predictions preserve the negative return-volatility correlation, a hallmark of asymmetric volatility dynamics. Moreover, analysis of the Hurst exponent and multifractal characteristics indicates that the predicted series, like the original synthetic data, exhibits anti-persistent behavior and retains its multifractal structure.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究采用单量子比特量子电路学习（QCL）对波动率时间序列进行建模。通过使用专门捕捉波动率不对称性的Rational GARCH模型生成合成数据进行评估，结果表明基于QCL的波动率预测保留了负收益-波动率相关性（这是不对称波动率动态的标志特征）。此外，对赫斯特指数和多重分形特征的分析表明，预测序列与原始合成数据类似，表现出反持续性行为并保留了其多重分形结构。",
    "fetch_date": "2025-12-27",
    "id": "20251227_568b9ce8"
  },
  {
    "title": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
    "url": "https://arxiv.org/pdf/2512.17929v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了在宏观经济关系不确定且随时间变化的情况下，中央银行应如何动态设定短期名义利率以稳定通胀和失业率。我们将货币政策建模为一个序贯决策问题，中央银行每季度观察宏观经济状况并选择利率调整。使用公开可用的历史联邦储备经济数据（FRED），我们构建了一个线性高斯转移模型，并实现了一个具有二次损失奖励函数的离散动作马尔可夫决策过程。我们选择了九种不同的强化学习方法与泰勒规则和朴素基线进行比较，包括表格Q学习变体、SARSA、Actor-Critic、深度Q网络、具有不确定性量化的贝叶斯Q学习以及具有部分可观测性的POMDP公式。令人惊讶的是，标准的表格Q学习取得了最佳性能（-615.13 ± 309.58 平均回报），优于增强的RL方法和传统政策规则。我们的结果表明，虽然复杂的RL技术在货币政策应用中显示出前景，但更简单的方法在应对不确定性时可能更稳健。",
    "fetch_date": "2025-12-27",
    "id": "20251227_ebb24070"
  },
  {
    "title": "EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction",
    "url": "https://arxiv.org/pdf/2512.12727v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "EXFormer：一种用于外汇收益率预测的多尺度趋势感知Transformer，具有动态变量选择功能。该论文提出了一种新颖的基于Transformer的架构，专门用于预测每日汇率收益率。它引入了多尺度趋势感知自注意力机制，采用具有不同感受野的并行卷积分支，以基于局部斜率对齐观测值，在保持长程依赖性的同时对制度转换保持敏感。动态变量选择器为28个与汇率收益率相关的外生协变量分配时变重要性权重，提供先验可解释性。嵌入的挤压-激励块重新校准通道响应，以强调信息特征并抑制预测中的噪声。使用欧元/美元、美元/日元和英镑/美元的每日数据，在五个不同的滑动窗口上进行了样本外评估。EXFormer始终优于随机游走和其他基线模型，提高了方向准确性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_95743fd6"
  },
  {
    "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2512.12250v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该研究提出了一种混合建模框架，将随机波动率模型与长短期记忆神经网络相结合，用于标普500指数的波动率预测。SV模型提升了统计精度并捕捉了潜在的波动动态，特别是在应对突发事件时；LSTM网络则增强了模型检测金融时间序列中复杂非线性模式的能力。研究采用滚动窗口方法训练模型并生成一步超前波动率预测，通过统计测试和投资模拟评估了混合SV-LSTM模型的性能。结果表明，该混合方法优于单独的SV和LSTM模型，为改进风险评估和战略投资规划提供了基础。",
    "fetch_date": "2025-12-26",
    "id": "20251226_36f39515"
  },
  {
    "title": "Deep Hedging with Reinforcement Learning: A Practical Framework for Option Risk Management",
    "url": "https://arxiv.org/pdf/2512.12420v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "We present a reinforcement-learning (RL) framework for dynamic hedging of equity index option exposures under realistic transaction costs and position limits. We hedge a normalized option-implied equity exposure (one unit of underlying delta, offset via SPY) by trading the underlying index ETF, using the option surface and macro variables only as state information and not as a direct pricing engine. Building on the \"deep hedging\" paradigm of Buehler et al. (2019), we design a leak-free environment, a cost-aware reward function, and a lightweight stochastic actor-critic agent trained on daily end-of-day panel data constructed from SPX/SPY implied volatility term structure, skew, realized volatility, and macro rate context. On a fixed train/validation/test split, the learned policy improves risk-adjusted performance versus no-hedge, momentum, and volatility-targeting baselines (higher point-estimate Sharpe); only the GAE policy's test-sample Sharpe is statistically distinguishable from zero, although confidence intervals overlap with a long-SPY benchmark so we stop short of claiming formal dominance. Turnover remains controlled and the policy is robust to doubled transaction costs. The modular codebase, comprising a data pipeline, simulator, and training scripts, is engineered for extensibility to multi-asset overlays, alternative objectives (e.g., drawdown or CVaR), and intraday data. From a portfolio management perspective, the learned overlay is designed to sit on top of an existing SPX or SPY allocation, improving the portfolio's mean-variance trade-off with controlled turnover and drawdowns. We discuss practical implications for portfolio overlays and outline avenues for future work.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于强化学习的深度对冲框架，用于在现实交易成本和头寸限制下动态管理股指期权风险敞口。该方法采用无泄漏环境设计、成本感知的奖励函数和轻量级随机演员-评论家智能体，使用SPX/SPY隐含波动率期限结构、偏度、已实现波动率和宏观利率等日频面板数据进行训练。在固定训练/验证/测试集划分下，学习到的策略相比无对冲、动量策略和波动率目标基准展现出更高的风险调整后绩效（夏普比率点估计值更高），且交易周转率受控，策略对加倍交易成本具有稳健性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_dfaa45fb"
  },
  {
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "url": "https://arxiv.org/pdf/2512.14744v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《VERAFI：通过神经符号策略生成实现可验证的代理金融智能》针对金融AI系统在推理中产生计算错误和违反监管规定（即使检索完美）的盲点，提出了一个结合神经符号策略生成的代理框架。VERAFI融合了最先进的密集检索与交叉编码器重排序、支持金融工具的代理，以及覆盖GAAP合规性、SEC要求和数学验证的自动化推理策略。在FinanceBench上的综合评估显示显著改进：传统密集检索加重排序仅实现52.4%的事实正确率，而VERAFI的综合方法达到94.7%，相对提升81%。神经符号策略层本身比纯代理处理贡献了4.3个百分点的增益，专门针对持续的数学和逻辑错误。通过将金融领域专业知识直接整合到推理过程中，VERAFI为在实战交易中实现可信赖的金融智能提供了一条实用路径。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b47b6951"
  },
  {
    "title": "Transfer Learning (Il)liquidity",
    "url": "https://arxiv.org/pdf/2512.11731v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "The estimation of the Risk Neutral Density (RND) implicit in option prices is challenging, especially in illiquid markets. We introduce the Deep Log-Sum-Exp Neural Network, an architecture that leverages Deep and Transfer learning to address RND estimation in the presence of irregular and illiquid strikes. We prove key statistical properties of the model and the consistency of the estimator. We illustrate the benefits of transfer learning to improve the estimation of the RND in severe illiquidity conditions through Monte Carlo simulations, and we test it empirically on SPX data, comparing it with popular estimation methods. Overall, our framework shows recovery of the RND in conditions of extreme illiquidity with as few as three option quotes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《迁移学习（非）流动性》提出了一种用于估计期权价格中隐含风险中性密度（RND）的深度对数求和指数神经网络架构，特别针对非流动性和不规则行权价市场。该模型利用深度学习和迁移学习技术，在极端流动性不足条件下（仅需三个期权报价）仍能有效恢复RND，并通过蒙特卡洛模拟和SPX数据实证验证了其优于传统方法的性能。",
    "fetch_date": "2025-12-26",
    "id": "20251226_f3dbe600"
  },
  {
    "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
    "url": "https://arxiv.org/pdf/2512.17945v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融机构在部署机器学习模型进行信用风险评估时，面临预测准确性与可解释性之间的权衡。单调性约束使模型行为与领域知识保持一致，但其性能成本——即“单调性的代价”——尚未得到充分量化。本文在五个公共数据集和三个库上，对信用违约概率的单调约束与无约束梯度提升模型进行了基准测试。我们将单调性代价定义为从无约束模型转向约束模型时标准性能指标的相对变化，通过配对比较和自助法不确定性进行估计。实验结果表明，AUC的单调性代价范围从基本为零到约2.9%：在大型数据集上约束几乎无成本（通常低于0.2%，常与零无异），而在约束覆盖广泛的小型数据集上成本最高（约2-3%）。因此，适当指定的单调性约束通常能以较小的准确性损失提供可解释性，特别是在大规模信用组合中。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b992ef93"
  },
  {
    "title": "The Impact of Bitcoin ETF Approval on Bitcoin's Hedging Properties Against Traditional Assets",
    "url": "https://arxiv.org/pdf/2512.12815v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "The approval of the Bitcoin Spot ETF in January 2024 marked a transformative event in cryptocurrency markets, signaling increased institutional adoption and integration into traditional finance. This study examines Bitcoin's changing relationships with traditional assets, including equities, gold, and fiat currencies, following this milestone. Using rolling correlation analysis, Chow tests, and DCC-GARCH models, we found that Bitcoin's correlation with the S\\&P 500 increased significantly post-ETF approval, indicating stronger alignment with equities. Its relationship with gold stabilized near zero, while its correlation with the U.S. Dollar Index remained consistently negative, reflecting its continued independence from fiat currencies. These findings offer insights into Bitcoin's evolving role in portfolios, implications for market stability, and future research opportunities on cryptocurrency integration into traditional financial systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "2024年1月比特币现货ETF获批标志着加密货币市场进入机构化与主流金融整合的新阶段。本研究通过滚动相关性分析、邹检验和DCC-GARCH模型，实证分析了该事件后比特币与传统资产（股票、黄金、法币）的动态关系。核心发现：比特币与标普500指数的相关性显著增强，表明其与股票市场联动性提升；与黄金的相关性稳定在零值附近；与美元指数的负相关性持续，保持对法币的独立性。这些结果为投资组合配置、市场稳定性评估及加密货币与传统金融体系融合研究提供了实证依据。",
    "fetch_date": "2025-12-26",
    "id": "20251226_aa031b92"
  },
  {
    "title": "Institutionalizing risk curation in decentralized credit",
    "url": "https://arxiv.org/pdf/2512.11976v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper maps the emerging market for decentralized credit in which ERC 4626 vaults and third-party curators, rather than monolithic lending protocols alone, increasingly determine underwriting and leverage decisions. We show that modular vaults differ in capital utilization, cross-chain and cross asset concentration, and liquidity risk structure. Further, we show that a small set of curators intermediates a disproportionate share of system TVL, exhibits clustered tail co movement, and captures markedly different fee margins despite broadly similar collateral composition. These findings indicate that the main locus of risk in DeFi lending has migrated upward from base protocols, where underwriting is effectively centralized in a single DAO governed parameter set, to a permissionless curator layer in which competing vault managers decide which assets and loans are originated. We argue that this shift requires a corresponding upgrade in transparency standards and outline a simple set of onchain disclosures that would allow users and DAOs to evaluate curator strategies on a comparable, money market style basis.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了去中心化信贷市场中ERC 4626金库和第三方策展人（而非单一借贷协议）日益主导承销和杠杆决策的新兴格局。研究发现模块化金库在资本利用率、跨链跨资产集中度及流动性风险结构方面存在差异，少数策展人中介了不成比例的系统总锁定价值（TVL），表现出聚集性尾部联动，且尽管抵押品构成相似却获得显著不同的费用边际。这表明DeFi借贷的主要风险点已从基础协议（承销权集中于单一DAO治理参数集）上移至无需许可的策展层，由竞争性金库管理者决定资产和贷款的发起。作者主张需相应提升透明度标准，并提出一套简单的链上披露方案，使用户和DAO能以可比的货币市场风格评估策展策略。",
    "fetch_date": "2025-12-26",
    "id": "20251226_470ff790"
  },
  {
    "title": "Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling",
    "url": "https://arxiv.org/pdf/2512.12526v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究对MSCI世界指数应用经验模态分解（EMD），将得到的本征模态函数（IMFs）转化为图表示，以便用图神经网络（GNNs）建模。使用CEEMDAN提取了九个IMF，涵盖高频波动到长期趋势。每个IMF通过四种时间序列到图的方法（自然可见性、水平可见性、递归图和转移图）转化为图。拓扑分析显示明显的尺度依赖结构：高频IMF产生密集、高度连接的小世界图，而低频IMF产生更稀疏、特征路径长度更长的网络。基于可见性的方法对振幅变化更敏感，通常产生更高的聚类，而递归图更好地保留了时间依赖性。这些结果为设计针对分解成分结构特性的GNN架构提供了指导，支持更有效的金融时间序列预测建模。",
    "fetch_date": "2025-12-26",
    "id": "20251226_17fa87b6"
  },
  {
    "title": "Explainable Prediction of Economic Time Series Using IMFs and Neural Networks",
    "url": "https://arxiv.org/pdf/2512.12499v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "This study investigates the contribution of Intrinsic Mode Functions (IMFs) derived from economic time series to the predictive performance of neural network models, specifically Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. To enhance interpretability, DeepSHAP is applied, which estimates the marginal contribution of each IMF while keeping the rest of the series intact. Results show that the last IMFs, representing long-term trends, are generally the most influential according to DeepSHAP, whereas high-frequency IMFs contribute less and may even introduce noise, as evidenced by improved metrics upon their removal. Differences between MLP and LSTM highlight the effect of model architecture on feature relevance distribution, with LSTM allocating importance more evenly across IMFs.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了从经济时间序列中提取的本征模态函数（IMFs）对神经网络模型（特别是多层感知机MLP和长短期记忆网络LSTM）预测性能的贡献。为增强可解释性，应用DeepSHAP方法评估每个IMF的边际贡献，同时保持序列其余部分不变。结果表明，根据DeepSHAP分析，代表长期趋势的最后几个IMF通常最具影响力，而高频IMF贡献较小甚至可能引入噪声——移除这些高频分量后模型指标得到改善。MLP与LSTM之间的差异凸显了模型架构对特征相关性分布的影响，其中LSTM在IMF间的权重分配更为均匀。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b8bacb63"
  },
  {
    "title": "Unified Approach to Portfolio Optimization using the `Gain Probability Density Function' and Applications",
    "url": "https://arxiv.org/pdf/2512.11649v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This article proposes a unified framework for portfolio optimization (PO), recognizing an object called the `gain probability density function (PDF)' as the fundamental object of the problem from which any objective function could be derived. The gain PDF has the advantage of being 1-dimensional for any given portfolio and thus is easy to visualize and interpret. The framework allows us to naturally incorporate all existing approaches (Markowitz, CVaR-deviation, higher moments...) and represents an interesting basis to develop new approaches. It leads us to propose a method to directly match a target PDF defined by the portfolio manager, giving them maximal control on the PO problem and moving beyond approaches that focus only on expected return and risk. As an example, we develop an application involving a new objective function to control high profits, to be applied after a conventional PO (including expected return and risk criteria) and thus leading to sub-optimality w.r.t. the conventional objective function. We then propose a methodology to quantify a cost associated with this optimality deviation in a common budget unit, providing a meaningful information to portfolio managers. Numerical experiments considering portfolios with energy-producing assets illustrate our approach. The framework is flexible and can be applied to other sectors (financial assets, etc).",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种基于“收益概率密度函数（PDF）”的统一投资组合优化框架，该框架将收益PDF视为问题的基本对象，任何目标函数均可从中推导。该PDF具有一维特性，易于可视化和解释，能够自然整合现有方法（如马科维茨、CVaR-偏差、高阶矩等），并为开发新方法提供了基础。文章提出了一种直接匹配投资组合经理定义的目标PDF的方法，使其能超越仅关注预期收益和风险的传统方法，对优化问题实现最大控制。作为示例，文章开发了一种在传统优化后控制高利润的新目标函数应用，这会导致相对于传统目标函数的次优性，并提出了一种以通用预算单位量化这种最优性偏差成本的方法，为投资组合经理提供有意义的信息。",
    "fetch_date": "2025-12-26",
    "id": "20251226_32dbca85"
  },
  {
    "title": "Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall",
    "url": "https://arxiv.org/pdf/2512.12334v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文扩展了动态贝叶斯网络（DBN）在计算市场风险中的应用，用于估计10天97.5%的预期损失（ES）和压力预期损失（SES）。研究以标普500指数为代理，比较了三种DBN结构学习算法与多种传统市场风险模型（使用正态或偏斜t分布）的表现。回测显示，所有模型在2.5%水平下均未能产生统计上准确的ES和SES预测，反映了建模极端尾部行为的困难。对于ES，EGARCH(1,1)模型（正态）预测最准确；对于SES，GARCH(1,1)模型（正态）表现最佳。所有依赖分布的模型在使用偏斜t分布时性能显著下降。DBN的表现与历史模拟法相当。",
    "fetch_date": "2025-12-26",
    "id": "20251226_5b6423f9"
  },
  {
    "title": "Universal Dynamics of Financial Bubbles in Isolated Markets: Evidence from the Iranian Stock Market",
    "url": "https://arxiv.org/pdf/2512.12054v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Speculative bubbles exhibit common statistical signatures across many financial markets, suggesting the presence of universal underlying mechanisms. We test this hypothesis in the Iranian stock market, an economy that is highly isolated, subject to capital controls, and largely inaccessible to foreign investors. Using the Log-Periodic Power Law Singularity (LPPLS) model, we analyze two major bubble episodes in 2020 and 2023. The estimated critical exponents beta around 0.46 and 0.20 fall within the empirical ranges documented for canonical historical bubbles such as the 1929 DJIA crash and the 2000 Nasdaq episode. The Tehran Stock Exchange displays clear LPPLS hallmarks, including faster-than-exponential price acceleration, log-periodic corrections, and stable estimates of the critical time horizon. These results indicate that endogenous herding, imitation, and positive-feedback dynamics, rather than exogenous shocks, play a dominant role even in politically and economically isolated markets. By showing that an emerging and semi-closed financial system conforms to the same dynamical patterns observed in global markets, this paper provides new empirical support for the universality of bubble dynamics. To the best of our knowledge, it also presents the first systematic LPPLS analysis of bubbles in the Tehran Stock Exchange. The findings highlight the usefulness of LPPLS-based diagnostic tools for monitoring systemic risk in emerging or restricted economies.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文通过应用对数周期幂律奇异性（LPPLS）模型，分析了伊朗股市（一个高度孤立、受资本管制且外国投资者难以进入的市场）在2020年和2023年的两次主要泡沫事件。研究发现，其估算的关键指数β值（约0.46和0.20）与历史上经典泡沫（如1929年道琼斯工业平均指数崩盘和2000年纳斯达克泡沫）的经验范围一致。德黑兰证券交易所显示出清晰的LPPLS特征，包括快于指数的价格加速、对数周期性修正以及关键时间范围的稳定估计。结果表明，即使是在政治和经济上孤立的市场中，内生的羊群效应、模仿行为和正反馈动态，而非外生冲击，也起着主导作用。通过展示一个新兴的半封闭金融体系遵循与全球市场相同的动态模式，该论文为泡沫动力学的普适性提供了新的实证支持。",
    "fetch_date": "2025-12-26",
    "id": "20251226_16dc055e"
  },
  {
    "title": "High-Frequency Analysis of a Trading Game with Transient Price Impact",
    "url": "https://arxiv.org/pdf/2512.11765v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study the high-frequency limit of an $n$-trader optimal execution game in discrete time. Traders face transient price impact of Obizhaeva--Wang type in addition to quadratic instantaneous trading costs $θ(ΔX_t)^2$ on each transaction $ΔX_t$. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval $[0,T]$, the discrete equilibrium inventories converge at rate $1/N$ to the continuous-time equilibrium of an Obizhaeva--Wang model with additional quadratic costs $\\vartheta_0(ΔX_0)^2$ and $\\vartheta_T(ΔX_T)^2$ on initial and terminal block trades, where $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case $n=2$ where $\\vartheta_0=\\vartheta_T=1/2$. In particular, we show how the coefficients $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$ arise endogenously in the high-frequency limit: the initial and terminal block costs of the continuous-time model are identified as the limits of the cumulative discrete instantaneous costs incurred over small neighborhoods of $0$ and $T$, respectively, and these limits are independent of $θ>0$. By contrast, when $θ=0$ the discrete-time equilibrium strategies and costs exhibit persistent oscillations and admit no high-frequency limit, mirroring the non-existence of continuous-time equilibria without boundary block costs. Our results show that two different types of trading frictions -- a fine time discretization and small instantaneous costs in continuous time -- have similar regularizing effects and select a canonical model in the limit.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了具有瞬态价格影响的n交易者最优执行博弈的高频极限。交易者面临Obizhaeva-Wang型瞬态价格影响以及每笔交易ΔX_t的二次瞬时交易成本θ(ΔX_t)^2。存在唯一的纳什均衡，交易者选择最小化预期执行成本的清算策略。在高频极限下，离散均衡库存以1/N的速率收敛于具有额外二次成本ϑ_0(ΔX_0)^2和ϑ_T(ΔX_T)^2的Obizhaeva-Wang模型的连续时间均衡，其中ϑ_0=(n-1)/2，ϑ_T=1/2。该结果扩展并改进了先前针对n=2情况的研究。",
    "fetch_date": "2025-12-26",
    "id": "20251226_54f3bc58"
  },
  {
    "title": "Risk Limited Asset Allocation with a Budget Threshold Utility Function and Leptokurtotic Distributions of Returns",
    "url": "https://arxiv.org/pdf/2512.11666v2",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "An analytical solution to single-horizon asset allocation for an investor with a piecewise-linear utility function, called herein the \"budget threshold utility,\" and exogenous position limits is presented. The resulting functional form has a surprisingly simple structure and can be readily interpreted as representing the addition of a simple \"risk cost\" to otherwise frictionless trading.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对具有预算阈值效用函数和外部头寸限制的投资者，提出了单期资产配置的解析解。该解具有简洁的结构，可解释为在无摩擦交易基础上增加了简单的“风险成本”。",
    "fetch_date": "2025-12-26",
    "id": "20251226_2daddc47"
  },
  {
    "title": "Sources and Nonlinearity of High Volume Return Premium: An Empirical Study on the Differential Effects of Investor Identity versus Trading Intensity (2020-2024)",
    "url": "https://arxiv.org/pdf/2512.14134v2",
    "source": "ArXiv",
    "date": "2025-12-16",
    "abstract": "Chae and Kang (2019, \\textit{Pacific-Basin Finance Journal}) documented a puzzling Low Volume Return Premium (LVRP) in Korea -- contradicting global High Volume Return Premium (HVRP) evidence. We resolve this puzzle. Using Korean market data (2020-2024), we demonstrate that HVRP exists in Korea but is masked by (1) pooling heterogeneous investor types and (2) using inappropriate intensity normalization. When institutional buying intensity is normalized by market capitalization rather than trading value, a perfect monotonic relationship emerges: highest-conviction institutional buying (Q4) generates +\\institutionLedQFourDayPlusFiftyCAR\\ cumulative abnormal returns over 50 days, while lowest-intensity trades (Q1) yield modest returns (+\\institutionLedQOneDayPlusFiftyCAR). Retail investors exhibit a flat pattern -- their trading generates near-zero returns regardless of conviction level -- confirming the pure noise trader hypothesis. During the Donghak Ant Movement (2020-2021), however, coordinated retail investors temporarily transformed from noise traders to liquidity providers, generating returns comparable to institutional trading. Our findings reconcile conflicting international evidence and demonstrate that detecting informed trading signals requires investor-type decomposition, nonlinear quartile analysis, and conviction-based (market cap) rather than participation-based (trading value) measurement.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该研究（2020-2024）通过分析韩国市场数据，解决了韩国市场低成交量回报溢价（LVRP）与全球高成交量回报溢价（HVRP）证据相矛盾的谜题。研究发现，当按市值（而非交易额）对机构买入强度进行标准化时，HVRP在韩国市场显现：机构最高确信度买入（Q4）在50天内产生显著累积异常收益（+\\institutionLedQFourDayPlusFiftyCAR），而最低强度交易（Q1）收益较低（+\\institutionLedQOneDayPlusFiftyCAR）。散户交易则呈现平坦模式，收益接近零，符合纯噪声交易者假说；但在东学蚂蚁运动（2020-2021）期间，协调行动的散户暂时转变为流动性提供者，产生与机构相当的收益。研究通过区分投资者身份（机构vs散户）和交易强度标准化方法，调和了国际上的矛盾发现，对实战交易中识别机构驱动信号、优化因子构建具有直接价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_63fdd5c4"
  },
  {
    "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
    "url": "https://arxiv.org/pdf/2512.12924v1",
    "source": "ArXiv",
    "date": "2025-12-15",
    "abstract": "We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种可解释的假设驱动交易框架，采用严格的向前滚动验证方法，旨在减少过拟合和前瞻性偏差。该框架结合了可解释的假设驱动信号生成与强化学习，并执行严格的样本外测试。方法包括：严格执行信息集纪律、在34个独立测试期进行滚动窗口验证、通过自然语言假设解释保持完全可解释性、纳入实际交易成本和头寸约束。在2015-2024年间对100只美国股票验证五种市场微观结构模式，系统产生适中的年化收益（0.55%，夏普比率0.33），具有出色的下行保护（最大回撤-2.76%）和市场中性特征（beta=0.058）。表现呈现强烈的制度依赖性，在高波动期（2020-2024年，季度收益0.60%）产生正收益，而在稳定市场（2015-2019年，-0.16%）表现不佳。报告统计上不显著的总体结果（p值0.34），以展示一个可重复、诚实的验证协议，优先考虑可解释性。",
    "fetch_date": "2025-12-25",
    "id": "20251225_8db06af9"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading: Challenges and opportunities",
    "url": "https://ieeexplore.ieee.org/abstract/document/9779600/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… rise of quantitative trading (QT), which refers to automatically generating trading signals with … for more than 70% and 40% of the trading volume in developed markets (eg, USA) and …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了深度强化学习在量化交易中的应用，量化交易指通过算法自动生成交易信号，在发达市场（如美国）已占据超过70%和40%的交易量。论文分析了深度强化学习在量化交易领域面临的挑战与机遇，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_e3153d4f"
  },
  {
    "title": "Trademaster: A holistic quantitative trading platform empowered by reinforcement learning",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b8f6f7f2ba4137124ac976286eacb611-Abstract-Datasets_and_Benchmarks.html",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… As shown in Figure 2, we formulate quantitative trading tasks as a Markov Decision Process (MDP) following a standard RL scenario, where an agent (investor) interacts with an …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "Trademaster是一个基于强化学习的综合性量化交易平台。该研究将量化交易任务建模为马尔可夫决策过程，遵循标准强化学习框架，其中智能体（投资者）与环境交互进行决策。",
    "fetch_date": "2025-12-25",
    "id": "20251225_fe85737d"
  },
  {
    "title": "FinRL: Deep reinforcement learning framework to automate trading in quantitative finance",
    "url": "https://dl.acm.org/doi/abs/10.1145/3490354.3494366",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this paper, we present the first open-source framework FinRL as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了首个开源框架FinRL，作为一个完整的流程，旨在帮助量化交易员克服陡峭的学习曲线。FinRL以简洁性为特点，利用深度强化学习自动化量化金融中的交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_f95bf03c"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading",
    "url": "https://ieeexplore.ieee.org/abstract/document/10626209/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this section, we explore the distinctive attributes of Quantitative Trading (QT) and elaborate on the rationale behind framing the entire QT process as a Partially Observable Markov …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了将深度强化学习应用于量化交易，将整个量化交易过程建模为部分可观测马尔可夫决策过程，具有实战价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_d085e88f"
  },
  {
    "title": "Reinforcement Learning Framework for Quantitative Trading",
    "url": "https://arxiv.org/abs/2411.07585",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… We investigate how a reinforcement learning agent can utilize financial indicators in specific market conditions and trends to enhance overall trading accuracy. By understanding the …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文研究强化学习智能体如何利用特定市场条件和趋势下的金融指标来提升整体交易准确性。通过理解市场动态，该框架旨在优化量化交易策略。",
    "fetch_date": "2025-12-25",
    "id": "20251225_c5574310"
  },
  {
    "title": "Deep reinforcement learning in quantitative algorithmic trading: A review",
    "url": "https://arxiv.org/abs/2106.00123",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… Deep Reinforcement Learning (DRL) agents proved to be to … reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度强化学习在量化算法交易中的应用综述：该论文聚焦于深度强化学习（DRL）在金融人工智能子领域——特别是自动化低频量化股票交易中的实际应用。研究表明，DRL智能体在该领域展现出潜力，通过结合强化学习与深度学习技术，探索在实战交易中生成阿尔法（Alpha）的策略，具有较高的实践参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_307e048c"
  },
  {
    "title": "Quantitative trading on stock market based on deep reinforcement learning",
    "url": "https://ieeexplore.ieee.org/abstract/document/8851831/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… intelligence, quantitative trading attracts … reinforcement learning in quantitative trading. A LSTM-based agent is proposed to learn the temporal pattern in data and automatically trades …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度强化学习的股票市场量化交易方法，采用LSTM智能体学习数据中的时序模式，实现自动化交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_92f14b99"
  }
]