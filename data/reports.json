[
  {
    "title": "The Limits of Complexity: Why Feature Engineering Beats Deep Learning in Investor Flow Prediction",
    "url": "https://arxiv.org/pdf/2601.07131v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "The application of machine learning to financial prediction has accelerated dramatically, yet the conditions under which complex models outperform simple alternatives remain poorly understood. This paper investigates whether advanced signal processing and deep learning techniques can extract predictive value from investor order flows beyond what simple feature engineering achieves. Using a comprehensive dataset of 2.79 million observations spanning 2,439 Korean equities from 2020--2024, we apply three methodologies: \\textit{Independent Component Analysis} (ICA) to recover latent market drivers, \\textit{Wavelet Coherence} analysis to characterize multi-scale correlation structure, and \\textit{Long Short-Term Memory} (LSTM) networks with attention mechanisms for non-linear prediction. Our results reveal a striking finding: a parsimonious linear model using market capitalization-normalized flows (``Matched Filter'' preprocessing) achieves a Sharpe ratio of 1.30 and cumulative return of 272.6\\%, while the full ICA-Wavelet-LSTM pipeline generates a Sharpe ratio of only 0.07 with a cumulative return of $-5.1\\%$. The raw LSTM model collapsed to predicting the unconditional mean, achieving a hit rate of 47.5\\% -- worse than random. We conclude that in low signal-to-noise financial environments, domain-specific feature engineering yields substantially higher marginal returns than algorithmic complexity. These findings establish important boundary conditions for the application of deep learning to financial prediction.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "论文《复杂性的局限：为何在投资者流量预测中特征工程优于深度学习》通过分析2020-2024年韩国2439只股票的279万条数据，对比了独立成分分析（ICA）、小波相干性分析和带注意力机制的LSTM网络等复杂方法，与简单的市值标准化流量（“匹配滤波器”预处理）线性模型。结果显示，简约线性模型的夏普比率达1.30，累计回报272.6%，而复杂深度学习流程的夏普比率仅0.07，累计回报-5.1%。该研究对实战交易具有重要价值，表明在特定金融预测任务中，精心设计的特征工程可能比过度复杂的深度学习模型更有效，为量化交易中的模型选择提供了实证依据。",
    "fetch_date": "2026-01-14",
    "id": "20260114_33e35d4c"
  },
  {
    "title": "Non-Convex Portfolio Optimization via Energy-Based Models: A Comparative Analysis Using the Thermodynamic HypergRaphical Model Library (THRML) for Index Tracking",
    "url": "https://arxiv.org/pdf/2601.07792v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "Portfolio optimization under cardinality constraints transforms the classical Markowitz mean-variance problem from a convex quadratic problem into an NP-hard combinatorial optimization problem. This paper introduces a novel approach using THRML (Thermodynamic HypergRaphical Model Library), a JAX-based library for building and sampling probabilistic graphical models that reformulates index tracking as probabilistic inference on an Ising Hamiltonian. Unlike traditional methods that seek a single optimal solution, THRML samples from the Boltzmann distribution of high-quality portfolios using GPU-accelerated block Gibbs sampling, providing natural regularization against overfitting.\n  We implement three key innovations: (1) dynamic coupling strength that scales inversely with market volatility (VIX), adapting diversification pressure to market regimes; (2) rebalanced bias weights prioritizing tracking quality over momentum for index replication; and (3) sector-aware post-processing ensuring institutional-grade diversification. Backtesting on a 100-stock S and P 500 universe from 2023 to 2025 demonstrates that THRML achieves 4.31 percent annualized tracking error versus 5.66 to 6.30 percent for baselines, while simultaneously generating 128.63 percent total return against the index total return of 79.61 percent. The Diebold-Mariano test confirms statistical significance with p less than 0.0001 across all comparisons. These results position energy-based models as a promising paradigm for portfolio construction, bridging statistical mechanics and quantitative finance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种基于THRML（热力学超图模型库）的非凸投资组合优化新方法，将指数跟踪问题转化为伊辛哈密顿量的概率推断问题。核心创新包括：1）动态耦合强度（与市场波动率VIX成反比），根据市场状况调整分散化压力；2）再平衡偏置权重，优先考虑跟踪质量而非动量；3）行业感知后处理，确保机构级分散化。在2023-2025年S&P 500的100只股票回测中，THRML实现了4.31%的年化跟踪误差，优于传统方法的5.66-6.30%。该方法通过GPU加速的块吉布斯采样从高质量投资组合的玻尔兹曼分布中采样，提供自然正则化以防止过拟合，对实战交易具有较高价值。",
    "fetch_date": "2026-01-14",
    "id": "20260114_fa0320b7"
  },
  {
    "title": "Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction",
    "url": "https://arxiv.org/pdf/2511.07014v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "Diffolio：一种用于多元概率金融时间序列预测和投资组合构建的扩散模型。该模型采用具有分层注意力架构的去噪网络，包含资产层面和市场层面。通过引入基于目标相关矩阵稳定估计的相关引导正则化器，更好地反映横截面相关性。实验表明，在12个行业投资组合的日超额收益上，Diffolio在多元预测准确性和投资组合表现上优于多种概率预测基准，构建的投资组合展现出持续稳健的性能。",
    "fetch_date": "2026-01-14",
    "id": "20260114_fcd6445c"
  },
  {
    "title": "Physics-Informed Singular-Value Learning for Cross-Covariances Forecasting in Financial Markets",
    "url": "https://arxiv.org/pdf/2601.07687v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "A new wave of work on covariance cleaning and nonlinear shrinkage has delivered asymptotically optimal analytical solutions for large covariance matrices. Building on this progress, these ideas have been generalized to empirical cross-covariance matrices, whose singular-value shrinkage characterizes comovements between one set of assets and another. Existing analytical cross-covariance cleaners are derived under strong stationarity and large-sample assumptions, and they typically rely on mesoscopic regularity conditions such as bounded spectra; macroscopic common modes (e.g., a global market factor) violate these conditions. When applied to real equity returns, where dependence structures drift over time and global modes are prominent, we find that these theoretically optimal formulas do not translate into robust out-of-sample performance. We address this gap by designing a random-matrix-inspired neural architecture that operates in the empirical singular-vector basis and learns a nonlinear mapping from empirical singular values to their corresponding cleaned values. By construction, the network can recover the analytical solution as a special case, yet it remains flexible enough to adapt to non-stationary dynamics and mode-driven distortions. Trained on a long history of equity returns, the proposed method achieves a more favorable bias-variance trade-off than purely analytical cleaners and delivers systematically lower out-of-sample cross-covariance prediction errors. Our results demonstrate that combining random-matrix theory with machine learning makes asymptotic theories practically effective in realistic time-varying markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于随机矩阵理论启发的神经网络架构，用于金融市场的交叉协方差矩阵预测。针对现有理论最优公式在真实股票收益数据（存在时变依赖结构和显著全局模式）中样本外表现不佳的问题，该方法在经验奇异向量基上操作，学习从经验奇异值到其清洗值的非线性映射。网络设计能恢复理论解作为特例，同时保持足够灵活性以适应实际市场条件，对实战交易中改进协方差估计和风险管理具有潜在应用价值。",
    "fetch_date": "2026-01-14",
    "id": "20260114_a78ab310"
  },
  {
    "title": "Emissions-Robust Portfolios",
    "url": "https://arxiv.org/pdf/2601.06507v1",
    "source": "ArXiv",
    "date": "2026-01-10",
    "abstract": "We study portfolio choice when firm-level emissions intensities are measured with error. We introduce a scope-specific penalty operator that rescales asset payoffs as a smooth function of revenue-normalized emissions intensity. Under payoff homogeneity, unit-scale invariance, mixture linearity, and a curvature semigroup axiom, the operator is unique and has the closed form $P^{(m)}_j(r,λ)=\\bigl(1-λ/λ_{\\max,j}\\bigr)^m r$. Combining this operator with norm- and moment-constrained ambiguity sets yields robust mean--variance and CVaR programs with exact linear and second-order cone reformulations and economically interpretable dual variables. In a U.S. large-cap equity universe with monthly rebalancing and uniform transaction costs, the resulting strategy reduces average Scope~1 emissions intensity by roughly 92\\% relative to equal weight while exhibiting no statistically detectable reduction in the Sharpe ratio under block-bootstrap inference and no statistically detectable change in average returns under HAC inference. We report the return--emissions Pareto frontier, sensitivity to robustness and turnover constraints, and uncertainty propagation from multiple imputation of emissions disclosures.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究当企业层面的排放强度存在测量误差时的投资组合选择问题。作者引入了一种特定范围的惩罚算子，将资产收益重新缩放为收入标准化排放强度的平滑函数。在收益同质性、单位尺度不变性、混合线性及曲率半群公理下，该算子具有唯一性，并给出闭式解。结合范数和矩约束的模糊集，该方法可转化为具有精确线性及二阶锥重构的稳健均值-方差和CVaR规划模型，其双重变量具有经济解释性。在美国大盘股月度再平衡及统一交易成本的实证中，该策略相对于等权重组合将平均Scope 1排放强度降低了约92%，同时基于块自助法推断的夏普比率无统计显著下降，基于HAC推断的平均收益也无统计显著变化。研究还报告了收益-排放的帕累托前沿、对稳健性和换手约束的敏感性，以及排放多重插补的不确定性传播。",
    "fetch_date": "2026-01-14",
    "id": "20260114_2c2dea9b"
  },
  {
    "title": "Cross-Market Alpha: Testing Short-Term Trading Factors in the U.S. Market via Double-Selection LASSO",
    "url": "https://arxiv.org/pdf/2601.06499v1",
    "source": "ArXiv",
    "date": "2026-01-10",
    "abstract": "Current asset pricing research exhibits a significant gap: a lack of sufficient cross-market validation regarding short-term trading-based factors. Against this backdrop, the development of the Chinese A-share market which is characterized by its retail-investor dominance, policy sensitivity, and high-frequency active trading -- has given rise to specific short-term trading-based factors. This study systematically examines the universality of factors from the Alpha191 library in the U.S. market, addressing the challenge of high-dimensional factor screening through the double-selection LASSO algorithm an established method for cross-market, high-dimensional research. After controlling for 151 fundamental factors from the U.S. equity factor zoo, 17 Alpha191 factors selected by this procedure exhibit significant incremental explanatory power for the cross-section of U.S. stock returns at the 5% level. Together these findings demonstrate that short-term trading-based factors, originating from the unique structure of the Chinese A-share market, provide incremental information not captured by existing mainstream pricing models, thereby enhancing the explanation of cross-sectional return differences.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "当前资产定价研究存在显著空白：缺乏关于短期交易因子的充分跨市场验证。在此背景下，以散户主导、政策敏感、高频活跃交易为特征的中国A股市场催生了特定的短期交易因子。本研究通过双选择LASSO算法（一种成熟的跨市场高维研究方法）系统检验了Alpha191因子库中因子在美国市场的普适性。在控制了美国股票因子动物园中的151个基本面因子后，该程序筛选出的17个Alpha191因子在5%显著性水平上对美国股票横截面收益具有显著的增量解释力。这些发现表明，源自中国A股市场独特结构的短期交易因子提供了现有主流定价模型未捕捉的增量信息，从而增强了对横截面收益的解释。",
    "fetch_date": "2026-01-14",
    "id": "20260114_b879dc2b"
  },
  {
    "title": "Deep Q-Network Based Resilient Drone Communication:Neutralizing First-Order Markov Jammers",
    "url": "https://arxiv.org/pdf/2601.06095v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Deep Reinforcement Learning based solution for jamming communications using Frequency Hopping Spread Spectrum technology in a 16 channel radio environment is presented. Deep Q Network based transmitter continuously selects the next frequency hopping channel while facing first order reactive jamming, which uses observed transition statistics to predict and interrupt transmissions. Through self training, the proposed agent learns a uniform random frequency hopping policy that effectively neutralizes the predictive advantage of the jamming. In the presence of Rayleigh fading and additive noise, the impact of forward error correction Bose Chaudhuri Hocquenghem type codes is systematically evaluated, demonstrating that even moderate redundancy significantly reduces packet loss. Extensive visualization of the learning dynamics, channel utilization distribution, epsilon greedy decay, cumulative reward, BER and SNR evolution, and detailed packet loss tables confirms convergence to a near optimal jamming strategy. The results provide a practical framework for autonomous resilient communications in modern electronic warfare scenarios.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于深度强化学习的抗干扰无人机通信方案，针对采用一阶马尔可夫模型的反应式干扰机。系统在16信道环境中使用跳频扩频技术，通过深度Q网络（DQN）发射机动态选择跳频信道，并利用自训练学习均匀随机跳频策略，有效抵消干扰机的预测优势。研究还系统评估了瑞利衰落和加性噪声下Bose-Chaudhuri-Hocquenghem（BCH）前向纠错码的影响，证明适度冗余能显著降低丢包率。通过学习动态、信道利用率分布、ε-贪婪衰减、累积奖励、误码率与信噪比演化等多维度可视化分析，验证了方案收敛至接近最优的抗干扰策略，为现代电子战场景下的自主弹性通信提供了实用框架。",
    "fetch_date": "2026-01-14",
    "id": "20260114_11f9766d"
  },
  {
    "title": "Correlation Structures and Regime Shifts in Nordic Stock Markets",
    "url": "https://arxiv.org/pdf/2601.06090v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Financial markets are complex adaptive systems characterized by collective behavior and abrupt regime shifts, particularly during crises. This paper studies time-varying dependencies in Nordic equity markets and examines whether correlation-eigenstructure dynamics can be exploited for regime-aware portfolio construction. Using two decades of daily data for the OMXS30, OMXC20, and OMXH25 universes, pronounced regime dependence in rolling correlation matrices is documented: crisis episodes are characterized by sharp increases in the leading eigenvalue and counter-cyclical behavior in the second eigenvalue. Eigenportfolio regressions further support a market-factor interpretation of the dominant eigenmode. Building on these findings, an adaptive portfolio allocation framework is proposed, combining correlation-matrix cleaning, an eigenvalue-ratio crisis indicator and long-only minimum-variance optimization with constraints that bound exposures to dominant eigenmodes. Backtesting results indicate improved downside protection and risk-adjusted performance during stress regimes, while remaining competitive with state-of-the-art benchmarks in tranquil periods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融市场作为复杂自适应系统，表现出集体行为和急剧的体制转换，尤其在危机期间。本文研究了北欧股票市场的时变依赖性，并探讨了能否利用相关性-特征结构动态进行体制感知的投资组合构建。基于OMXS30、OMXC20和OMXH25指数二十年的日度数据，研究发现滚动相关性矩阵具有显著的体制依赖性：危机时期表现为领先特征值急剧上升和第二特征值的逆周期行为。特征投资组合回归进一步支持了主导特征模式的市场因子解释。基于这些发现，本文提出了一个自适应投资组合配置框架，结合相关性矩阵清洗、特征值比率危机指标以及仅做多的最小方差优化，并约束对主导特征模式的敞口。回测结果表明，在压力体制下该框架能改善下行保护和风险调整后表现，同时在平静期与先进基准保持竞争力。",
    "fetch_date": "2026-01-14",
    "id": "20260114_217d6bf3"
  },
  {
    "title": "Who sets the range? Funding mechanics and 4h context in crypto markets",
    "url": "https://arxiv.org/pdf/2601.06084v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Financial markets often appear chaotic, yet ranges are rarely accidental. They emerge from structured interactions between market context and capital conditions. The four-hour timeframe provides a critical lens for observing this equilibrium zone where institutional positioning, leveraged exposure, and liquidity management converge. Funding mechanisms, especially in perpetual futures, act as disciplinary forces that regulate trader behavior, impose economic costs, and shape directional commitment. When funding aligns with the prevailing 4H context, price expansion becomes possible; when it diverges, compression and range-bound behavior dominate. Ranges therefore represent controlled balance rather than indecision, reflecting strategic positioning by informed participants. Understanding how 4H context and funding operate as market governors is essential for interpreting cryptocurrency price action as a rational, power-mediated process.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融市场常显混沌，但价格区间鲜为偶然。它们源于市场背景与资本条件的结构化互动。四小时时间框架为观察这一均衡区提供了关键视角，机构头寸、杠杆敞口和流动性管理在此交汇。资金机制（尤其是永续合约中的）作为纪律性力量，调节交易者行为、施加经济成本并塑造方向性承诺。当资金机制与主导的4H背景一致时，价格扩张成为可能；当两者背离时，压缩和区间震荡行为占主导。因此，区间代表受控平衡而非犹豫不决，反映了知情参与者的战略布局。理解4H背景和资金机制如何作为市场调节器运作，对于将加密货币价格行为解读为理性、权力中介的过程至关重要。",
    "fetch_date": "2026-01-14",
    "id": "20260114_c5b798d0"
  },
  {
    "title": "Crypto Pricing with Hidden Factors",
    "url": "https://arxiv.org/pdf/2601.07664v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "We estimate risk premia in the cross-section of cryptocurrency returns using the Giglio-Xiu (2021) three-pass approach, allowing for omitted latent factors alongside observed stock-market and crypto-market factors. Using weekly data on a broad universe of large cryptocurrencies, we find that crypto expected returns load on both crypto-specific factors and selected equity-industry factors associated with technology and profitability, consistent with increased integration between crypto and traditional markets. In addition, we study non-tradable state variables capturing investor sentiment (Fear and Greed), speculative rotation (Altcoin Season Index), and security shocks (hacked value scaled by market capitalization), which are new to the literature. Relative to conventional Fama-MacBeth estimates, the latent-factor approach yields materially different premia for key factors, highlighting the importance of controlling for unobserved risks in crypto asset pricing.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究采用Giglio-Xiu（2021）三通方法，在控制股票市场和加密货币市场观测因子的同时，允许存在未被观测的潜在因子，以估计加密货币横截面收益的风险溢价。基于广泛大型加密货币的周度数据，研究发现加密货币预期收益同时受加密特定因子以及与技术和盈利能力相关的选定股票行业因子影响，这符合加密与传统市场日益融合的趋势。此外，研究还引入了文献中首次探讨的非交易性状态变量，包括投资者情绪（恐惧与贪婪指数）、投机轮动（山寨币季节指数）和安全冲击（黑客攻击损失市值占比）。相较于传统的Fama-MacBeth估计，潜在因子方法对关键因子的风险溢价估计存在显著差异，凸显了在加密资产定价中控制未观测风险的重要性。",
    "fetch_date": "2026-01-14",
    "id": "20260114_f4ddbdff"
  },
  {
    "title": "Tab-TRM: Tiny Recursive Model for Insurance Pricing on Tabular Data",
    "url": "https://arxiv.org/pdf/2601.07675v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "We introduce Tab-TRM (Tabular-Tiny Recursive Model), a network architecture that adapts the recursive latent reasoning paradigm of Tiny Recursive Models (TRMs) to insurance modeling. Drawing inspiration from both the Hierarchical Reasoning Model (HRM) and its simplified successor TRM, the Tab-TRM model makes predictions by reasoning over the input features. It maintains two learnable latent tokens - an answer token and a reasoning state - that are iteratively refined by a compact, parameter-efficient recursive network. The recursive processing layer repeatedly updates the reasoning state given the full token sequence and then refines the answer token, in close analogy with iterative insurance pricing schemes. Conceptually, Tab-TRM bridges classical actuarial workflows - iterative generalized linear model fitting and minimum-bias calibration - on the one hand, and modern machine learning, in terms of Gradient Boosting Machines, on the other.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "Tab-TRM：一种针对表格数据的保险定价微小递归模型。该网络架构将微小递归模型（TRMs）的递归潜在推理范式应用于保险建模，灵感来源于分层推理模型（HRM）及其简化后继TRM。模型通过两个可学习的潜在标记（答案标记和推理状态）进行预测，这些标记由紧凑、参数高效的递归网络迭代优化。递归处理层在给定完整标记序列的情况下重复更新推理状态，然后优化答案标记，类似于迭代保险定价方案。概念上，Tab-TRM一方面连接了经典精算工作流程（迭代广义线性模型拟合和最小偏差校准），另一方面连接了现代机器学习（如梯度提升机）。",
    "fetch_date": "2026-01-14",
    "id": "20260114_c5945c37"
  },
  {
    "title": "Temporal-Aligned Meta-Learning for Risk Management: A Stacking Approach for Multi-Source Credit Scoring",
    "url": "https://arxiv.org/pdf/2601.07588v1",
    "source": "ArXiv",
    "date": "2026-01-12",
    "abstract": "This paper presents a meta-learning framework for credit risk assessment of Italian Small and Medium Enterprises (SMEs) that explicitly addresses the temporal misalignment of credit scoring models.\n  The approach aligns financial statement reference dates with evaluation dates, mitigating bias arising from publication delays and asynchronous data sources. It is based on a two-step temporal decomposition that at first estimates annual probabilities of default (PDs) anchored to balance-sheet reference dates (December 31st) through a static model. Then it models the monthly evolution of PDs using higher-frequency behavioral data. Finally, we employ stacking-based architecture to aggregate multiple scoring systems, each capturing complementary aspects of default risk, into a unified predictive model. In this way, first level model outputs are treated as learned representations that encode non-linear relationships in financial and behavioral indicators, allowing integration of new expert-based features without retraining base models. This design provides a coherent and interpretable solution to challenges typical of low-default environments, including heterogeneous default definitions and reporting delays. Empirical validation shows that the framework effectively captures credit risk evolution over time, improving temporal consistency and predictive stability relative to standard ensemble methods.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于意大利中小企业信用风险评估的元学习框架，专门解决信用评分模型的时间错位问题。该方法通过两步时间分解：首先使用静态模型估计基于资产负债表参考日期（12月31日）的年度违约概率（PD），然后利用高频行为数据建模PD的月度演变。最后采用基于堆叠的架构整合多个评分系统，将一级模型输出作为学习表示，编码财务和行为指标的非线性关系，允许集成新的专家特征而无需重新训练基础模型。该设计为低违约环境中的典型挑战提供了连贯且可解释的解决方案。",
    "fetch_date": "2026-01-14",
    "id": "20260114_31d1358a"
  },
  {
    "title": "A Three--Dimensional Efficient Surface for Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2601.06271v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "The classical mean-variance framework characterizes portfolio risk solely through return variance and the covariance matrix, implicitly assuming that all relevant sources of risk are captured by second moments. In modern financial markets, however, shocks often propagate through complex networks of interconnections, giving rise to systemic and spillover risks that variance alone does not reflect.\n  This paper develops a unified portfolio optimization framework that incorporates connectedness risk alongside expected return and variance. Using a quadratic measure of network spillovers derived from a connectedness matrix, we formulate a three-objective optimization problem and characterize the resulting three-dimensional efficient surface. We establish existence, uniqueness, and continuity of optimal portfolios under mild regularity conditions and derive closed-form solutions when short-selling is allowed. The trade-off between variance and connectedness is shown to be strictly monotone except in degenerate cases, yielding a well-defined risk-risk frontier.\n  Under simultaneous diagonalizability of the covariance and connectedness matrices, we prove a three-fund separation theorem: all efficient portfolios can be expressed as affine combinations of a minimum-variance portfolio, a minimum-connectedness portfolio, and the tangency portfolio. The framework clarifies how network-based risk alters classical diversification results and provides a transparent theoretical foundation for incorporating systemic connectedness into portfolio choice.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种统一投资组合优化框架，在传统均值-方差框架基础上引入了连通性风险作为第三维度。通过从连通性矩阵导出的网络溢出二次度量，构建了三维目标优化问题，并刻画了三维有效前沿。论文证明了最优投资组合的存在性、唯一性和连续性，在允许卖空条件下给出了闭式解，并展示了方差与连通性之间的严格单调权衡关系。在协方差矩阵与连通性矩阵同时可对角化条件下，证明了三维基金分离定理。该研究主要贡献在于理论框架构建和数学性质证明，未涉及具体交易策略、回测验证或市场应用案例。",
    "fetch_date": "2026-01-14",
    "id": "20260114_1a604acf"
  },
  {
    "title": "PriceSeer: Evaluating Large Language Models in Real-Time Stock Prediction",
    "url": "https://arxiv.org/pdf/2601.06088v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Stock prediction, a subject closely related to people's investment activities in fully dynamic and live environments, has been widely studied. Current large language models (LLMs) have shown remarkable potential in various domains, exhibiting expert-level performance through advanced reasoning and contextual understanding. In this paper, we introduce PriceSeer, a live, dynamic, and data-uncontaminated benchmark specifically designed for LLMs performing stock prediction tasks. Specifically, PriceSeer includes 110 U.S. stocks from 11 industrial sectors, with each containing 249 historical data points. Our benchmark implements both internal and external information expansion, where LLMs receive extra financial indicators, news, and fake news to perform stock price prediction. We evaluate six cutting-edge LLMs under different prediction horizons, demonstrating their potential in generating investment strategies after obtaining accurate price predictions for different sectors. Additionally, we provide analyses of LLMs' suboptimal performance in long-term predictions, including the vulnerability to fake news and specific industries. The code and evaluation data will be open-sourced at https://github.com/BobLiang2113/PriceSeer.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了PriceSeer，一个专为大型语言模型（LLMs）执行股票预测任务设计的实时、动态且数据无污染的基准测试。该基准包含来自11个工业部门的110只美国股票，每只股票包含249个历史数据点，并通过内部和外部信息扩展（如额外金融指标、新闻和虚假新闻）来评估LLMs在不同预测周期下的表现。研究展示了LLMs在获取准确价格预测后生成投资策略的潜力，并分析了其在长期预测中的不足，包括对虚假新闻和特定行业的脆弱性。代码和评估数据将开源。",
    "fetch_date": "2026-01-14",
    "id": "20260114_0822b69a"
  },
  {
    "title": "A Clarifying Note on Long-Horizon Investment and Dollar-Cost Averaging: An Effective Investment Exposure Perspective",
    "url": "https://arxiv.org/pdf/2601.06074v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "It is widely claimed in investment education and practice that extending the investment horizon reduces risk, and that diversifying investment timing, for example through dollar-cost averaging (DCA), further mitigates investment risk. Although such claims are intuitively appealing, they are often stated without precise definitions of risk or a clear separation between risk and uncertainty.\n  This paper revisits these two beliefs within a unified probabilistic framework. We define risk at the expectation level as a property of the generating distribution of cumulative investment outcomes, and distinguish it from uncertainty, understood as the dispersion of realized outcomes across possible paths. To enable meaningful comparisons across horizons and investment schedules, we introduce the notion of effective investment exposure, defined as time-integrated invested capital.\n  Under stationary return processes with finite variance, we show that extending the investment horizon does not alter expected risk, expected return, or the risk-return ratio on a per-unit-exposure basis. In contrast, different investment timing strategies can induce distinct exposure profiles over time. As a result, lump-sum investment and dollar-cost averaging may differ not only in uncertainty but also in expected risk when compared at equal return exposure, although the resulting risk differences are of constant order and do not grow with the investment horizon.\n  These results clarify why common narratives surrounding long-horizon investment and dollar-cost averaging are conceptually misleading, while also explaining why adopting such strategies under budgetary or timing constraints need not be regarded as irrational.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在统一的概率框架下重新审视了“延长投资期限降低风险”和“分散投资时机（如定投）进一步降低风险”这两种常见观点。作者明确定义了风险（期望层面的累积投资结果分布特性）与不确定性（实现结果在不同路径上的离散度），并引入了“有效投资暴露”（时间积分投资资本）的概念进行比较分析。研究表明：在有限方差的平稳收益过程中，延长投资期限不会改变单位暴露的预期风险、预期收益或风险收益比；而不同的投资时机策略会产生随时间变化的暴露特征。",
    "fetch_date": "2026-01-14",
    "id": "20260114_a6e0d2ea"
  },
  {
    "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
    "url": "https://arxiv.org/pdf/2601.05975v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们提出DeePM（深度投资组合管理器），一种结构化深度学习宏观投资组合管理器，通过端到端训练以最大化稳健的风险调整后效用。DeePM解决了金融学习中的三个基本挑战：（1）通过定向延迟（因果筛选）机制解决异步“不规则过滤”问题，优先考虑因果脉冲响应学习而非信息新鲜度；（2）通过宏观经济图先验对抗低信噪比，根据经济第一原理正则化跨资产依赖性；（3）优化分布稳健目标，其中平滑的最差窗口惩罚作为熵风险价值（EVaR）的可微代理——一种窗口稳健的效用，鼓励在最不利历史子期间表现强劲。在2010-2025年对50种多元化期货进行的大规模回测中，DeePM仅使用每日收盘价就实现了净风险调整后收益，大约是经典趋势跟踪策略和被动基准的两倍。此外，DeePM相对于最先进的动量Transformer架构提升了约50%。",
    "fetch_date": "2026-01-13",
    "id": "20260113_f8fa8183"
  },
  {
    "title": "Forecast-to-Fill: Benchmark-Neutral Alpha and Billion-Dollar Capacity in Gold Futures (2015-2025)",
    "url": "https://arxiv.org/pdf/2511.08571v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We test whether simple, interpretable state variables-trend and momentum-can generate durable out-of-sample alpha in one of the world's most liquid assets, gold. Using a rolling 10-year training and 6-month testing walk-forward from 2015 to 2025 (2,793 trading days), we convert a smoothed trend-momentum regime signal into volatility-targeted, friction-aware positions through fractional, impact-adjusted Kelly sizing and ATR-based exits. Out of sample, the strategy delivers a Sharpe ratio of 2.88 and a maximum drawdown of 0.52 percent, net of 0.7 basis-point linear cost and a square-root impact term (gamma = 0.02). A regression on spot-gold returns yields a 43 percent annualized return (CAGR approximately 43 percent) and a 37 percent alpha (Sharpe = 2.88, IR = 2.09) at a 15 percent volatility target with beta approximately 0.03, confirming benchmark-neutral performance. Bootstrap confidence intervals ([2.49, 3.27]) and SPA tests (p = 0.000) confirm statistical significance and robustness to latency, reversal, and cost stress. We conclude that forecast-to-fill engineering-linking transparent signals to executable trades with explicit risk, cost, and impact control-can transform modest predictability into allocator-grade, billion-dollar-scalable alpha.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们测试了简单、可解释的状态变量——趋势和动量——能否在全球最具流动性的资产之一黄金中产生持久的样本外阿尔法。采用2015年至2025年滚动10年训练和6个月测试的前向验证（2,793个交易日），通过分数、影响调整的凯利头寸规模和基于ATR的退出，将平滑的趋势-动量机制信号转化为波动率目标、考虑摩擦的仓位。样本外，该策略在扣除0.7个基点的线性成本和平方根影响项（gamma = 0.02）后，实现了2.88的夏普比率和0.52%的最大回撤。对现货黄金收益的回归显示，在15%的波动率目标下，年化收益率为43%（复合年增长率约43%），阿尔法为37%（夏普比率=2.88，信息比率=2.09），贝塔约0.03，确认了基准中性表现。自助法置信区间（[2.49, 3.27]）和SPA检验（p = 0.000）证实了统计显著性以及对延迟、反转和成本压力的稳健性。我们得出结论，预测到执行工程——将透明信号与具有明确风险、成本和影响控制的可执行交易联系起来——可以转化适度的信号为实际交易价值。",
    "fetch_date": "2026-01-13",
    "id": "20260113_bf90557f"
  },
  {
    "title": "Multi-Period Martingale Optimal Transport: Classical Theory, Neural Acceleration, and Financial Applications",
    "url": "https://arxiv.org/pdf/2601.05290v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "This paper develops a computational framework for Multi-Period Martingale Optimal Transport (MMOT), addressing convergence rates, algorithmic efficiency, and financial calibration. Our contributions include: (1) Theoretical analysis: We establish discrete convergence rates of $O(\\sqrt{Δt} \\log(1/Δt))$ via Donsker's principle and linear algorithmic convergence of $(1-κ)^{2/3}$; (2) Algorithmic improvements: We introduce incremental updates ($O(M^2)$ complexity) and adaptive sparse grids; (3) Numerical implementation: A hybrid neural-projection solver is proposed, combining transformer-based warm-starting with Newton-Raphson projection. Once trained, the pure neural solver achieves a $1{,}597\\times$ online inference speedup ($4.7$s $\\to 2.9$ms) suitable for real-time applications, while the hybrid solver ensures martingale constraints to $10^{-6}$ precision. Validated on 12,000 synthetic instances (GBM, Merton, Heston) and 120 real market scenarios.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文开发了多周期鞅最优传输（MMOT）的计算框架，重点解决收敛速度、算法效率和金融校准问题。主要贡献包括：（1）理论分析：通过Donsker原理建立离散收敛速度O(√Δt log(1/Δt))和线性算法收敛速度(1-κ)^{2/3}；（2）算法改进：引入增量更新（O(M²)复杂度）和自适应稀疏网格；（3）数值实现：提出混合神经-投影求解器，结合基于Transformer的预热启动与牛顿-拉夫森投影。训练后，纯神经求解器实现1,597倍在线推理加速（4.7秒→2.9毫秒），适用于实时应用，而混合求解器确保鞅约束达到10^{-6}精度。在12,000个合成实例（GBM、Merton、Heston）和120个真实市场场景中验证。",
    "fetch_date": "2026-01-13",
    "id": "20260113_218c7ab8"
  },
  {
    "title": "Forecasting implied volatility surface with generative diffusion models",
    "url": "https://arxiv.org/pdf/2511.07571v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We introduce a conditional Denoising Diffusion Probabilistic Model (DDPM) for generating arbitrage-free implied volatility (IV) surfaces, offering a more stable and accurate alternative to existing GAN-based approaches. To capture the path-dependent nature of volatility dynamics, our model is conditioned on a rich set of market variables, including exponential weighted moving averages (EWMAs) of historical surfaces, returns and squared returns of underlying asset, and scalar risk indicators like VIX. Empirical results demonstrate our model significantly outperforms leading GAN-based models in capturing the stylized facts of IV dynamics. A key challenge is that historical data often contains small arbitrage opportunities in the earlier dataset for training, which conflicts with the goal of generating arbitrage-free surfaces. We address this by incorporating a standard arbitrage penalty into the loss function, but apply it using a novel, parameter-free weighting scheme based on the signal-to-noise ratio (SNR) that dynamically adjusts the penalty's strength across the diffusion process. We also show a formal analysis of this trade-off and provide a proof of convergence showing that the penalty introduces a small, controllable bias that steers the model toward the manifold of arbitrage-free surfaces while ensuring the generated distribution remains close to the real-world data.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于生成无套利隐含波动率曲面的条件去噪扩散概率模型，相比现有基于GAN的方法更稳定、准确。模型通过包含历史曲面指数加权移动平均、标的资产收益率及平方收益率、VIX等风险指标的市场变量来捕捉波动率的路径依赖特性。实证表明该模型在捕捉隐含波动率动态特征方面显著优于主流GAN模型。针对历史数据存在微小套利机会的问题，作者在损失函数中引入标准套利惩罚项，并采用基于信噪比的无参数加权方案动态调整惩罚强度。文中还对该权衡进行了形式化分析并提供了收敛性证明。",
    "fetch_date": "2026-01-13",
    "id": "20260113_2dc4728a"
  },
  {
    "title": "Deep Neural Operator Learning for Probabilistic Models",
    "url": "https://arxiv.org/pdf/2511.07235v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们提出了一种用于广义概率模型的深度神经算子框架。在算子满足全局Lipschitz条件且适用于广泛概率模型的情况下，我们建立了该架构的通用逼近定理，并给出了明确的网络规模边界。底层随机过程仅需满足可积性和一般尾部概率条件。我们在前向-后向随机微分方程（FBSDE）框架下验证了这些假设对欧式和美式期权定价问题的适用性，这进而涵盖了由抛物型偏微分方程（无论是否存在自由边界）产生的一类广泛算子。最后，我们提供了一个美式期权篮子的数值示例，证明学习到的模型无需重新训练即可为新执行价格生成最优停止边界。",
    "fetch_date": "2026-01-13",
    "id": "20260113_276bbc39"
  },
  {
    "title": "When the Rules Change: Adaptive Signal Extraction via Kalman Filtering and Markov-Switching Regimes",
    "url": "https://arxiv.org/pdf/2601.05716v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "Static linear models of order flow assume constant parameters, failing precisely when they are needed most: during periods of market stress and structural change. This paper proposes a dynamic, state-dependent framework for order flow signal extraction that adapts to shifting market conditions in the Korean stock market. Using daily transaction data from 2020--2024 covering 2,439 stocks and 2.79 million stock-day observations, we implement three complementary methodologies: (1) an Adaptive Kalman Filter where measurement noise variance is explicitly coupled to market volatility; (2) a three-state Markov-Switching model identifying Bull, Normal, and Crisis regimes; and (3) an Asymmetric Response Function capturing differential investor reactions to positive versus negative shocks. We find that foreign investor predictive power increases 8.9-fold during crisis periods relative to bull markets ($β_{crisis}=0.00204$ vs. $β_{bull}=0.00023$), while individual investors exhibit momentum-chasing behavior with 6.3 times stronger response to positive shocks. The integrated ``All-Weather'' strategy provides modest drawdown reduction during extreme market events, though challenges remain in the post-COVID high-rate environment.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种动态、状态依赖的订单流信号提取框架，旨在适应韩国股市的市场条件变化。通过结合自适应卡尔曼滤波（测量噪声方差与市场波动率耦合）、三状态马尔可夫转换模型（识别牛市、常态和危机状态）以及非对称响应函数（捕捉投资者对正负冲击的差异反应），该研究揭示了危机期间外国投资者预测能力显著增强（β值增长8.9倍），而个人投资者表现出更强的动量追逐行为。综合的“全天候”策略在极端市场事件中能适度减少回撤，但实际应用仍面临挑战。",
    "fetch_date": "2026-01-13",
    "id": "20260113_f6ee0520"
  },
  {
    "title": "Generative Pricing of Basket Options via Signature-Conditioned Mixture Density Networks",
    "url": "https://arxiv.org/pdf/2511.09061v1",
    "source": "ArXiv",
    "date": "2025-11-12",
    "abstract": "We present a generative framework for pricing European-style basket options by learning the conditional terminal distribution of the log arithmetic-weighted basket return. A Mixture Density Network (MDN) maps time-varying market inputs encoded via truncated path signatures to the full terminal density in a single forward pass. Traditional approaches either impose restrictive assumptions or require costly re-simulation whenever inputs change, limiting real-time use. Trained on Monte Carlo (MC) under GBM with time-varying volatility or local volatility, the MDN acts as a reusable surrogate distribution: once trained, it prices new scenarios by integrating the learned density. Across maturities, correlations, and basket weights, the learned densities closely match MC (low KL) and produce small pricing errors, while enabling \\emph{train-once, price-anywhere} reuse at inference-time latency.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于生成式框架的篮子期权定价方法，通过学习对数算术加权篮子收益的条件终端分布，利用混合密度网络（MDN）将经截断路径签名编码的时变市场输入映射至完整终端密度。该方法克服了传统方法在输入变化时需重新模拟的高成本限制，实现了“一次训练、随处定价”的实时应用。在GBM模型下通过蒙特卡洛模拟训练，该网络作为可重用的代理分布，在不同期限、相关性和篮子权重下均表现出与蒙特卡洛模拟相近的密度估计和较小的定价误差。",
    "fetch_date": "2026-01-13",
    "id": "20260113_4dd3024a"
  },
  {
    "title": "A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications",
    "url": "https://arxiv.org/pdf/2511.08735v2",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "In this work, we extend deep learning-based numerical methods to fully coupled forward-backward stochastic differential equations (FBSDEs) within a non-Markovian framework. Error estimates and convergence are provided. In contrast to the existing literature, our approach not only analyzes the non-Markovian framework but also addresses fully coupled settings, in which both the drift and diffusion coefficients of the forward process may be random and depend on the backward components $Y$ and $Z$. Furthermore, we illustrate the practical applicability of our framework by addressing utility maximization problems under rough volatility, which are solved numerically with the proposed deep learning-based methods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究将基于深度学习的数值方法扩展到非马尔可夫框架下的完全耦合前向-后向随机微分方程（FBSDEs），提供了误差估计和收敛性分析。与现有文献相比，该方法不仅分析了非马尔可夫框架，还处理了完全耦合设置，其中前向过程的漂移和扩散系数可以是随机的，并依赖于后向分量Y和Z。此外，通过解决粗糙波动率下的效用最大化问题，并采用所提出的基于深度学习的数值方法求解，展示了该框架的实际应用价值。",
    "fetch_date": "2026-01-13",
    "id": "20260113_a521f976"
  },
  {
    "title": "Levy-stable scaling of risk and performance functionals",
    "url": "https://arxiv.org/pdf/2511.07834v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We develop a finite-horizon model in which liquid-asset returns exhibit Levy-stable scaling on a data-driven window [tau_UV, tau_IR] and aggregate into a finite-variance regime outside. The window and the tail index alpha are identified from the log-log slope of the central body and a two-segment fit of scale versus horizon. With an anchor horizon tau_0, we derive horizon-correct formulas for Value-at-Risk, Expected Shortfall, Sharpe and Information ratios, Kelly under a Value-at-Risk constraint, and one-step drawdown, where each admits a closed-form Gaussian-bias term driven by the exponent gap (1/alpha - 1/2). The implementation is nonparametric up to alpha and fixed tail quantiles. The formulas are reproducible across horizons on the Levy window.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文开发了一个有限期限模型，其中流动性资产收益在数据驱动的窗口[tau_UV, tau_IR]内表现出Levy稳定标度，并在外部聚合成有限方差机制。通过中心体的对数-对数斜率和尺度与期限的两段拟合，识别出窗口和尾部指数alpha。基于锚定期限tau_0，推导了期限校正公式，包括风险价值、预期短缺、夏普和信息比率、风险价值约束下的凯利准则以及一步回撤，每个公式均包含由指数间隙(1/alpha - 1/2)驱动的高斯偏差项。实现过程在alpha和固定尾部分位数方面是非参数的。这些公式在Levy窗口内跨期限可重现。",
    "fetch_date": "2026-01-13",
    "id": "20260113_9550bf42"
  },
  {
    "title": "Geopolitical and Institutional Constraints on Adaptive Market Efficiency -- A Feasibility Diagnostic for Robust Portfolio Construction",
    "url": "https://arxiv.org/pdf/2601.05924v1",
    "source": "ArXiv",
    "date": "2026-01-09",
    "abstract": "This paper develops a structural framework for characterizing the informational feasibility of financial markets under heterogeneous institutional and geopolitical conditions. Departing from the assumption of uniform and time-invariant market efficiency, adaptive efficiency is conceptualized as a localized and state-dependent property emerging from the interaction between economic scale, institutional enforcement, and geopolitical embedding. To operationalize this perspective, the paper introduces the Geopolitical-Adaptive Efficiency Ratio (GAER), a descriptive cross-sectional indicator measuring the concentration of adaptive-efficiency-supporting mass within institutionally and geopolitically central assets. GAER is not a return-predictive signal, factor, or regime classifier. Instead, it functions as a diagnostic boundary condition, delimiting the domain in which ranking-based and robustness-oriented portfolio construction methods are plausibly applicable. The framework integrates insights from adaptive market theory, institutional economics, and political economy, linking disclosure continuity, liquidity provision, and enforcement credibility to the persistence of informational signals in asset prices. GAER is formalized, its theoretical properties are discussed, and its interpretation is illustrated using a global equity snapshot based on publicly observable information. The contribution separates informational feasibility from portfolio construction and execution, providing a conceptual foundation for constraint-aware financial modeling without reliance on forecast-driven assumptions or parametric optimization.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个结构性框架，用于描述在异质性制度和地缘政治条件下金融市场的可行性。该框架摒弃了市场效率统一且时不变的假设，将适应性效率概念化为一种局部化、状态依赖的属性，源于经济规模、制度执行力和地缘政治嵌入之间的相互作用。为操作化这一视角，论文引入了地缘政治-适应性效率比率（GAER），这是一个描述性的横截面指标，用于衡量制度和地缘政治核心资产中支持适应性效率的集中度。GAER并非回报预测信号、因子或制度分类器，而是作为诊断性边界条件，界定基于排序和稳健性导向的投资组合构建方法可能适用的领域。该框架整合了适应性市场理论、制度经济学和政治经济学的见解，将信息披露连续性、流动性提供和执行可信度与信息持久性联系起来。",
    "fetch_date": "2026-01-13",
    "id": "20260113_3e7739f8"
  },
  {
    "title": "Equilibrium Strategies for Singular Dividend Control Problems under the Mean-Variance Criterion",
    "url": "https://arxiv.org/pdf/2511.08433v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We revisit the optimal dividend problem of de Finetti by adding a variance term to the usual criterion of maximizing the expected discounted dividends paid until ruin, in a singular control framework. Investors do not like variability in their dividend distribution, and the mean-variance (MV) criterion balances the desire for large expected dividend payments with small variability in those payments. The resulting MV singular dividend control problem is time-inconsistent, and we follow a game-theoretic approach to find a time-consistent equilibrium strategy. Our main contribution is a new verification theorem for the novel dividend problem, in which the MV criterion is applied to an integral of the control until ruin, a random time that is endogenous to the problem. We demonstrate the use of the verification theorem in two cases for which we obtain the equilibrium dividend strategy (semi-)explicitly, and we provide a numerical example to illustrate our results.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在de Finetti最优分红问题的基础上，在奇异控制框架下，将方差项加入最大化破产前预期折现分红的传统准则中，提出均值-方差（MV）奇异分红控制问题。投资者偏好分红分配的稳定性，MV准则平衡了大额预期分红支付与支付波动性之间的需求。该问题具有时间不一致性，作者采用博弈论方法寻找时间一致均衡策略。主要贡献是针对这一新型分红问题提出了新的验证定理，其中MV准则应用于控制积分直至破产（问题的内生随机时间）。作者通过两个案例（半）显式地获得均衡分红策略，并提供了数值示例来展示结果。",
    "fetch_date": "2026-01-13",
    "id": "20260113_5f20f5ab"
  },
  {
    "title": "Robust distortion risk metrics and portfolio optimization",
    "url": "https://arxiv.org/pdf/2511.08662v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We establish sharp upper and lower bounds for distortion risk metrics under distributional uncertainty. The uncertainty sets are characterized by four key features of the underlying distribution: mean, variance, unimodality, and Wasserstein distance to a reference distribution.\n  We first examine very general distortion risk metrics, assuming only finite variation for the underlying distortion function and without requiring continuity or monotonicity. This broad framework includes notable distortion risk metrics such as range value-at-risk, glue value-at-risk, Gini deviation, mean-median deviation and inter-quantile difference. In this setting, when the uncertainty set is characterized by a fixed mean, variance and a Wasserstein distance, we determine both the worst- and best-case values of a given distortion risk metric and identify the corresponding extremal distribution. When the uncertainty set is further constrained by unimodality with a fixed inflection point, we establish for the case of absolutely continuous distortion functions the extremal values along with their respective extremal distributions. We apply our results to robust portfolio optimization and model risk assessment offering improved decision-making under model uncertainty.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在分布不确定性条件下，为失真风险度量建立了尖锐的上界和下界。不确定性集由基础分布的四个关键特征刻画：均值、方差、单峰性以及与参考分布的Wasserstein距离。研究首先考察了非常一般的失真风险度量，仅假设基础失真函数具有有限变差，无需连续性或单调性。这一广泛框架包含了多种重要的失真风险度量，如范围风险价值、胶合风险价值、基尼偏差、均值-中位数偏差和分位数间差。在此设定下，当不确定性集由固定均值、方差和Wasserstein距离刻画时，确定了给定失真风险度量的最坏情况和最佳情况值，并识别了相应的极值分布。当不确定性集进一步受到具有固定拐点的单峰性约束时，针对绝对连续失真函数的情况，建立了极值及其相应的极值分布。研究结果应用于鲁棒投资组合优化和模型风险评估。",
    "fetch_date": "2026-01-13",
    "id": "20260113_0c20b0ff"
  },
  {
    "title": "\"It Looks All the Same to Me\": Cross-index Training for Long-term Financial Series Prediction",
    "url": "https://arxiv.org/pdf/2511.08658v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "We investigate a number of Artificial Neural Network architectures (well-known and more ``exotic'') in application to the long-term financial time-series forecasts of indexes on different global markets. The particular area of interest of this research is to examine the correlation of these indexes' behaviour in terms of Machine Learning algorithms cross-training. Would training an algorithm on an index from one global market produce similar or even better accuracy when such a model is applied for predicting another index from a different market? The demonstrated predominately positive answer to this question is another argument in favour of the long-debated Efficient Market Hypothesis of Eugene Fama.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨了多种人工神经网络架构（包括经典与前沿模型）在全球不同市场指数长期金融时间序列预测中的应用。研究的核心关注点在于，通过机器学习算法的跨市场训练，检验这些指数行为之间的相关性。具体而言，研究试图回答：在一个全球市场的指数上训练的算法模型，当应用于预测另一个不同市场的指数时，是否会产生相似甚至更高的预测准确性？研究结果表明，答案总体上是肯定的。这一发现为长期备受争议的尤金·法玛有效市场假说提供了新的支持论据。",
    "fetch_date": "2026-01-13",
    "id": "20260113_0158cd7b"
  },
  {
    "title": "An extreme Gradient Boosting (XGBoost) Trees approach to Detect and Identify Unlawful Insider Trading (UIT) Transactions",
    "url": "https://arxiv.org/pdf/2511.08306v1",
    "source": "ArXiv",
    "date": "2025-11-11",
    "abstract": "Corporate insiders have control of material non-public preferential information (MNPI). Occasionally, the insiders strategically bypass legal and regulatory safeguards to exploit MNPI in their execution of securities trading. Due to a large volume of transactions a detection of unlawful insider trading becomes an arduous task for humans to examine and identify underlying patterns from the insider's behavior. On the other hand, innovative machine learning architectures have shown promising results for analyzing large-scale and complex data with hidden patterns. One such popular technique is eXtreme Gradient Boosting (XGBoost), the state-of-the-arts supervised classifier. We, hence, resort to and apply XGBoost to alleviate challenges of identification and detection of unlawful activities. The results demonstrate that XGBoost can identify unlawful transactions with a high accuracy of 97 percent and can provide ranking of the features that play the most important role in detecting fraudulent activities.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种基于极端梯度提升（XGBoost）树的方法，用于检测和识别非法内幕交易（UIT）交易。公司内部人员掌握重要的非公开优先信息（MNPI），有时会绕过法律和监管保障措施，利用MNPI进行证券交易。由于交易量巨大，人工检测非法内幕交易并识别内部人员行为中的潜在模式变得困难。而创新的机器学习架构在分析具有隐藏模式的大规模和复杂数据方面显示出有希望的结果。XGBoost作为最先进的监督分类器，被应用于缓解识别和检测非法活动的挑战。结果表明，XGBoost能以97%的高准确率识别非法交易，并能提供在检测欺诈活动中起最重要作用的特征排名。",
    "fetch_date": "2026-01-13",
    "id": "20260113_a2cbd2ec"
  },
  {
    "title": "Machine-learning a family of solutions to an optimal pension investment problem",
    "url": "https://arxiv.org/pdf/2511.07045v1",
    "source": "ArXiv",
    "date": "2025-11-10",
    "abstract": "We use a neural network to identify the optimal solution to a family of optimal investment problems, where the parameters determining an investor's risk and consumption preferences are given as inputs to the neural network in addition to economic variables. This is used to develop a practical tool that can be used to explore how pension outcomes vary with preference parameters. We use a Black-Scholes economic model so that we may validate the accuracy of network using a classical and provably convergent numerical method developed using the duality approach.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究使用神经网络求解一类最优养老金投资问题，将投资者的风险偏好、消费偏好参数与经济变量一同作为网络输入，从而识别最优投资策略。研究基于Black-Scholes经济模型，并利用对偶方法构建的经典数值解法验证网络准确性，旨在开发可探索养老金结果随偏好参数变化的实用工具。",
    "fetch_date": "2026-01-13",
    "id": "20260113_6b0aba1f"
  },
  {
    "title": "Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy",
    "url": "https://arxiv.org/pdf/2511.12120v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种用于自动化股票交易的集成策略，采用深度强化学习方案，通过最大化投资回报来学习交易策略。该策略训练了一个深度强化学习智能体，并利用三种基于演员-评论家（actor-critic）的算法——近端策略优化（PPO）、优势演员-评论家（A2C）和深度确定性策略梯度（DDPG）——构建集成交易策略。该集成策略继承并整合了三种算法的最佳特性，从而能够稳健地适应不同的市场情况。为避免在连续动作空间中训练网络时消耗大量内存，采用了按需加载技术来处理超大规模数据。研究在30只具有充足流动性的道琼斯股票上测试了算法，评估了不同强化学习算法下交易智能体的表现，并与道琼斯工业平均指数及传统的最小方差投资组合配置策略进行了比较。",
    "fetch_date": "2026-01-12",
    "id": "20260112_0631a907"
  },
  {
    "title": "Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling",
    "url": "https://arxiv.org/pdf/2511.10501v2",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文全面综述了图神经网络（GNN）、深度强化学习（DRL）和概率主题建模（PTM）在战略多智能体环境中的应用潜力，重点关注其在战略对手建模中的价值。研究强调：（1）利用机器学习方法揭示未知模型结构，以适应对手建模任务；（2）结合博弈论概念，避免依赖现实场景中常无效的假设（如共同先验假设CPA和自利假设SIH）。论文分析了处理不确定性和异质性（现实应用中的常见特征）以及可扩展性的能力，并倡导使用图神经网络（GNN）作为有效建模多智能体环境中关系和交互的潜在解决方案。GNN专为处理图结构数据设计，在节点分类和链接预测等任务中表现出强大能力。此外，论文还回顾了强化学习（RL）领域，特别是多智能体强化学习（MARL）在动态决策中的应用。",
    "fetch_date": "2026-01-12",
    "id": "20260112_44f05882"
  },
  {
    "title": "Noise-proofing Universal Portfolio Shrinkage",
    "url": "https://arxiv.org/pdf/2511.10478v1",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "We enhance the Universal Portfolio Shrinkage Approximator (UPSA) of Kelly et al. (2023) by making it more robust with respect to estimation noise and covariate shift. UPSA optimizes the realized Sharpe ratio using a relatively small calibration window, leveraging ridge penalties and cross-validation to yield better portfolios. Yet, it still suffers from the staggering amount of noise in financial data. We propose two methods to make UPSA more robust and improve its efficiency: time-averaging of the optimal penalty weights and using the Average Oracle correlation eigenvalues to make covariance matrices less noisy and more robust to covariate shift. Combining these two long-term averages outperforms UPSA by a large margin in most specifications.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们通过增强对估计噪声和协变量漂移的鲁棒性，改进了Kelly等人（2023）提出的通用投资组合收缩近似器（UPSA）。UPSA利用相对较小的校准窗口优化已实现夏普比率，通过岭惩罚和交叉验证获得更好的投资组合。然而，它仍受金融数据中大量噪声的影响。我们提出两种方法使UPSA更鲁棒并提高其效率：对最优惩罚权重进行时间平均，以及使用平均Oracle相关特征值来降低协方差矩阵的噪声并增强对协变量漂移的鲁棒性。结合这两种长期平均方法，在大多数设定下显著优于UPSA。",
    "fetch_date": "2026-01-12",
    "id": "20260112_c57261ce"
  },
  {
    "title": "FCOC: A Fractal-Chaotic Co-driven Framework for Financial Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2511.10365v2",
    "source": "ArXiv",
    "date": "2025-11-13",
    "abstract": "This paper introduces the Fractal-Chaotic Oscillation Co-driven (FCOC) framework, a novel paradigm for financial volatility forecasting that systematically resolves the dual challenges of feature fidelity and model responsiveness. FCOC synergizes two core innovations: our novel Fractal Feature Corrector (FFC), engineered to extract high-fidelity fractal signals, and a bio-inspired Chaotic Oscillation Component (COC) that replaces static activations with a dynamic processing system. Empirically validated on the S\\&P 500 and DJI, the FCOC framework demonstrates profound and generalizable impact. The framework fundamentally transforms the performance of previously underperforming architectures, such as the Transformer, while achieving substantial improvements in key risk-sensitive metrics for state-of-the-art models like Mamba. These results establish a powerful co-driven approach, where models are guided by superior theoretical features and powered by dynamic internal processors, setting a new benchmark for risk-aware forecasting.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为FCOC（分形-混沌协同驱动）的新框架，用于金融波动率预测。该框架通过结合分形特征校正器（FFC）提取高保真分形信号，以及采用受生物启发的混沌振荡组件（COC）替代静态激活函数，构建动态处理系统，旨在系统性地解决特征保真度与模型响应性的双重挑战。实证研究基于标普500和道琼斯工业平均指数，结果表明FCOC框架对Transformer等先前表现不佳的架构有根本性提升，同时对Mamba等前沿模型在关键风险敏感指标上也有显著改进。这确立了一种强大的协同驱动方法，为风险感知预测设立了新基准。",
    "fetch_date": "2026-01-12",
    "id": "20260112_25e39469"
  },
  {
    "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2511.12351v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种用于多元时间序列异常检测的深度强化学习框架，结合了变分自编码器（VAE）、基于LSTM的深度Q网络（DQN）、动态奖励塑形和主动学习模块。核心贡献是实现动态奖励缩放（DRSMT），VAE用于捕获紧凑的潜在表示并降噪，DQN实现自适应序列异常分类，动态奖励塑形通过调整重构和分类信号的重要性来平衡训练中的探索与利用，主动学习则识别最不确定的样本进行标注以减少人工监督需求。在SMD和Water Distribution Test两个基准数据集上的实验验证了该框架的有效性。",
    "fetch_date": "2026-01-12",
    "id": "20260112_055c5560"
  },
  {
    "title": "A Practical Machine Learning Approach for Dynamic Stock Recommendation",
    "url": "https://arxiv.org/pdf/2511.12129v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Dynamic-Stock-Recommendation-Machine_Learning-Published-Paper-IEEE}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种实用的机器学习方法用于动态股票推荐。核心方案是动态买入并持有排名前20%的S&P 500股票。首先筛选具有良好解释力的代表性股票指标；其次采用线性回归、岭回归、逐步回归、随机森林和广义提升回归五种常用机器学习方法，在滚动窗口中对股票指标与季度对数收益率进行建模；然后选择每期均方误差最低的模型对股票进行排序；最后通过等权重、均值-方差和最小方差等投资组合分配方法对所选股票进行测试。实证结果表明，该方案在夏普比率和累计收益方面均优于S&P 500指数的纯多头策略。",
    "fetch_date": "2026-01-12",
    "id": "20260112_6a992cf9"
  },
  {
    "title": "Risk-Aware Deep Reinforcement Learning for Dynamic Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.11481v1",
    "source": "ArXiv",
    "date": "2025-11-14",
    "abstract": "This paper presents a deep reinforcement learning (DRL) framework for dynamic portfolio optimization under market uncertainty and risk. The proposed model integrates a Sharpe ratio-based reward function with direct risk control mechanisms, including maximum drawdown and volatility constraints. Proximal Policy Optimization (PPO) is employed to learn adaptive asset allocation strategies over historical financial time series. Model performance is benchmarked against mean-variance and equal-weight portfolio strategies using backtesting on high-performing equities. Results indicate that the DRL agent stabilizes volatility successfully but suffers from degraded risk-adjusted returns due to over-conservative policy convergence, highlighting the challenge of balancing exploration, return maximization, and risk mitigation. The study underscores the need for improved reward shaping and hybrid risk-aware strategies to enhance the practical deployment of DRL-based portfolio allocation models.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于深度强化学习（DRL）的动态投资组合优化框架，用于应对市场不确定性和风险。该模型将基于夏普比率的奖励函数与直接风险控制机制（包括最大回撤和波动率约束）相结合，并采用近端策略优化（PPO）算法在历史金融时间序列上学习自适应资产配置策略。通过在高绩效股票上进行回测，模型表现与均值-方差和等权重投资组合策略进行了基准比较。结果表明，DRL代理能够有效稳定波动率，但由于策略收敛过于保守，风险调整后收益有所下降，凸显了在探索、收益最大化和风险缓解之间取得平衡的挑战。研究强调需要改进奖励塑造和采用混合风险感知策略，以增强基于DRL的投资组合分配模型的实际部署价值。",
    "fetch_date": "2026-01-12",
    "id": "20260112_23a85aee"
  },
  {
    "title": "On the utility problem in a market where price impact is transient",
    "url": "https://arxiv.org/pdf/2511.12093v1",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "We consider a discrete-time model of a financial market where a risky asset is bought and sold with transactions having a transient price impact. It is shown that the corresponding utility maximization problem admits a solution. We manage to remove some unnatural restrictions on the market depth and resilience processes that were present in earlier work. A non-standard feature of the problem is that the set of attainable portfolio values may fail the convexity property.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个离散时间金融市场模型，其中风险资产的买卖交易会产生暂时性价格冲击。研究表明，相应的效用最大化问题存在解。作者成功移除了先前研究中关于市场深度和恢复过程的一些非自然限制。该问题的一个非标准特征是，可实现的投资组合价值集合可能不满足凸性。",
    "fetch_date": "2026-01-12",
    "id": "20260112_3f0c1711"
  },
  {
    "title": "Deep Learning Based Hybrid Transformer Model for Stock Price Prediction",
    "url": "https://ieeexplore.ieee.org/abstract/document/10894439/",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… The backtesting of this study on global indices further displays that the Transformer proves to be even more superior to traditional methods and can offer investors with avenues to earn …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于深度学习的混合Transformer模型用于股价预测。该研究在全球指数上进行回测，结果显示Transformer模型显著优于传统方法，能为投资者提供盈利途径。",
    "fetch_date": "2026-01-11",
    "id": "20260111_1c77b35c"
  },
  {
    "title": "Automate strategy finding with llm in quant investment",
    "url": "https://arxiv.org/abs/2409.06289",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… based on LLM, to utilize the strong exploratory power of LLM to … We introduce the multi-Agent approach to the financial do… alpha factors and strategies in quantitative trading. It begins by …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于LLM（大语言模型）的量化投资策略自动化发现方法，利用LLM强大的探索能力，引入多智能体（multi-Agent）方法于金融领域，旨在自动生成alpha因子和量化交易策略。",
    "fetch_date": "2026-01-11",
    "id": "20260111_2da79e52"
  },
  {
    "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
    "url": "https://arxiv.org/abs/2508.02366",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Second, we propose a hybrid LLM+RL architecture in which the LLM provides guidance by augmenting the observation space of an RL agent. This design enables the …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种混合LLM+RL架构，其中大型语言模型通过增强强化学习智能体的观察空间来提供指导。这种设计使RL代理能够利用LLM的语义理解能力，在量化交易中实现更智能的决策。",
    "fetch_date": "2026-01-11",
    "id": "20260111_f9bf84fa"
  },
  {
    "title": "Language model guided reinforcement learning in quantitative trading: LLM-guided intelligence bridging strategy and safety in trading",
    "url": "https://www.um.edu.mt/library/oar/handle/123456789/141862",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Building on this capacity, this research investigates whether LLM agents can guide a RL agent to produce coherent, economically grounded strategies aligned with investor risk …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该研究探讨了大型语言模型（LLM）智能体能否指导强化学习（RL）智能体，在量化交易中生成连贯、基于经济原理的策略，并与投资者的风险偏好保持一致，旨在通过LLM引导的智能桥接策略与交易安全性。",
    "fetch_date": "2026-01-11",
    "id": "20260111_c491b345"
  },
  {
    "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
    "url": "https://aclanthology.org/anthology-files/anthology-files/pdf/findings/2025.findings-emnlp.945.pdf",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Through this approach, we hope narrow the gap between LLM-based agents and human … Trademaster: A holistic quantitative trading platform empowered by reinforcement learning. In …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出QuantAgents多智能体金融系统，通过模拟交易缩小基于大语言模型的智能体与人类交易员之间的差距，并整合了Trademaster这一由强化学习驱动的综合量化交易平台，对实战交易具有较高的应用价值。",
    "fetch_date": "2026-01-11",
    "id": "20260111_3c84d91d"
  },
  {
    "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading",
    "url": "https://arxiv.org/abs/2509.05080",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)… expert agents. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "MM-DREX：一种多模态驱动的动态路由大语言模型专家系统用于金融交易。传统依赖固定结构和单模态数据的量化交易模型难以适应市场状态转换，而本文提出的方法利用多模态数据（如文本、时序、市场数据）动态路由至不同LLM专家代理，旨在提升交易策略的适应性和性能。",
    "fetch_date": "2026-01-11",
    "id": "20260111_dcfba4b6"
  },
  {
    "title": "Hedgeagents: A balanced-aware multi-agent financial trading system",
    "url": "https://dl.acm.org/doi/abs/10.1145/3701716.3715232",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… for a single agent. We have integrated the LLM-based intelligent investment agent into a … TradeMaster: A holistic quantitative trading platform empowered by reinforcement learning. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种平衡感知的多智能体金融交易系统（Hedgeagents），将基于大语言模型（LLM）的智能投资代理集成到TradeMaster平台中。TradeMaster是一个由强化学习（RL）驱动的整体量化交易平台。该系统通过多智能体协同和强化学习技术，旨在提升实战交易中的决策能力和适应性。",
    "fetch_date": "2026-01-11",
    "id": "20260111_0829c581"
  },
  {
    "title": "Quantagent: Price-driven multi-agent llms for high-frequency trading",
    "url": "https://arxiv.org/abs/2509.09995",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… To bridge the gap between traditional high-frequency quantitative trading and recent advances in multi-agent LLM systems, we introduce QuantAgent, a collaborative framework for low-…",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "为弥合传统高频量化交易与多智能体大语言模型系统最新进展之间的差距，本文提出QuantAgent——一个用于低频延迟交易场景的协作框架。该框架通过价格驱动机制协调多个LLM智能体，旨在提升交易决策的适应性和鲁棒性，属于将前沿AI技术应用于实际交易策略的探索性研究。",
    "fetch_date": "2026-01-11",
    "id": "20260111_aac51ed5"
  },
  {
    "title": "A Multi-agent System Based On LLM For Trading Financial Assets.",
    "url": "https://www.researchgate.net/profile/Simona-Oprea/publication/389806855_A_Multi-agent_System_Based_On_LLM_For_Trading_Financial_Assets/links/68218750bfbe974b23c7fc56/A-Multi-agent-System-Based-On-LLM-For-Trading-Financial-Assets.pdf",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… Quantitative trading consists of using quantitative analysis and mathematical models to make trading decisions. It relies on data and algorithms, can be short-term or long-term, variable …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "基于大语言模型（LLM）的多智能体系统用于金融资产交易。量化交易利用定量分析和数学模型进行交易决策，依赖数据和算法，可应用于短期或长期策略。",
    "fetch_date": "2026-01-11",
    "id": "20260111_66e0ae60"
  },
  {
    "title": "TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis",
    "url": "https://arxiv.org/abs/2508.17565",
    "source": "Scholar",
    "date": "2026-01-11",
    "abstract": "… LLM-based agents: FinMem integrates hierarchical memory and role prompts into the agent … We have introduced TradingGroup, an innovative multi-agent system for quantitative trading…",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "本文介绍TradingGroup：一种创新的多智能体量化交易系统，具备自我反思和数据合成能力。该系统基于LLM智能体，其中FinMem组件将分层记忆和角色提示集成到智能体中。该系统旨在通过多智能体协作提升量化交易性能。",
    "fetch_date": "2026-01-11",
    "id": "20260111_b0e7b1e4"
  },
  {
    "title": "Deep Reinforcement Learning for Optimum Order Execution: Mitigating Risk and Maximizing Returns",
    "url": "https://arxiv.org/pdf/2601.04896v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "Optimal Order Execution is a well-established problem in finance that pertains to the flawless execution of a trade (buy or sell) for a given volume within a specified time frame. This problem revolves around optimizing returns while minimizing risk, yet recent research predominantly focuses on addressing one aspect of this challenge. In this paper, we introduce an innovative approach to Optimal Order Execution within the US market, leveraging Deep Reinforcement Learning (DRL) to effectively address this optimization problem holistically. Our study assesses the performance of our model in comparison to two widely employed execution strategies: Volume Weighted Average Price (VWAP) and Time Weighted Average Price (TWAP). Our experimental findings clearly demonstrate that our DRL-based approach outperforms both VWAP and TWAP in terms of return on investment and risk management. The model's ability to adapt dynamically to market conditions, even during periods of market stress, underscores its promise as a robust solution.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文提出了一种基于深度强化学习（DRL）的创新方法，用于解决美国市场中的最优订单执行问题。该方法旨在整体优化回报并降低风险，通过动态适应市场条件（包括市场压力时期）来提升性能。实验结果表明，该DRL模型在投资回报和风险管理方面均优于两种广泛使用的执行策略：成交量加权平均价格（VWAP）和时间加权平均价格（TWAP）。这突显了其作为稳健解决方案的潜力，对实战交易具有较高价值。",
    "fetch_date": "2026-01-10",
    "id": "20260110_464a514c"
  },
  {
    "title": "Forecasting Equity Correlations with Hybrid Transformer Graph Neural Network",
    "url": "https://arxiv.org/pdf/2601.04602v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "This paper studies forward-looking stock-stock correlation forecasting for S\\&P 500 constituents and evaluates whether learned correlation forecasts can improve graph-based clustering used in basket trading strategies. We cast 10-day ahead correlation prediction in Fisher-z space and train a Temporal-Heterogeneous Graph Neural Network (THGNN) to predict residual deviations from a rolling historical baseline. The architecture combines a Transformer-based temporal encoder, which captures non-stationary, complex, temporal dependencies, with an edge-aware graph attention network that propagates cross-asset information over the equity network. Inputs span daily returns, technicals, sector structure, previous correlations, and macro signals, enabling regime-aware forecasts and attention-based feature and neighbor importance to provide interpretability. Out-of-sample results from 2019-2024 show that the proposed model meaningfully reduces correlation forecasting error relative to rolling-window estimates. When integrated into a graph-based clustering framework, forward-looking correlations produce adaptable and economically meaningfully baskets, particularly during periods of market stress. These findings suggest that improvements in correlation forecasts translate into meaningful gains during portfolio construction tasks.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文研究标普500成分股的股票间前瞻性相关性预测，并评估所学相关性预测是否能改进基于图的聚类在篮子交易策略中的应用。研究将10日超前相关性预测置于Fisher-z空间，训练一个时序异构图神经网络（THGNN）来预测相对于滚动历史基线的残差偏差。该架构结合了基于Transformer的时序编码器（捕捉非平稳、复杂的时序依赖）与边感知图注意力网络（在股票网络中传播跨资产信息）。输入涵盖日收益率、技术指标、行业结构、历史相关性和宏观信号，支持基于市场状态的预测，并通过注意力机制提供特征和邻居重要性的可解释性。2019-2024年样本外结果显示，该模型显著降低了相关性预测误差。当整合到基于图的聚类框架中时，前瞻性相关性能够生成适应性强且具有经济意义的篮子组合，尤其在市场压力时期表现突出。",
    "fetch_date": "2026-01-10",
    "id": "20260110_7bd13459"
  },
  {
    "title": "Discovery of a 13-Sharpe OOS Factor: Drift Regimes Unlock Hidden Cross-Sectional Predictability",
    "url": "https://arxiv.org/pdf/2511.12490v1",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "We document a high-performing cross-sectional equity factor that achieves out-of-sample Sharpe ratios above 13 through regime-conditional signal activation. The strategy combines value and short-term reversal signals only during stock-specific drift regimes, defined as periods when individual stocks show more than 60 percent positive days in trailing 63-day windows. Under these conditions, the factor delivers annualized returns of 158.6 percent with 12.0 percent volatility and a maximum drawdown of minus 11.9 percent. Using rigorous walk-forward validation across 20 years of S&P 500 data (2004 to 2024), we show performance roughly 13 times stronger than market benchmarks on a risk-adjusted basis, produced entirely out-of-sample with frozen parameters. The factor passes extensive robustness tests, including 1,000 randomization trials with p-values below 0.001, and maintains Sharpe ratios above 7 even under 30 percent parameter perturbations. Exposure to standard risk factors is negligible, with total R-squared values below 3 percent. We provide mechanistic evidence that drift regimes reshape market microstructure by amplifying behavioral biases, altering liquidity patterns, and creating conditions where cross-sectional price discovery becomes systematically exploitable. Conservative capacity estimates indicate deployable capital of 100 to 500 million dollars before noticeable performance degradation.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文发现了一个高绩效的横截面股票因子，通过制度条件信号激活，在样本外实现了超过13的夏普比率。该策略仅在股票特定的漂移制度（定义为个股在63天滚动窗口内正收益日占比超过60%的时期）下，结合价值和短期反转信号。在这些条件下，该因子年化收益率为158.6%，波动率为12.0%，最大回撤为-11.9%。通过对20年标普500数据（2004-2024年）进行严格的向前验证，该因子在风险调整后的表现比市场基准强约13倍，且完全在样本外使用固定参数实现。该因子通过了广泛的稳健性测试，包括1000次随机化试验（p值低于0.001），即使在30%的参数扰动下，夏普比率仍保持在7以上。对标准风险因子的暴露可忽略不计，总R平方值低于3%。机制证据表明，漂移制度通过放大行为偏差、改变流动性模式重塑市场微观结构。",
    "fetch_date": "2026-01-10",
    "id": "20260110_b932feb7"
  },
  {
    "title": "Trading Electrons: Predicting DART Spread Spikes in ISO Electricity Markets",
    "url": "https://arxiv.org/pdf/2601.05085v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "We study the problem of forecasting and optimally trading day-ahead versus real-time (DART) price spreads in U.S. wholesale electricity markets. Building on the framework of Galarneau-Vincent et al., we extend spike prediction from a single zone to a multi-zone setting and treat both positive and negative DART spikes within a unified statistical model. To translate directional signals into economically meaningful positions, we develop a structural and market-consistent price impact model based on day-ahead bid stacks. This yields closed-form expressions for the optimal vector of zonal INC/DEC quantities, capturing asymmetric buy/sell impacts and cross-zone congestion effects. When applied to NYISO, the resulting impact-aware strategy significantly improves the risk-return profile relative to unit-size trading and highlights substantial heterogeneity across markets and seasons.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们研究了在美国批发电力市场中预测和优化交易日前与实时（DART）价差的问题。基于Galarneau-Vincent等人的框架，我们将尖峰预测从单一区域扩展到多区域设置，并在统一的统计模型中处理正负DART尖峰。为了将方向性信号转化为具有经济意义的头寸，我们基于日前投标堆栈开发了一个结构化和市场一致的价格影响模型。这产生了区域增/减量的最优向量闭式表达式，捕捉了不对称的买卖影响和跨区域拥堵效应。当应用于纽约独立系统运营商（NYISO）时，所得的影响感知策略相对于单位规模交易显著改善了风险回报特征，并突显了市场和季节间的显著异质性。",
    "fetch_date": "2026-01-10",
    "id": "20260110_93896dc0"
  },
  {
    "title": "Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms",
    "url": "https://arxiv.org/pdf/2511.20678v1",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种基于强化学习的加密货币投资组合管理框架，采用Soft Actor-Critic（SAC）和Deep Deterministic Policy Gradient（DDPG）算法。针对加密货币市场的高波动性和非线性动态特性，传统投资组合优化方法往往难以适应。为此，研究设计了一个智能体，通过模拟交易环境与历史市场数据交互，直接学习连续交易动作。该智能体优化投资组合权重，旨在最大化累积收益，同时最小化下行风险和交易成本。在多种加密货币上的实验评估表明，SAC和DDPG智能体在表现上优于等权重和均值-方差等基准策略。其中，SAC算法凭借其熵正则化目标，在嘈杂市场条件下展现出比DDPG更高的稳定性和鲁棒性。这些结果凸显了深度强化学习在加密货币市场中实现自适应和数据驱动型投资组合管理的潜力。",
    "fetch_date": "2026-01-10",
    "id": "20260110_17518719"
  },
  {
    "title": "Intraday Limit Order Price Change Transition Dynamics Across Market Capitalizations Through Markov Analysis",
    "url": "https://arxiv.org/pdf/2601.04959v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "Quantitative understanding of stochastic dynamics in limit order price changes is essential for execution strategy design. We analyze intraday transition dynamics of ask and bid orders across market capitalization tiers using high-frequency NASDAQ100 tick data. Employing a discrete-time Markov chain framework, we categorize consecutive price changes into nine states and estimate transition probability matrices (TPMs) for six intraday intervals across High ($\\mathtt{HMC}$), Medium ($\\mathtt{MMC}$), and Low ($\\mathtt{LMC}$) market cap stocks. Element-wise TPM comparison reveals systematic patterns: price inertia peaks during opening and closing hours, stabilizing midday. A capitalization gradient is observed: $\\mathtt{HMC}$ stocks exhibit the strongest inertia, while $\\mathtt{LMC}$ stocks show lower stability and wider spreads. Markov metrics, including spectral gap, entropy rate, and mean recurrence times, quantify these dynamics. Clustering analysis identifies three distinct temporal phases on the bid side -- Opening, Midday, and Closing, and four phases on the ask side by distinguishing Opening, Midday, Pre-Close, and Close. This indicates that sellers initiate end-of-day positioning earlier than buyers. Stationary distributions show limit order dynamics are dominated by neutral and mild price changes. Jensen-Shannon divergence confirms the closing hour as the most distinct phase, with capitalization modulating temporal contrasts and bid-ask asymmetry. These findings support capitalization-aware and time-adaptive execution algorithms.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过马尔可夫分析研究纳斯达克100指数成分股日内限价单价格变动的转移动态。使用高频tick数据，将连续价格变动分为九个状态，并针对高、中、低市值股票在六个日内时段估计转移概率矩阵。研究发现：价格惯性在开盘和收盘时段最强，午间趋于稳定；高市值股票惯性最强，低市值股票稳定性较低且价差更宽。通过谱隙、熵率和平均递归时间等马尔可夫指标量化动态特征。聚类分析识别出买方三个时间阶段（开盘、午间、收盘）和卖方四个阶段（开盘、午间、预收盘、收盘），表明卖方在收盘前主动调整头寸。",
    "fetch_date": "2026-01-10",
    "id": "20260110_c17f32da"
  },
  {
    "title": "Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach",
    "url": "https://arxiv.org/pdf/2601.04608v1",
    "source": "ArXiv",
    "date": "2026-01-08",
    "abstract": "We study U.S. Treasury yield curve forecasting under distributional uncertainty and recast forecasting as an operations research and managerial decision problem. Rather than minimizing average forecast error, the forecaster selects a decision rule that minimizes worst case expected loss over an ambiguity set of forecast error distributions. To this end, we propose a distributionally robust ensemble forecasting framework that integrates parametric factor models with high dimensional nonparametric machine learning models through adaptive forecast combinations. The framework consists of three machine learning components. First, a rolling window Factor Augmented Dynamic Nelson Siegel model captures level, slope, and curvature dynamics using principal components extracted from economic indicators. Second, Random Forest models capture nonlinear interactions among macro financial drivers and lagged Treasury yields. Third, distributionally robust forecast combination schemes aggregate heterogeneous forecasts under moment uncertainty, penalizing downside tail risk via expected shortfall and stabilizing second moment estimation through ridge regularized covariance matrices. The severity of the worst case criterion is adjustable, allowing the forecaster to regulate the trade off between robustness and statistical efficiency. Using monthly data, we evaluate out of sample forecasts across maturities and horizons from one to twelve months ahead. Adaptive combinations deliver superior performance at short horizons, while Random Forest forecasts dominate at longer horizons. Extensions to global sovereign bond yields confirm the stability and generalizability of the proposed framework.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究在分布不确定性下美国国债收益率曲线的预测问题，将其重新构建为运筹学和管理决策问题。预测者选择决策规则，以最小化预测误差分布模糊集上的最坏情况期望损失，而非最小化平均预测误差。为此，提出一个分布鲁棒的集成预测框架，通过自适应预测组合将参数化因子模型与高维非参数机器学习模型相结合。框架包含三个机器学习组件：1) 滚动窗口因子增强动态Nelson-Siegel模型，利用从经济指标中提取的主成分捕捉水平、斜率和曲率动态；2) 随机森林模型，捕捉宏观金融驱动因素与滞后国债收益率之间的非线性相互作用；3) 分布鲁棒的预测组合方案，在矩不确定性下聚合异质预测，通过预期短缺惩罚下行尾部风险，并通过岭正则化协方差矩阵稳定二阶矩估计。",
    "fetch_date": "2026-01-10",
    "id": "20260110_2eb95a5e"
  },
  {
    "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects",
    "url": "https://arxiv.org/pdf/2511.12865v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了一个具有随机活动持续时间和现金流的项目优化问题，项目活动需满足先后约束并产生现金流入和流出。目标是通过加速现金流入和推迟现金流出来最大化预期净现值（NPV）。作者将问题建模为离散时间马尔可夫决策过程（MDP），并提出了一种双深度Q网络（DDQN）方法。对比实验表明，DDQN在大型或高度不确定的环境中优于传统的刚性和动态策略，展现出卓越的计算能力、策略可靠性和适应性。消融研究进一步揭示，双网络架构减轻了动作值的高估，而目标网络显著提高了训练收敛性和鲁棒性。这些结果表明，DDQN不仅在复杂项目优化中实现了更高的预期NPV，还为稳定有效的策略实施提供了可靠框架。",
    "fetch_date": "2026-01-10",
    "id": "20260110_e12bdf0a"
  },
  {
    "title": "Basis Immunity: Isotropy as a Regularizer for Uncertainty",
    "url": "https://arxiv.org/pdf/2511.13334v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.\n  In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.\n  Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.\n  This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种将均值-方差优化与各向同性约束相结合的框架，作为对抗信号不确定性的几何正则化器。该方法通过可调的各向同性惩罚，在完全各向同性（闭式各向同性-均值分配）与纯均值-方差之间平滑插值，自然地分解为规范投资组合。论文回顾并澄清了各向同性、规范投资组合、主投资组合、原始与对偶表示以及收益、风险和各项同性的内在基不变度量之间的基本联系，并将其应用于行业趋势分析。",
    "fetch_date": "2026-01-10",
    "id": "20260110_eaf0fc71"
  },
  {
    "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
    "url": "https://arxiv.org/pdf/2511.13322v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新的模型无关方法，通过Voronoi状态划分将强化学习策略蒸馏为局部专用线性策略，以增强可解释性。该方法将状态空间划分为多个区域，在每个区域内使用可解释的线性模型近似原始深度强化学习控制器的性能，在网格世界环境和经典控制任务中验证了其有效性。",
    "fetch_date": "2026-01-10",
    "id": "20260110_96d20d08"
  },
  {
    "title": "Impact by design: translating Lead times in flux into an R handbook with code",
    "url": "https://arxiv.org/pdf/2511.12763v2",
    "source": "ArXiv",
    "date": "2025-11-16",
    "abstract": "This commentary translates the central ideas in Lead times in flux into a practice ready handbook in R. The original article measures change in the full distribution of booking lead times with a normalized L1 distance and tracks that divergence across months relative to year over year and to a fixed 2018 reference. It also provides a bound that links divergence and remaining horizon to the relative error of pickup forecasts. We implement these ideas end to end in R, using a minimal data schema and providing runnable scripts, simulated examples, and a prespecified evaluation plan. All results use synthetic data so the exposition is fully reproducible without reference to proprietary sources.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文将《Lead times in flux》中的核心思想转化为R语言的实践手册。原文章通过归一化L1距离测量预订提前期的完整分布变化，并追踪该差异相对于同比和2018年固定基准的月度变化。同时提供了一个将差异与剩余时间范围关联到拾取预测相对误差的边界。我们在R中端到端实现了这些思想，使用最小数据模式，并提供可运行脚本、模拟示例和预设评估计划。所有结果均使用合成数据，因此无需参考专有来源即可完全复现。",
    "fetch_date": "2026-01-10",
    "id": "20260110_ffd51d03"
  },
  {
    "title": "Technology Adoption and Network Externalities in Financial Systems: A Spatial-Network Approach",
    "url": "https://arxiv.org/pdf/2601.04246v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "This paper develops a unified framework for analyzing technology adoption in financial networks that incorporates spatial spillovers, network externalities, and their interaction. The framework characterizes adoption dynamics through a master equation whose solution admits a Feynman-Kac representation as expected cumulative adoption pressure along stochastic paths through spatial-network space. From this representation, I derive the Adoption Amplification Factor -- a structural measure of technology leadership that captures the ratio of total system-wide adoption to initial adoption following a localized shock. A Levy jump-diffusion extension with state-dependent jump intensity captures critical mass dynamics: below threshold, adoption evolves through gradual diffusion; above threshold, cascade dynamics accelerate adoption through discrete jumps. Applying the framework to SWIFT gpi adoption among 17 Global Systemically Important Banks, I find strong support for the two-regime characterization. Network-central banks adopt significantly earlier ($ρ= -0.69$, $p = 0.002$), and pre-threshold adopters have significantly higher amplification factors than post-threshold adopters (11.81 versus 7.83, $p = 0.010$). Founding members, representing 29 percent of banks, account for 39 percent of total system amplification -- sufficient to trigger cascade dynamics. Controlling for firm size and network position, CEO age delays adoption by 11-15 days per year.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个统一框架，用于分析金融网络中的技术采用，该框架整合了空间溢出效应、网络外部性及其相互作用。通过主方程及其Feynman-Kac表示，框架描述了采用动态，并推导出“采用放大因子”——一种衡量技术领导力的结构指标，反映了局部冲击后系统总采用量与初始采用量的比率。通过Levy跳扩散扩展（具有状态依赖的跳跃强度）捕捉临界质量动态：阈值以下，采用通过渐进扩散演变；阈值以上，级联动态通过离散跳跃加速采用。将该框架应用于17家全球系统重要性银行的SWIFT gpi采用，发现强烈支持两阶段特征：网络中心银行显著更早采用（ρ = -0.69，p = 0.002），且阈值前采用者的放大因子显著高于阈值后采用者（11.81对比...）。",
    "fetch_date": "2026-01-10",
    "id": "20260110_65a2bfc9"
  },
  {
    "title": "CBDC Stress Test in a Dual-Currency Setting",
    "url": "https://arxiv.org/pdf/2511.13384v4",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "This study explores the potential impact of introducing a Central Bank Digital Currency (CBDC) on financial stability in an emerging dual-currency economy (Romania), where the domestic currency (RON) coexists with the euro. It develops an integrated analytical framework combining econometrics, machine learning, and behavioural modelling. CBDC adoption probabilities are estimated using XGBoost and logistic regression models trained on behavioural and macro-financial indicators rather than survey data. Liquidity stress simulations assess how banks would respond to deposit withdrawals resulting from CBDC adoption, while VAR, MSVAR, and SVAR models capture the macro-financial transmission of liquidity shocks into credit contraction and changes in monetary conditions. The findings indicate that CBDC uptake (co-circulating Digital RON and Digital EUR) would be moderate at issuance, amounting to around EUR 1 billion, primarily driven by digital readiness and trust in the central bank. The study concludes that a non-remunerated, capped CBDC, designed primarily as a means of payment rather than a store of value, can be introduced without compromising financial stability. In dual currency economies, differentiated holding limits for domestic and foreign digital currencies (e.g., Digital RON versus Digital Euro) are crucial to prevent uncontrolled euroisation and preserve monetary sovereignty. A prudent design with moderate caps, non remuneration, and macroprudential coordination can transform CBDC into a digital liquidity buffer and a complementary monetary policy instrument that enhances resilience and inclusion rather than destabilising the financial system.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨了在罗马尼亚这一新兴的双货币经济体（本国货币RON与欧元共存）中，引入央行数字货币（CBDC）对金融稳定的潜在影响。它构建了一个结合计量经济学、机器学习和行为建模的综合分析框架。CBDC的采用概率是通过使用XGBoost和逻辑回归模型，基于行为和宏观金融指标（而非调查数据）进行估计的。流动性压力模拟评估了银行如何应对因CBDC采用而导致的存款提取，而VAR、MSVAR和SVAR模型则捕捉了流动性冲击向信贷收缩和货币条件变化的宏观金融传导。研究结果表明，CBDC（数字RON和数字欧元共同流通）在发行初期的采用将是温和的，规模约为10亿欧元，主要由数字化准备程度和对央行的信任驱动。研究结论是，一种非付息、有上限的CBDC，主要设计为支付手段而非价值储存手段，可以在不损害金融稳定性的情况下引入。",
    "fetch_date": "2026-01-10",
    "id": "20260110_5001cae3"
  },
  {
    "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
    "url": "https://arxiv.org/pdf/2511.13214v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种利用图神经网络与深度强化学习解决资源受限项目调度问题（RCPSP）中任务工期不确定性的方法。该方法旨在通过已知概率建模不确定工期，最小化项目期望总工期，并生成可重复使用的基线调度方案。研究开发了名为Wheatley的框架，在标准基准测试中表现出优越性能与泛化能力。",
    "fetch_date": "2026-01-10",
    "id": "20260110_d52b1ee6"
  },
  {
    "title": "Sharpening Shapley Allocation: from Basel 2.5 to FRTB",
    "url": "https://arxiv.org/pdf/2511.12391v3",
    "source": "ArXiv",
    "date": "2025-11-15",
    "abstract": "Risk allocation, the decomposition of a portfolio-wide risk measure into component contributions, is a fundamental problem in financial risk management due to the non-additive nature of risk measures, the layered organizational structures of financial institutions, and the range of possible allocation strategies characterized by different rationales and properties.\n  In this work, we conduct a systematic review of the major risk allocation strategies typically used in finance, comparing their theoretical properties, practical advantages, and limitations. To this scope we set up a specific testing framework, including both simplified settings, designed to highlight basic intrinsic behaviours, and realistic financial portfolios under different risk regulations, i.e. Basel 2.5 and FRTB. Furthermore, we develop and test novel practical solutions to manage the issue of negative risk allocations and of multi-level risk allocation in the layered organizational structure of financial institutions, while preserving the additivity property. Finally, we devote particular attention to the computational aspects of risk allocation.\n  Our results show that, in this context, the Shapley allocation strategy offers the best compromise between simplicity, mathematical properties, risk representation and computational cost. The latter is still acceptable even in the challenging case of many business units, provided that an efficient Monte Carlo simulation is employed, which offers excellent scaling and convergence properties. While our empirical applications focus on market risk, our methodological framework is fully general and applicable to other financial context such as valuation risk, liquidity risk, credit risk, and counterparty credit risk.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "风险分配是将组合层面的风险度量分解为各组成部分贡献的基本问题，源于风险度量的非可加性、金融机构的层级组织结构以及不同分配策略的多样性。本文系统回顾了金融领域常用的主要风险分配策略，比较其理论特性、实践优势和局限性。通过建立测试框架，包括简化场景和基于不同风险监管（如Basel 2.5和FRTB）的现实金融组合，评估了Shapley分配方法。研究还提出了管理负风险分配和多层级风险分配的新解决方案，并重点关注了风险分配的计算方面。",
    "fetch_date": "2026-01-10",
    "id": "20260110_79273fc4"
  },
  {
    "title": "Uni-FinLLM: A Unified Multimodal Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment",
    "url": "https://arxiv.org/pdf/2601.02677v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified multimodal large language model that uses a shared Transformer backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified multimodal LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "Uni-FinLLM：一种统一的多模态大语言模型，采用模块化任务头，用于微观股票预测和宏观系统性风险评估。该模型通过共享Transformer骨干网络和跨模态注意力机制，联合处理金融文本、数值时间序列、基本面数据和视觉数据，实现微观、中观和宏观层面的统一预测。在股票方向预测准确率（67.4%）、信用风险评估准确率（84.1%）和宏观预警准确率（82.3%）上均显著超越基线模型，验证了统一多模态LLM在联合建模资产行为与系统性风险方面的有效性，为金融领域提供了可扩展的决策支持引擎。",
    "fetch_date": "2026-01-09",
    "id": "20260109_e295a081"
  },
  {
    "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
    "url": "https://arxiv.org/pdf/2601.02310v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文提出了一种用于高频限价订单簿（LOB）预测的时序Kolmogorov-Arnold网络（T-KAN），通过用可学习的B样条激活函数替代标准LSTM的固定线性权重，使模型能学习市场信号的“形状”而不仅仅是幅度。在FI-2010数据集上，T-KAN在k=100时间窗口的F1分数相对提升了19.1%，并在1.0基点交易成本下实现了132.48%的回报（对比DeepLOB的-82.76%回撤）。模型具有较好的可解释性（样条中的“死区”清晰可见），且架构针对低延迟FPGA实现进行了独特优化。这些特性直接针对实战交易中的噪声处理、Alpha衰减和延迟挑战，具有明确的实战价值。",
    "fetch_date": "2026-01-09",
    "id": "20260109_234a7923"
  },
  {
    "title": "Smart Predict--then--Optimize Paradigm for Portfolio Optimization in Real Markets",
    "url": "https://arxiv.org/pdf/2601.04062v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Improvements in return forecast accuracy do not always lead to proportional improvements in portfolio decision quality, especially under realistic trading frictions and constraints. This paper adopts the Smart Predict--then--Optimize (SPO) paradigm for portfolio optimization in real markets, which explicitly aligns the learning objective with downstream portfolio decision quality rather than pointwise prediction accuracy. Within this paradigm, predictive models are trained using an SPO-based surrogate loss that directly reflects the performance of the resulting investment decisions. To preserve interpretability and robustness, we employ linear predictors built on return-based and technical-indicator features and integrate them with portfolio optimization models that incorporate transaction costs, turnover control, and regularization. We evaluate the proposed approach on U.S. ETF data (2015--2025) using a rolling-window backtest with monthly rebalancing. Empirical results show that decision-focused training consistently improves risk-adjusted performance over predict--then--optimize baselines and classical optimization benchmarks, and yields strong robustness during adverse market regimes (e.g., the 2020 COVID-19). These findings highlight the practical value of the Smart Predict--then--Optimize paradigm for portfolio optimization in realistic and non-stationary financial environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文采用智能预测-优化（SPO）范式进行真实市场中的投资组合优化，该范式明确将学习目标与下游投资组合决策质量（而非逐点预测精度）对齐。在此范式中，预测模型使用基于SPO的代理损失进行训练，该损失直接反映最终投资决策的表现。为保持可解释性和鲁棒性，我们采用基于收益率和技术指标特征构建的线性预测器，并将其与包含交易成本、换手率控制和正则化的投资组合优化模型相结合。我们在美国ETF数据（2015-2025）上使用滚动窗口回测（每月再平衡）评估所提方法。实证结果表明，与预测-优化基线和经典优化基准相比，决策导向的训练能持续提升风险调整后表现，并在不利市场条件下展现出强鲁棒性。",
    "fetch_date": "2026-01-09",
    "id": "20260109_72f42092"
  },
  {
    "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
    "url": "https://arxiv.org/pdf/2601.03948v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "Trade-R1：通过过程级推理验证将可验证奖励桥接至随机环境。强化学习（RL）使大语言模型（LLM）在数学和编程等可验证奖励提供清晰信号的领域取得显著推理成就，但将其扩展至金融决策面临市场随机性的挑战：奖励可验证但本质嘈杂，导致标准RL退化为奖励黑客行为。为此，我们提出Trade-R1，一种通过过程级推理验证将可验证奖励桥接至随机环境的模型训练框架。核心创新是一种验证方法，将评估长篇金融文档推理的问题转化为结构化检索增强生成（RAG）任务。我们构建三角一致性度量，评估检索证据、推理链和决策之间的两两对齐，作为嘈杂市场回报的有效性过滤器。我们探索两种奖励整合策略：用于稳定对齐信号的固定效应语义奖励（FSR），以及用于耦合幅度优化的动态效应语义奖励（DSR）。不同国家资产选择实验表明...",
    "fetch_date": "2026-01-09",
    "id": "20260109_e30b157b"
  },
  {
    "title": "Market-Dependent Communication in Multi-Agent Alpha Generation",
    "url": "https://arxiv.org/pdf/2511.13614v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "多策略对冲基金面临一个根本性的组织选择：生成交易策略的分析师是否应该沟通，以及如何沟通？我们使用基于5个智能体的LLM交易系统，在21个月内进行了450次实验，比较了从孤立基线到协作和竞争对话的五种组织结构。研究表明，沟通能提高绩效，但最优的沟通设计取决于市场特征。竞争性对话在波动性大的科技股中表现出色，而协作性对话在稳定的普通股中占主导。金融股对所有沟通干预都表现出抵抗性。令人惊讶的是，所有结构（包括孤立智能体）都收敛到相似的策略对齐，这挑战了透明度会导致有害多样性损失的假设。绩效差异源于行为机制：竞争性智能体专注于股票层面的配置，而协作性智能体则发展技术框架。对话质量得分与回报零相关。这些发现表明，最优的沟通设计必须匹配市场波动特征，而复杂的讨论并不保证更好的交易结果。",
    "fetch_date": "2026-01-09",
    "id": "20260109_939fc4c3"
  },
  {
    "title": "Enhancing stock price prediction using GANs and transformer-based attention mechanisms",
    "url": "https://link.springer.com/article/10.1007/s00181-024-02644-6",
    "source": "Scholar",
    "date": "2026-01-09",
    "abstract": "… of Reinforcement Learning methodologies, fine-tuning of hyperparameters, and expansion to non-technology industries, we seek to develop more robust and accurate stock prediction …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种结合生成对抗网络（GANs）和基于Transformer的注意力机制来增强股票价格预测的方法。研究重点包括强化学习（RL）方法的应用、超参数微调，以及将模型扩展到非科技行业，旨在开发更稳健、准确的股票预测模型。",
    "fetch_date": "2026-01-09",
    "id": "20260109_abf31a5a"
  },
  {
    "title": "A comprehensive review and analysis of different modeling approaches for financial index tracking problem",
    "url": "https://arxiv.org/pdf/2601.03927v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Index tracking, also known as passive investing, has gained significant traction in financial markets due to its cost-effective and efficient approach to replicating the performance of a specific market index. This review paper provides a comprehensive overview of the various modeling approaches and strategies developed for index tracking, highlighting the strengths and limitations of each approach. We categorize the index tracking models into three broad frameworks: optimization-based models, statistical-based models and machine learning based data-driven approach. A comprehensive empirical study conducted on the S\\&P 500 dataset demonstrates that the tracking error volatility model under the optimization-based framework delivers the most precise index tracking, the convex co-integration model, under the statistical-based framework achieves the strongest return-risk balance, and the deep neural network with fixed noise model within the data-driven framework provides a competitive performance with notably low turnover and high computational efficiency. By combining a critical review of the existing literature with comparative empirical analysis, this paper aims to provide insights into the evolving landscape of index tracking and its practical implications for investors and fund managers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "这篇题为《金融指数跟踪问题的不同建模方法全面回顾与分析》的综述论文，系统梳理了指数跟踪（被动投资）的三大建模框架：基于优化的模型、基于统计的模型和基于机器学习的数据驱动方法。通过对标普500数据集的实证研究表明：优化框架下的跟踪误差波动率模型具有最高跟踪精度，统计框架下的凸协整模型实现了最优的风险收益平衡，而数据驱动框架下的带固定噪声深度神经网络模型在保持竞争力的同时，展现出显著的低换手率和高计算效率。该论文通过文献综述与实证对比，为实战交易中指数跟踪策略的选择与优化提供了有价值的参考。",
    "fetch_date": "2026-01-09",
    "id": "20260109_e69c36db"
  },
  {
    "title": "Breaking the Dimensional Barrier: Dynamic Portfolio Choice with Parameter Uncertainty via Pontryagin Projection",
    "url": "https://arxiv.org/pdf/2601.03175v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "We study continuous-time portfolio choice in diffusion markets with parameter $θ\\in Θ$ and uncertainty law $q(dθ)$. Nature draws latent $θ\\sim q$ at time 0; the investor cannot observe it and must deploy a single $θ$-blind feedback policy maximizing an ex-ante CRRA objective averaged over diffusion noise and $θ$. Our methods access $q$ only by sampling and assume no parametric form. We extend Pontryagin-Guided Direct Policy Optimization (PG-DPO) by sampling $θ$ inside the simulator and computing discrete-time gradients via backpropagation through time (BPTT), and we propose projected PG-DPO (P-PGDPO) that projects costate estimates to satisfy the $q$-aggregated Pontryagin first-order condition, yielding a deployable rule. We prove a BPTT-PMP correspondence uniform on compacts and a residual-based $θ$-blind policy-gap bound under local stability with explicit discretization/Monte Carlo errors; experiments show projection-driven stability and accurate decision-time benchmark recovery in high dimensions.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究在参数θ不确定的扩散市场中的连续时间投资组合选择问题。投资者无法观测潜在参数θ，只能采用单一θ盲反馈策略，最大化事前CRRA目标函数。方法通过采样访问不确定性分布q，无需参数形式假设。扩展了Pontryagin引导直接策略优化(PG-DPO)，在模拟器中采样θ并通过时间反向传播(BPTT)计算离散时间梯度，提出投影PG-DPO(P-PGDPO)，将协态估计投影以满足q聚合的Pontryagin一阶条件，得到可部署规则。证明了BPTT与Pontryagin极大值原理在紧集上的一致性，以及在局部稳定性下的θ盲策略间隙界限，包含显式离散化/蒙特卡洛误差。实验显示投影驱动稳定性和高维决策时基准的准确恢复。",
    "fetch_date": "2026-01-09",
    "id": "20260109_b823004b"
  },
  {
    "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2601.02850v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种神经符号深度强化学习方法，通过整合背景符号知识来提升样本效率和泛化能力。具体方法是将简单场景中定义的部分策略表示为逻辑规则，通过在线推理在训练过程中引导智能体：在探索阶段偏置动作分布，在利用阶段重新缩放Q值。这种神经符号集成增强了可解释性和可信度，同时加速了收敛，特别是在稀疏奖励环境和长规划视野的任务中。论文在挑战性任务上进行了实证验证，展示了该方法在避免从头调整DRL参数、加速复杂场景学习方面的潜力。",
    "fetch_date": "2026-01-09",
    "id": "20260109_db8b5b68"
  },
  {
    "title": "Class of topological portfolios: Are they better than classical portfolios?",
    "url": "https://arxiv.org/pdf/2601.03974v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Topological Data Analysis (TDA), an emerging field in investment sciences, harnesses mathematical methods to extract data features based on shape, offering a promising alternative to classical portfolio selection methodologies. We utilize persistence landscapes, a type of summary statistics for persistent homology, to capture the topological variation of returns, blossoming a novel concept of ``Topological Risk\". Our proposed topological risk then quantifies portfolio risk by tracking time-varying topological properties of assets through the $L_p$ norm of the persistence landscape. Through optimization, we derive an optimal portfolio that minimizes this topological risk. Numerical experiments conducted using nearly a decade long S\\&P 500 data demonstrate the superior performance of our TDA-based portfolios in comparison to the seven popular portfolio optimization models and two benchmark portfolio strategies, the naive $1/N$ portfolio and the S\\&P 500 market index, in terms of excess mean return, and several financial ratios. The outcome remains consistent through out the computational analysis conducted for the varying size of holding and investment time horizon. These results underscore the potential of our TDA-based topological risk metric in providing a more comprehensive understanding of portfolio dynamics than traditional statistical measures. As such, it holds significant relevance for modern portfolio management practices.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出了一种基于拓扑数据分析（TDA）的投资组合优化方法，通过持久性景观（persistence landscapes）捕捉收益的拓扑变化，并定义了“拓扑风险”概念。利用L_p范数量化时变拓扑特性，通过优化最小化该风险构建投资组合。基于近十年标普500数据的数值实验表明，该方法在超额平均收益和多个财务比率上优于七种经典投资组合模型及两种基准策略（1/N组合和标普500指数），且结果在不同持仓规模和投资期限下保持稳健。",
    "fetch_date": "2026-01-09",
    "id": "20260109_4315c79c"
  },
  {
    "title": "Optimal execution on Uniswap v2/v3 under transient price impact",
    "url": "https://arxiv.org/pdf/2601.03799v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "We study the optimal liquidation of a large position on Uniswap v2 and Uniswap v3 in discrete time. The instantaneous price impact is derived from the AMM pricing rule. Transient impact is modeled to capture either exponential or approximately power-law decay, together with a permanent component. In the Uniswap v2 setting, we obtain optimal strategies in closed-form under general price dynamics. For Uniswap v3, we consider a two-layer liquidity framework, which naturally extends to multiple layers. We address the problem using dynamic programming under geometric Brownian motion dynamics and approximate the solution numerically using a discretization scheme. We obtain optimal strategies akin to classical ones in the LOB literature, with features specific to Uniswap. In particular, we show how the liquidity profile influences them.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究了在Uniswap v2/v3上大额头寸的最优清算问题。瞬时价格影响源自AMM定价规则，瞬态影响模型捕捉指数或近似幂律衰减，并包含永久性成分。在Uniswap v2中，我们获得了通用价格动态下的闭式最优策略；在Uniswap v3中，我们采用双层流动性框架（可扩展至多层），基于几何布朗运动动态规划，并通过离散化方案数值近似求解。所得最优策略类似于LOB文献中的经典方法，但具有Uniswap特有特征，特别是流动性分布如何影响策略。",
    "fetch_date": "2026-01-09",
    "id": "20260109_759461fb"
  },
  {
    "title": "Quantum computing for multidimensional option pricing: End-to-end pipeline",
    "url": "https://arxiv.org/pdf/2601.04049v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "This work introduces an end-to-end framework for multi-asset option pricing that combines market-consistent risk-neutral density recovery with quantum-accelerated numerical integration. We first calibrate arbitrage-free marginal distributions from European option quotes using the Normal Inverse Gaussian (NIG) model, leveraging its analytical tractability and ability to capture skewness and fat tails. Marginals are coupled via a Gaussian copula to construct joint distributions. To address the computational bottleneck of the high-dimensional integration required to solve the option pricing formula, we employ Quantum Accelerated Monte Carlo (QAMC) techniques based on Quantum Amplitude Estimation (QAE), achieving quadratic convergence improvements over classical Monte Carlo (CMC) methods. Theoretical results establish accuracy bounds and query complexity for both marginal density estimation (via cosine-series expansions) and multidimensional pricing. Empirical tests on liquid equity entities (Credit Agricole, AXA, Michelin) confirm high calibration accuracy and demonstrate that QAMC requires 10-100 times fewer queries than classical methods for comparable precision. This study provides a practical route to integrate arbitrage-aware modelling with quantum computing, highlighting implications for scalability and future extensions to complex derivatives.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种用于多资产期权定价的端到端框架，该框架结合了市场一致的风险中性密度恢复与量子加速数值积分。首先使用正态逆高斯（NIG）模型从欧式期权报价中校准无套利边际分布，利用其解析易处理性及捕捉偏度和厚尾的能力。通过高斯Copula耦合边际分布以构建联合分布。为解决求解期权定价公式所需的高维积分计算瓶颈，采用了基于量子振幅估计（QAE）的量子加速蒙特卡洛（QAMC）技术，相比经典蒙特卡洛（CMC）方法实现了二次收敛改进。理论结果建立了边际密度估计（通过余弦级数展开）和多维定价的精度界限及查询复杂度。对流动性股票实体（法国农业信贷银行、安盛、米其林）的实证测试证实了高校准精度，并表明在可比精度下，QAMC所需的查询次数比经典方法少10-100倍。",
    "fetch_date": "2026-01-09",
    "id": "20260109_cad71305"
  },
  {
    "title": "Diversification Preferences and Risk Attitudes",
    "url": "https://arxiv.org/pdf/2601.04067v1",
    "source": "ArXiv",
    "date": "2026-01-07",
    "abstract": "Portfolio diversification is a cornerstone of modern finance, while risk aversion is central to decision theory; both concepts are long-standing and foundational. We investigate their connections by studying how different forms of diversification correspond to notions of risk aversion. We focus on the classical distinctions between weak and strong risk aversion, and consider diversification preferences for pairs of risks that are identically distributed, comonotonic, antimonotonic, independent, or exchangeable, as well as their intersections. Under a weak continuity condition and without assuming completeness of preferences, diversification for antimonotonic and identically distributed pairs implies weak risk aversion, and diversification for exchangeable pairs is equivalent to strong risk aversion. The implication from diversification for independent pairs to weak risk aversion requires a stronger continuity. We further provide results and examples that clarify the relationships between various diversification preferences and risk attitudes, in particular justifying the one-directional nature of many implications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了投资组合分散化与风险厌恶之间的理论联系，研究了在不同风险分布（如反单调、同分布、可交换、独立等）下，分散化偏好如何对应弱风险厌恶与强风险厌恶的概念。在弱连续性条件下，反单调和同分布风险对的分散化偏好意味着弱风险厌恶，而可交换风险对的分散化偏好则等价于强风险厌恶。独立风险对的分散化偏好推导出弱风险厌恶需要更强的连续性条件。论文通过结果和示例澄清了各种分散化偏好与风险态度之间的关系，特别是论证了多数蕴含关系的单向性。",
    "fetch_date": "2026-01-09",
    "id": "20260109_19a96837"
  },
  {
    "title": "Trading with market resistance and concave price impact",
    "url": "https://arxiv.org/pdf/2601.03215v1",
    "source": "ArXiv",
    "date": "2026-01-06",
    "abstract": "We consider an optimal trading problem under a market impact model with endogenous market resistance generated by a sophisticated trader who (partially) detects metaorders and trades against them to exploit price overreactions induced by the order flow. The model features a concave transient impact driven by a power-law propagator with a resistance term responding to the trader's rate via a fixed-point equation involving a general resistance function. We derive a (non)linear stochastic Fredholm equation as the first-order optimality condition satisfied by optimal trading strategies. Existence and uniqueness of the optimal control are established when the resistance function is linear, and an existence result is obtained when it is strictly convex using coercivity and weak lower semicontinuity of the associated profit-and-loss functional. We also propose an iterative scheme to solve the nonlinear stochastic Fredholm equation and prove an exponential convergence rate. Numerical experiments confirm this behavior and illustrate optimal round-trip strategies under \"buy\" signals with various decay profiles and different market resistance specifications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一种具有内生市场阻力的市场冲击模型下的最优交易问题。该模型假设存在一个能够（部分）探测到大额订单（元订单）并与之反向交易以利用订单流引发的价格过度反应的复杂交易者，从而产生市场阻力。模型特征包括由幂律传播子驱动的凹型瞬时冲击，以及一个通过涉及一般阻力函数的定点方程对交易者速率做出响应的阻力项。作者推导出一个（非）线性随机Fredholm方程作为最优交易策略满足的一阶最优性条件。当阻力函数为线性时，建立了最优控制的存在性和唯一性；当阻力函数严格凸时，利用相关损益泛函的强制性和弱下半连续性得到了存在性结果。此外，作者提出了一个迭代方案来求解该非线性随机Fredholm方程，并证明了指数收敛速度。数值实验验证了该行为，并展示了在不同衰减曲线和市场阻力设定下，基于'买入'信号的最优往返交易策略。",
    "fetch_date": "2026-01-09",
    "id": "20260109_6dd1b64d"
  },
  {
    "title": "A Quantum Model for Constrained Markowitz Modern Portfolio Using Slack Variables to Process Mixed-Binary Optimization under QAOA",
    "url": "https://arxiv.org/pdf/2601.03278v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "Effectively encoding inequality constraints is a primary obstacle in applying quantum algorithms to financial optimization. A quantum model for Markowitz portfolio optimization is presented that resolves this by embedding slack variables directly into the problem Hamiltonian. The method maps each slack variable to a dedicated ancilla qubit, transforming the problem into a Quadratic Unconstrained Binary Optimization (QUBO) formulation suitable for the Quantum Approximate Optimization Algorithm (QAOA). This process internalizes the constraints within the quantum state, altering the problem's energy landscape to facilitate optimization. The model is empirically validated through simulation, showing it consistently finds the optimal portfolio where a standard penalty-based QAOA fails. This work demonstrates that modifying the Hamiltonian architecture via a slack-ancilla scheme provides a robust and effective pathway for solving constrained optimization problems on quantum computers. A fundamental quantum limit on the simultaneous precision of portfolio risk and return is also posited.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于约束型马科维茨现代投资组合优化的量子模型，通过引入松弛变量并将其直接嵌入问题哈密顿量中，解决了量子算法在金融优化中处理不等式约束的主要障碍。该方法将每个松弛变量映射到一个专用的辅助量子比特，将问题转化为适合量子近似优化算法（QAOA）的二次无约束二进制优化（QUBO）形式。该模型通过仿真进行了实证验证，表明其能持续找到最优投资组合，而标准的基于惩罚的QAOA则无法做到。这项工作表明，通过松弛-辅助方案修改哈密顿量架构，为在量子计算机上解决约束优化问题提供了一条稳健有效的途径。文中还提出了关于投资组合风险与回报同时精度的基本量子极限。",
    "fetch_date": "2026-01-09",
    "id": "20260109_ad68ae27"
  },
  {
    "title": "Algorithmic trading: winning strategies and their rationale",
    "url": "https://books.google.com/books?hl=en&lr=&id=CIwCTVqEj4oC&oi=fnd&pg=PR9&dq=%22algorithmic+trading%22+strategy+after:2024&ots=kVDIJtGBzE&sig=h_GIptC7VR5rAizz-bZt0STkCfI",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… to algorithmic trading strategies that can be readily implemented by both retail and institutional traders alike. More than an academic treatise on financial theory, Algorithmic Trading is …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "《算法交易：制胜策略及其原理》聚焦于可直接由零售和机构交易者实施的算法交易策略，超越金融理论的学术论述，强调实战应用。",
    "fetch_date": "2026-01-08",
    "id": "20260108_f8028324"
  },
  {
    "title": "Intelligent algorithmic trading strategy using reinforcement learning and directional change",
    "url": "https://ieeexplore.ieee.org/abstract/document/9514595/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… , employing an algorithmic trading strategy remains a challenge … algorithmic trading strategy using the proposed DCRL model, specifically, we present two algorithmic trading strategies …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于强化学习（RL）和方向变化（Directional Change）的智能算法交易策略，通过DCRL模型设计了两种具体的算法交易策略，旨在解决实际交易中算法策略应用的挑战。",
    "fetch_date": "2026-01-08",
    "id": "20260108_cefab63f"
  },
  {
    "title": "Optimal profit-making strategies in stock market with algorithmic trading",
    "url": "https://www.aimspress.com/aimspress-data/qfe/2024/3/PDF/QFE-08-03-021.pdf",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… former for strategy construction and use the machine learning model constructed in the previous chapter to realize strategy returns by … Our construction of the timing strategy is as follows: …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了在股票市场中通过算法交易实现最优盈利策略的方法。它结合了机器学习模型进行策略构建，并利用前一章节构建的模型来实现策略收益。其中，择时策略的构建方法被详细阐述，表明其具有实际应用潜力，适用于实战交易场景。",
    "fetch_date": "2026-01-08",
    "id": "20260108_672ad619"
  },
  {
    "title": "Deep Convolutional Transformer Network for Stock Movement Prediction",
    "url": "https://www.mdpi.com/2079-9292/13/21/4225",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… In [64], FDG-trans is built by integrating GRU, LSTM, and multi-head attention Transformers. Both TN and DT are stock prediction models based on the standard Transformer architecture…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度卷积Transformer网络的股票走势预测模型。通过整合GRU、LSTM和多头注意力Transformer等深度学习架构，构建了FDG-trans等股票预测模型，这些模型均基于标准Transformer架构。该研究对实战交易具有较高价值，因为它直接应用于股票市场预测这一量化交易核心领域，并采用了当前前沿的深度学习技术。",
    "fetch_date": "2026-01-08",
    "id": "20260108_ec739ba3"
  },
  {
    "title": "Hybrid LSTM-Transformer Model for Stock Market Prediction: A Deep Learning Approach",
    "url": "https://ieeexplore.ieee.org/abstract/document/11127241/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… This paper proposes a novel LSTM-Transformer hybrid model … LSTM, GRU, and Transformer baselines, achieving an MSE of … Below, we discuss key advancements in stock prediction …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种新颖的LSTM-Transformer混合模型用于股票市场预测。通过深度学习的方法，该模型在LSTM、GRU和Transformer基准模型上取得了较低的MSE（均方误差），展示了在股票预测领域的关键进展。",
    "fetch_date": "2026-01-08",
    "id": "20260108_15b45001"
  },
  {
    "title": "Global Stock Market Prediction Using Transformer-Based Deep Learning Techniques",
    "url": "https://ieeexplore.ieee.org/abstract/document/10724893/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… The Transformer framework, initially created for natural … The well-known Transformer architecture, which excels at … The outcomes demonstrate that the Transformer performs better than …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文研究基于Transformer的深度学习技术在全球股市预测中的应用。Transformer架构最初为自然语言处理设计，以其在序列建模中的卓越能力而闻名。结果表明，Transformer在预测性能上优于其他方法，显示出其在实战交易中捕捉市场时序模式和生成交易信号的潜力。",
    "fetch_date": "2026-01-08",
    "id": "20260108_d43c562e"
  },
  {
    "title": "Enhancing multi-factor stock selection with transformer networks: A comparative analysis against traditional machine learning models",
    "url": "https://www.sciencedirect.com/science/article/pii/S187705092502438X",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… Recent studies have begun to validate the Transformer's potential in finance. For instance, Transformer-based models have been shown to outperform LSTMs in stock prediction [5], and …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文《利用Transformer网络增强多因子选股：与传统机器学习模型的比较分析》探讨了Transformer架构在金融量化领域的应用潜力。研究表明，基于Transformer的模型在股票预测方面优于LSTM等传统时序模型，通过对比分析验证了其在多因子选股策略中的有效性，对实战交易中因子挖掘和预测模型优化具有直接参考价值。",
    "fetch_date": "2026-01-08",
    "id": "20260108_183ffad8"
  },
  {
    "title": "Llm-augmented linear transformer–cnn for enhanced stock price prediction",
    "url": "https://www.mdpi.com/2227-7390/13/3/487",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… from two additional branches: the Linear Transformer branch, which captures long-term … the proposed framework significantly improves stock prediction accuracy by effectively capturing …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "该论文提出了一种LLM增强的线性Transformer-CNN框架，通过线性Transformer分支捕捉长期依赖关系，并结合CNN处理局部模式，显著提升了股价预测的准确性。该方法融合了深度学习技术，对实战交易中的预测模型优化具有参考价值。",
    "fetch_date": "2026-01-08",
    "id": "20260108_a0e7c66b"
  },
  {
    "title": "Stock price prediction with LLM-guided market movement signals and transformer model",
    "url": "https://ojs.bonviewpress.com/index.php/FSI/article/view/5703",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… However, most Transformer-based stock prediction models … -generated sentiment features into stock prediction models [18]. … are not applied to directly output the final stock prediction. …",
    "broker": "Google Scholar",
    "score": 7,
    "summary": "该论文提出了一种结合LLM生成的市场情绪信号与Transformer模型的股价预测方法。虽然摘要指出现有Transformer模型通常不直接输出最终股价预测，但该方法通过整合LLM引导的市场动向信号，旨在提升预测性能。对实战交易的价值在于：可能提供更丰富的市场情绪特征，增强模型对市场动态的捕捉能力，但需验证其在实时交易环境中的稳定性和泛化性。",
    "fetch_date": "2026-01-08",
    "id": "20260108_90875abc"
  },
  {
    "title": "Algorithmic trading and quantitative strategies",
    "url": "https://www.taylorfrancis.com/books/mono/10.1201/9780429183942/algorithmic-trading-quantitative-strategies-raja-velu",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… Hardy is responsible for the development of the algorithmic trading strategies and models underpinning the agency electronic execution products for the Equities and Futures divisions …",
    "broker": "Google Scholar",
    "score": 6,
    "summary": "该论文聚焦于算法交易与量化策略的开发，由Hardy负责为股票和期货部门的机构电子执行产品构建算法交易策略和模型。内容涉及实际应用层面的策略与模型开发，但未明确提及强化学习、深度学习或Alpha生成等前沿实战技术，因此具有一定的实战参考价值，但可能偏向传统量化方法。",
    "fetch_date": "2026-01-08",
    "id": "20260108_8062999b"
  },
  {
    "title": "Stock price forecast based on improved Transformer",
    "url": "https://books.google.com/books?hl=en&lr=&id=XZEXEQAAQBAJ&oi=fnd&pg=PA170&dq=transformers+for+%22stock+prediction%22+after:2024&ots=A6yPxbhQ6b&sig=fEqjpsiwiEq0xwuCx805RMP8tHg",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… a stock prediction system based on five basic trading indicators of historical stock trading data, which provides investors with stock prediction … This paper use transformer model as the …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "本文基于历史股票交易数据的五个基本交易指标，构建了一个基于改进Transformer模型的股票价格预测系统，旨在为投资者提供股票预测。",
    "fetch_date": "2026-01-08",
    "id": "20260108_922c857b"
  },
  {
    "title": "Stock Price Prediction Using Transformers",
    "url": "https://ieeexplore.ieee.org/abstract/document/10911285/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… study focuses on the capabilities of Transformer architecture to handle intricate, non-linear trends effectively. Traditional approaches to stock prediction, including autoregressive and …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该研究聚焦于Transformer架构在有效处理复杂非线性趋势方面的能力。传统的股票预测方法包括自回归模型等。",
    "fetch_date": "2026-01-08",
    "id": "20260108_81c4d64f"
  },
  {
    "title": "Analyzing the impact of algorithmic trading on stock market behavior: A comprehensive review",
    "url": "https://pdfs.semanticscholar.org/5893/a486749185dfd0ac0453cdd89a0b4d0ef73e.pdf",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… interplay between algorithmic trading strategies and market … examination of algorithmic trading's principles, strategies, … The findings reveal that algorithmic trading, characterized by …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "该论文全面综述了算法交易对股市行为的影响，探讨了算法交易策略与市场之间的相互作用，并分析了算法交易的原则与策略。研究发现，算法交易以...为特征。",
    "fetch_date": "2026-01-08",
    "id": "20260108_20f72e0a"
  },
  {
    "title": "Systematic Review on Algorithmic Trading",
    "url": "https://www.ceeol.com/search/article-detail?id=1365022",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… for academics and practitioners to innovate and optimize algorithmic trading strategies. … interested in algorithmic trading, traders looking to optimize their strategies and developers …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "本文是一篇关于算法交易的系统性综述，主要面向学术界和从业者，旨在为算法交易策略的创新与优化提供参考。对算法交易感兴趣的研究人员、寻求策略优化的交易员以及相关开发者可能从中受益。",
    "fetch_date": "2026-01-08",
    "id": "20260108_548b7780"
  },
  {
    "title": "Algorithmic trading",
    "url": "https://ieeexplore.ieee.org/abstract/document/5696713/",
    "source": "Scholar",
    "date": "2026-01-08",
    "abstract": "… will shape the design of the algorithmic trading strategy; typically, broker AT systems seek to minimize the cost of trading by optimizing the execution strategy (ie minimize market impact …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "算法交易：论文探讨了算法交易策略的设计，通常经纪商的算法交易系统通过优化执行策略（即最小化市场冲击）来降低交易成本。",
    "fetch_date": "2026-01-08",
    "id": "20260108_a8706ded"
  },
  {
    "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
    "url": "https://arxiv.org/pdf/2601.02061v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "深度强化学习智能体常表现出不稳定、高频的控制行为，这在实际部署中会因过度能耗和机械磨损而受限。本文通过高阶导数惩罚系统研究动作平滑正则化，从连续控制基准的理论理解推进到建筑能源管理的实际验证。在四个连续控制环境中的综合评估表明，三阶导数惩罚（急动度最小化）能持续实现更优的平滑性，同时保持竞争力性能。我们将这些发现扩展到HVAC控制系统，其中平滑策略将设备开关次数减少60%，转化为显著的运营效益。我们的工作确立了高阶动作正则化作为RL优化与能源关键应用中操作约束之间的有效桥梁。",
    "fetch_date": "2026-01-07",
    "id": "20260107_8d61df9e"
  },
  {
    "title": "Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance",
    "url": "https://arxiv.org/pdf/2601.01709v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文扩展了Black-Scholes框架下的Q学习器（QLBS），通过引入风险厌恶和交易成本，并提出了新颖的期权定价复制学习（RLOP）方法。两种方法均与标准强化学习算法完全兼容，并在市场摩擦条件下运行。使用SPY和XOP期权数据进行评估，结果显示Adaptive-QLBS在隐含波动率空间实现了更高的静态定价精度，而RLOP通过降低短缺概率提供了更优的动态对冲表现。这些结果强调了评估期权定价模型时需超越静态拟合，重视实际对冲效果的重要性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_33758e27"
  },
  {
    "title": "On lead-lag estimation of non-synchronously observed point processes",
    "url": "https://arxiv.org/pdf/2601.01871v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "This paper introduces a new theoretical framework for analyzing lead-lag relationships between point processes, with a special focus on applications to high-frequency financial data. In particular, we are interested in lead-lag relationships between two sequences of order arrival timestamps. The seminal work of Dobrev and Schaumburg proposed model-free measures of cross-market trading activity based on cross-counts of timestamps. While their method is known to yield reliable results, it faces limitations because its original formulation inherently relies on discrete-time observations, an issue we address in this study. Specifically, we formulate the problem of estimating lead-lag relationships in two point processes as that of estimating the shape of the cross-pair correlation function (CPCF) of a bivariate stationary point process, a quantity well-studied in the neuroscience and spatial statistics literature. Within this framework, the prevailing lead-lag time is defined as the location of the CPCF's sharpest peak. Under this interpretation, the peak location in Dobrev and Schaumburg's cross-market activity measure can be viewed as an estimator of the lead-lag time in the aforementioned sense. We further propose an alternative lead-lag time estimator based on kernel density estimation and show that it possesses desirable theoretical properties and delivers superior numerical performance. Empirical evidence from high-frequency financial data demonstrates the effectiveness of our proposed method.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种新的理论框架，用于分析点过程之间的领先-滞后关系，特别关注高频金融数据的应用。研究重点在于两个订单到达时间戳序列之间的领先-滞后关系。Dobrev和Schaumburg的开创性工作提出了基于时间戳交叉计数的跨市场交易活动无模型度量方法。虽然他们的方法已知能产生可靠结果，但由于其原始公式本质上依赖于离散时间观测而面临局限性，本研究正是为了解决这一问题。具体而言，我们将估计两个点过程中领先-滞后关系的问题表述为估计二元平稳点过程的交叉配对相关函数形状的问题，这一量在神经科学和空间统计学文献中已有深入研究。在此框架下，主导的领先-滞后时间被定义为CPCF最尖锐峰值的位置。在这种解释下，Dobrev和Schaumburg的跨市场活动度量中的峰值位置可被视为领先-滞后时间的估计量。",
    "fetch_date": "2026-01-07",
    "id": "20260107_5419ed89"
  },
  {
    "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions",
    "url": "https://arxiv.org/pdf/2601.01803v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "论文《Moments Matter: Stabilizing Policy Optimization using Return Distributions》提出了一种通过利用高阶矩（偏度和峰度）来偏置PPO算法优势函数的方法，旨在解决深度强化学习（RL）中因环境和算法噪声导致的策略不稳定问题。该方法通过建模状态-动作回报分布来减少策略更新引起的变异性，从而提高策略优化的稳定性。对于实战交易，该技术有助于在复杂、高噪声的市场环境中训练更稳健的交易策略，减少策略性能的剧烈波动，提升算法在实盘中的可靠性和可迁移性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_62ba9edb"
  },
  {
    "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives",
    "url": "https://arxiv.org/pdf/2601.01665v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种面向多目标组合优化问题（MOCOPs）的偏好条件深度强化学习（DRL）求解器的统一鲁棒性框架。该框架包含：1）基于偏好的对抗攻击方法，用于生成暴露求解器弱点的困难实例，并通过帕累托前沿质量退化量化攻击影响；2）防御策略，将难度感知偏好选择融入对抗训练，以减少对受限偏好区域的过拟合并提升分布外性能。实验在多目标旅行商问题（MOTSP）、多目标容量约束车辆路径问题（MOCVRP）和多目标背包问题（MOKP）上验证了攻击方法能有效针对不同求解器生成困难实例，且防御方法显著增强了鲁棒性和泛化能力。",
    "fetch_date": "2026-01-07",
    "id": "20260107_53571319"
  },
  {
    "title": "Can Large Language Models Improve Venture Capital Exit Timing After IPO?",
    "url": "https://arxiv.org/pdf/2601.00810v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "IPO后退出时机是风险投资（VC）最关键决策之一，现有研究主要描述VC何时退出而非评估其经济最优性。大型语言模型（LLMs）在整合复杂金融数据和文本信息方面展现潜力，但尚未应用于IPO后退出决策。本研究提出一个框架，利用LLMs分析IPO后月度财务表现、文件、新闻和市场信号，估算VC最优退出时间并建议卖出或继续持有。通过比较LLM生成建议与实际VC退出日期，计算两种策略的回报差异，量化遵循LLM指导的收益或损失，为AI驱动指导能否改善退出时机提供证据，并补充风险投资研究中的传统风险模型和实物期权模型。",
    "fetch_date": "2026-01-07",
    "id": "20260107_1799a059"
  },
  {
    "title": "Statistical and economic evaluation of forecasts in electricity markets: beyond RMSE and MAE",
    "url": "https://arxiv.org/pdf/2511.13616v1",
    "source": "ArXiv",
    "date": "2025-11-17",
    "abstract": "In recent years, a rapid development of forecasting methods has led to an increase in the accuracy of predictions. In the literature, forecasts are typically evaluated using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). While appropriate for statistical assessment, these measures do not adequately reflect the economic value of forecasts. This study addresses the decision-making problem faced by a battery energy storage system, which must determine optimal charging and discharging times based on day-ahead electricity price forecasts. To explore the relationship between forecast accuracy and economic value, we generate a pool of 192 forecasts. These are evaluated using seven statistical metrics that go beyond RMSE and MAE, capturing various characteristics of the predictions and associated errors. We calculate the dynamic correlation between the statistical measures and gained profits to reveal that both RMSE and MAE are only weakly correlated with revenue. In contrast, measures that assess the alignment between predicted and actual daily price curves have a stronger relationship with profitability and are thus more effective for selecting optimal forecasts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "近年来，预测方法的快速发展提高了预测准确性。文献中通常使用均方根误差（RMSE）和平均绝对误差（MAE）等指标评估预测。虽然这些指标适用于统计评估，但不能充分反映预测的经济价值。本研究针对电池储能系统面临的决策问题，该系统必须基于日前电价预测确定最佳充放电时间。为探索预测准确性与经济价值之间的关系，我们生成了192个预测池，并使用七种超越RMSE和MAE的统计指标进行评估，这些指标捕捉了预测及相关误差的各种特征。通过计算统计指标与获得利润之间的动态相关性，我们发现RMSE和MAE与收入仅呈弱相关。相比之下，评估预测与实际日价格曲线一致性的指标与盈利能力有更强的关系，因此在选择最优预测方法时更有效。",
    "fetch_date": "2026-01-07",
    "id": "20260107_fdc6cf29"
  },
  {
    "title": "Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects",
    "url": "https://arxiv.org/pdf/2601.01783v1",
    "source": "ArXiv",
    "date": "2026-01-05",
    "abstract": "The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the \"too-similar-to-fail\" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过时变参数向量自回归（TVP-VAR）模型和30天滚动窗口，分析了2023年美国银行业危机期间的风险传染机制。研究发现，危机传播主要通过高频信息传染渠道而非直接金融关联，支持“太相似而不能倒”假说——在利率压力下，业务模式相似的银行间风险溢出显著。研究识别出SVB、FRC、WAL为主要风险净输出方，其相似同行成为净接收方，这种动态系统性脆弱指标无法通过逐资产分析捕获。市场情绪和政策冲击进一步放大了风险溢出效应。",
    "fetch_date": "2026-01-07",
    "id": "20260107_81f579ca"
  },
  {
    "title": "Almost-Exact Simulation Scheme for Heston-type Models: Bermudan and American Option Pricing",
    "url": "https://arxiv.org/pdf/2601.00815v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Recently, an Almost-Exact Simulation (AES) scheme was introduced for the Heston stochastic volatility model and tested for European option pricing. This paper extends this scheme for pricing Bermudan and American options under both Heston and double Heston models. The AES improves Monte Carlo simulation efficiency by using the non-central chi-square distribution for the variance process. We derive the AES scheme for the double Heston model and compare the performance of the AES schemes under both models with the Euler scheme. Our numerical experiments validate the effectiveness of the AES scheme in providing accurate option prices with reduced computational time, highlighting its robustness for both models. In particular, the AES achieves higher accuracy and computational efficiency when the number of simulation steps matches the exercise dates for Bermudan options.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文将Heston模型的几乎精确模拟（AES）方案扩展到双Heston模型，用于百慕大和美式期权定价。AES利用方差过程的非中心卡方分布提高蒙特卡洛模拟效率，相比欧拉方案在计算时间和精度上表现更优，尤其当模拟步数与行权日期匹配时。",
    "fetch_date": "2026-01-07",
    "id": "20260107_cbbee458"
  },
  {
    "title": "Object-Centric World Models for Causality-Aware Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.14262v2",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \\emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "世界模型旨在支持样本高效的深度强化学习智能体，但现有模型在准确模拟高维、非平稳且包含多个具有丰富交互的物体的环境方面仍面临挑战，因为它们通常学习所有环境组件的整体表示。相比之下，人类通过将环境分解为离散物体来感知环境，从而促进高效决策。受此启发，我们提出了STICA（Slot Transformer Imagination with CAusality-aware reinforcement learning），这是一个统一框架，其中以物体为中心的Transformer作为世界模型以及因果感知的策略和价值网络。STICA将每个观察表示为一组以物体为中心的token，以及表示智能体动作和所得奖励的token，使世界模型能够预测token级别的动态和交互。策略和价值网络随后估计token级别的因果关系，并在注意力层中使用它们，从而实现因果引导的决策。在物体丰富的基准测试中，STICA始终表现出色。",
    "fetch_date": "2026-01-07",
    "id": "20260107_38bd7ac8"
  },
  {
    "title": "Wasserstein Distributionally Robust Rare-Event Simulation",
    "url": "https://arxiv.org/pdf/2601.01642v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "标准罕见事件模拟技术需要精确的分布设定，这在存在分布不确定性时限制了其有效性。为此，我们开发了一个新颖的框架，用于估计受此类分布模型风险影响的罕见事件概率。具体而言，我们专注于计算最坏情况下的罕见事件概率，该概率被定义为针对以特定名义分布为中心的Wasserstein模糊集的一个分布鲁棒边界。通过利用该边界的对偶特性，我们提出了分布鲁棒重要性采样（DRIS），这是一种计算上易于处理的方法，旨在显著降低与估计对偶分量相关的方差。所提出的方法易于实现且采样成本低。最重要的是，它实现了消失的相对误差，这是罕见事件模拟中极难建立的最强效率保证。我们的数值研究证实了DRIS相对于现有基准的优越性能。",
    "fetch_date": "2026-01-07",
    "id": "20260107_6c6507be"
  },
  {
    "title": "Chaos and Synchronization in Financial Leverages Dynamics: Modeling Systemic Risk with Coupled Unimodal Maps",
    "url": "https://arxiv.org/pdf/2601.01505v1",
    "source": "ArXiv",
    "date": "2026-01-04",
    "abstract": "Systemic financial risk refers to the simultaneous failure or destabilization of multiple financial institutions, often triggered by contagion mechanisms or common exposures to shocks. In this paper, we present a dynamical model of bank leverage (the ratio of asset holdings to equity) a quantity that both reflects and drives risk dynamics. We model how banks, constrained by Value-at-Risk (VaR) regulations, adjust their leverage in response to changes in the price of a single asset, assumed to be held in fixed proportion across banks. This leverage-targeting behavior introduces a procyclical feedback loop between asset prices and leverage. In the dynamics, this can manifest as logistic-like behavior with a rich bifurcation structure across model parameters. By analyzing these coupled dynamics in both isolated and interconnected bank models, we outline a framework for understanding how systemic risk can emerge from seemingly rational micro-level behavior.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "系统性金融风险指多个金融机构同时失败或失稳，通常由传染机制或对冲击的共同敞口引发。本文提出一个银行杠杆（资产持有与权益之比）的动态模型，该指标既反映也驱动风险动态。我们模拟了受风险价值（VaR）监管约束的银行，如何根据单一资产价格变化调整其杠杆（假设各银行按固定比例持有该资产）。这种杠杆目标行为在资产价格与杠杆之间引入了顺周期反馈循环。在动态中，这可能表现为类似逻辑斯蒂的行为，具有丰富的分岔结构。通过分析孤立和互连银行模型中的这些耦合动态，我们概述了一个理解系统性风险如何从看似理性的微观行为中产生的框架。",
    "fetch_date": "2026-01-07",
    "id": "20260107_c9ba9ad2"
  },
  {
    "title": "Critical volatility threshold for log-normal to power-law transition",
    "url": "https://arxiv.org/pdf/2601.01269v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "Random walk models with log-normal outcomes fit local market observations remarkably well. Yet interconnected or recursive structures - layered derivatives, leveraged positions, iterative funding rounds - periodically produce power-law distributed events. We show that the transition from log-normal to power-law dynamics requires only three conditions: randomness in the underlying process, rectification of payouts, and iterative feed-forward of expected values. Using an infinite option-on-option chain as an illustrative model, we derive a critical volatility threshold at $σ^* = \\sqrt{2π} \\approx 250.66\\%$ for the unconditional case. With selective survival - where participants require minimum returns to continue - the critical threshold drops discontinuously to $σ_{\\text{th}}^{*} = \\sqrt{π/2} \\approx 125.3\\%$, and can decrease further with higher survival thresholds. The resulting outcomes follow what we term the Critical Volatility ($V^*$) Distribution - a power-law whose exponent admits closed-form expression in terms of survival pressure and conditional expected growth. The result suggests that fat tails may be an emergent property of iterative log-normal processes with selection rather than an exogenous feature.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了金融市场中从对数正态分布向幂律分布的动态转变。研究发现，这种转变仅需三个条件：基础过程的随机性、收益的整流以及期望值的迭代前馈。通过一个无限期权链模型，论文推导出无条件情况下的临界波动率阈值σ* ≈ 250.66%，而在存在选择性生存（参与者需要最低回报以继续参与）的情况下，该阈值会不连续地降至σ_th* ≈ 125.3%，并可能随着生存阈值的提高而进一步降低。由此产生的结果遵循所谓的“临界波动率分布”——一种幂律分布，其指数可通过生存压力和条件期望增长以闭式表达。研究结果表明，厚尾现象可能是具有选择机制的迭代对数正态过程的一种涌现特性。",
    "fetch_date": "2026-01-07",
    "id": "20260107_ec011e94"
  },
  {
    "title": "European Options in Market Models with Multiple Defaults: the BSDE approach",
    "url": "https://arxiv.org/pdf/2601.01250v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "We study non-linear Backward Stochastic Differential Equations (BSDEs) driven by a Brownian motion and p default martingales. The driver of the BSDE with multiple default jumps can take a generalized form involving an optional finite variation process. We first show existence and uniqueness. We then establish comparison and strict comparison results for these BSDEs, under a suitable assumption on the driver. In the case of a linear driver, we derive an explicit formula for the first component of the BSDE using an adjoint exponential semimartingale. The representation depends on whether the finite variation process is predictable or only optional. We apply our results to the problem of pricing and hedging a European option in a linear complete market with two defaultable assets and in a non-linear complete market with p defaultable assets. Two examples of the latter market model are provided: an example where the seller of the option is a large investor influencing the probability of default of a single asset and an example where the large seller's strategy affects the default probabilities of all p assets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究由布朗运动和p个违约鞅驱动的非线性倒向随机微分方程（BSDE）。在驱动项包含可选有限变差过程的广义形式下，证明了存在唯一性、比较定理和严格比较定理。针对线性驱动情形，利用伴随指数半鞅给出了BSDE第一分量的显式公式，该表示取决于有限变差过程是可预测还是仅可选。应用结果研究了含两个违约资产的线性完全市场及含p个违约资产的非线性完全市场中欧式期权的定价与对冲问题，并提供了两个非线性市场实例：一是期权卖方作为大投资者影响单一资产违约概率；二是大卖方策略影响所有p个资产的违约概率。",
    "fetch_date": "2026-01-07",
    "id": "20260107_5b2b83ef"
  },
  {
    "title": "Order-Constrained Spectral Causality in Multivariate Time Series",
    "url": "https://arxiv.org/pdf/2601.01216v1",
    "source": "ArXiv",
    "date": "2026-01-03",
    "abstract": "We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种基于算子理论的多元时间序列因果分析框架，通过顺序约束谱非不变性定义方向性影响。该方法在线性高斯假设下等同于线性格兰杰因果性，但能捕捉超越该范围的集体和非线性方向依赖关系。论文建立了统计推断的理论基础，包括存在性、一致性和有效性，并通过模拟验证了其在非线性依赖检测上的优势。",
    "fetch_date": "2026-01-07",
    "id": "20260107_594d59c5"
  },
  {
    "title": "Uncertainty-Adjusted Sorting for Asset Pricing with Machine Learning",
    "url": "https://arxiv.org/pdf/2601.00593v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Machine learning is central to empirical asset pricing, but portfolio construction still relies on point predictions and largely ignores asset-specific estimation uncertainty. We propose a simple change: sort assets using uncertainty-adjusted prediction bounds instead of point predictions alone. Across a broad set of ML models and a U.S. equity panel, this approach improves portfolio performance relative to point-prediction sorting. These gains persist even when bounds are built from partial or misspecified uncertainty information. They arise mainly from reduced volatility and are strongest for flexible machine learning models. Identification and robustness exercises show that these improvements are driven by asset-level rather than time or aggregate predictive uncertainty.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "机器学习在实证资产定价中至关重要，但投资组合构建仍依赖点预测，并基本忽略了资产特定的估计不确定性。我们提出一个简单的改变：使用不确定性调整的预测边界而非仅点预测来对资产进行排序。在广泛的机器学习模型和美国股票面板数据中，该方法相对于点预测排序改善了投资组合表现。即使边界基于部分或错误指定的不确定性信息构建，这些收益依然存在。它们主要源于波动性的降低，并且对于灵活的机器学习模型效果最强。识别和稳健性检验表明，这些改进是由资产层面而非时间或总体预测不确定性驱动的。",
    "fetch_date": "2026-01-06",
    "id": "20260106_6f536e4c"
  },
  {
    "title": "Ultimate Forward Rate Prediction and its Application to Bond Yield Forecasting: A Machine Learning Perspective",
    "url": "https://arxiv.org/pdf/2601.00011v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "This study focuses on forecasting the ultimate forward rate (UFR) and developing a UFRbased bond yield prediction model using data from Chinese treasury bonds and macroeconomic variables spanning from December 2009 to December 2024. The de Kort-Vellekooptype methodology is applied to estimate the UFR, incorporating the optimal turning parameter determination technique proposed in this study, which helps mitigate anomalous fluctuations. In addition, both linear and nonlinear machine learning techniques are employed to forecast the UFR and ultra-long-term bond yields. The results indicate that nonlinear machine learning models outperform their linear counterparts in forecasting accuracy. Incorporating macroeconomic variables, particularly price index-related variables, significantly improves the accuracy of predictions. Finally, a novel UFR-based bond yield forecasting model is developed, demonstrating superior performance across different bond maturities.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究聚焦于预测终极远期利率（UFR）并开发基于UFR的债券收益率预测模型，使用中国国债和宏观经济变量数据（2009年12月至2024年12月）。应用de Kort-Vellekoop类型方法估计UFR，结合本研究提出的最优转折参数确定技术，有助于减轻异常波动。此外，采用线性和非线性机器学习技术预测UFR和超长期债券收益率。结果表明，非线性机器学习模型在预测准确性上优于线性模型。纳入宏观经济变量，特别是价格指数相关变量，显著提高了预测准确性。最后，开发了一种新颖的基于UFR的债券收益率预测模型，在不同债券期限上表现出优越性能。",
    "fetch_date": "2026-01-06",
    "id": "20260106_6f58b720"
  },
  {
    "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2601.00770v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "投资组合优化是各大金融机构的核心任务，其中基数约束均值-方差投资组合优化（CCPO）作为混合整数二次规划问题，传统精确求解器难以处理，通常依赖启发式算法获取近似解。本文提出一种新颖的智能体框架，针对CCPO问题探索多种具体架构，旨在自动化复杂工作流程并优化启发式算法开发，通过整合多算法解决方案提升有效前沿表现。研究表明智能体框架在组合优化领域展现出自动化工作流与算法开发方面的潜力，部分性能甚至超越人工水平。",
    "fetch_date": "2026-01-06",
    "id": "20260106_3f86608c"
  },
  {
    "title": "Second Thoughts: How 1-second subslots transform CEX-DEX Arbitrage on Ethereum",
    "url": "https://arxiv.org/pdf/2601.00738v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agent's DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereum's default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了以太坊区块时间缩短对去中心化交易所活动的影响，重点关注中心化交易所与去中心化交易所之间的套利行为。作者构建了一个交易模型，其中代理在DEX上的交易执行存在不确定性，代理在决定是否追求套利机会时会明确考虑这种执行风险。通过对比以太坊默认12秒区块时间环境与提供1秒子区块执行的更快机制下的代理行为，基于2025年7月至9月币安和Uniswap v3数据的模拟显示：更快的区块时间使套利交易数量平均增加535%，交易量平均增加203%。1秒子区块机制下CEX-DEX套利活动的增加，源于成功和失败交易结果方差的降低，这提高了风险调整后收益，使CEX-DEX套利更具吸引力。",
    "fetch_date": "2026-01-06",
    "id": "20260106_9f1da085"
  },
  {
    "title": "HODL Strategy or Fantasy? 480 Million Crypto Market Simulations and the Macro-Sentiment Effect",
    "url": "https://arxiv.org/pdf/2512.02029v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Crypto enthusiasts claim that buying and holding crypto assets yields high returns, often citing Bitcoin's past performance to promote other tokens and fuel fear of missing out. However, understanding the real risk-return trade-off and what factors affect future crypto returns is crucial as crypto becomes increasingly accessible to retail investors through major brokerages. We examine the HODL strategy through two independent analyses. First, we implement 480 million Monte Carlo simulations across 378 non-stablecoin crypto assets, net of trading fees and the opportunity cost of 1-month Treasury bills, and find strong evidence of survivorship bias and extreme downside concentration. At the 2-3 year horizon, the median excess return is -28.4 percent, the 1 percent conditional value at risk indicates that tail scenarios wipe out principal after all costs, and only the top quartile achieves very large gains, with a mean excess return of 1,326.7 percent. These results challenge the HODL narrative: across a broad set of assets, simple buy-and-hold loads extreme downside risk onto most investors, and the miracles mostly belong to the luckiest quarter. Second, using a Bayesian multi-horizon local projection framework, we find that endogenous predictors based on realized risk-return metrics have economically negligible and unstable effects, while macro-finance factors, especially the 24-week exponential moving average of the Fear and Greed Index, display persistent long-horizon impacts and high cross-basket stability. Where significant, a one-standard-deviation sentiment shock reduces forward top-quartile mean excess returns by 15-22 percentage points and median returns by 6-10 percentage points over 1-3 year horizons, suggesting that macro-sentiment conditions, rather than realized return histories, are the dominant indicators for future outcomes.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过两项独立分析评估HODL策略的实际价值。首先，对378种非稳定币加密资产进行了4.8亿次蒙特卡洛模拟（扣除交易费用和1个月国债机会成本），发现存在显著的生存偏差和极端下行风险集中：在2-3年持有期内，中位数超额收益为-28.4%，1%条件风险价值显示尾部情景会完全侵蚀本金，仅前四分之一资产获得平均1326.7%的超额收益。其次，采用贝叶斯多时间框架分析宏观情绪效应。研究结论挑战了HODL叙事，表明简单买入持有策略对多数投资者带来极端下行风险，超额收益主要集中于最幸运的少数资产。",
    "fetch_date": "2026-01-06",
    "id": "20260106_4a5f9385"
  },
  {
    "title": "Capital allocation and tail central moments for the multivariate normal mean-variance mixture distribution",
    "url": "https://arxiv.org/pdf/2601.00568v1",
    "source": "ArXiv",
    "date": "2026-01-02",
    "abstract": "Capital allocation is a procedure used to assess the risk contributions of individual risk components to the total risk of a portfolio. While the conditional tail expectation (CTE)-based capital allocation is arguably the most popular capital allocation method, its inability to reflect important tail behaviour of losses necessitates a more accurate approach. In this paper, we introduce a new capital allocation method based on the tail central moments (TCM), generalising the tail covariance allocation informed by the tail variance. We develop analytical expressions of the TCM as well as the TCM-based capital allocation for the class of normal mean-variance mixture distributions, which is widely used to model asymmetric and heavy-tailed data in finance and insurance. As demonstrated by a numerical analysis, the TCM-based capital allocation captures several significant patterns in the tail region of equity losses that remain undetected by the CTE, enhancing the understanding of the tail risk contributions of risk components.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "资本配置是评估投资组合中单个风险成分对总风险贡献的程序。虽然基于条件尾部期望（CTE）的资本配置方法最为流行，但其无法反映损失的重要尾部行为，因此需要更精确的方法。本文引入了一种基于尾部中心矩（TCM）的新资本配置方法，推广了由尾部方差驱动的尾部协方差配置。我们为广泛应用于金融和保险中建模非对称和厚尾数据的正态均值-方差混合分布类，开发了TCM以及基于TCM的资本配置的解析表达式。数值分析表明，基于TCM的资本配置捕捉了股权损失尾部区域的几个重要模式，这些模式未被CTE检测到，从而增强了对风险成分尾部风险贡献的理解。",
    "fetch_date": "2026-01-06",
    "id": "20260106_85310112"
  },
  {
    "title": "Causal Inference in Financial Event Studies",
    "url": "https://arxiv.org/pdf/2511.15123v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Financial event studies, ubiquitous in finance research, typically use linear factor models with known factors to estimate abnormal returns and identify causal effects of information events. This paper demonstrates that when factor models are misspecified -- an almost certain reality -- traditional event study estimators produce inconsistent estimates of treatment effects. The bias is particularly severe during volatile periods, over long horizons, and when event timing correlates with market conditions. We derive precise conditions for identification and expressions for asymptotic bias. As an alternative, we propose synthetic control methods that construct replicating portfolios from control securities without imposing specific factor structures. Revisiting four empirical applications, we show that some established findings may reflect model misspecification rather than true treatment effects. While traditional methods remain reliable for short-horizon studies with random event timing, our results suggest caution when interpreting long-horizon or volatile-period event studies and highlight the importance of quasi-experimental designs when available.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "金融事件研究通常使用已知因子的线性因子模型来估计异常回报并识别信息事件的因果效应。本文证明，当因子模型设定错误时（几乎是必然的现实），传统的事件研究估计量会产生不一致的处理效应估计。这种偏差在波动期、长期窗口以及事件时机与市场状况相关时尤为严重。我们推导了识别的精确条件和渐近偏差的表达式。作为替代方案，我们提出了合成控制方法，该方法从控制证券构建复制投资组合，而不强加特定的因子结构。通过重新审视四个实证应用，我们表明一些已确立的发现可能反映的是模型设定错误而非真实的处理效应。虽然传统方法对于事件时机随机的短期研究仍然可靠，但我们的结果表明在解释长期或波动期事件研究时应谨慎，并强调了准实验设计的重要性。",
    "fetch_date": "2026-01-06",
    "id": "20260106_be952b51"
  },
  {
    "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2511.15002v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "下一代网络采用开放式无线接入网络（O-RAN）架构，通过无线接入网络智能控制器（RIC）实现动态资源管理。尽管深度强化学习（DRL）模型在优化网络资源方面展现出潜力，但在动态环境中常面临鲁棒性和泛化性不足的问题。本文提出一种新颖的资源管理方法，在分布式多智能体强化学习（MARL）框架中，将锐度感知最小化（SAM）技术融入软演员-评论家（SAC）算法。该方法引入自适应选择性SAM机制，其正则化由时序差分（TD）误差方差驱动，确保仅对环境复杂度高的智能体进行正则化。这种针对性策略减少了不必要的开销，提高了训练稳定性，并在不牺牲学习效率的前提下增强了泛化能力。此外，还结合了动态ρ调度方案，以优化各智能体间的探索-利用权衡。实验结果表明，该方法显著优于传统DRL方法，在资源分配方面实现了高达22%的性能提升。",
    "fetch_date": "2026-01-06",
    "id": "20260106_708e569e"
  },
  {
    "title": "Multimodal Insights into Credit Risk Modelling: Integrating Climate and Text Data for Default Prediction",
    "url": "https://arxiv.org/pdf/2601.00478v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Credit risk assessment increasingly relies on diverse sources of information beyond traditional structured financial data, particularly for micro and small enterprises (mSEs) with limited financial histories. This study proposes a multimodal framework that integrates structured credit variables, climate panel data, and unstructured textual narratives within a unified learning architecture. Specifically, we use long short-term memory (LSTM), the gated recurrent unit (GRU), and transformer models to analyse the interplay between these data modalities. The empirical results demonstrate that unimodal models based on climate or text data outperform those relying solely on structured data, while the integration of multiple data modalities yields significant improvements in credit default prediction. Using SHAP-based explainability methods, we find that physical climate risks play an important role in default prediction, with water-logging by rain emerging as the most influential factor. Overall, this study demonstrates the potential of multimodal approaches in AI-enabled decision-making, which provides robust tools for credit risk assessment while contributing to the broader integration of environmental and textual insights into predictive analytics.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究提出了一种多模态信用风险建模框架，整合结构化信用变量、气候面板数据和非结构化文本叙述，采用LSTM、GRU和Transformer模型分析数据模态间的交互作用。实证结果表明，基于气候或文本数据的单模态模型优于仅依赖结构化数据的模型，而多模态数据整合能显著提升信用违约预测性能。通过SHAP可解释性方法发现，物理气候风险（特别是雨水内涝）在违约预测中起重要作用。该研究展示了多模态方法在AI决策中的潜力，为信用风险评估提供了更稳健的工具。",
    "fetch_date": "2026-01-06",
    "id": "20260106_25e03aa2"
  },
  {
    "title": "Core-Periphery Dynamics in Market-Conditioned Financial Networks: A Conditional P-Threshold Mutual Information Approach",
    "url": "https://arxiv.org/pdf/2601.00395v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "This study investigates how financial market structure reorganizes during the COVID-19 crash using a conditional p-threshold mutual information (MI) based Minimum Spanning Tree (MST) framework. We analyze nonlinear dependencies among the largest stocks from four diverse QUAD countries: the US, Japan, Australia, and India. Crashes are identified using the Hellinger distance and Hilbert spectrum; a crash occurs when HD = mu\\_H + 2*sigma\\_H, segmenting data into pre-crash, crash, and post-crash periods. Conditional p-threshold MI filters out common market effects and applies permutation-based significance testing. Resulting validated dependencies are used to construct MST networks for comparison across periods. Networks become more integrated during the crash, with shorter path lengths, higher centrality, and lower algebraic connectivity, indicating fragility. Core-periphery structure declines, with increased periphery vulnerability, and disassortative mixing facilitates shock transmission. Post-crash networks show only partial recovery. Aftershock analysis using the Gutenberg-Richter law indicates higher relative frequency of large volatility events following the crash. Results are consistent across all markets, highlighting the conditional p-threshold MI framework for capturing nonlinear interdependencies and systemic vulnerability.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究采用基于条件p阈值互信息的最小生成树框架，分析COVID-19崩盘期间金融市场的结构重组。通过Hellinger距离和Hilbert谱识别崩盘时段，将数据分为崩盘前、崩盘中、崩盘后三期。条件p阈值互信息方法过滤了共同市场效应，并应用基于排列的显著性检验。构建的MST网络显示：崩盘期间网络整合度提高（路径更短、中心性更高、代数连通性更低），核心-边缘结构减弱，边缘脆弱性增加，非同类混合促进了冲击传导。崩盘后网络仅部分恢复。基于古登堡-里克特定律的余震分析表明，崩盘后大型波动事件的相对频率更高。",
    "fetch_date": "2026-01-06",
    "id": "20260106_cba3ac3f"
  },
  {
    "title": "Option Pricing beyond Black-Scholes Model:Quantum Mechanics Approach",
    "url": "https://arxiv.org/pdf/2601.00293v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "Based on the analog between the stochastic dynamics and quantum harmonic oscillator, we propose a market force driving model to generalize the Black-Scholes model in finance market. We give new schemes of option pricing, in which we can take various unexpected market behaviors into account to modify the option pricing. As examples, we present several market forces to analyze their effects on the option pricing. These results provide us two practical applications. One is to be used as a new scheme of option pricing when we can predict some hidden market forces or behaviors emerging. The other implies the existence of some risk premium when some unexpected forces emerge.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "基于随机动力学与量子谐振子的类比，提出市场驱动力模型以推广金融市场的Black-Scholes模型。给出新的期权定价方案，可纳入各种意外市场行为修正定价。通过示例分析多种市场力对期权定价的影响。结果提供两个实际应用：一是当预测到某些隐藏市场力或行为出现时，可作为新的期权定价方案；二是暗示意外市场力出现时存在风险溢价。",
    "fetch_date": "2026-01-06",
    "id": "20260106_f02bcbfc"
  },
  {
    "title": "A Global Optimal Theory of Portfolio beyond R-$σ$ Model",
    "url": "https://arxiv.org/pdf/2601.00281v1",
    "source": "ArXiv",
    "date": "2026-01-01",
    "abstract": "The deviation of the efficient market hypothesis (EMH) for the practical economic system allows us gain the arbitrary or risk premium in finance markets. We propose the triplet $(R,H,σ)$ theory to give the local and global optimal portfolio, which eneralize from the $(R,σ)$ model. We present the formulation of the triplet $(R,H,σ)$ model and give the Pareto optimal solution as well as comparing it with the numerical investigations for the Chinese stock market. We define the local optimal weights of the triplet $(\\mathbf{w}_{R},\\mathbf{w}_{H},\\mathbf{w}_σ)$, which constructs the triangle of the quasi-optimal investing subspace such that we further define the centroid of the triangle or the incenter of the triangle as the optimal investing weights, which optimizes the mean return, the arbitrary or risk premium and the volatility risk. By investigating numerically the Chinese stock market as an example we demonstrate the validity of the formulation and obtain the global optimal strategy and quasi-optimal investing subspace. The theory provides an efficient way to design the portfolio for different style investors, conservative or aggressive investors, in finance market to maximize the mean return and arbitrary or risk premium with a small volatility risk.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种超越传统R-σ模型的全局最优投资组合理论，引入了(R,H,σ)三元组模型，其中H代表套利或风险溢价。通过定义局部最优权重(w_R,w_H,w_σ)构建准最优投资子空间三角形，并以三角形重心或内心作为全局最优投资权重，同时优化均值收益、套利/风险溢价和波动风险。论文以中国股市为例进行了数值验证，展示了该理论为保守型或激进型投资者设计投资组合的有效方法。",
    "fetch_date": "2026-01-06",
    "id": "20260106_0fe72c01"
  },
  {
    "title": "Full grid solution for multi-asset options pricing with tensor networks",
    "url": "https://arxiv.org/pdf/2601.00009v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出使用量化张量列车（QTT）方法解决多资产期权定价中的维度灾难问题。传统基于Black-Scholes PDE的全网格求解器受限于维度指数增长，通常只能处理3个标的资产，实践中多依赖蒙特卡洛方法。研究表明，QTT可将d资产Black-Scholes PDE转化为个人计算机可处理的高维问题，构建的算子、收益函数和边界条件表示其秩随d多项式增长、随网格大小多对数增长。论文开发了两种求解器：欧式和美式期权的时间步进算法，以及欧式期权的时空算法。在3-5维相关篮子期权和最大-最小期权上实现了高精度全网格价格和希腊值计算，该方法可扩展至10-15个标的资产的全网格求解。",
    "fetch_date": "2026-01-06",
    "id": "20260106_dd97f056"
  },
  {
    "title": "Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework",
    "url": "https://arxiv.org/pdf/2511.14980v1",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种用于期权定价模型校准的选择性遗忘（机器去学习）框架。当市场报价过时、损坏或需要删除时，现有校准流程通常需要完全重新训练模型。本文引入了一种基于算子理论的高斯-牛顿方法，允许仅移除部分数据而不必重建整个非线性最小二乘问题，并提供了稳定性保证和扰动边界。",
    "fetch_date": "2026-01-06",
    "id": "20260106_05ad2997"
  },
  {
    "title": "The Hidden Constant of Market Rhythms: How $1-1/e$ Defines Scaling in Intrinsic Time",
    "url": "https://arxiv.org/pdf/2511.14408v1",
    "source": "ArXiv",
    "date": "2025-11-18",
    "abstract": "Directional-change Intrinsic Time analysis has long revealed scaling laws in market microstructure, but the origin of their stability remains elusive. This article presents evidence that Intrinsic Time can be modeled as a memoryless exponential hazard process. Empirically, the proportion of directional changes to total events stabilizes near $1 - 1/e = 0.632$, matching the probability that a Poisson process completes one mean interval. This constant provides a natural heuristic to identify scaling regimes across thresholds and supports an interpretation of market activity as a renewal process in intrinsic time.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文通过实证研究发现，市场内在时间中的方向性变化与总事件的比例稳定在$1-1/e≈0.632$附近，这与泊松过程完成一个平均区间的概率相匹配。该常数揭示了市场微观结构中的标度律稳定性，支持将市场活动解释为内在时间中的更新过程，为识别不同阈值下的标度机制提供了自然启发式方法。",
    "fetch_date": "2026-01-06",
    "id": "20260106_ccb77cc5"
  },
  {
    "title": "Scaling Conditional Autoencoders for Portfolio Optimization via Uncertainty-Aware Factor Selection",
    "url": "https://arxiv.org/pdf/2511.17462v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "Conditional Autoencoders (CAEs) offer a flexible, interpretable approach for estimating latent asset-pricing factors from firm characteristics. However, existing studies usually limit the latent factor dimension to around K=5 due to concerns that larger K can degrade performance. To overcome this challenge, we propose a scalable framework that couples a high-dimensional CAE with an uncertainty-aware factor selection procedure. We employ three models for quantile prediction: zero-shot Chronos, a pretrained time-series foundation model (ZS-Chronos), gradient-boosted quantile regression trees using XGBoost and RAPIDS (Q-Boost), and an I.I.D bootstrap-based sample mean model (IID-BS). For each model, we rank factors by forecast uncertainty and retain the top-k most predictable factors for portfolio construction, where k denotes the selected subset of factors. This pruning strategy delivers substantial gains in risk-adjusted performance across all forecasting models. Furthermore, due to each model's uncorrelated predictions, a performance-weighted ensemble consistently outperforms individual models with higher Sharpe, Sortino, and Omega ratios.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种可扩展的框架，将高维条件自编码器（CAE）与不确定性感知因子选择程序相结合，用于投资组合优化。通过使用三种分位数预测模型（零样本Chronos、基于XGBoost和RAPIDS的梯度提升分位数回归树、基于I.I.D自举的样本均值模型），根据预测不确定性对因子进行排序，并保留前k个最可预测的因子进行投资组合构建。这种剪枝策略在所有预测模型中均显著提升了风险调整后的绩效。此外，由于各模型的预测不相关，性能加权集成模型在夏普比率、索提诺比率和欧米茄比率方面持续优于单个模型。",
    "fetch_date": "2026-01-05",
    "id": "20260105_13b5c54e"
  },
  {
    "title": "Statistical Arbitrage in Polish Equities Market Using Deep Learning Techniques",
    "url": "https://arxiv.org/pdf/2512.02037v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "We study a systematic approach to a popular Statistical Arbitrage technique: Pairs Trading. Instead of relying on two highly correlated assets, we replace the second asset with a replication of the first using risk factor representations. These factors are obtained through Principal Components Analysis (PCA), exchange traded funds (ETFs), and, as our main contribution, Long Short Term Memory networks (LSTMs). Residuals between the main asset and its replication are examined for mean reversion properties, and trading signals are generated for sufficiently fast mean reverting portfolios.\n  Beyond introducing a deep learning based replication method, we adapt the framework of Avellaneda and Lee (2008) to the Polish market. Accordingly, components of WIG20, mWIG40, and selected sector indices replace the original S&P500 universe, and market parameters such as the risk free rate and transaction costs are updated to reflect local conditions.\n  We outline the full strategy pipeline: risk factor construction, residual modeling via the Ornstein Uhlenbeck process, and signal generation. Each replication technique is described together with its practical implementation. Strategy performance is evaluated over two periods: 2017-2019 and the recessive year 2020.\n  All methods yield profits in 2017-2019, with PCA achieving roughly 20 percent cumulative return and an annualized Sharpe ratio of up to 2.63. Despite multiple adaptations, our conclusions remain consistent with those of the original paper. During the COVID-19 recession, only the ETF based approach remains profitable (about 5 percent annual return), while PCA and LSTM methods underperform. LSTM results, although negative, are promising and indicate potential for future optimization.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于深度学习的统计套利策略，应用于波兰股市。核心创新在于：使用长短期记忆网络（LSTM）等风险因子（包括PCA和ETF）复制目标资产，替代传统配对交易中的高相关性资产；通过Ornstein-Uhlenbeck过程建模残差的均值回归特性，生成交易信号。研究将Avellaneda和Lee（2008）的框架适配至波兰市场（覆盖WIG20、mWIG40及行业指数），并更新了无风险利率和交易成本等本地参数。论文详细描述了从风险因子构建到信号生成的完整策略流程，强调了LSTM在资产复制中的实践应用，对实战交易具有较高的参考价值。",
    "fetch_date": "2026-01-05",
    "id": "20260105_cb8264be"
  },
  {
    "title": "Integration of LSTM Networks in Random Forest Algorithms for Stock Market Trading Predictions",
    "url": "https://arxiv.org/pdf/2512.02036v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "The aim of this paper is the analysis and selection of stock trading systems that combine different models with data of different nature, such as financial and microeconomic information. Specifically, based on previous work by the authors and applying advanced techniques of Machine Learning and Deep Learning, our objective is to formulate trading algorithms for the stock market with empirically tested statistical advantages, thus improving results published in the literature. Our approach integrates Long Short-Term Memory (LSTM) networks with algorithms based on decision trees, such as Random Forest and Gradient Boosting. While the former analyze price patterns of financial assets, the latter are fed with economic data of companies. Numerical simulations of algorithmic trading with data from international companies and 10-weekday predictions confirm that an approach based on both fundamental and technical variables can outperform the usual approaches, which do not combine those two types of variables. In doing so, Random Forest turned out to be the best performer among the decision trees. We also discuss how the prediction performance of such a hybrid approach can be boosted by selecting the technical variables.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文旨在通过整合不同性质的数据（如金融和微观经济信息）来分析和选择股票交易系统。具体方法是将长短期记忆（LSTM）网络与基于决策树的算法（如随机森林和梯度提升）相结合。LSTM用于分析金融资产的价格模式，而决策树算法则输入公司的经济数据。通过使用国际公司数据进行数值模拟和10个交易日的预测，结果表明，结合基本面和技术面变量的方法优于不结合这两类变量的常规方法，其中随机森林在决策树中表现最佳。论文还讨论了如何通过选择增强这种混合方法的预测性能。",
    "fetch_date": "2026-01-05",
    "id": "20260105_e66f400c"
  },
  {
    "title": "Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution",
    "url": "https://arxiv.org/pdf/2511.15262v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究探讨了强化学习在最优执行大宗订单中的应用，旨在逐步执行大额订单的同时，在较长时间内最小化执行差额和市场冲击。与传统参数化价格动态和冲击建模方法不同，本研究采用无模型、数据驱动的框架。由于策略优化需要历史数据无法提供的反事实反馈，我们采用队列反应模型来生成真实且可处理的限价订单簿模拟，涵盖瞬时价格冲击以及非线性、动态的订单流响应。在方法上，我们在包含时间、库存、价格和深度变量的状态空间上训练了一个双深度Q网络智能体，并评估其相对于现有基准的性能。数值模拟结果表明，该智能体学习到的策略兼具战略性和战术性，能有效适应订单簿条件，并在多种训练配置下优于标准方法。这些发现有力地证明，无模型强化学习能够为最优执行问题提供自适应且稳健的解决方案。",
    "fetch_date": "2026-01-05",
    "id": "20260105_8157885d"
  },
  {
    "title": "Financial Information Theory",
    "url": "https://arxiv.org/pdf/2511.16339v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This paper introduces a comprehensive framework for Financial Information Theory by applying information-theoretic concepts such as entropy, Kullback-Leibler divergence, mutual information, normalized mutual information, and transfer entropy to financial time series. We systematically derive these measures with complete mathematical proofs, establish their theoretical properties, and propose practical algorithms for estimation. Using S&P 500 data from 2000 to 2025, we demonstrate empirical usefulness for regime detection, market efficiency testing, and portfolio construction. We show that normalized mutual information (NMI) behaves as a powerful, bounded, and interpretable measure of temporal dependence, highlighting periods of structural change such as the 2008 financial crisis and the COVID-19 shock. Our entropy-adjusted Value at Risk, information-theoretic diversification criterion, and NMI-based market efficiency test provide actionable tools for risk management and asset allocation. We interpret NMI as a quantitative diagnostic of the Efficient Market Hypothesis and demonstrate that information-theoretic methods offer superior regime detection compared to traditional autocorrelation- or volatility-based approaches. All theoretical results include rigorous proofs, and empirical findings are validated across multiple market regimes spanning 25 years of daily returns.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出金融信息理论的综合框架，将信息论概念（如熵、Kullback-Leibler散度、互信息、归一化互信息、传递熵）应用于金融时间序列。通过系统推导这些度量（含完整数学证明），建立理论性质并提出实用估计算法。基于2000-2025年标普500数据的实证分析显示，该方法在机制检测、市场效率检验和投资组合构建中具有实用价值：归一化互信息（NMI）作为有界可解释的时序依赖性度量，能有效识别结构变化期（如2008年金融危机和COVID-19冲击）；熵调整VaR、信息论分散化准则及NMI市场效率检验为风险管理和资产配置提供可操作工具。研究将NMI解释为有效市场假说的定量诊断指标，并证明信息论方法在机制检测上优于传统自相关方法。",
    "fetch_date": "2026-01-05",
    "id": "20260105_36c1b600"
  },
  {
    "title": "Corporate Earnings Calls and Analyst Beliefs",
    "url": "https://arxiv.org/pdf/2511.15214v2",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "Economic behavior is shaped not only by quantitative information but also by the narratives through which such information is communicated and interpreted (Shiller, 2017). I show that narratives extracted from earnings calls significantly improve the prediction of both realized earnings and analyst expectations. To uncover the underlying mechanisms, I introduce a novel text-morphing methodology in which large language models generate counterfactual transcripts that systematically vary topical emphasis (the prevailing narrative) while holding quantitative content fixed. This framework allows me to precisely measure how analysts under- and over-react to specific narrative dimensions. The results reveal systematic biases: analysts over-react to sentiment (optimism) and under-react to narratives of risk and uncertainty. Overall, the analysis offers a granular perspective on the mechanisms of expectation formation through the competing narratives embedded in corporate communication.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过文本变形方法，利用大型语言模型生成反事实收益电话会议记录，系统性地改变主题重点（主流叙事），同时保持定量内容不变。研究发现分析师对情绪（乐观）反应过度，而对风险和不确定性叙事反应不足。这些系统性偏差的识别为量化交易提供了可操作的Alpha信号，可通过文本分析预测分析师预期修正和实际收益，具有直接的实战应用价值。",
    "fetch_date": "2026-01-05",
    "id": "20260105_fc5bbd62"
  },
  {
    "title": "Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems",
    "url": "https://arxiv.org/pdf/2511.16657v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种针对欧元-美元货币对在高频外汇市场环境中实施的高级人工智能算法交易系统。其方法论核心在于整合一套全面的输入特征集：包括从欧元区和美国收集的关键宏观经济基本面变量（例如国内生产总值和失业率），以及一套全面的技术变量（包括指标、振荡器、斐波那契水平和价格背离）。通过标准机器学习指标量化预测准确性，并利用历史数据进行回测模拟以评估交易盈利能力和风险，从而评估所得算法的性能。研究最后通过比较分析来确定哪一类输入特征（基本面或技术面）能为生成盈利交易信号提供更强、更可靠的预测能力。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f79ba632"
  },
  {
    "title": "Probability Weighting Meets Heavy Tails: An Econometric Framework for Behavioral Asset Pricing",
    "url": "https://arxiv.org/pdf/2511.16563v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "We develop an econometric framework integrating heavy-tailed Student's $t$ distributions with behavioral probability weighting while preserving infinite divisibility. Using 432{,}752 observations across 86 assets (2004--2024), we demonstrate Student's $t$ specifications outperform Gaussian models in 88.4\\% of cases. Bounded probability-weighting transformations preserve mathematical properties required for dynamic pricing. Gaussian models underestimate 99\\% Value-at-Risk by 19.7\\% versus 3.2\\% for our specification. Joint estimation procedures identify tail and behavioral parameters with established asymptotic properties. Results provide robust inference for asset-pricing applications where heavy tails and behavioral distortions coexist.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "我们开发了一个计量经济学框架，将重尾的Student's t分布与行为概率加权相结合，同时保持无限可分性。使用86种资产（2004-2024年）的432,752个观测值，我们证明Student's t模型在88.4%的情况下优于高斯模型。有界概率加权变换保留了动态定价所需的数学性质。高斯模型低估了99%的风险价值（VaR）19.7%，而我们的模型仅低估3.2%。联合估计程序可识别尾部参数和行为参数，并具有已建立的渐近性质。结果为重尾和行为扭曲共存的资产定价应用提供了稳健的推断。",
    "fetch_date": "2026-01-05",
    "id": "20260105_bac290b9"
  },
  {
    "title": "Quantitative Geometric Market Structuralism: A Framework for Detecting Structural Endpoints in Financial Markets",
    "url": "https://arxiv.org/pdf/2511.16319v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "This study introduces the Quantitative Geometric Market Structuralist (QGMS) framework a hybrid analytical methodology integrating geometric pattern recognition with quantitative mathematical modeling to identify terminal zones of large-scale market movements. Unlike conventional econometric or signal-based models, the QGMS framework conceptualizes market dynamics as evolving geometric structures governed by self-organizing principles of price formation.\n  To preserve the proprietary nature of its internal mathematical architecture, the methodology employs a blind-testing validation process, wherein price, symbol, and temporal identifiers are concealed during analysis. This design ensures objective verification without revealing the underlying algorithmic core. The frameworks predictive robustness has been empirically examined across multiple financial crises, including the 2008 Global Financial Collapse, the 2015 EUR CHF SNB event, the 2016 Brexit referendum, and the 2020 COVID-19 market crash. In each case, the system consistently identified structural endpoints preceding major market reversals.\n  The findings suggest that geometric quantitative market interpretation may offer a new class of predictive tools bridging the gap between mathematical formalism and empirical price behavior. By combining academic testability with intellectual property protection, the QGMS framework establishes a viable foundation for institutional evaluation and further research into nonlinear structural forecasting models.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究提出定量几何市场结构主义（QGMS）框架，这是一种混合分析方法，将几何模式识别与定量数学模型相结合，用于识别大规模市场运动的终端区域。与传统计量经济学或基于信号的模型不同，QGMS框架将市场动态概念化为由价格形成的自组织原则支配的演化几何结构。为保护其内部数学架构的专有性质，该方法采用盲测验证过程，在分析过程中隐藏价格、符号和时间标识符。该设计确保客观验证而不揭示底层算法核心。该框架的预测稳健性已在多次金融危机中进行了实证检验，包括2008年全球金融危机、2015年欧元/瑞郎瑞士央行事件、2016年英国脱欧公投和2020年COVID-19市场崩盘。在每种情况下，该系统始终识别出主要市场反转前的结构端点。研究结果表明，几何定量市场解释可能提供一种新的市场分析方法。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f8fb4b4e"
  },
  {
    "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining",
    "url": "https://arxiv.org/pdf/2511.15456v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "随着去中心化金融（DeFi）的发展，理解DeFi交易背后的用户意图至关重要，但由于复杂的智能合约交互、多方面的链上/链下因素以及不透明的十六进制日志，这变得极具挑战性。现有方法缺乏深入的语义洞察。为此，我们提出了交易意图挖掘（TIM）框架。TIM利用基于扎根理论构建的DeFi意图分类法和多智能体大型语言模型（LLM）系统，稳健地推断用户意图。元级规划器动态协调领域专家，将多视角特定的意图分析分解为可解决的子任务。问题求解器处理多模态链上/链下数据的任务。认知评估器则减轻LLM幻觉并确保可验证性。实验表明，TIM显著优于机器学习模型、单一LLM和单一智能体基线。我们还分析了意图推断中的核心挑战。这项工作有助于更可靠地理解DeFi中的用户动机，为复杂的区块链活动提供情境感知的解释。",
    "fetch_date": "2026-01-05",
    "id": "20260105_0a511ca3"
  },
  {
    "title": "Law-Strength Frontiers and a No-Free-Lunch Result for Law-Seeking Reinforcement Learning on Volatility Law Manifolds",
    "url": "https://arxiv.org/pdf/2511.17304v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "We study reinforcement learning (RL) on volatility surfaces through the lens of Scientific AI. We ask whether axiomatic no-arbitrage laws, imposed as soft penalties on a learned world model, can reliably align high-capacity RL agents, or mainly create Goodhart-style incentives to exploit model errors. From classical static no-arbitrage conditions we build a finite-dimensional convex volatility law manifold of admissible total-variance surfaces, together with a metric law-penalty functional and a Graceful Failure Index (GFI) that normalizes law degradation under shocks. A synthetic generator produces law-consistent trajectories, while a recurrent neural world model trained without law regularization exhibits structured off-manifold errors. On this testbed we define a Goodhart decomposition \\(r = r^{\\mathcal{M}} + r^\\perp\\), where \\(r^\\perp\\) is ghost arbitrage from off-manifold prediction error. We prove a ghost-arbitrage incentive theorem for PPO-type agents, a law-strength trade-off theorem showing that stronger penalties eventually worsen P\\&L, and a no-free-lunch theorem: under a law-consistent world model and law-aligned strategy class, unconstrained law-seeking RL cannot Pareto-dominate structural baselines on P\\&L, penalties, and GFI. In experiments on an SPX/VIX-like world model, simple structural strategies form the empirical law-strength frontier, while all law-seeking RL variants underperform and move into high-penalty, high-GFI regions. Volatility thus provides a concrete case where reward shaping with verifiable penalties is insufficient for robust law alignment.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文从科学AI的视角研究波动率曲面上的强化学习。核心探讨了将无套利公理作为软惩罚施加于学习的世界模型时，能否可靠地引导高容量RL智能体，还是主要产生Goodhart式激励以利用模型误差。作者构建了有限维凸波动率定律流形，定义了度量定律惩罚函数和优雅失效指数，并证明了三个定理：PPO类智能体的幽灵套利激励定理、定律强度权衡定理（更强惩罚最终会恶化盈亏），以及一个无免费午餐定理——在定律一致的世界模型和定律对齐的策略类下，无约束的定律寻求RL无法实现帕累托改进。论文主要贡献在于理论框架和定理证明，而非具体的实战交易策略或算法实现。",
    "fetch_date": "2026-01-05",
    "id": "20260105_48cdc0b0"
  },
  {
    "title": "Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building",
    "url": "https://arxiv.org/pdf/2511.17654v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Conflict resolution and consensus building represent critical challenges in multi-agent systems, negotiations, and collaborative decision-making processes. This paper introduces Dialogue Diplomats, a novel end-to-end multi-agent reinforcement learning (MARL) framework designed for automated conflict resolution and consensus building in complex, dynamic environments. The proposed system integrates advanced deep reinforcement learning architectures with dialogue-based negotiation protocols, enabling autonomous agents to engage in sophisticated conflict resolution through iterative communication and strategic adaptation. We present three primary contributions: first, a novel Hierarchical Consensus Network (HCN) architecture that combines attention mechanisms with graph neural networks to model inter-agent dependencies and conflict dynamics. second, a Progressive Negotiation Protocol (PNP) that structures multi-round dialogue interactions with adaptive concession strategies; and third, a Context-Aware Reward Shaping mechanism that balances individual agent objectives with collective consensus goals.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "《对话外交官：一种用于自动冲突解决与共识构建的端到端多智能体强化学习系统》提出了一种新颖的端到端多智能体强化学习框架，旨在解决复杂动态环境中的冲突与共识构建问题。该系统整合了先进的深度强化学习架构与基于对话的协商协议，使自主智能体能够通过迭代通信和策略适应进行复杂的冲突解决。主要贡献包括：1）结合注意力机制与图神经网络以建模智能体间依赖关系和冲突动态的层次化共识网络架构；2）采用自适应让步策略构建多轮对话交互的渐进式协商协议；3）平衡个体智能体目标与集体共识目标的上下文感知奖励塑造机制。",
    "fetch_date": "2026-01-05",
    "id": "20260105_d98d9b99"
  },
  {
    "title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense",
    "url": "https://arxiv.org/pdf/2511.16483v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在复杂动态环境中为自主网络攻防学习智能体设计奖励函数对领域专家而言具有挑战性。本研究提出基于大语言模型（LLM）的奖励设计方法，在深度强化学习（DRL）驱动的实验仿真环境中生成自主网络防御策略。通过构建反映智能体行为异质性的多类攻防角色，首先生成包含网络仿真环境上下文信息的LLM引导奖励设计方案，随后在DRL驱动的攻防仿真环境中利用这些奖励结构学习网络防御策略集合。结果表明，LLM引导的奖励设计能够针对多样化对抗行为生成有效防御策略。",
    "fetch_date": "2026-01-05",
    "id": "20260105_039df9ff"
  },
  {
    "title": "Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements",
    "url": "https://arxiv.org/pdf/2511.15960v1",
    "source": "ArXiv",
    "date": "2025-11-20",
    "abstract": "Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "二元期权交易常被宣传为预测模型可产生稳定利润的领域，但二元期权的内在随机性和随机性质使价格走势高度不可预测，给任何预测方法带来重大挑战。本研究证明，机器学习算法在预测二元期权走势方面难以超越简单基线。使用2021年至2023年EUR/USD货币对数据集，我们测试了多种模型，包括随机森林、逻辑回归、梯度提升和k-最近邻（kNN），均在超参数优化前后进行。此外，评估了多种神经网络架构，包括多层感知器（MLP）和长短期记忆（LSTM）网络在不同训练条件下。尽管这些努力详尽，但所有模型均未超越ZeroR基线准确率，突显了二元期权的内在随机性。这些发现强化了二元期权缺乏可预测模式的观点，使其不适合基于机器学习的预测。",
    "fetch_date": "2026-01-05",
    "id": "20260105_2fb789d3"
  },
  {
    "title": "Anonymization and Information Loss",
    "url": "https://arxiv.org/pdf/2511.15364v1",
    "source": "ArXiv",
    "date": "2025-11-19",
    "abstract": "We show that while anonymization effectively obscures firm identity, it significantly reduces the power of textual understanding, thereby diminishing models' ability to extract meaningful economic signals from financial texts. This information loss is particularly severe when numerical and object entities are removed from texts and is amplified in texts characterized by high linguistic uncertainty and firm specificity. Importantly, in the setting of sentiment extraction from earnings call transcripts, we find that information loss induced by anonymization is more pervasive and severe than the effects of look-ahead bias, suggesting that the costs of anonymization may outweigh its benefits in certain financial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了金融文本匿名化处理导致的信息损失问题。研究表明，虽然匿名化能有效隐藏公司身份，但会显著削弱文本理解能力，降低模型从金融文本中提取有效经济信号的能力。这种信息损失在移除文本中的数值和实体对象时尤为严重，且在语言不确定性高、公司特异性强的文本中会被放大。特别在盈利电话会议记录的情感提取场景中，匿名化造成的信息损失比前瞻性偏差的影响更普遍且严重，表明在某些金融应用中，匿名化的成本可能超过其收益。",
    "fetch_date": "2026-01-05",
    "id": "20260105_f31b728b"
  },
  {
    "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.17963v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该论文提出了一种融合长短期记忆网络预测与近端策略优化强化学习的混合框架，用于动态投资组合优化。该框架利用深度循环网络捕捉时序依赖性的预测能力，同时PPO智能体在连续动作空间中自适应调整资产配置，使系统既能预测趋势又能动态适应市场变化。基于2018年1月至2024年12月涵盖美国与印尼股票、美国国债及主要加密货币的多资产数据集，模型在考虑交易成本后，通过年化收益率、波动率、夏普比率和最大回撤等指标进行评估。结果显示，该混合架构在非平稳市场环境下实现了更高收益和更强韧性，展现了其作为稳健AI驱动交易策略的潜力。",
    "fetch_date": "2026-01-04",
    "id": "20260104_05180ddb"
  },
  {
    "title": "Re(Visiting) Time Series Foundation Models in Finance",
    "url": "https://arxiv.org/pdf/2511.18578v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Financial time series forecasting is central to trading, portfolio optimization, and risk management, yet it remains challenging due to noisy, non-stationary, and heterogeneous data. Recent advances in time series foundation models (TSFMs), inspired by large language models, offer a new paradigm for learning generalizable temporal representations from large and diverse datasets. This paper presents the first comprehensive empirical study of TSFMs in global financial markets. Using a large-scale dataset of daily excess returns across diverse markets, we evaluate zero-shot inference, fine-tuning, and pre-training from scratch against strong benchmark models. We find that off-the-shelf pre-trained TSFMs perform poorly in zero-shot and fine-tuning settings, whereas models pre-trained from scratch on financial data achieve substantial forecasting and economic improvements, underscoring the value of domain-specific adaptation. Increasing the dataset size, incorporating synthetic data augmentation, and applying hyperparameter tuning further enhance performance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "金融时间序列预测是交易、投资组合优化和风险管理的核心，但由于数据噪声大、非平稳且异质性强，该任务极具挑战。受大语言模型启发，时间序列基础模型（TSFMs）的最新进展为从大规模多样化数据集中学习可泛化的时序表征提供了新范式。本文首次对TSFMs在全球金融市场中的应用进行了全面的实证研究。基于涵盖多个市场的日超额收益大规模数据集，我们评估了零样本推理、微调及从零开始预训练模型与强基准模型的性能。研究发现，现成的预训练TSFMs在零样本和微调设置下表现不佳，而在金融数据上从零开始预训练的模型则实现了显著的预测和经济收益提升，强调了领域特定适应的重要性。增加数据集规模、引入合成数据增强以及应用超参数调优可进一步提升模型性能。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d10039fd"
  },
  {
    "title": "Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons",
    "url": "https://arxiv.org/pdf/2511.18076v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究提出了一种增强的G-Learning算法，结合GIRL算法进行参数优化，用于投资组合优化。目标是在目标日期前最大化投资组合价值，同时最小化投资者的定期贡献。模型在高度波动的市场中运行，通过多元化投资组合确保低风险水平，并利用强化学习动态调整持仓。结果显示，夏普比率从0.42提升至0.483，在高度波动的多元化投资组合市场中表现显著。GIRL与G-Learning的对比表明，虽然GIRL优化了奖励函数参数，但对投资组合性能的影响有限，说明强化学习方法如G-Learning已能实现稳健优化。",
    "fetch_date": "2026-01-04",
    "id": "20260104_429bc0cd"
  },
  {
    "title": "Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints",
    "url": "https://arxiv.org/pdf/2511.17892v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "我们开发了一个基于Heath-Jarrow-Morton（HJM）期限结构模型和动态Nelson-Siegel参数化远期利率的无套利深度学习框架，用于收益率曲线和债券价格预测。该方法通过将卡尔曼滤波器、扩展卡尔曼滤波器、粒子滤波器与循环神经网络（LSTM/CLSTM）相结合，将无套利漂移限制嵌入神经状态空间架构，并在训练中引入显式套利误差正则化（AER）项。模型应用于美国国债和公司债券数据，评估其在1天和5天期限的收益率空间和价格空间预测性能。实证表明，套利正则化在短期期限（尤其是5天预测）中带来最显著的改进，提高了以买卖价差命中率衡量的市场一致性，并降低了美元计价的预测误差。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d5fc23a3"
  },
  {
    "title": "Partial multivariate transformer as a tool for cryptocurrencies time series prediction",
    "url": "https://arxiv.org/pdf/2512.04099v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "Forecasting cryptocurrency prices is hindered by extreme volatility and a methodological dilemma between information-scarce univariate models and noise-prone full-multivariate models. This paper investigates a partial-multivariate approach to balance this trade-off, hypothesizing that a strategic subset of features offers superior predictive power. We apply the Partial-Multivariate Transformer (PMformer) to forecast daily returns for BTCUSDT and ETHUSDT, benchmarking it against eleven classical and deep learning models. Our empirical results yield two primary contributions. First, we demonstrate that the partial-multivariate strategy achieves significant statistical accuracy, effectively balancing informative signals with noise. Second, we experiment and discuss an observable disconnect between this statistical performance and practical trading utility; lower prediction error did not consistently translate to higher financial returns in simulations. This finding challenges the reliance on traditional error metrics and highlights the need to develop evaluation criteria more aligned with real-world financial objectives.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "预测加密货币价格面临极端波动性和方法论困境：信息稀缺的单变量模型与噪声充斥的全多变量模型之间的权衡。本文研究了一种部分多变量方法以平衡这一权衡，假设策略性特征子集能提供更优的预测能力。我们应用部分多变量Transformer（PMformer）预测BTCUSDT和ETHUSDT的日收益率，并以十一种经典和深度学习模型为基准。实证结果有两个主要贡献：首先，我们证明部分多变量策略实现了显著的统计准确性，有效平衡了信息信号与噪声；其次，我们实验并讨论了这种统计性能与实际交易效用之间的可观察脱节——较低的预测误差在模拟中并未一致转化为更高的财务回报。这一发现挑战了对传统误差指标的依赖，并强调需要开发更符合现实世界金融目标的评估标准。",
    "fetch_date": "2026-01-04",
    "id": "20260104_210cbaf1"
  },
  {
    "title": "Optimal dividend and capital injection under self-exciting claims",
    "url": "https://arxiv.org/pdf/2511.19701v1",
    "source": "ArXiv",
    "date": "2025-11-24",
    "abstract": "In this paper, we study an optimal dividend and capital-injection problem in a Cramér--Lundberg model where claim arrivals follow a Hawkes process, capturing clustering effects often observed in insurance portfolios. We establish key analytical properties of the value function and characterise the optimal capital-injection strategy through an explicit threshold. We also show that the value function is the unique viscosity solution of the associated HJB variational inequality. For numerical purposes, we first compute a benchmark solution via a monotone finite-difference scheme with Howard's policy iteration. We then develop a reinforcement learning approach based on policy-gradient and actor-critic methods. The learned strategies closely match the PDE benchmark and remain stable across initial conditions. The results highlight the relevance of policy-gradient techniques for dividend optimisation under self-exciting claim dynamics and point toward scalable methods for higher-dimensional extensions.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究了一个最优分红与资本注入问题，采用Cramér–Lundberg模型，其中索赔到达遵循Hawkes过程，以捕捉保险组合中常见的聚集效应。论文建立了价值函数的关键解析性质，并通过显式阈值刻画了最优资本注入策略，证明价值函数是相关HJB变分不等式的唯一粘性解。数值计算方面，首先通过带Howard策略迭代的单调有限差分法获得基准解，随后开发了基于策略梯度和演员-评论家方法的强化学习算法。学习得到的策略与PDE基准高度吻合，且在不同初始条件下保持稳定。结果表明策略梯度技术在处理自激励索赔动态下的分红优化问题中具有适用性，并为高维扩展提供了可扩展的方法。",
    "fetch_date": "2026-01-04",
    "id": "20260104_b78c6b9f"
  },
  {
    "title": "A calibrated model of debt recycling with interest costs and tax shields: viability under different fiscal regimes and jurisdictions",
    "url": "https://arxiv.org/pdf/2511.18614v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Debt recycling is a leveraged equity management strategy in which homeowners use accumulated home equity to finance investments, applying the resulting returns to accelerate mortgage repayment. We propose a novel framework to model equity and mortgage dynamics in presence of mortgage interest rates, borrowing costs on equity-backed credit lines, and tax shields arising from interest deductibility. The model is calibrated on three jurisdictions -- Australia, Germany, and Switzerland -- representing diverse interest rate environments and fiscal regimes. Results demonstrate that introducing positive interest rates without tax shields contracts success regions and lengthens repayment times, while tax shields partially reverse these effects by reducing effective borrowing costs and adding equity boosts from mortgage interest deductibility. Country-specific outcomes vary systematically, and rental properties consistently outperform owner-occupied housing due to mortgage interest deductibility provisions.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "债务回收是一种杠杆股权管理策略，房主利用累积的房屋净值融资投资，并将所得回报用于加速抵押贷款还款。本文提出了一个新颖框架，在抵押贷款利率、股权支持信贷额度的借款成本以及利息可抵扣性产生的税收盾的背景下，模拟股权和抵押贷款的动态变化。该模型在澳大利亚、德国和瑞士三个司法管辖区进行了校准，代表了不同的利率环境和财政制度。结果表明，在没有税收盾的情况下引入正利率会缩小成功区域并延长还款时间，而税收盾通过降低有效借款成本和增加抵押贷款利息可抵扣性带来的股权提升，部分逆转了这些影响。特定国家的结果存在系统性差异，由于抵押贷款利息可抵扣性条款，租赁房产的表现始终优于自住住房。",
    "fetch_date": "2026-01-04",
    "id": "20260104_38111ddb"
  },
  {
    "title": "Limit Order Book Dynamics in Matching Markets: Microstructure, Spread, and Execution Slippage",
    "url": "https://arxiv.org/pdf/2511.20606v2",
    "source": "ArXiv",
    "date": "2025-11-25",
    "abstract": "Conventional models of matching markets assume that monetary transfers can clear markets by compensating for utility differentials. However, empirical patterns show that such transfers often fail to close structural preference gaps. This paper introduces a market microstructure framework that models matching decisions as a limit order book system with rigid bid ask spreads. Individual preferences are represented by a latent preference state matrix, where the spread between an agent's internal ask price (the unconditional maximum) and the market's best bid (the reachable maximum) creates a structural liquidity constraint. We establish a Threshold Impossibility Theorem showing that linear compensation cannot close these spreads unless it induces a categorical identity shift. A dynamic discrete choice execution model further demonstrates that matches occur only when the market to book ratio crosses a time decaying liquidity threshold, analogous to order execution under inventory pressure. Numerical experiments validate persistent slippage, regional invariance of preference orderings, and high tier zero spread executions. The model provides a unified microstructure explanation for matching failures, compensation inefficiency, and post match regret in illiquid order driven environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文《匹配市场中的限价订单簿动态：微观结构、价差与执行滑点》提出了一种市场微观结构框架，将匹配决策建模为具有刚性买卖价差的限价订单簿系统。个体偏好由潜在偏好状态矩阵表示，其中代理内部要价（无条件最大值）与市场最佳出价（可达最大值）之间的价差构成了结构性流动性约束。阈值不可能定理表明，线性补偿无法消除这些价差，除非引发分类身份转变。动态离散选择执行模型进一步证明，只有当市场与订单簿比率超过随时间衰减的流动性阈值时，匹配才会发生，类似于库存压力下的订单执行。数值实验验证了持续的滑点、偏好排序的区域不变性以及高层级零价差执行。该模型为匹配市场的微观结构提供了统一解释。",
    "fetch_date": "2026-01-04",
    "id": "20260104_190db185"
  },
  {
    "title": "Carbon-Penalised Portfolio Insurance Strategies in a Stochastic Factor Model with Partial Information",
    "url": "https://arxiv.org/pdf/2511.19186v1",
    "source": "ArXiv",
    "date": "2025-11-24",
    "abstract": "Given the increasing importance of environmental, social and governance (ESG) factors, particularly carbon emissions, we investigate optimal proportional portfolio insurance (PPI) strategies accounting for carbon footprint reduction. PPI strategies enable investors to mitigate downside risk while retaining the potential for upside gains. This paper aims to determine the multiplier of the PPI strategy to maximise the expected utility of the terminal cushion, where the terminal cushion is penalised proportionally to the realised volatility of stocks issued by firms operating in carbon-intensive sectors. We model the risky assets' dynamics using geometric Brownian motions whose drift rates are modulated by an unobservable common stochastic factor to capture market-specific or economy-wide state variables that are typically not directly observable. Using classical stochastic filtering theory, we formulate a suitable optimization problem and solve it for CRRA utility function. We characterise optimal carbon penalised PPI strategies and optimal value functions under full and partial information and quantify the loss of utility due incomplete information. Finally, we carry a numerical analysis showing that the proposed strategy reduces carbon emission intensity without compromising financial performance.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "鉴于环境、社会和治理（ESG）因素，特别是碳排放的重要性日益增加，本研究探讨了考虑碳足迹减少的最优比例投资组合保险（PPI）策略。PPI策略使投资者能够减轻下行风险，同时保留上行收益的潜力。本文旨在确定PPI策略的乘数，以最大化终端缓冲的期望效用，其中终端缓冲根据碳密集型行业公司发行的股票的实际波动率按比例惩罚。我们使用几何布朗运动对风险资产的动态进行建模，其漂移率由不可观测的公共随机因子调节，以捕捉通常无法直接观测的市场特定或经济范围内的状态变量。利用经典随机滤波理论，我们构建了一个合适的优化问题，并针对CRRA效用函数进行求解。我们刻画了在完全信息和部分信息下的最优碳惩罚PPI策略和最优价值函数，并量化了由于信息不完全导致的效用损失。最后，我们进行了数值分析。",
    "fetch_date": "2026-01-04",
    "id": "20260104_746f0249"
  },
  {
    "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization",
    "url": "https://arxiv.org/pdf/2512.20623v1",
    "source": "ArXiv",
    "date": "2025-11-23",
    "abstract": "Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出BitRL-Light框架，将1-bit量化大型语言模型（LLMs）与深度Q网络（DQN）强化学习结合，用于边缘设备上的实时智能家居照明控制。该系统在Raspberry Pi硬件上部署1-bit量化的Llama-3.2-1B模型，相比全精度模型实现71.4倍能耗降低，并通过多目标强化学习从用户反馈中学习最优照明策略，平衡能耗、舒适度和昼夜节律对齐。实验结果显示，相比基于规则的系统节能32%，在Raspberry Pi 4上推理延迟低于200ms，用户满意度达95%。系统通过Google Home/IFTTT集成处理自然语言命令，并通过手动覆盖学习隐式反馈。比较分析表明，1-bit模型在ARM处理器上比2-bit方案提速5.07倍，同时保持92%任务准确率。",
    "fetch_date": "2026-01-04",
    "id": "20260104_a67c750a"
  },
  {
    "title": "Superhedging under Proportional Transaction Costs in Continuous Time",
    "url": "https://arxiv.org/pdf/2511.18169v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "We revisit the well-studied superhedging problem under proportional transaction costs in continuous time using the recently developed tools of set-valued stochastic analysis. By relying on a simple Black-Scholes-type market model for mid-prices and using continuous trading schemes, we define a dynamic family of superhedging sets in continuous time and express them in terms of set-valued integrals. We show that these sets, defined as subsets of Lebesgue spaces at different times, form a dynamic set-valued risk measure with multi-portfolio time-consistency. Finally, we transfer the problem formulation to a path-space setting and introduce approximate versions of superhedging sets that will involve relaxing the superhedging inequality, the superhedging probability, and the solvency requirement for the superhedging strategy with a predetermined error level. In this more technical framework, we are able to relate the approximate superhedging sets at different times by means of a set-valued Bellman's principle, which we believe will pave the way for a set-valued differential structure that characterizes the superhedging sets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文在连续时间下，利用集合值随机分析工具重新研究了比例交易成本下的超对冲问题。基于简单的Black-Scholes型中间价市场模型和连续交易策略，定义了连续时间下的动态超对冲集合，并用集合值积分表示。研究表明，这些在不同时间作为Lebesgue空间子集定义的集合构成了具有多投资组合时间一致性的动态集合值风险度量。最后，将问题表述转移到路径空间设置，引入了超对冲集合的近似版本，通过预设误差水平放宽超对冲不等式、超对冲概率和超对冲策略的偿付要求。在此技术框架下，通过集合值Bellman原理关联不同时间的近似超对冲集合，为表征超对冲集合的集合值微分结构奠定了基础。",
    "fetch_date": "2026-01-04",
    "id": "20260104_367cade6"
  },
  {
    "title": "Random processes for long-term market simulations",
    "url": "https://arxiv.org/pdf/2511.18125v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "For long term investments, model portfolios are defined at the level of indexes, a setup known as Strategic Asset Allocation (SAA). The possible outcomes at a scale of a few decades can be obtained by Monte Carlo simulations, resulting in a probability density for the possible portfolio values at the investment horizon. Such studies are critical for long term wealth plannings, for example in the financial component of social insurances or in accumulated capital for retirement. The quality of the results depends on two inputs: the process used for the simulations and its parameters. The base model is a constant drift, a constant covariance and normal innovations, as pioneered by Bachelier. Beyond this model, this document presents in details a multivariate process that incorporate the most recent advances in the models for financial time series. This includes the negative correlations of the returns at a scale of a few years, the heteroskedasticity (i.e. the volatility' dynamics), and the fat tails and asymmetry for the distributions of returns. For the parameters, the quantitative outcomes depend critically on the estimate for the drift, because this is a non random contribution acting at each time step. Replacing the point forecast by a probabilistic forecast allows us to analyze the impact of the drift values, and then to incorporate this uncertainty in the Monte Carlo simulations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了用于长期市场模拟的随机过程，核心应用于战略资产配置（SAA）层面的长期投资组合建模。通过蒙特卡洛模拟生成数十年投资期限的可能结果概率分布，适用于养老金、退休储蓄等长期财富规划。论文详细介绍了超越基础模型（恒定漂移、协方差及正态创新）的多变量过程，纳入了金融时间序列的最新进展，包括数年尺度的收益负相关性、异方差性（波动率动态）以及收益分布的厚尾和不对称性。论文强调，模拟结果的定量输出关键取决于漂移参数的估计，因为这是非随机贡献。",
    "fetch_date": "2026-01-04",
    "id": "20260104_9fb61e40"
  },
  {
    "title": "Diffusive Limit of Hawkes Driven Order Book Dynamics With Liquidity Migration",
    "url": "https://arxiv.org/pdf/2511.18117v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "This paper develops a theoretical mesoscopic model of the limit order book driven by multivariate Hawkes processes, designed to capture temporal self-excitation and the spatial propagation of order flow across price levels. In contrast to classical zero-intelligence or Poisson based queueing models, the proposed framework introduces mathematically defined migration events between neighbouring price levels, whose intensities are themselves governed by the underlying Hawkes structure. This provides a principled stochastic mechanism for modeling interactions between order arrivals, cancellations, and liquidity movement across adjacent queues.\n  Starting from a microscopic specification of Hawkes driven order flow, we derive a diffusion approximation which yields a reflected mesoscopic stochastic differential equation (SDE) system for queue volumes. The limiting generator is obtained through a Taylor expansion of the microscopic generator, demonstrating how temporal excitation together with spatial migration determine the drift and diffusion structure of the limit order book in the mesoscopic regime. The resulting model extends existing diffusion limits by incorporating correlated excitations and price level to price level liquidity movement within a unified Hawkes based formulation.\n  By establishing this diffusive limit, the paper provides a mathematically consistent bridge between high frequency event based models and macroscopic stochastic descriptions of market microstructure. The work is entirely theoretical and lays a foundation for future analytical and numerical developments without relying on empirical calibration.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种由多元霍克斯过程驱动的限价订单簿理论介观模型，旨在捕捉订单流的时间自激效应及其在价格层级间的空间传播。与经典的零智能或基于泊松的排队模型不同，该框架引入了相邻价格层级间数学定义的迁移事件，其强度由底层霍克斯结构控制，为建模订单到达、取消及相邻队列间流动性移动的相互作用提供了原则性随机机制。从霍克斯驱动订单流的微观设定出发，推导出扩散近似，得到队列量的反射介观随机微分方程系统。通过微观生成元的泰勒展开获得极限生成元，展示了时间激励与空间迁移如何共同决定介观状态下限价订单簿的漂移和扩散结构。该模型通过纳入相关激励和流动性迁移，扩展了现有的扩散极限。",
    "fetch_date": "2026-01-04",
    "id": "20260104_d92a3607"
  },
  {
    "title": "A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization",
    "url": "https://arxiv.org/pdf/2511.18093v1",
    "source": "ArXiv",
    "date": "2025-11-22",
    "abstract": "Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于深度强化学习（DRL）的新型误差时序差分（ETD）算法，旨在解决微电网优化中预测模型不确定性导致控制策略次优的问题。首先，建立了集成可再生能源（RES）和储能系统（ESS）的微电网系统及其马尔可夫决策过程（MDP）模型；其次，提出了一种基于深度Q网络（DQN）的预测控制方法，其中设计了加权平均算法和ETD算法分别量化和处理预测不确定性；最后，基于美国真实数据集的仿真表明，ETD算法有效提升了DRL在优化微电网运行中的性能。",
    "fetch_date": "2026-01-04",
    "id": "20260104_e4bba900"
  },
  {
    "title": "Emergence of Randomness in Temporally Aggregated Financial Tick Sequences",
    "url": "https://arxiv.org/pdf/2511.17479v1",
    "source": "ArXiv",
    "date": "2025-11-21",
    "abstract": "Markets efficiency implies that the stock returns are intrinsically unpredictable, a property that makes markets comparable to random number generators. We present a novel methodology to investigate ultra-high frequency financial data and to evaluate the extent to which tick by tick returns resemble random sequences. We extend the analysis of ultra high-frequency stock market data by applying comprehensive sets of randomness tests, beyond the usual reliance on serial correlation or entropy measures. Our purpose is to extensively analyze the randomness of these data using statistical tests from standard batteries that evaluate different aspects of randomness.\n  We illustrate the effect of time aggregation in transforming highly correlated high-frequency trade data to random streams. More specifically, we use many of the tests in the NIST Statistical Test Suite and in the TestU01 battery (in particular the Rabbit and Alphabit sub-batteries), to prove that the degree of randomness of financial tick data increases together with the increase of the aggregation level in transaction time. Additionally, the comprehensive nature of our tests also uncovers novel patterns, such as non-monotonic behaviors in predictability for certain assets. This study demonstrates a model-free approach for both assessing randomness in financial time series and generating pseudo-random sequences from them, with potential relevance in several applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "市场效率意味着股票回报本质上是不可预测的，这一特性使市场类似于随机数生成器。我们提出了一种新颖的方法来研究超高频金融数据，并评估逐笔回报与随机序列的相似程度。通过应用全面的随机性测试集（超越通常依赖的序列相关性或熵度量），我们扩展了对超高频股票市场数据的分析。我们的目的是使用标准测试套件中的统计测试来广泛分析这些数据的随机性，这些测试评估随机性的不同方面。我们说明了时间聚合在将高度相关的高频交易数据转换为随机流中的作用。具体而言，我们使用NIST统计测试套件和TestU01测试套件（特别是Rabbit和Alphabit子套件）中的许多测试，证明金融逐笔数据的随机性程度随着交易时间聚合水平的增加而增加。此外，我们测试的全面性还揭示了新的模式，例如非单调性。",
    "fetch_date": "2026-01-04",
    "id": "20260104_75507072"
  },
  {
    "title": "A3T-GCN for FTSE100 Components Price Forecasting",
    "url": "https://arxiv.org/pdf/2511.21873v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We examine the predictive power of a novel hybrid A3T-GCN architecture for forecasting closing stock prices of FTSE100 constituents. The dataset comprises 79 companies and 375,329 daily observations from 2007 to 2024, with node features including technical indicators (RSI, MACD), normalized and log returns, and annualized log returns over multiple windows (ALR1W, ALR2W, ALR1M, ALR2M). Graphs are constructed based on sector classifications and correlations of returns or financial ratios. Our results show that the A3T-GCN model using annualized log-returns and shorter sequence lengths improves prediction accuracy while reducing computational requirements. Additionally, longer historical sequences yield only modest improvements, highlighting their importance for longer-term forecasts.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了一种新型混合A3T-GCN架构在预测富时100指数成分股收盘价方面的预测能力。数据集包含2007年至2024年79家公司的375,329个日度观测值，节点特征包括技术指标（RSI、MACD）、归一化和对数收益率，以及多个时间窗口的年化对数收益率（ALR1W、ALR2W、ALR1M、ALR2M）。图结构基于行业分类和收益率或财务比率的相关系数构建。结果表明，使用年化对数收益率和较短序列长度的A3T-GCN模型提高了预测精度，同时降低了计算需求。此外，较长的历史序列仅带来有限的改进，突显了其对长期预测的重要性。",
    "fetch_date": "2026-01-03",
    "id": "20260103_2b03ff62"
  },
  {
    "title": "Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba",
    "url": "https://arxiv.org/pdf/2511.22101v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "The report goes through the main steps of replicating and improving the article \"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.\" The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过复制并改进《在Uniswap V3中使用深度强化学习的自适应流动性提供》一文，提出了一种结合Mamba与DDQN（Dueling Double Deep Q-networks）及新奖励函数的新结构。论文涵盖了从Uniswap Subgraph获取数据、实现细节、结果分析，并引入了两个新基准进行比较。尽管尚未在所有数据集上应用，但新模型在部分测试中表现更优，且具有更强的理论支持。",
    "fetch_date": "2026-01-03",
    "id": "20260103_cb6973a7"
  },
  {
    "title": "LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline",
    "url": "https://arxiv.org/pdf/2512.07867v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文开发了一种基于LLM的透明、可审计的宏观金融压力测试管道，结合结构化提示与可选的国家基本面及新闻检索。该系统为G7国家生成机器可读的宏观经济情景（涵盖GDP增长、通胀及政策利率），并通过基于因子的映射将其转化为投资组合损失，从而支持相对于传统计量经济学基线的在险价值（VaR）和预期短缺（ES）评估。研究表明，LLM能生成连贯且针对特定国家的压力叙事，产生稳定的尾部风险放大效应，且对检索选择的敏感性有限。全面的合理性检查、情景诊断和基于ANOVA的方差分解表明，风险变异主要由投资组合构成和提示设计驱动，而非检索机制。该管道包含快照、确定性模式和哈希验证工件，以确保可重复性和可审计性。总体而言，结果表明，当LLM生成的宏观情景与透明结构和严格验证相结合时，可为实战交易提供有价值的风险模拟工具。",
    "fetch_date": "2026-01-03",
    "id": "20260103_0b660ac8"
  },
  {
    "title": "Black-Litterman and ESG Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2511.21850v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "We introduce a simple portfolio optimization strategy using ESG data with the Black-Litterman allocation framework. ESG scores are used as a bias for Stein shrinkage estimation of equilibrium risk premiums used in assigning Black-Litterman asset weights. Assets are modeled as multivariate affine normal-inverse Gaussian variables using CVaR as a risk measure. This strategy, though very simple, when employed with a soft turnover constraint is exceptionally successful. Portfolios are reallocated daily over a 4.7 year period, each with a different set of hyperparameters used for optimization. The most successful strategies have returns of approximately 40-45% annually.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种结合ESG数据的简单投资组合优化策略，采用Black-Litterman资产配置框架。ESG评分被用作Stein收缩估计的偏差，用于计算Black-Litterman资产权重中的均衡风险溢价。资产被建模为多元仿射正态逆高斯变量，并使用CVaR作为风险度量。该策略虽然简单，但在采用软换手率约束时表现异常出色。投资组合在4.7年期间每日重新配置，每次使用不同的超参数进行优化。最成功的策略年化回报率约为40-45%。",
    "fetch_date": "2026-01-03",
    "id": "20260103_44a40e03"
  },
  {
    "title": "Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting",
    "url": "https://arxiv.org/pdf/2512.07860v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "This paper investigates an optimal integration of deep learning with financial models for robust asset price forecasting. Specifically, we developed a hybrid framework combining a Long Short-Term Memory (LSTM) network with the Merton-Lévy jump-diffusion model. To optimise this framework, we employed the Grey Wolf Optimizer (GWO) for the LSTM hyperparameter tuning, and we explored three calibration methods for the Merton-Levy model parameters: Artificial Neural Networks (ANNs), the Marine Predators Algorithm (MPA), and the PyTorch-based TorchSDE library. To evaluate the predictive performance of our hybrid model, we compared it against several benchmark models, including a standard LSTM and an LSTM combined with the Fractional Heston model. This evaluation used three real-world financial datasets: Brent oil prices, the STOXX 600 index, and the IT40 index. Performance was assessed using standard metrics, including Mean Squared Error (MSE), Mean Absolute Error(MAE), Mean Squared Percentage Error (MSPE), and the coefficient of determination (R2). Our experimental results demonstrate that the hybrid model, combining a GWO-optimized LSTM network with the Levy-Merton Jump-Diffusion model calibrated using an ANN, outperformed the base LSTM model and all other models developed in this study.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文研究了一种将深度学习与金融模型相结合进行资产价格稳健预测的混合框架。具体而言，作者开发了一个结合长短期记忆网络与Merton-Lévy跳跃扩散模型的混合模型。为优化该框架，采用灰狼优化器进行LSTM超参数调优，并探索了三种Merton-Levy模型参数校准方法：人工神经网络、海洋捕食者算法和基于PyTorch的TorchSDE库。通过在布伦特原油价格、STOXX 600指数和IT40指数三个真实金融数据集上的实验评估，使用均方误差、平均绝对误差、均方百分比误差和决定系数等标准指标，与标准LSTM及LSTM结合分数Heston模型等基准模型进行比较。实验结果表明，结合GWO优化LSTM与Levy-Merton跳跃扩散模型的混合框架在预测性能上表现优异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_b3fb4ca0"
  },
  {
    "title": "Portfolio Optimization via Transfer Learning",
    "url": "https://arxiv.org/pdf/2511.21221v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "认识到资产市场通常表现出共享的信息特征，我们开发了一种基于迁移学习的投资组合策略，该策略利用跨市场信息，通过前向验证来提升目标市场的投资表现。我们的策略渐近地识别并利用信息丰富的数据集，选择性地纳入有效信息，同时摒弃误导性信息。这使得我们的策略能够渐近地实现最大夏普比率。通过数值研究以及两个投资组合的案例研究（一个由A股和H股双重上市的股票组成，另一个包含美国各行业的股票），展示了该策略的优异表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_d33d2a08"
  },
  {
    "title": "Constrained deep learning for pricing and hedging european options in incomplete markets",
    "url": "https://arxiv.org/pdf/2511.20837v1",
    "source": "ArXiv",
    "date": "2025-11-25",
    "abstract": "In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在不完全金融市场中，欧式期权的定价和对冲因不可对冲风险而缺乏唯一无套利解。本文提出一种约束深度学习方法来最小化损益分布，使用单一神经网络表示期权价格函数，其梯度作为对冲策略，通过强制自融资组合条件的损失函数进行优化。针对期权收益非光滑特性（如香草看涨期权在平价点不可微，数字期权不连续）与神经网络固有平滑性的冲突，比较了无约束网络与显式嵌入终端收益条件的约束架构，借鉴偏微分方程求解技术。框架假设两种可交易资产：标的资产和捕捉波动率动态的流动性看涨期权。数值实验评估了该方法在具有不同非光滑性的简单期权、奇异Equinox期权及市场跳跃场景下的表现。",
    "fetch_date": "2026-01-03",
    "id": "20260103_061536e2"
  },
  {
    "title": "Evolutionary Discovery of Heuristic Policies for Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2511.23122v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\\textbf{\\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \\method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《Evolutionary Discovery of Heuristic Policies for Traffic Signal Control》提出了一种名为Temporal Policy Evolution for Traffic的方法，旨在解决交通信号控制（TSC）中经典启发式方法过于简化、深度强化学习（DRL）泛化能力差且策略不透明，以及在线大型语言模型（LLMs）延迟高且缺乏环境特定优化的问题。该方法利用LLMs作为进化引擎，通过结构化状态抽象（SSA）将高维交通数据转换为时序逻辑事实，并结合信用分配反馈（CAF）追踪微观决策错误与宏观结果的关系，从而在无需训练的情况下生成轻量级、鲁棒的启发式策略，针对特定交通环境进行优化。",
    "fetch_date": "2026-01-03",
    "id": "20260103_4318e585"
  },
  {
    "title": "Factors Influencing Cryptocurrency Prices: Evidence from Bitcoin, Ethereum, Dash, Litecoin, and Monero",
    "url": "https://arxiv.org/pdf/2511.22782v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "This paper examines factors that influence prices of most common five cryptocurrencies such as Bitcoin, Ethereum, Dash, Litecoin, and Monero over 2010-2018 using weekly data. The study employs ARDL technique and documents several findings. First, cryptomarket-related factors such as market beta, trading volume, and volatility appear to be significant determinant for all five cryptocurrencies both in short- and long-run. Second, attractiveness of cryptocurrencies also matters in terms of their price determination, but only in long-run. This indicates that formation (recognition) of the attractiveness of cryptocurrencies are subjected to time factor. In other words, it travels slowly within the market. Third, SP500 index seems to have weak positive long-run impact on Bitcoin, Ethereum, and Litcoin, while its sign turns to negative losing significance in short-run, except Bitcoin that generates an estimate of -0.20 at 10% significance level. Lastly, error-correction models for Bitcoin, Etherem, Dash, Litcoin, and Monero show that cointegrated series cannot drift too far apart, and converge to a long-run equilibrium at a speed of 23.68%, 12.76%, 10.20%, 22.91%, and 14.27% respectively.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文使用ARDL技术分析了2010-2018年比特币、以太坊、达世币、莱特币和门罗币这五种主要加密货币的周度数据，探讨了影响其价格的因素。研究发现：1）加密货币市场相关因素（如市场贝塔、交易量、波动性）在短期和长期对所有五种货币均有显著影响；2）加密货币的吸引力（如认可度）仅在长期影响价格，表明市场认知形成缓慢；3）标普500指数对比特币、以太坊和莱特币有微弱的长期正向影响，但短期影响不显著（除比特币在10%显著性水平下为负向）；4）误差修正模型显示各币种均存在长期均衡关系，收敛速度在10.20%-23.68%之间。",
    "fetch_date": "2026-01-03",
    "id": "20260103_72eb24b5"
  },
  {
    "title": "Beta-Dependent Gamma Feedback and Endogenous Volatility Amplification in Option Markets",
    "url": "https://arxiv.org/pdf/2511.22766v1",
    "source": "ArXiv",
    "date": "2025-11-27",
    "abstract": "We develop a theoretical framework that aims to link micro-level option hedging and stock-specific factor exposure with macro-level market turbulence and explain endogenous volatility amplification during gamma-squeeze events. By explicitly modeling market-maker delta-neutral hedging and incorporating beta-dependent volatility normalization, we derive a stability condition that characterizes the onset of a gamma-squeeze event. The model captures a nonlinear recursive feedback loop between market-maker hedging and price movements and the resulting self-reinforcing dynamics. From a complex-systems perspective, the dynamics represent a bounded nonlinear response in which effective gain depends jointly on beta-normalized shock perception and gamma-scaled sensitivity. Our analysis highlights that low-beta stocks exhibit disproportionately strong feedback even for modest absolute price movements.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个理论框架，旨在将微观层面的期权对冲和股票特定因子暴露与宏观层面的市场动荡联系起来，解释伽玛挤压事件中的内生波动率放大。通过显式建模做市商的Delta中性对冲并纳入Beta依赖的波动率归一化，推导出表征伽玛挤压事件发生的稳定性条件。模型捕捉了做市商对冲与价格变动之间的非线性递归反馈循环及其产生的自我强化动态。从复杂系统视角看，该动态代表了一种有界非线性响应，其中有效增益同时取决于Beta归一化的冲击感知和伽玛缩放敏感性。分析强调，低Beta股票即使面对温和的绝对价格变动，也会表现出不成比例的强烈反馈效应。",
    "fetch_date": "2026-01-03",
    "id": "20260103_40682348"
  },
  {
    "title": "The Risk-Adjusted Intelligence Dividend: A Quantitative Framework for Measuring AI Return on Investment Integrating ISO 42001 and Regulatory Exposure",
    "url": "https://arxiv.org/pdf/2511.21975v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Organizations investing in artificial intelligence face a fundamental challenge: traditional return on investment calculations fail to capture the dual nature of AI implementations, which simultaneously reduce certain operational risks while introducing novel exposures related to algorithmic malfunction, adversarial attacks, and regulatory liability. This research presents a comprehensive financial framework for quantifying AI project returns that explicitly integrates changes in organizational risk profiles. The methodology addresses a critical gap in current practice where investment decisions rely on optimistic benefit projections without accounting for the probabilistic costs of AI-specific threats including model drift, bias-related litigation, and compliance failures under emerging regulations such as the European Union Artificial Intelligence Act and ISO/IEC 42001. Drawing on established risk quantification methods, including annual loss expectancy calculations and Monte Carlo simulation techniques, this framework enables practitioners to compute net benefits that incorporate both productivity gains and the delta between pre-implementation and post-implementation risk exposures. The analysis demonstrates that accurate AI investment evaluation requires explicit modeling of control effectiveness, reserve requirements for algorithmic failures, and the ongoing operational costs of maintaining model performance. Practical implications include specific guidance for establishing governance structures, conducting phased validations, and integrating risk-adjusted metrics into capital allocation decisions, ultimately enabling evidence-based AI portfolio management that satisfies both fiduciary responsibilities and regulatory mandates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "组织投资人工智能面临一个根本性挑战：传统的投资回报率计算未能捕捉AI实施的双重性——在降低某些运营风险的同时，引入了与算法故障、对抗性攻击和监管责任相关的新型风险敞口。本研究提出了一个全面的财务框架，用于量化AI项目回报，明确整合组织风险状况的变化。该方法解决了当前实践中的一个关键空白，即投资决策依赖于乐观的效益预测，而未考虑AI特定威胁（包括模型漂移、偏见相关诉讼以及欧盟人工智能法案和ISO/IEC 42001等新兴法规下的合规失败）的概率成本。借鉴已建立的风险量化方法，包括年度损失预期计算和蒙特卡洛模拟技术，该框架使从业者能够计算净效益，既包含生产力收益，也包含实施前和实施后风险敞口之间的差异。",
    "fetch_date": "2026-01-03",
    "id": "20260103_dbd49bab"
  },
  {
    "title": "Extended Convolution Bounds on the Fréchet Problem: Robust Risk Aggregation and Risk Sharing",
    "url": "https://arxiv.org/pdf/2511.21929v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "In this paper, we provide extended convolution bounds for the Fréchet problem and discuss related implications in quantitative risk management. First, we establish a new form of inequality for the Range-Value-at-Risk (RVaR). Based on this inequality, we obtain bounds for robust risk aggregation with dependence uncertainty for (i) RVaR, (ii) inter-RVaR difference and (iii) inter-quantile difference, and provide sharpness conditions. These bounds are called extended convolution bounds, which not only complement the results in the literature (convolution bounds in Blanchet et al. (2025)) but also offer results for some variability measures. Next, applying the above inequality, we study the risk sharing for the averaged quantiles (corresponding to risk sharing for distortion risk measures with special inverse S-shaped distortion functions), which is a non-convex optimization problem. We obtain the expression of the minimal value of the risk sharing and the explicit expression for the corresponding optimal allocation, which is comonotonic risk sharing for large losses and counter-comonotonic risk sharing for small losses or large gains. Finally, we explore the dependence structure for the optimal allocations, showing that the optimal allocation does not exist if the risk is not bounded from above.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对Fréchet问题提出了扩展卷积界，并探讨了其在量化风险管理中的应用。首先，建立了Range-Value-at-Risk（RVaR）的新不等式，并基于此获得了在依赖不确定性下关于（i）RVaR、（ii）RVaR间差值及（iii）分位数间差值的稳健风险聚合界，同时给出了锐度条件。这些扩展卷积界不仅补充了现有文献结果，还为一些变异性度量提供了结论。其次，应用上述不等式研究了平均分位数的风险分担问题（对应于具有特殊逆S形扭曲函数的扭曲风险度量的风险分担），这是一个非凸优化问题。我们获得了风险分担的最小值表达式及相应最优分配的显式表达式，即对大损失采用共单调风险分担，对小损失或大收益采用反共单调风险分担。最后，探讨了相关依赖结构。",
    "fetch_date": "2026-01-03",
    "id": "20260103_a2552928"
  },
  {
    "title": "Informative Risk Measures in the Banking Industry: A Proposal based on the Magnitude-Propensity Approach",
    "url": "https://arxiv.org/pdf/2511.21556v1",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Despite decades of research in risk management, most of the literature has focused on scalar risk measures (like e.g. Value-at-Risk and Expected Shortfall). While such scalar measures provide compact and tractable summaries, they provide a poor informative value as they miss the intrinsic multivariate nature of risk.To contribute to a paradigmatic enhancement, and building on recent theoretical work by Faugeras and Pagés (2024), we propose a novel multivariate representation of risk that better reflects the structure of potential portfolio losses, while maintaining desirable properties of interpretability and analytical coherence. The proposed framework extends the classical frequency-severity approach and provides a more comprehensive characterization of extreme events. Several empirical applications based on real-world data demonstrate the feasibility, robustness and practical relevance of the methodology, suggesting its potential for both regulatory and managerial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "尽管风险管理研究已有数十年，但多数文献聚焦于标量风险度量（如风险价值和预期缺口）。虽然这些标量度量提供了简洁易处理的总结，但其信息价值有限，因为它们忽略了风险固有的多元性。为促进范式改进，并基于Faugeras和Pagés（2024）的最新理论工作，本文提出了一种新颖的多元风险表示方法，能更好地反映潜在投资组合损失的结构，同时保持可解释性和分析一致性的理想特性。该框架扩展了经典的频率-严重性方法，提供了对极端事件的更全面刻画。基于真实数据的多个实证应用证明了该方法的可行性、稳健性和实际相关性，表明其在监管和管理应用中的潜力。",
    "fetch_date": "2026-01-03",
    "id": "20260103_200d1274"
  },
  {
    "title": "The Quantum Network of Assets: A Non-Classical Framework for Market Correlation and Structural Risk",
    "url": "https://arxiv.org/pdf/2511.21515v2",
    "source": "ArXiv",
    "date": "2025-11-26",
    "abstract": "Classical correlation matrices capture only linear and pairwise co-movements, leaving higher-order, nonlinear, and state-dependent interactions of financial markets unrepresented. This paper introduces the Quantum Network of Assets (QNA), a density-matrix based framework that embeds cross-asset dependencies into a quantum-information representation. The approach does not assume physical quantum effects but uses the mathematical structure of density operators, entropy, and mutual information to describe market organisation at a structural level.\n  Within this framework we define two structural measures: the Entanglement Risk Index (ERI), which summarises global non-separability and the compression of effective market degrees of freedom, and the Quantum Early-Warning Signal (QEWS), which tracks changes in entropy to detect latent information build-up. These measures reveal dependency geometry that classical covariance-based tools cannot capture.\n  Using NASDAQ-100 data from 2024-2025, we show that quantum entropy displays smoother evolution and clearer regime distinctions than classical entropy, and that ERI rises during periods of structural tightening even when volatility remains low. Around the 2025 US tariff announcement, QEWS shows a marked pre-event increase in structural tension followed by a sharp collapse after the announcement, indicating that structural transitions can precede price movements without implying predictive modelling.\n  QNA therefore provides a structural diagnostic of market fragility, regime shifts, and latent information flow. The framework suggests new directions for systemic risk research by linking empirical asset networks with tools from quantum information theory.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "经典相关性矩阵仅捕捉线性和成对的共动，未能体现金融市场的高阶、非线性和状态依赖的相互作用。本文引入资产量子网络（QNA），一种基于密度矩阵的框架，将跨资产依赖性嵌入量子信息表示中。该方法不假设物理量子效应，而是利用密度算子、熵和互信息的数学结构来描述市场在结构层面的组织。在此框架内，我们定义了两个结构性度量：纠缠风险指数（ERI），用于总结全局不可分性和有效市场自由度的压缩；以及量子早期预警信号（QEWS），通过追踪熵的变化来检测潜在信息的积累。这些度量揭示了基于经典协方差的工具无法捕捉的依赖性几何结构。使用2024-2025年的纳斯达克100指数数据，我们表明量子熵比经典熵展现出更平滑的演化和更清晰的制度区分，并且ERI在结构紧缩期间上升，即使波动率保持不变。",
    "fetch_date": "2026-01-03",
    "id": "20260103_ee106c42"
  },
  {
    "title": "Algorithmic trading and ai: A review of strategies and market impact",
    "url": "https://www.researchgate.net/profile/Titilola-Falaiye/publication/378548435_Algorithmic_Trading_and_AI_A_Review_of_Strategies_and_Market_Impact/links/65e60893e7670d36abfd1738/Algorithmic-Trading-and-AI-A-Review-of-Strategies-and-Market-Impact.pdf",
    "source": "Scholar",
    "date": "2026-01-03",
    "abstract": "… This review seeks to delve into the intricate strategies employed in algorithmic trading, … redefined these strategies. Furthermore, it aims to unravel the impact of algorithmic trading on …",
    "broker": "Google Scholar",
    "score": 4,
    "summary": "该论文综述了算法交易中采用的复杂策略，并探讨了人工智能（特别是机器学习）如何重新定义这些策略。同时，文章旨在揭示算法交易对市场的影响，包括市场效率、流动性和波动性等方面。",
    "fetch_date": "2026-01-03",
    "id": "20260103_fc1d08db"
  },
  {
    "title": "Retail Investor Horizon and Earnings Announcements",
    "url": "https://arxiv.org/pdf/2512.00280v2",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "This paper moves beyond aggregate measures of retail intensity to explore investment horizon as a distinguishing feature of earnings-related return patterns. Using self-reported holding periods from StockTwits (2010-2021), we observe that separating retail activity into \"long-horizon\" and \"short-horizon\" cohorts reveals divergent price anomalies. Long-horizon composition is associated with underreaction, characterized by larger initial reactions and pronounced Post-Earnings Announcement Drift (PEAD), suggesting a slow but persistent convergence toward fundamental value. In contrast, short-horizon activity parallels sentiment-driven overreaction, where elevated pre-event sentiment precedes weaker subsequent performance and price reversals. A zero-cost strategy exploiting this heterogeneity, going long on long-horizon stocks and short on short-horizon stocks, yields risk-adjusted alphas of 0.43% per month. These findings suggest that accounting for investment horizon helps disentangles the fundamental signal in retail flow from speculative noise.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文超越零售投资者整体参与度的衡量，通过投资期限（基于StockTwits 2010-2021年自报持有期）区分“长期”与“短期”零售投资者活动，揭示了财报公告前后不同的价格异常模式。长期投资者活动与反应不足相关，表现为较大的初始反应和显著的财报公告后漂移（PEAD），暗示向基本面价值的缓慢持续收敛；而短期投资者活动则与情绪驱动的过度反应相关，表现为事件前情绪高涨、后续表现疲软及价格反转。利用这种异质性构建的零成本策略（做多长期股票、做空短期股票）每月可获得0.43%的风险调整后阿尔法收益。这表明，考虑投资期限有助于从零售资金流中分离基本面信号与投机噪音。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b70a95e0"
  },
  {
    "title": "Forward-Oriented Causal Observables for Non-Stationary Financial Markets",
    "url": "https://arxiv.org/pdf/2512.24621v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We study short-horizon forecasting in financial time series under strict causal constraints, treating the market as a non-stationary stochastic system in which any predictive observable must be computable online from information available up to the decision time. Rather than proposing a machine-learning predictor or a direct price-forecast model, we focus on \\emph{constructing} an interpretable causal signal from heterogeneous micro-features that encode complementary aspects of the dynamics (momentum, volume pressure, trend acceleration, and volatility-normalized price location). The construction combines (i) causal centering, (ii) linear aggregation into a composite observable, (iii) causal stabilization via a one-dimensional Kalman filter, and (iv) an adaptive ``forward-like'' operator that mixes the composite signal with a smoothed causal derivative term. The resulting observable is mapped into a transparent decision functional and evaluated through realized cumulative returns and turnover. An application to high-frequency EURUSDT (1-minute) illustrates that causally constructed observables can exhibit substantial economic relevance in specific regimes, while degrading under subsequent regime shifts, highlighting both the potential and the limitations of causal signal design in non-stationary markets.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们研究在严格因果约束下的金融时间序列短期预测，将市场视为一个非平稳随机系统，其中任何预测性可观测值必须能够在线从决策时间前可获得的信息中计算得出。我们并非提出一个机器学习预测器或直接的价格预测模型，而是专注于从编码动态互补方面（动量、成交量压力、趋势加速和波动率归一化价格位置）的异构微观特征中构建一个可解释的因果信号。该构建结合了（i）因果中心化，（ii）线性聚合为复合可观测值，（iii）通过一维卡尔曼滤波器进行因果稳定化，以及（iv）一个将复合信号与平滑因果导数项混合的自适应“前向”算子。所得可观测值被映射到一个透明的决策函数中，并通过实现的累积收益和换手率进行评估。对高频EURUSDT（1分钟）的应用表明，因果构建的可观测值在特定制度下可展现出显著的经济相关性，而在次优条件下则会退化。",
    "fetch_date": "2026-01-02",
    "id": "20260102_fb4c17f1"
  },
  {
    "title": "Generative AI-enhanced Sector-based Investment Portfolio Construction",
    "url": "https://arxiv.org/pdf/2512.24526v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025).\n  Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency.\n  This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了如何将领先提供商（OpenAI、Google、Anthropic、DeepSeek和xAI）的大型语言模型（LLMs）应用于量化行业投资组合构建。研究使用LLMs识别标普500行业指数中的可投资股票范围，并评估其选股结果与经典投资组合优化方法结合后的表现。每个模型被提示为每个行业选择和加权20只股票，结果投资组合在两个不同的样本外时期（稳定市场阶段：2025年1月至3月，波动阶段：2025年4月至6月）与其相应的行业指数进行比较。结果显示，LLM投资组合表现具有强烈的时间依赖性：在稳定市场条件下，LLM加权投资组合在累计回报和风险调整（夏普比率）指标上经常优于行业指数；但在波动时期，许多LLM投资组合表现不佳，表明当前模型可能难以适应其训练数据中代表性不足的制度转换或高波动环境。重要的是，当基于LLM的选股与传统优化方法结合时，表现有所改善。",
    "fetch_date": "2026-01-02",
    "id": "20260102_388c3be1"
  },
  {
    "title": "Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem",
    "url": "https://arxiv.org/pdf/2512.24251v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于深度强化学习（DRL）的方法，用于解决车队规模与混合车辆路径问题（FSMVRP）。该方法将问题建模为马尔可夫决策过程（MDP），并开发了一种名为FRIPN的新型策略网络，能够同时处理车队组合和路径规划决策。论文通过随机生成实例和基准数据集进行了全面实验，结果表明该方法能在几秒内生成接近最优的解决方案，适用于短期车辆租赁和按需物流等实际场景。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6fbbf264"
  },
  {
    "title": "Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach",
    "url": "https://arxiv.org/pdf/2512.00243v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "Our work investigates the economic efficiency of the prevailing \"ladder-step\" investment strategy in oil and gas exploration, which advocates for the incremental acquisition of geological information throughout the project lifecycle. By employing a multi-agent Deep Reinforcement Learning (DRL) framework, we model an alternative strategy that prioritizes the early acquisition of high-quality information assets. We simulate the entire upstream value chain-comprising competitive bidding, exploration, and development phases-to evaluate the economic impact of this approach relative to traditional methods. Our results demonstrate that front-loading information investment significantly reduces the costs associated with redundant data acquisition and enhances the precision of reserve valuation. Specifically, we find that the alternative strategy outperforms traditional methods in highly competitive environments by mitigating the \"winner's curse\" through more accurate bidding. Furthermore, the economic benefits are most pronounced during the development phase, where superior data quality minimizes capital misallocation. These findings suggest that optimal investment timing is structurally dependent on market competition rather than solely on price volatility, offering a new paradigm for capital allocation in extractive industries.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究通过多智能体深度强化学习（DRL）框架，评估了石油天然气勘探领域传统“阶梯式”信息资产投资策略的经济效率，并提出了一种优先早期获取高质量信息资产的替代策略。模拟上游价值链（包括竞争性投标、勘探和开发阶段）显示，该替代策略能显著降低冗余数据采集成本，提高储量评估精度，在高度竞争环境中通过更精准的投标缓解“赢家诅咒”，并在开发阶段因数据质量优化而减少资本错配，表明信息投资时机优化具有结构性经济价值。",
    "fetch_date": "2026-01-02",
    "id": "20260102_b7002262"
  },
  {
    "title": "Stochastic factors can matter: improving robust growth under ergodicity",
    "url": "https://arxiv.org/pdf/2512.24906v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strategies obtained from portfolio optimization are very sensitive to them. To mitigate this well-known phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price process $X$, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of admissible models allows $X$ to depend on a multivariate stochastic factor $Y$ and fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor process $Y$. A principal motivation of this framework comes from pairs trading, where $X$ is the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certain partial differential equation (PDE). We demonstrate that utilizing the stochastic factor leads to improvement in robust growth complementing the conclusions of the previous study by Itkin et. al. (arXiv:2211.15628 [q-fin.MF], forthcoming in $\\textit{Finance and Stochastics}$), which additionally robustified the dynamics of the stochastic factor leading to $Y$-independent optimal strategies. Our analysis leads to new financial insights, quantifying the improvement in growth the investor can achieve by optimally incorporating stochastic factors into their trading decisions. We illustrate our theoretical results on several numerical examples including an application to pairs trading.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文针对资产收益率漂移难以准确建模且投资组合优化策略对其高度敏感的问题，研究了在高维不完全市场中、资产价格过程X存在漂移不确定性下的稳健增长优化，并引入了遍历性假设。该框架允许X依赖于多元随机因子Y，并固定了(a)联合波动结构、(b)长期联合遍历密度和(c)随机因子过程Y的动力学。主要动机来自配对交易，其中X为价差过程。研究确定了稳健最优增长率，构建了最坏情况下的可接受模型，并通过求解特定偏微分方程(PDE)来表征稳健增长最优策略。结果表明，利用随机因子可提升稳健增长，补充了先前研究的结论。",
    "fetch_date": "2026-01-02",
    "id": "20260102_7acb0abd"
  },
  {
    "title": "Signature approach for pricing and hedging path-dependent options with frictions",
    "url": "https://arxiv.org/pdf/2511.23295v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "We introduce a novel signature approach for pricing and hedging path-dependent options with instantaneous and permanent market impact under a mean-quadratic variation criterion. Leveraging the expressive power of signatures, we recast an inherently nonlinear and non-Markovian stochastic control problem into a tractable form, yielding hedging strategies in (possibly infinite) linear feedback form in the time-augmented signature of the control variables, with coefficients characterized by non-standard infinite-dimensional Riccati equations on the extended tensor algebra. Numerical experiments demonstrate the effectiveness of these signature-based strategies for pricing and hedging general path-dependent payoffs in the presence of frictions. In particular, market impact naturally smooths optimal trading strategies, making low-truncated signature approximations highly accurate and robust in frictional markets, contrary to the frictionless case.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种新颖的签名方法，用于在均值-二次变分准则下，对具有瞬时和永久市场摩擦影响的路径依赖期权进行定价和对冲。利用签名的表达能力，我们将一个本质非线性且非马尔可夫性的随机控制问题转化为可处理的形式，得到了以控制变量的时间增强签名的（可能无限）线性反馈形式表示的对冲策略，其系数由扩展张量代数上的非标准无限维Riccati方程表征。数值实验证明了这些基于签名的策略在存在摩擦的情况下，对一般路径依赖收益进行定价和对冲的有效性。特别是，市场摩擦自然地平滑了最优交易策略，使得低阶截断签名近似在高摩擦市场中具有高精度和鲁棒性，这与无摩擦情况相反。",
    "fetch_date": "2026-01-02",
    "id": "20260102_647600a8"
  },
  {
    "title": "Boundary error control for numerical solution of BSDEs by the convolution-FFT method",
    "url": "https://arxiv.org/pdf/2512.24714v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文首先回顾了基于卷积快速傅里叶变换（CFFT）的倒向随机微分方程（BSDE）数值解法，随后提出了一种改进期权定价中边界误差的方法。通过优化原方法中的阻尼与平移方案，将目标函数转化为有界周期函数以适用傅里叶变换，其中时变平移方案显著降低了边界误差。数值实验与误差分析验证了改进后方法在精度和收敛性上的提升。",
    "fetch_date": "2026-01-02",
    "id": "20260102_1ef00a6e"
  },
  {
    "title": "Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.24580v1",
    "source": "ArXiv",
    "date": "2025-12-31",
    "abstract": "We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种用于风险敏感强化学习（RSRL）的新框架，该框架结合了对状态转移不确定性的鲁棒性。作者定义了两个不同但耦合的风险度量：一个内部风险度量处理状态和成本的随机性，一个外部风险度量捕捉状态转移动态的不确定性。该框架通过允许对内部和外部风险度量使用一般的相干风险度量，统一并推广了大多数现有的强化学习框架。在此框架内，作者构建了一个风险敏感鲁棒马尔可夫决策过程（RSRMDP），推导了其贝尔曼方程，并在给定后验分布下提供了误差分析。作者进一步开发了一种贝叶斯动态规划（Bayesian DP）算法，该算法在后验更新和价值迭代之间交替进行。该方法采用了一个基于风险的贝尔曼算子估计器，该估计器结合了蒙特卡洛采样和凸优化，作者为此证明了强一致性保证。此外，作者证明了该算法在训练环境中收敛到接近最优的策略，并在狄利克雷后验和条件风险价值（CVaR）下分析了样本复杂度和计算复杂度。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9e6c8c0d"
  },
  {
    "title": "Minimal Solutions to the Skorokhod Reflection Problem Driven by Jump Processes and an Application to Reinsurance",
    "url": "https://arxiv.org/pdf/2512.24491v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider a reflected process in the positive orthant driven by an exogenous jump process. For a given input process, we show that there exists a unique minimal strong solution to the given particle system up until a certain maximal stopping time, which is stated explicitly in terms of the dual formulation of a linear programming problem associated with the state of the system. We apply this model to study the ruin time of interconnected insurance firms, where the stopping time can be interpreted as the failure time of a reinsurance agreement between the firms. Our work extends the analysis of the particle system in Baker, Hambly, and Jettkant (2025) to the case of jump driving processes, and the existence result of Reiman (1984) beyond the case of sub-stochastic reflection matrices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了由外生跳跃过程驱动的正象限反射过程。对于给定的输入过程，我们证明了在某个最大停时之前，该粒子系统存在唯一的最小强解，该停时通过系统状态相关的线性规划问题的对偶形式明确给出。我们将该模型应用于研究互联保险公司的破产时间，其中停时可解释为公司间再保险协议的失效时间。我们的工作将Baker、Hambly和Jettkant（2025）对粒子系统的分析扩展到跳跃驱动过程的情况，并将Reiman（1984）的存在性结果推广到非次随机反射矩阵的情形。",
    "fetch_date": "2026-01-02",
    "id": "20260102_230af948"
  },
  {
    "title": "Utility Maximisation with Model-independent Constraints",
    "url": "https://arxiv.org/pdf/2512.24371v1",
    "source": "ArXiv",
    "date": "2025-12-30",
    "abstract": "We consider an agent who has access to a financial market, including derivative contracts, who looks to maximise her utility. Whilst the agent looks to maximise utility over one probability measure, or class of probability measures, she must also ensure that the mark-to-market value of her portfolio remains above a given threshold. When the mark-to-market value is based on a more pessimistic valuation method, such as model-independent bounds, we recover a novel optimisation problem for the agent where the agents investment problem must satisfy a pathwise constraint.\n  For complete markets, the expression of the optimal terminal wealth is given, using the max-plus decomposition for supermartingales. Moreover, for the Black-Scholes-Merton model the explicit form of the process involved in such decomposition is obtained, and we are able to investigate numerically optimal portfolios in the presence of options which are mispriced according to the agent's beliefs.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文探讨了一个在金融市场（包括衍生品合约）中寻求效用最大化的代理人问题。代理人虽然在一个概率测度或一类概率测度下最大化效用，但必须同时确保其投资组合的盯市价值维持在给定阈值之上。当盯市价值基于更悲观的估值方法（如模型无关边界）时，我们为代理人推导出一个新颖的优化问题，其中投资问题必须满足路径约束。对于完全市场，利用超鞅的最大加分解给出了最优终端财富的表达式。此外，对于Black-Scholes-Merton模型，我们获得了该分解中涉及过程的显式形式，并能够数值研究在代理人认为期权被错误定价情况下的最优投资组合。",
    "fetch_date": "2026-01-02",
    "id": "20260102_de031d92"
  },
  {
    "title": "A Test of Lookahead Bias in LLM Forecasts",
    "url": "https://arxiv.org/pdf/2512.23847v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We develop a statistical test to detect lookahead bias in economic forecasts generated by large language models (LLMs). Using state-of-the-art pre-training data detection techniques, we estimate the likelihood that a given prompt appeared in an LLM's training corpus, a statistic we term Lookahead Propensity (LAP). We formally show that a positive correlation between LAP and forecast accuracy indicates the presence and magnitude of lookahead bias, and apply the test to two forecasting tasks: news headlines predicting stock returns and earnings call transcripts predicting capital expenditures. Our test provides a cost-efficient, diagnostic tool for assessing the validity and reliability of LLM-generated forecasts.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们开发了一种统计检验方法，用于检测大型语言模型（LLMs）生成的经济预测中的前瞻性偏差。利用最先进的预训练数据检测技术，我们估计给定提示出现在LLM训练语料库中的可能性，这一统计量我们称为前瞻倾向（LAP）。我们正式证明，LAP与预测准确性之间的正相关表明前瞻性偏差的存在及其程度，并将该检验应用于两个预测任务：新闻标题预测股票收益和财报电话会议记录预测资本支出。我们的检验为评估LLM生成预测的有效性和可靠性提供了一种成本效益高的诊断工具。",
    "fetch_date": "2026-01-02",
    "id": "20260102_9a4a2330"
  },
  {
    "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations",
    "url": "https://arxiv.org/pdf/2512.00249v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "在支持兵棋推演的作战模拟领域，智能体开发长期以基于规则的脚本方法为主，深度强化学习（RL）方法近期才被引入。脚本智能体在受控环境中提供可预测性和一致性，但在动态复杂场景中因其固有僵化性而表现不足。相反，RL智能体在适应性和学习方面表现优异，在处理意外情况方面具有改进潜力，但面临重大挑战，如黑盒决策过程和在大型模拟环境中的可扩展性问题。本文提出了一种新颖的分层混合人工智能（AI）方法，将脚本智能体的可靠性和可预测性与RL的动态自适应学习能力相结合。通过分层构建AI系统，该方法旨在利用脚本智能体处理常规战术级决策，利用RL智能体处理更高级别的战略决策，从而在利用各自优势的同时解决每种方法的局限性。",
    "fetch_date": "2026-01-02",
    "id": "20260102_f8dabb31"
  },
  {
    "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions",
    "url": "https://arxiv.org/pdf/2512.00142v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究介绍了DeFi TrustBoost框架，该框架结合区块链技术和可解释AI，旨在解决贷款机构在审批来自低收入家庭的小企业贷款申请时面临的挑战。该框架强调满足区块链和AI系统的四个关键要求：机密性、数据保护法规合规性、对抗攻击抵抗力和监管审计合规性。它提出了一种用于自动化AI决策防篡改审计的技术，以及一种链上（区块链内）和链下数据存储策略，以促进金融组织内部及跨组织的协作。",
    "fetch_date": "2026-01-02",
    "id": "20260102_82b264a1"
  },
  {
    "title": "Responsible LLM Deployment for High-Stake Decisions by Decentralized Technologies and Human-AI Interactions",
    "url": "https://arxiv.org/pdf/2512.04108v1",
    "source": "ArXiv",
    "date": "2025-11-28",
    "abstract": "High-stakes decision domains are increasingly exploring the potential of Large Language Models (LLMs) for complex decision-making tasks. However, LLM deployment in real-world settings presents challenges in data security, evaluation of its capabilities outside controlled environments, and accountability attribution in the event of adversarial decisions. This paper proposes a framework for responsible deployment of LLM-based decision-support systems through active human involvement. It integrates interactive collaboration between human experts and developers through multiple iterations at the pre-deployment stage to assess the uncertain samples and judge the stability of the explanation provided by post-hoc XAI techniques. Local LLM deployment within organizations and decentralized technologies, such as Blockchain and IPFS, are proposed to create immutable records of LLM activities for automated auditing to enhance security and trace back accountability. It was tested on Bert-large-uncased, Mistral, and LLaMA 2 and 3 models to assess the capability to support responsible financial decisions on business lending.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "论文《通过去中心化技术和人机交互实现高风险决策的负责任大语言模型部署》探讨了在金融等高风险决策领域部署大语言模型（LLMs）的挑战，包括数据安全、模型能力评估和决策问责。提出一个负责任部署框架，强调人类专家在部署前阶段的主动参与，通过多轮迭代评估不确定样本和事后可解释人工智能（XAI）技术的稳定性。建议在组织内部署本地LLMs，并利用区块链和IPFS等去中心化技术创建不可篡改的记录，以增强安全性和可追溯性。该框架在Bert-large-uncased、Mistral和LLaMA 2/3模型上进行了测试，用于评估其在商业贷款等金融决策中的支持能力。",
    "fetch_date": "2026-01-02",
    "id": "20260102_6751ad03"
  },
  {
    "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
    "url": "https://arxiv.org/pdf/2512.01565v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种名为Deep FlexQP的加速非线性规划方法，通过深度展开技术改进QP优化器。该方法基于对QP约束的精确松弛，无论原始约束是否可行，都能找到最优解或最小化约束违反的稀疏解。Deep FlexQP具有良好的可扩展性、鲁棒性和热启动能力，通过数据驱动技术学习维度无关的反馈策略，能够泛化到更大维度的问题，并在投资组合优化等基准测试中优于现有方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2e864486"
  },
  {
    "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets",
    "url": "https://arxiv.org/pdf/2512.10971v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AI-Trader：实时金融市场中自主智能体的基准测试》提出首个完全自动化、实时且数据无污染的LLM智能体金融决策评估基准。该基准覆盖美股、A股和加密货币三大市场，采用多粒度交易模拟实时金融环境，并引入革命性的全自主最小信息范式——智能体仅接收基本上下文，需独立搜索、验证和整合实时市场信息。研究评估了六种主流LLM，旨在解决动态实时环境中决策制定的挑战，对量化交易实战具有直接应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23a72d9d"
  },
  {
    "title": "Autodeleveraging: Impossibilities and Optimization",
    "url": "https://arxiv.org/pdf/2512.01112v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by $\\approx 28\\times$ relative to our optimal policy, imposing roughly \\$653 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "自动去杠杆化（ADL）是永续期货交易所的最后损失社会化机制，在维持偿付能力的清算失败时触发。本文首次对ADL进行严格建模，证明了ADL机制面临一个根本性的“三难困境”：没有任何政策能同时满足交易所的偿付能力、收入和交易者的公平性。这一不可能定理意味着，随着参与规模扩大，一种新型的“道德风险”会渐近增长，使得“零损失”社会化变得不可能。建设性地，本文展示了三类ADL机制可以最优地应对这一三难困境，提供公平性、对价格冲击的鲁棒性以及交易所收入最大化。通过分析2025年10月10日Hyperliquid数据集（当时ADL在12分钟内反复用于平仓21亿美元头寸），本文实证比较了这些ADL机制与实践中使用的标准方法。",
    "fetch_date": "2026-01-01",
    "id": "20260101_dd83332c"
  },
  {
    "title": "Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model",
    "url": "https://arxiv.org/pdf/2512.00630v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文评估了基于Qwen3-8B模型通过rLoRA微调进行金融文本分类的性能。该方法采用噪声嵌入指令微调增强鲁棒性，结合秩稳定低秩优化与FlashAttention提升训练效率，适用于金融情感分析和新闻分类等量化交易相关任务。研究对比了传统Transformer模型及多个大模型，展示了该技术在实战交易系统中的潜在应用价值。",
    "fetch_date": "2026-01-01",
    "id": "20260101_f74585e2"
  },
  {
    "title": "An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts",
    "url": "https://arxiv.org/pdf/2512.00916v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "评估罕见事件预测具有挑战性，因为标准指标在事件发生率下降时会失效。F1分数、AUPRC、MCC和准确率等指标会导致退化阈值（收敛于0或1），其值被类别不平衡而非尾部判别能力主导。我们开发了一族罕见事件稳定（RES）指标，其最优阈值在事件概率趋近于零时仍保持严格内部性，确保在极端罕见情况下的连贯决策规则。模拟显示RES指标保持稳定阈值、一致的模型排名和近乎完全的流行度不变性，而传统指标则表现出统计显著的阈值漂移和结构崩溃。信用违约应用证实了这些结果：RES指标产生可解释的违约概率阈值（4-9%），并在子采样下保持稳健，而经典指标在操作上失效。RES框架为评估极端风险预测提供了原则性的、流行度不变的基础。",
    "fetch_date": "2026-01-01",
    "id": "20260101_70ccf6da"
  },
  {
    "title": "Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election",
    "url": "https://arxiv.org/pdf/2512.00893v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "We study how the 2024 U.S. presidential election, viewed as a major political risk event, affected cryptocurrency markets by distinguishing human-driven peer-to-peer stablecoin transactions from automated algorithmic activity. Using structural break analysis, we find that human-driven Ethereum Request for Comment 20 (ERC-20) transactions shifted on November 3, two days before the election, while exchange trading volumes reacted only on Election Day. Automated smart-contract activity adjusted much later, with structural breaks appearing in January 2025. We validate these shifts using surrogate-based robustness tests. Complementary energy-spectrum analysis of Bitcoin and Ethereum identifies pronounced post-election turbulence, and a structural vector autoregression confirms a regime shift in stablecoin dynamics. Overall, human-driven stablecoin flows act as early-warning indicators of political stress, preceding both exchange behavior and algorithmic responses.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本研究探讨了2024年美国总统大选这一重大政治风险事件对加密货币市场的影响，通过区分人类驱动的点对点稳定币交易与自动化算法活动。利用结构断点分析发现，人类驱动的ERC-20交易在选举前两天（11月3日）已出现变化，而交易所交易量仅在选举日才反应。自动化智能合约活动调整更晚，结构断点出现在2025年1月。通过替代性稳健性检验验证了这些变化。对比特币和以太坊的互补能量谱分析识别出显著的选举后市场波动，结构向量自回归模型确认了稳定币动态中的制度转变。总体而言，人类驱动的稳定币流动可作为政治压力的早期预警指标，领先于交易所行为和算法响应。",
    "fetch_date": "2026-01-01",
    "id": "20260101_1eadb97f"
  },
  {
    "title": "Efficient Calibration in the rough Bergomi model by Wasserstein distance",
    "url": "https://arxiv.org/pdf/2512.00448v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Despite the empirical success in modeling volatility of the rough Bergomi (rBergomi) model, it suffers from pricing and calibration difficulties stemming from its non-Markovian structure. To address this, we propose a comprehensive computational framework that enhances both simulation and calibration. First, we develop a modified Sum-of-Exponentials (mSOE) Monte Carlo scheme which hybridizes an exact simulation of the singular kernel near the origin with a multi-factor approximation for the remainder. This method achieves high accuracy, particularly for out-of-the-money options, with an $\\mathcal{O}(n)$ computational cost. Second, based on this efficient pricing engine, we then propose a distribution-matching calibration scheme by using Wasserstein distance as the optimization objective. This leverages a minimax formulation against Lipschitz payoffs, which effectively distributes pricing errors and improving robustness. Our numerical results confirm the mSOE scheme's convergence and demonstrate that the calibration algorithm reliably identifies model parameters and generalizes well to path-dependent options, which offers a powerful and generic tool for practical model fitting.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "针对粗糙Bergomi模型在定价和校准上的困难，本文提出了一个综合计算框架。首先，开发了一种改进的指数和蒙特卡洛方案，通过混合原点附近奇异核的精确模拟与剩余部分的多因子近似，实现了高精度（尤其对价外期权）和O(n)计算成本。其次，基于此高效定价引擎，提出了一种使用Wasserstein距离作为优化目标的分布匹配校准方案，通过极小极大公式对抗Lipschitz收益，有效分配定价误差并提高鲁棒性。数值结果验证了方案的收敛性，并表明校准算法能可靠识别模型参数并良好推广至路径依赖期权，为实际模型拟合提供了强大通用工具。",
    "fetch_date": "2026-01-01",
    "id": "20260101_5a90d3c8"
  },
  {
    "title": "Does it take two to tango: Interaction between Credit Default Swaps and National Stock Indices",
    "url": "https://arxiv.org/pdf/2512.07887v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper investigates both short and long-run interaction between BIST-100 index and CDS prices over January 2008 to May 2015 using ARDL technique. The paper documents several findings. First, ARDL analysis shows that 1 TL increase in CDS shrinks BIST-100 index by 22.5 TL in short-run and 85.5 TL in long-run. Second, 1000 TL increase in BIST index price causes 25 TL and 44 TL reducation in Turkey's CDS prices in short- and long-run respectively. Third, a percentage increase in interest rate shrinks BIST index by 359 TL and a percentage increase in inflation rate scales CDS prices up to 13.34 TL both in long-run. In case of short-run, these impacts are limited with 231 TL and 5.73 TL respectively. Fourth, a kurush increase in TL/USD exchange rate leads 24.5 TL (short-run) and 78 TL (long-run) reductions in BIST, while it augments CDS prices by 2.5 TL (short-run) and 3 TL (long-run) respectively. Fifth, each negative political events decreases BIST by 237 TL in short-run and 538 TL in long-run, while it increases CDS prices by 33 TL in short-run and 89 TL in long-run. These findings imply the highly dollar indebted capital structure of Turkish firms, and overly sensitivity of financial markets to the uncertainties in political sphere. Finally, the paper provides evidence for that BIST and CDS with control variables drift too far apart, and converge to a long-run equilibrium at a moderate monthly speed.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文使用ARDL技术研究了2008年1月至2015年5月期间土耳其BIST-100指数与CDS价格之间的短期和长期互动关系。主要发现包括：1）CDS价格每上涨1土耳其里拉，BIST-100指数在短期和长期分别下跌22.5里拉和85.5里拉；2）BIST指数每上涨1000里拉，土耳其CDS价格在短期和长期分别下降25里拉和44里拉；3）利率每上升1个百分点，BIST指数在长期下跌359里拉；通胀率每上升1个百分点，CDS价格在长期上涨13.34里拉；4）土耳其里拉兑美元汇率每上涨1库鲁什，BIST指数在短期和长期分别下跌24.5里拉和78里拉，同时CDS价格分别上涨2.5里拉和3里拉；5）每个负面政治事件使BIST指数在短期和长期分别下跌237里拉和538里拉，同时使CDS价格分别上涨33里拉和89里拉。这些发现揭示了土耳其企业高度美元化的资本结构及其对金融市场的过度敏感性。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2689c421"
  },
  {
    "title": "A Hybrid Architecture for Options Wheel Strategy Decisions: LLM-Generated Bayesian Networks for Transparent Trading",
    "url": "https://arxiv.org/pdf/2512.01123v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Large Language Models (LLMs) excel at understanding context and qualitative nuances but struggle with the rigorous and transparent reasoning required in high-stakes quantitative domains such as financial trading. We propose a model-first hybrid architecture for the options \"wheel\" strategy that combines the strengths of LLMs with the robustness of a Bayesian Network. Rather than using the LLM as a black-box decision-maker, we employ it as an intelligent model builder. For each trade decision, the LLM constructs a context-specific Bayesian network by interpreting current market conditions, including prices, volatility, trends, and news, and hypothesizing relationships among key variables. The LLM also selects relevant historical data from an 18.75-year, 8,919-trade dataset to populate the network's conditional probability tables. This selection focuses on scenarios analogous to the present context. The instantiated Bayesian network then performs transparent probabilistic inference, producing explicit probability distributions and risk metrics to support decision-making. A feedback loop enables the LLM to analyze trade outcomes and iteratively refine subsequent network structures and data selection, learning from both successes and failures. Empirically, our hybrid system demonstrates effective performance on the wheel strategy. Over nearly 19 years of out-of-sample testing, it achieves a 15.3% annualized return with significantly superior risk-adjusted performance (Sharpe ratio 1.08 versus 0.62 for market benchmarks) and dramatically lower drawdown (-8.2% versus -60%) while maintaining a 0% assignment rate through strategic option rolling. Crucially, each trade decision is fully explainable, involving on average 27 recorded decision factors (e.g., volatility level, option premium, risk indicators, market context).",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于期权“轮动”策略决策的混合架构，结合了大型语言模型（LLM）与贝叶斯网络的优势。LLM不直接作为黑盒决策器，而是作为智能模型构建器，根据当前市场条件（如价格、波动率、趋势、新闻）构建情境特定的贝叶斯网络，并假设关键变量间的关系。LLM还从18.75年、8,919笔交易的历史数据集中选取类似情境的数据，填充网络的条件概率表。实例化的贝叶斯网络执行透明的概率推理，生成明确的概率分布和风险指标以支持决策。反馈循环使LLM能分析交易结果并迭代优化后续网络结构。",
    "fetch_date": "2026-01-01",
    "id": "20260101_2daebce1"
  },
  {
    "title": "Arbitrage-Free Option Price Surfaces via Chebyshev Tensor Bases and a Hamiltonian Fog Post-Fit",
    "url": "https://arxiv.org/pdf/2512.01967v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We study the construction of arbitrage-free option price surfaces from noisy bid-ask quotes across strike and maturity. Our starting point is a Chebyshev representation of the call price surface on a warped log-moneyness/maturity rectangle, together with linear sampling and no-arbitrage operators acting on a collocation grid. Static no-arbitrage requirements are enforced as linear inequalities, while the surface is fitted directly to prices via a coverage-seeking quadratic objective that trades off squared band misfit against spectral and transport-inspired regularisation of the Chebyshev coefficients. This yields a strictly convex quadratic program in the modal coefficients, solvable at practical scales with off-the-shelf solvers (OSQP).\n  On top of the global backbone, we introduce a local post-fit layer based on a discrete fog of risk-neutral densities on a three-dimensional lattice (m,t,u) and an associated Hamiltonian-type energy. On each patch of the (m,t) plane, the fog variables are coupled to a nodal price field obtained from the baseline surface, yielding a joint convex optimisation problem that reweights noisy quotes and applies noise-aware local corrections while preserving global static no-arbitrage and locality.\n  The method is designed such that for equity options panels, the combined procedure achieves high inside-spread coverage in stable regimes (in calm years, 98-99% of quotes are priced inside the bid-ask intervals) and low rates of static no-arbitrage violations (below 1%). In stressed periods, the fog layer provides a mechanism for controlled leakage outside the band: when local quotes are mutually inconsistent or unusually noisy, the optimiser allocates fog mass outside the bid-ask tube and justifies small out-of-band deviations of the post-fit surface, while preserving a globally arbitrage-free and well-regularised description of the option surface.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "该论文提出了一种构建无套利期权价格曲面的方法，通过切比雪夫张量基和哈密顿雾后拟合技术。核心包括：使用切比雪夫基在扭曲的对数货币性/期限矩形上表示看涨期权价格曲面，结合线性采样和无套利算子；通过线性不等式强制静态无套利约束，采用覆盖性二次目标函数直接拟合价格，平衡带状误差平方与谱正则化；最终转化为严格凸二次规划问题。在全局基础上，引入基于三维格点风险中性密度雾和哈密顿型能量的局部后拟合层，通过联合凸优化对噪声报价进行重加权和局部修正。该方法理论性强，但未涉及实际交易策略或市场动态，对实战交易的价值主要体现在模型构建层面。",
    "fetch_date": "2026-01-01",
    "id": "20260101_9c7bbf8b"
  },
  {
    "title": "Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections",
    "url": "https://arxiv.org/pdf/2512.01408v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao \\cite{KZ98}-type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文通过贝叶斯分布鲁棒视角重新审视默顿连续时间投资组合选择问题。核心创新在于：在贝叶斯默顿模型中，将单一模糊集置于先验漂移项上（而非传统的时间矩形分布鲁棒控制），从而保留学习能力与可处理性。通过极小极大交换将鲁棒控制简化为优化先验的非线性泛函，并利用Wasserstein不确定性下的显式渐近最优推前映射来表征小半径最坏情况先验。该方法通过针对默顿泛函定制的非线性Wasserstein投影校准模糊半径，实验表明相较于分布鲁棒控制（DRC）减少了悲观性并提升了性能。",
    "fetch_date": "2026-01-01",
    "id": "20260101_582de585"
  },
  {
    "title": "The Endogenous Constraint: Hysteresis, Stagflation, and the Structural Inhibition of Monetary Velocity in the Bitcoin Network (2016-2025)",
    "url": "https://arxiv.org/pdf/2512.07886v1",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "Bitcoin operates as a macroeconomic paradox: it combines a strictly predetermined, inelastic monetary issuance schedule with a stochastic, highly elastic demand for scarce block space. This paper empirically validates the Endogenous Constraint Hypothesis, positing that protocol-level throughput limits generate a non-linear negative feedback loop between network friction and base-layer monetary velocity. Using a verified Transaction Cost Index (TCI) derived from Blockchain.com on-chain data and Hansen's (2000) threshold regression, we identify a definitive structural break at the 90th percentile of friction (TCI ~ 1.63). The analysis reveals a bifurcation in network utility: while the network exhibits robust velocity growth of +15.44% during normal regimes, this collapses to +6.06% during shock regimes, yielding a statistically significant Net Utility Contraction of -9.39% (p = 0.012). Crucially, Instrumental Variable (IV) tests utilizing Hashrate Variation as a supply-side instrument fail to detect a significant relationship in a linear specification (p=0.196), confirming that the velocity constraint is strictly a regime-switching phenomenon rather than a continuous linear function. Furthermore, we document a \"Crypto Multiplier\" inversion: high friction correlates with a +8.03% increase in capital concentration per entity, suggesting that congestion forces a substitution from active velocity to speculative hoarding.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "比特币网络的内生约束：滞后效应、滞胀与货币流通速度的结构性抑制（2016-2025）。本文实证验证了内生约束假说，认为协议层面的吞吐量限制在网络摩擦与基础层货币流通速度之间形成了非线性负反馈循环。通过使用基于Blockchain.com链上数据验证的交易成本指数（TCI）和Hansen（2000）的门槛回归，我们在摩擦的90百分位数（TCI约1.63）处识别出明确的结构性断点。分析揭示了网络效用的分叉：在正常状态下，网络表现出+15.44%的强劲流通速度增长，而在冲击状态下则崩溃至+6.06%，产生了统计显著的净效用收缩-9.39%（p=0.012）。关键的是，利用哈希率变化作为供给侧工具变量的工具变量（IV）测试在线性规范中未检测到显著关系（p=0.196），证实了流通速度约束严格是一种状态转换现象，而非连续线性函数。",
    "fetch_date": "2026-01-01",
    "id": "20260101_95084b35"
  },
  {
    "title": "Equilibrium Investment with Random Risk Aversion: (Non-)uniqueness, Optimality, and Comparative Statics",
    "url": "https://arxiv.org/pdf/2512.00830v2",
    "source": "ArXiv",
    "date": "2025-11-30",
    "abstract": "This paper investigates infinite-dimensional portfolio selection problem under a general distribution of the risk aversion parameter. We provide a complete characterization of all deterministic equilibrium investment strategies. Our results reveal that the solution structure depends critically on the distribution of risk aversion: the equilibrium is unique whenever it exists in the case of finite expected risk aversion, whereas an infinite expectation can lead to infinitely many equilibria or to a unique trivial one (pi equals 0). To address this multiplicity, we introduce three optimality criteria-optimal, uniformly optimal, and uniformly strictly optimal-and explicitly characterize the existence and uniqueness of the corresponding equilibria. Under the same necessary and sufficient condition, the optimal and uniformly optimal equilibria exist uniquely and coincide. Furthermore, by additionally assuming that the market price of risk is non-zero near the terminal time, we show that the optimal (and hence uniformly optimal) equilibrium is also uniformly strictly optimal. Finally, we perform comparative statics to demonstrate that a risk aversion distribution dominating another in the reverse hazard rate order leads to a less aggressive equilibrium strategy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在风险厌恶参数服从一般分布下的无限维投资组合选择问题。我们完整刻画了所有确定性均衡投资策略。结果表明，解的结构关键取决于风险厌恶的分布：当期望风险厌恶有限时，均衡存在则唯一；而无限期望可能导致无穷多均衡或唯一平凡均衡（π=0）。针对多重性，我们引入了三种最优性准则——最优、一致最优和一致严格最优——并明确刻画了相应均衡的存在性与唯一性。在相同充要条件下，最优与一致最优均衡存在唯一且重合。进一步假设市场风险价格在终端时刻附近非零，我们证明最优（从而一致最优）均衡也是一致严格最优的。最后，通过比较静态分析表明，若一个风险厌恶分布随机占优于另一个，则其均衡投资策略在随机占优意义下更大。",
    "fetch_date": "2026-01-01",
    "id": "20260101_33a484bf"
  },
  {
    "title": "Convergence Rates of Turnpike Theorems for Portfolio Choice in Stochastic Factor Models",
    "url": "https://arxiv.org/pdf/2512.00346v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "Turnpike theorems state that if an investor's utility is asymptotically equivalent to a power utility, then the optimal investment strategy converges to the CRRA strategy as the investment horizon tends to infinity. This paper aims to derive the convergence rates of the turnpike theorem for optimal feedback functions in stochastic factor models. In these models, optimal feedback functions can be decomposed into two terms: myopic portfolios and excess hedging demands. We obtain convergence rates for myopic portfolios in nonlinear stochastic factor models and for excess hedging demands in quadratic term structure models, where the interest rate is a quadratic function of a multivariate Ornstein-Uhlenbeck process. We show that the convergence rates are determined by (i) the decay speed of the price of a zero-coupon bond and (ii) how quickly the investor's utility becomes power-like at high levels of wealth. As an application, we consider optimal collective investment problems and show that sharing rules for terminal wealth affect convergence rates.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了随机因子模型中投资组合选择的“大道定理”收敛速率。大道定理指出，若投资者效用渐近等价于幂效用，则最优投资策略在投资期限趋于无穷时收敛于CRRA策略。论文旨在推导随机因子模型中最优反馈函数收敛速率的理论结果，将最优反馈分解为短视组合与超额对冲需求两部分，并在非线性随机因子模型和二次期限结构模型中分别获得其收敛速率。收敛速率取决于（i）零息债券价格的衰减速度，以及（ii）投资者效用在高财富水平下趋近幂效用的速度。作为应用，论文探讨了最优集体投资问题，表明终端财富的分享规则会影响收敛速率。",
    "fetch_date": "2026-01-01",
    "id": "20260101_fd006166"
  },
  {
    "title": "Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network",
    "url": "https://arxiv.org/pdf/2512.00299v1",
    "source": "ArXiv",
    "date": "2025-11-29",
    "abstract": "We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sion's maxmin theorem. For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了在随机占优约束下的S型非凹效用最大化静态投资组合选择问题。主要贡献包括：1）用一阶随机占优约束替代清算边界要求，为风险管理提供替代方案；2）提出数值算法检测二阶随机占优约束下的表现不佳区域，通过修改该区域分布获得（次）优解。核心金融洞见是：决策者应在表现不佳情景中遵循随机占优约束，在表现良好情景中追求效用最大化。该研究属于理论优化方法，未涉及强化学习/深度学习/Alpha生成等实战交易技术。",
    "fetch_date": "2026-01-01",
    "id": "20260101_23e46ee2"
  },
  {
    "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.23515v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "Alpha-R1：通过强化学习实现基于LLM推理的Alpha因子筛选。信号衰减和市场机制转换是数据驱动投资策略在非平稳市场中面临的持续挑战。传统时间序列和机器学习方法主要依赖历史相关性，当经济环境变化时往往难以泛化。虽然大语言模型在处理非结构化信息方面具有强大能力，但其通过明确的经济推理支持量化因子筛选的潜力尚未充分开发。现有基于因子的方法通常将Alpha简化为数值时间序列，忽略了决定因子何时具有经济相关性的语义逻辑。我们提出Alpha-R1，一个通过强化学习训练的80亿参数推理模型，用于上下文感知的Alpha筛选。Alpha-R1基于因子逻辑和实时新闻进行推理，评估不断变化的市场条件下的Alpha相关性，根据上下文一致性选择性地激活或停用因子。在多个资产池中的实证结果表明，Alpha-R1持续优于基准策略，并表现出对Alpha衰减的改进鲁棒性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_7845ee87"
  },
  {
    "title": "Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading",
    "url": "https://arxiv.org/pdf/2512.02227v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, while the S&P 500 index yielded a return of $15.97\\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, whereas the BTC price increased by $3.80\\%$. Our code is available on \\href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场因其时间动态性和低信噪比特性，成为AI智能体的关键应用领域。传统算法交易系统通常需要专业团队多年开发和测试。本文提出一个金融智能体编排框架，旨在将金融智能民主化给公众。该框架将传统算法交易系统的各个组件映射为智能体，包括规划器、编排器、阿尔法智能体、风险智能体、投资组合智能体、回测智能体、执行智能体、审计智能体和记忆智能体。论文展示了两个内部交易实例：在股票交易任务（2024年4月至12月每小时数据）中，该方法实现了20.42%的收益率、2.63的夏普比率和-3.59%的最大回撤，而同期标普500指数收益率为15.97%；在BTC交易任务（2025年7月27日至8月13日每分钟数据）中，实现了8.39%的收益率、0.38的夏普比率和-2.80%的最大回撤，而同期BTC价格上涨3.80%。代码已在GitHub开源。",
    "fetch_date": "2025-12-31",
    "id": "20251231_c175bb6a"
  },
  {
    "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction",
    "url": "https://arxiv.org/pdf/2512.23596v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究非平稳环境下股票收益预测的机器学习模型，揭示了一个根本性的非平稳性-复杂度权衡：复杂模型能减少设定误差，但需要更长的训练窗口，这会引入更强的非平稳性。作者通过一种新颖的模型选择方法解决了这一矛盾，该方法使用锦标赛程序联合优化模型类别和训练窗口大小，在非平稳验证数据上自适应评估候选模型。理论分析表明，该方法能平衡设定误差、估计方差和非平稳性，表现接近事后最佳模型。将该方法应用于17个行业投资组合收益，持续优于标准的滚动窗口基准，样本外R²平均提高14-23%。在NBER指定的衰退期间，改进尤为显著：例如，在海湾战争衰退期间，该方法实现了正的R²，而基准为负；在2001年衰退期间，R²绝对值至少提高80个基点；在2008年金融危机期间也表现优异。从经济角度看，基于该方法的交易策略具有实际应用价值。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbb43c5"
  },
  {
    "title": "Squeezed Covariance Matrix Estimation: Analytic Eigenvalue Control",
    "url": "https://arxiv.org/pdf/2512.23021v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "We revisit Gerber's Informational Quality (IQ) framework, a data-driven approach for constructing correlation matrices from co-movement evidence, and address two obstacles that limit its use in portfolio optimization: guaranteeing positive semidefinite ness (PSD) and controlling spectral conditioning. We introduce a squeezing identity that represents IQ estimators as a convex-like combination of structured channel matrices, and propose an atomic-IQ parameterization in which each channel-class matrix is built from PSD atoms with a single class-level normalization. This yields constructive PSD guarantees over an explicit feasibility region, avoiding reliance on ex-post projection. To regulate conditioning, we develop an analytic eigen floor that targets either a minimum eigenvalue or a desired condition number and, when necessary, repairs PSD violations in closed form while remaining compatible with the squeezing identity. In long-only tangency back tests with transaction costs, atomic-IQ improves out-of-sample Sharpe ratios and delivers a more stable risk profile relative to a broad set of standard covariance estimators.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视Gerber的信息质量（IQ）框架——一种基于数据驱动方法从共动证据构建相关矩阵的框架，并解决了限制其在投资组合优化中应用的两个障碍：保证正半定性（PSD）和控制谱条件数。作者引入了一种压缩恒等式，将IQ估计量表示为结构化通道矩阵的类凸组合，并提出了一种原子IQ参数化方法，其中每个通道类矩阵由具有单一类级别归一化的PSD原子构建。这在一个明确的可行区域内提供了构造性的PSD保证，避免依赖事后投影。为了调节条件数，作者开发了一种解析特征值下限方法，旨在实现最小特征值或期望的条件数，并在必要时以闭式形式修复PSD违规，同时保持与压缩恒等式的兼容性。在考虑交易成本的仅多头切线投资组合回测中，原子IQ方法相对于一系列标准协方差估计器，提高了样本外夏普比率，并提供了更稳定的风险特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_cefa3cff"
  },
  {
    "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.22895v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\\% higher Return, 5\\% higher Sharpe ratio, 5\\% higher Sortino ratio, and 2\\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "SAMP-HDRL：通过分层深度强化学习实现多智能体投资组合管理的分段分配与动量调整效用。该框架针对非平稳市场中的投资组合优化挑战（如制度转换、动态相关性和DRL策略可解释性有限），提出动态资产分组方法，将市场划分为高质量和普通子集。上层智能体提取全局市场信号，下层智能体在掩码约束下执行组内分配。基于效用的资本分配机制整合风险资产与无风险资产，确保全局与局部决策的协调一致。在三种市场制度（2019-2021）的回测中，SAMP-HDRL在波动和震荡条件下持续优于9个传统基准和9个DRL基准，相比最强基准至少实现5%的更高回报、夏普比率、索提诺比率和2%的更高欧米茄比率，在动荡市场中收益尤为显著。消融研究证实了各模块的有效性。",
    "fetch_date": "2025-12-31",
    "id": "20251231_b5de9d1a"
  },
  {
    "title": "AutoQuant: An Auditable Expert-System Framework for Execution-Constrained Auto-Tuning in Cryptocurrency Perpetual Futures",
    "url": "https://arxiv.org/pdf/2512.22476v1",
    "source": "ArXiv",
    "date": "2025-12-27",
    "abstract": "Backtests of cryptocurrency perpetual futures are fragile when they ignore microstructure frictions and reuse evaluation windows during parameter search. We study four liquid perpetuals (BTC/USDT, ETH/USDT, SOL/USDT, AVAX/USDT) and quantify how execution delay, funding, fees, and slippage can inflate reported performance. We introduce AutoQuant, an execution-centric, alpha-agnostic framework for auditable strategy configuration selection. AutoQuant encodes strict T+1 execution semantics and no-look-ahead funding alignment, runs Bayesian optimization under realistic costs, and applies a two-stage double-screening protocol across held-out rolling windows and a cost-sensitivity grid. We show that fee-only and zero-cost backtests can materially overestimate annualized returns relative to a fully costed configuration, and that double screening tends to reduce drawdowns under the same strict semantics even when returns are not higher. A CSCV/PBO diagnostic indicates substantial residual overfitting risk, motivating AutoQuant as validation and governance infrastructure rather than a claim of persistent alpha. Returns are reported for small-account simulations with linear trading costs and without market impact or capacity modeling.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《AutoQuant：一种用于加密货币永续期货执行约束自动调参的可审计专家系统框架》针对加密货币永续期货回测中忽略微观结构摩擦（如执行延迟、资金费、手续费、滑点）导致性能虚高的问题，提出以执行为中心、与阿尔法无关的可审计策略配置选择框架。该框架采用严格的T+1执行语义和无前瞻性资金费对齐，在真实成本下进行贝叶斯优化，并通过保留滚动窗口和成本敏感性网格的双阶段双重筛选协议。研究表明，仅考虑手续费和零成本的回测会显著高估年化收益，而双重筛选在相同严格语义下即使收益未提高也能降低回撤。CSCV/PBO诊断显示存在显著的残余过拟合风险，因此AutoQuant更适合作为验证和治理基础设施，而非声称持续阿尔法的工具。收益报告基于小账户模拟和线性交易成本。",
    "fetch_date": "2025-12-31",
    "id": "20251231_1842d9c9"
  },
  {
    "title": "Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling",
    "url": "https://arxiv.org/pdf/2512.21798v2",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data provides a practical solution to the privacy, accessibility, and reproducibility challenges that often constrain empirical research in quantitative finance. This paper investigates the use of deep generative models, specifically Time-series Generative Adversarial Networks (TimeGAN) and Variational Autoencoders (VAEs) to generate realistic synthetic financial return series for portfolio construction and risk modeling applications. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean--variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据为解决量化金融实证研究中的隐私、可访问性和可复现性挑战提供了实用方案。本文研究了深度生成模型，特别是时间序列生成对抗网络（TimeGAN）和变分自编码器（VAE），在生成用于投资组合构建和风险建模应用的现实合成金融收益序列方面的应用。以标普500的历史日收益率为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。研究表明，TimeGAN生成的合成数据在分布形状、波动模式和自相关行为方面接近真实收益。当应用于均值-方差投资组合优化时，所得合成数据集产生的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE提供更稳定的训练，但倾向于平滑极端市场波动，从而影响风险估计。",
    "fetch_date": "2025-12-31",
    "id": "20251231_3fd18e9d"
  },
  {
    "title": "Beyond Binary Screens: A Continuous Shariah Compliance Index for Asset Pricing and Portfolio Design",
    "url": "https://arxiv.org/pdf/2512.22858v1",
    "source": "ArXiv",
    "date": "2025-12-28",
    "abstract": "Binary Shariah screens vary across standards and apply hard thresholds that create discontinuous classifications. We construct a Continuous Shariah Compliance Index (CSCI) in $[0,1]$ by mapping standard screening ratios to smooth scores between conservative ``comfort'' bounds and permissive outer bounds, and aggregating them conservatively with a sectoral activity factor. Using CRSP/Compustat U.S. equities (1999-2024) with lagged accounting inputs and monthly rebalancing, we find that CSCI-based long-only portfolios have historical risk-adjusted performance similar to an emulated binary Islamic benchmark. Tightening the minimum compliance threshold reduces the investable universe and diversification and is associated with lower Sharpe ratios. The framework yields a practical compliance gradient that supports portfolio construction, constraint design, and cross-standard comparisons without reliance on pass/fail screening.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种连续伊斯兰教法合规指数（CSCI），用于替代传统的二元筛选方法。该指数在[0,1]区间内对标准筛选比率进行平滑评分，并通过行业活动因子进行保守聚合。基于美国股票数据（1999-2024年）的回测显示，CSCI构建的纯多头投资组合在历史风险调整后表现与模拟的二元伊斯兰基准相似。提高最低合规阈值会缩小可投资范围、降低分散化程度，并与较低的夏普比率相关。该框架提供了一种实用的合规梯度，支持投资组合构建、约束设计及跨标准比较，无需依赖通过/失败筛选。",
    "fetch_date": "2025-12-31",
    "id": "20251231_a943828e"
  },
  {
    "title": "Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%",
    "url": "https://arxiv.org/pdf/2512.03107v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出ECLIPSE框架，将AI幻觉视为模型语义熵与可用证据容量之间的不匹配。该框架结合多样本聚类的熵估计和一种新颖的困惑度分解方法，以衡量模型如何使用检索到的证据。在受控的金融问答数据集上，ECLIPSE实现了0.89的ROC AUC和0.90的平均精度，显著优于仅基于语义熵的基线（AUC 0.50）。然而，该方法的有效性依赖于校准的令牌级不确定性，在缺乏令牌级对数概率的模型上性能会显著下降。",
    "fetch_date": "2025-12-31",
    "id": "20251231_36769bcf"
  },
  {
    "title": "Broken Symmetry of Stock Returns -- a Modified Jones-Faddy Skew t-Distribution",
    "url": "https://arxiv.org/pdf/2512.23640v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We argue that negative skew and positive mean of the distribution of stock returns are largely due to the broken symmetry of stochastic volatility governing gains and losses. Starting with stochastic differential equations for stock returns and for stochastic volatility we argue that the distribution of stock returns can be effectively split in two -- for gains and losses -- assuming difference in parameters of their respective stochastic volatilities. A modified Jones-Faddy skew t-distribution utilized here allows to reflect this in a single organic distribution which tends to meaningfully capture this asymmetry. We illustrate its application on distribution of daily S&P500 returns, including analysis of its tails.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出股票收益率分布的负偏和正均值主要源于控制收益与损失的随机波动率对称性破缺。通过建立股票收益率和随机波动率的随机微分方程，作者认为可将收益率分布有效拆分为收益和损失两部分，并假设其各自随机波动率参数存在差异。采用的修正Jones-Faddy偏斜t分布能够以单一有机分布反映这种不对称性，并有效捕捉该特征。论文以标普500日收益率分布为例进行了应用展示，包括尾部分析。",
    "fetch_date": "2025-12-31",
    "id": "20251231_efbf4bea"
  },
  {
    "title": "Impact of Volatility on Time-Based Transaction Ordering Policies",
    "url": "https://arxiv.org/pdf/2512.23386v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "We study Arbitrum's Express Lane Auction (ELA), an ahead-of-time second-price auction that grants the winner an exclusive latency advantage for one minute. Building on a single-round model with risk-averse bidders, we propose a hypothesis that the value of priority access is discounted relative to risk-neutral valuation due to the difficulty of forecasting short-horizon volatility and bidders' risk aversion. We test these predictions using ELA bid records matched to high-frequency ETH prices and find that the result is consistent with the model.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了Arbitrum的快速通道拍卖（ELA），这是一种提前进行的次高价拍卖，获胜者将获得一分钟的独家延迟优势。基于风险厌恶投标人的单轮模型，我们提出一个假设：由于预测短期波动性的困难以及投标人的风险厌恶，优先访问的价值相对于风险中性估值有所折价。我们使用与高频ETH价格匹配的ELA投标记录来测试这些预测，发现结果与模型一致。",
    "fetch_date": "2025-12-31",
    "id": "20251231_688d9f03"
  },
  {
    "title": "Lambda Expected Shortfall",
    "url": "https://arxiv.org/pdf/2512.23139v1",
    "source": "ArXiv",
    "date": "2025-12-29",
    "abstract": "The Lambda Value-at-Risk (Lambda$-VaR) is a generalization of the Value-at-Risk (VaR), which has been actively studied in quantitative finance. Over the past two decades, the Expected Shortfall (ES) has become one of the most important risk measures alongside VaR because of its various desirable properties in the practice of optimization, risk management, and financial regulation. Analogously to the intimate relation between ES and VaR, we introduce the Lambda Expected Shortfall (Lambda-ES), as a generalization of ES and a counterpart to Lambda-VaR. Our definition of Lambda-ES has an explicit formula and many convenient properties, and we show that it is the smallest quasi-convex and law-invariant risk measure dominating Lambda-VaR under mild assumptions. We examine further properties of Lambda-ES, its dual representation, and related optimization problems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "Lambda期望损失（Lambda-ES）是期望损失（ES）的推广，作为Lambda风险价值（Lambda-VaR）的对应概念。该定义具有显式公式和多种便利性质，在温和假设下是最小化Lambda-VaR的拟凸且律不变风险度量。论文探讨了Lambda-ES的进一步性质、对偶表示及相关优化问题。",
    "fetch_date": "2025-12-31",
    "id": "20251231_6138efd6"
  },
  {
    "title": "A Note on the Conditions for COS Convergence",
    "url": "https://arxiv.org/pdf/2512.02745v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We study the truncation error of the COS method and give simple, verifiable conditions that guarantee convergence. In one dimension, COS is admissible when the density belongs to both L1 and L2 and has a finite weighted L2 moment of order strictly greater than one. We extend the result to multiple dimensions by requiring the moment order to exceed the dimension. These conditions enlarge the class of densities covered by previous analyses and include heavy-tailed distributions such as Student t with small degrees of freedom.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了COS方法的截断误差，并给出了保证收敛的简单可验证条件。在一维情况下，当密度函数同时属于L1和L2空间且具有严格大于1阶的有限加权L2矩时，COS方法是可接受的。通过要求矩的阶数超过维度，我们将结果扩展到多维情况。这些条件扩大了先前分析所涵盖的密度函数类别，包括具有小自由度的Student t等重尾分布。",
    "fetch_date": "2025-12-31",
    "id": "20251231_8fd2e7eb"
  },
  {
    "title": "Visibility-Graph Asymmetry as a Structural Indicator of Volatility Clustering",
    "url": "https://arxiv.org/pdf/2512.02352v2",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Volatility clustering is one of the most robust stylized facts of financial markets, yet it is typically detected using moment-based diagnostics or parametric models such as GARCH. This paper shows that clustered volatility also leaves a clear imprint on the time-reversal symmetry of horizontal visibility graphs (HVGs) constructed on absolute returns in physical time. For each time point, we compute the maximal forward and backward visibility distances, $L^{+}(t)$ and $L^{-}(t)$, and use their empirical distributions to build a visibility-asymmetry fingerprint comprising the Kolmogorov--Smirnov distance, variance difference, entropy difference, and a ratio of extreme visibility spans. In a Monte Carlo study, these HVG asymmetry features sharply separate volatility-clustered GARCH(1,1) dynamics from i.i.d.\\ Gaussian noise and from randomly shuffled GARCH series that preserve the marginal distribution but destroy temporal dependence; a simple linear classifier based on the fingerprint achieves about 90\\% in-sample accuracy. Applying the method to daily S\\&P500 data reveals a pronounced forward--backward imbalance, including a variance difference $Δ\\mathrm{Var}$ that exceeds the simulated GARCH values by two orders of magnitude and vanishes after shuffling. Overall, the visibility-graph asymmetry fingerprint emerges as a simple, model-free, and geometrically interpretable indicator of volatility clustering and time irreversibility in financial time series.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种基于水平可见性图（HVG）非对称性的新方法来检测金融市场中的波动率聚集现象。通过计算绝对收益序列在物理时间上的前向和后向最大可见距离，并构建包含Kolmogorov-Smirnov距离、方差差、熵差和极端可见跨度比率的非对称性指纹，该方法能够有效区分具有波动率聚集的GARCH(1,1)动态与独立同分布的高斯噪声。在蒙特卡洛研究中，基于该指纹的简单线性分类器实现了约90%的样本内准确率。应用于标普500日度数据时，该方法揭示了显著的前后向不平衡特征。",
    "fetch_date": "2025-12-31",
    "id": "20251231_0d518006"
  },
  {
    "title": "The Three-Dimensional Decomposition of Volatility Memory",
    "url": "https://arxiv.org/pdf/2512.02166v1",
    "source": "ArXiv",
    "date": "2025-12-01",
    "abstract": "This paper develops a three-dimensional decomposition of volatility memory into orthogonal components of level, shape, and tempo. The framework unifies regime-switching, fractional-integration, and business-time approaches within a single canonical representation that identifies how each dimension governs persistence strength, long-memory form, and temporal speed. We establish conditions for existence, uniqueness, and ergodicity of this decomposition and show that all GARCH-type processes arise as special cases. Empirically, applications to SPY and EURUSD (2005--2024) reveal that volatility memory is state-dependent: regime and tempo gates dominate in equities, while fractional-memory gates prevail in foreign exchange. The unified tri-gate model jointly captures these effects. By formalizing volatility dynamics through a level--shape--tempo structure, the paper provides a coherent link between information flow, market activity, and the evolving memory of financial volatility.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种将波动率记忆分解为水平、形态和节奏三个正交维度的三维分解框架。该框架在单一规范表示中统一了机制转换、分数积分和业务时间方法，识别了每个维度如何控制持续性强度、长记忆形式和时间速度。我们建立了该分解的存在性、唯一性和遍历性条件，并证明所有GARCH类过程都是其特例。对SPY和EURUSD（2005-2024）的实证应用表明，波动率记忆具有状态依赖性：机制和节奏维度在股票市场中占主导，而分数记忆维度在外汇市场中更为显著。统一的“三闸门”模型共同捕捉了这些效应。通过将波动率动态形式化为水平-形态-节奏结构，本文为信息流、市场活动和金融波动率演化记忆之间提供了连贯的联系。",
    "fetch_date": "2025-12-31",
    "id": "20251231_587625c2"
  },
  {
    "title": "Hidden Order in Trades Predicts the Size of Price Moves",
    "url": "https://arxiv.org/pdf/2512.15720v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p < 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "金融市场呈现一个明显悖论：虽然方向性价格变动基本不可预测（符合弱式有效市场假说），但价格变动幅度却显示出系统性结构。本文证明，基于15状态马尔可夫转移矩阵以秒级分辨率计算的实时订单流熵，能够预测日内收益的幅度而不提供方向性信息。对36个交易日内3850万笔SPY交易的分析显示，在熵低于第5百分位的条件下，后续5分钟绝对收益增加2.89倍（t=12.41，p<0.0001），而方向性准确率保持在45.0%——与随机水平无统计学差异（p=0.12）。这种解耦源于基本对称性：熵在符号置换下保持不变，可检测知情交易的存在而不揭示其方向。五个非重叠测试期的前向验证确认了样本外可预测性，标签置换安慰剂检验得出z=14.4。这些发现表明，信息论度量可作为市场微观结构中的波动率状态变量。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a096c37e"
  },
  {
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
    "url": "https://arxiv.org/pdf/2512.21878v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "MASFIN是一个模块化多智能体框架，将大型语言模型（LLMs）与结构化金融指标和非结构化新闻相结合，同时嵌入明确的偏差缓解协议。该系统利用GPT-4.1-nano实现可重复性和成本效益推理，生成包含15-30只股票的周度投资组合，其配置权重针对短期表现进行了优化。在为期八周的评估中，MASFIN实现了7.33%的累计回报，在八周中的六周内表现优于标普500、纳斯达克100和道琼斯基准，尽管波动性较高。这些发现展示了具有偏差意识的生成式AI框架在金融预测中的潜力，并凸显了模块化多智能体设计在推进实用、透明和可重复方法方面的机遇。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0237ad76"
  },
  {
    "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling",
    "url": "https://arxiv.org/pdf/2512.21791v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种统一的合成金融数据多标准评估框架，并应用于三种代表性生成范式：统计ARIMA-GARCH基线、变分自编码器（VAEs）和时间序列生成对抗网络（TimeGAN）。基于标普500历史日数据，评估了保真度（最大均值差异MMD）、时间结构（自相关和波动率聚集）以及在下游任务中的实际效用，特别是均值-方差投资组合优化和波动率预测。实证结果表明：ARIMA-GARCH能捕捉线性趋势和条件波动率但无法复现非线性动态；VAEs生成平滑轨迹但低估极端事件；TimeGAN在真实性和时间连贯性之间取得了最佳平衡（如TimeGAN获得最低MMD：1.84e-3，5次种子平均）。最后，论文根据应用需求和计算约束提出了生成模型选择的实用指南。统一的评估协议和可复现性为实战交易中解决数据稀缺和保密性问题提供了直接价值，特别是对需要大量数据进行模型训练和压力测试的量化策略开发。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a7fa7b3b"
  },
  {
    "title": "Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification",
    "url": "https://arxiv.org/pdf/2512.22109v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We study the construction and rebalancing of sparse index-tracking portfolios from an operational research perspective, with explicit emphasis on uncertainty quantification and implementability. The decision variables are portfolio weights constrained to sum to one; the aims are to track a reference index closely while controlling the number of names and the turnover induced by rebalancing. We cast index tracking as a high-dimensional linear regression of index returns on constituent returns, and employ a sparsity-inducing Laplace prior on the weights. A single global shrinkage parameter controls the trade-off between tracking error and sparsity, and is calibrated by an empirical-Bayes stochastic approximation scheme. Conditional on this calibration, we approximate the posterior distribution of the portfolio weights using proximal Langevin-type Markov chain Monte Carlo algorithms tailored to the budget constraint. This yields posterior uncertainty on tracking error, portfolio composition and prospective rebalancing moves. Building on these posterior samples, we propose rules for rebalancing that gate trades through magnitude-based thresholds and posterior activation probabilities, thereby trading off expected tracking error against turnover and portfolio size. A case study on tracking the S&P~500 index is carried out to showcase how our tools shape the decision process from portfolio construction to rebalancing.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文从运筹学视角研究稀疏指数跟踪投资组合的构建与再平衡，重点强调不确定性量化与可实施性。决策变量为总和为1的投资组合权重；目标是在控制持仓数量与再平衡引起的换手率的同时，紧密跟踪基准指数。作者将指数跟踪建模为指数收益对成分股收益的高维线性回归，并对权重施加诱导稀疏性的拉普拉斯先验。单一全局收缩参数控制跟踪误差与稀疏性之间的权衡，并通过经验贝叶斯随机逼近方案进行校准。基于此校准，作者采用针对预算约束设计的近似近端朗之万型马尔可夫链蒙特卡洛算法，近似投资组合权重的后验分布，从而获得关于跟踪误差、投资组合构成及预期再平衡操作的后验不确定性。基于这些后验样本，作者提出了通过基于幅度的阈值和后验激活概率来门控交易的再平衡规则。",
    "fetch_date": "2025-12-30",
    "id": "20251230_7977417e"
  },
  {
    "title": "Applications of synthetic financial data in portfolio and risk modeling",
    "url": "https://arxiv.org/pdf/2512.21798v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Synthetic financial data offers a practical way to address the privacy and accessibility challenges that limit research in quantitative finance. This paper examines the use of generative models, in particular TimeGAN and Variational Autoencoders (VAEs), for creating synthetic return series that support portfolio construction, trading analysis, and risk modeling. Using historical daily returns from the S and P 500 as a benchmark, we generate synthetic datasets under comparable market conditions and evaluate them using statistical similarity metrics, temporal structure tests, and downstream financial tasks. The study shows that TimeGAN produces synthetic data with distributional shapes, volatility patterns, and autocorrelation behaviour that are close to those observed in real returns. When applied to mean-variance portfolio optimization, the resulting synthetic datasets lead to portfolio weights, Sharpe ratios, and risk levels that remain close to those obtained from real data. The VAE provides more stable training but tends to smooth extreme market movements, which affects risk estimation. Finally, the analysis supports the use of synthetic datasets as substitutes for real financial data in portfolio analysis and risk simulation, particularly when models are able to capture temporal dynamics. Synthetic data therefore provides a privacy-preserving, cost-effective, and reproducible tool for financial experimentation and model development.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "合成金融数据在投资组合和风险建模中的应用。该论文探讨了使用生成模型（特别是TimeGAN和变分自编码器VAE）创建合成收益率序列，以支持投资组合构建、交易分析和风险建模。研究以标普500历史日收益率作为基准，在可比市场条件下生成合成数据集，并通过统计相似性指标、时间结构测试和下游金融任务进行评估。结果表明，TimeGAN生成的合成数据在分布形态、波动率模式和自相关行为方面接近真实收益率。应用于均值-方差投资组合优化时，合成数据集得出的投资组合权重、夏普比率和风险水平与真实数据结果相近。VAE训练更稳定但倾向于平滑极端市场波动，从而影响风险估计。分析支持将合成数据集作为真实金融数据的替代品，用于解决隐私和可访问性限制。",
    "fetch_date": "2025-12-30",
    "id": "20251230_c2ad24df"
  },
  {
    "title": "FX Market Making with Internal Liquidity",
    "url": "https://arxiv.org/pdf/2512.04603v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "As the FX markets continue to evolve, many institutions have started offering passive access to their internal liquidity pools. Market makers act as principal and have the opportunity to fill those orders as part of their risk management, or they may choose to adjust pricing to their external OTC franchise to facilitate the matching flow. It is, a priori, unclear how the strategies managing internal liquidity should depend on market condions, the market maker's risk appetite, and the placement algorithms deployed by participating clients. The market maker's actions in the presence of passive orders are relevant not only for their own objectives, but also for those liquidity providers who have certain expectations of the execution speed. In this work, we investigate the optimal multi-objective strategy of a market maker with an option to take liquidity on an internal exchange, and draw important qualitative insights for real-world trading.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "随着外汇市场不断发展，许多机构开始提供对其内部流动性池的被动访问。做市商作为主体，有机会将这些订单作为其风险管理的一部分进行成交，或者他们可以选择调整其外部场外交易特许经营的价格以促进匹配流动。从先验角度看，管理内部流动性的策略应如何依赖市场条件、做市商的风险偏好以及参与客户部署的配置算法尚不明确。做市商在被动订单存在下的行为不仅与其自身目标相关，也与对执行速度有特定预期的流动性提供者相关。在本研究中，我们探讨了做市商在可选择从内部交易所获取流动性时的最优多目标策略，并为实际交易提供了重要的定性见解。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4668657e"
  },
  {
    "title": "Investigating Conditional Restricted Boltzmann Machines in Regime Detection",
    "url": "https://arxiv.org/pdf/2512.21823v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "This study investigates the efficacy of Conditional Restricted Boltzmann Machines (CRBMs) for modeling high-dimensional financial time series and detecting systemic risk regimes. We extend the classical application of static Restricted Boltzmann Machines (RBMs) by incorporating autoregressive conditioning and utilizing Persistent Contrastive Divergence (PCD) to incorporate complex temporal dependency structures. Comparing a discrete Bernoulli-Bernoulli architecture against a continuous Gaussian-Bernoulli variant across a multi-asset dataset spanning 2013-2025, we observe a dichotomy between generative fidelity and regime detection. While the Gaussian CRBM successfully preserves static asset correlations, it exhibits limitations in generating long-range volatility clustering. Thus, we analyze the free energy as a relative negative log-likelihood (surprisal) under a fixed, trained model. We demonstrate that the model's free energy serves as a robust, regime stability metric. By decomposing the free energy into quadratic (magnitude) and structural (correlation) components, we show that the model can distinguish between pure magnitude shocks and market regimes. Our findings suggest that the CRBM offers a valuable, interpretable diagnostic tool for monitoring systemic risk, providing a supplemental metric to implied volatility metrics like the VIX.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了条件受限玻尔兹曼机（CRBMs）在建模高维金融时间序列和检测系统性风险状态方面的有效性。通过引入自回归条件和使用持续对比散度（PCD），扩展了静态受限玻尔兹曼机（RBMs）的经典应用，以纳入复杂的时间依赖结构。在2013-2025年跨资产数据集中，比较离散伯努利-伯努利架构与连续高斯-伯努利变体，观察到生成保真度与状态检测之间的二分法。虽然高斯CRBM成功保留了静态资产相关性，但在生成长程波动率聚类方面存在局限性。因此，我们在固定训练模型下分析自由能作为相对负对数似然（惊奇度）。研究表明，模型的自由能可作为稳健的状态稳定性度量。通过将自由能分解为二次（幅度）和结构（相关性）分量，模型能够区分纯幅度冲击和市场状态。",
    "fetch_date": "2025-12-30",
    "id": "20251230_137a0b37"
  },
  {
    "title": "S&P 500 Stock's Movement Prediction using CNN",
    "url": "https://arxiv.org/pdf/2512.21804v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].\n  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文使用卷积神经网络（CNN）预测标普500指数成分股的股价走势，创新之处在于直接利用包含股票拆分/股息事件的多维原始市场数据，而非传统工程化金融数据。虽然CNN在图像分类领域表现优异，但论文未深入探讨金融数据的复杂性，且主要基于单维度数据，对实战交易的价值有限，更多是为未来研究奠定基础。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4893f447"
  },
  {
    "title": "A High-Level Framework for Practically Model-Independent Pricing",
    "url": "https://arxiv.org/pdf/2512.15718v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "We present a high-level framework that explains why, in practice, different pricing models calibrated to the same vanilla surface tend to produce similar valuations for exotic derivatives. Our approach acts as an overlay on the Monte Carlo infrastructure already used in banks, combining path reweighting with a conic optimisation layer without requiring any changes to existing code. This construction delivers narrow, practically model-independent price bands for exotics, reconciling front-office practice with the robust, model-independent ideas developed in the academic literature.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种高层框架，解释了为何在实践中，校准至相同普通期权曲面的不同定价模型往往对奇异衍生品给出相近的估值。该方法作为银行现有蒙特卡洛基础设施的覆盖层，结合路径重加权与锥优化层，无需修改现有代码。该框架为奇异品生成狭窄、近乎模型独立的价格区间，将前台实践与学术文献中发展的稳健、模型独立理念相统一。",
    "fetch_date": "2025-12-30",
    "id": "20251230_877a57bf"
  },
  {
    "title": "Enhancing trading strategies: a multi-indicator analysis for profitable algorithmic trading",
    "url": "https://link.springer.com/article/10.1007/s10614-024-10669-3",
    "source": "Scholar",
    "date": "2025-12-30",
    "abstract": "… backtesting to compare the strategy's historical performance to benchmarks. The … algorithmic trading models. This research contributes to understanding of algorithmic trading strategies …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该论文通过多指标分析增强交易策略，采用回测方法比较策略历史表现与基准，为理解算法交易模型提供了理论贡献。",
    "fetch_date": "2025-12-30",
    "id": "20251230_0b3c59aa"
  },
  {
    "title": "Variational Quantum Eigensolver for Real-World Finance: Scalable Solutions for Dynamic Portfolio Optimization Problems",
    "url": "https://arxiv.org/pdf/2512.22001v1",
    "source": "ArXiv",
    "date": "2025-12-26",
    "abstract": "We present a scalable, hardware-aware methodology for extending the Variational Quantum Eigensolver (VQE) to large, realistic Dynamic Portfolio Optimization (DPO) problems. Building on the scaling strategy from our previous work, where we tailored a VQE workflow to both the DPO formulation and the target QPU, we now put forward two significant advances. The first is the implementation of the Ising Sample-based Quantum Configuration Recovery (ISQR) routine, which improves solution quality in Quadratic Unconstrained Binary Optimization problems. The second is the use of the VQE Constrained method to decompose the optimization task, enabling us to handle DPO instances with more variables than the available qubits on current hardware. These advances, which are broadly applicable to other optimization problems, allow us to address a portfolio with a size relevant to the financial industry, consisting of up to 38 assets and covering the full Spanish stock index (IBEX 35). Our results, obtained on a real Quantum Processing Unit (IBM Fez), show that this tailored workflow achieves financial performance on par with classical methods while delivering a broader set of high-quality investment strategies, demonstrating a viable path towards obtaining practical advantage from quantum optimization in real financial applications.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种可扩展、硬件感知的方法，将变分量子本征求解器（VQE）应用于大规模、现实的动态投资组合优化（DPO）问题。通过引入伊辛样本量子配置恢复（ISQR）例程和改进的VQE约束方法，该研究成功处理了包含多达38种资产（覆盖西班牙IBEX 35指数）的投资组合，并在真实量子处理单元（IBM Fez）上实现了与经典方法相当的财务绩效。",
    "fetch_date": "2025-12-30",
    "id": "20251230_4c468636"
  },
  {
    "title": "Mean-Field Price Formation on Trees with a Network of Relative Performance Concerns",
    "url": "https://arxiv.org/pdf/2512.21621v1",
    "source": "ArXiv",
    "date": "2025-12-25",
    "abstract": "Financial firms and institutional investors are routinely evaluated based on their performance relative to their peers. These relative performance concerns significantly influence risk-taking behavior and market dynamics. While the literature studying Nash equilibrium under such relative performance competitions is extensive, its effect on asset price formation remains largely unexplored. This paper investigates mean-field equilibrium price formation of a single risky stock in a discrete-time market where agents exhibit exponential utility and relative performance concerns. Unlike existing literature that typically treats asset prices as exogenous, we impose a market-clearing condition to determine the price dynamics endogenously within a relative performance equilibrium. Using a binomial tree framework, we establish the existence and uniqueness of the market-clearing mean-field equilibrium in both single- and multi-population settings. Finally, we provide illustrative numerical examples demonstrating the equilibrium price distributions and agents' optimal position sizes.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "金融机构和机构投资者通常根据其相对于同行的表现进行评估。这些相对绩效关注显著影响风险承担行为和市场动态。虽然研究此类相对绩效竞争下纳什均衡的文献广泛，但其对资产价格形成的影响在很大程度上仍未得到探索。本文研究了在离散时间市场中，具有指数效用和相对绩效关注的单一风险股票的平均场均衡价格形成。与通常将资产价格视为外生变量的现有文献不同，我们施加了市场出清条件，以在相对绩效均衡内内生地确定价格动态。使用二叉树框架，我们建立了单群体和多群体设置下市场出清平均场均衡的存在性和唯一性。最后，我们提供了说明性的数值示例，展示了均衡价格分布和代理人的最优头寸规模。",
    "fetch_date": "2025-12-30",
    "id": "20251230_f73cd03a"
  },
  {
    "title": "A Co-evolutionary Approach for Heston Calibration",
    "url": "https://arxiv.org/pdf/2512.03922v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "We evaluate a co-evolutionary calibration framework for the Heston model in which a genetic algorithm (GA) over parameters is coupled to an evolving neural inverse map from option surfaces to parameters. While GA-history sampling can reduce training loss quickly and yields strong in-sample fits to the target surface, learning-curve diagnostics show a widening train--validation gap across generations, indicating substantial overfitting induced by the concentrated and less diverse dataset. In contrast, a broad, space-filling dataset generated via Latin hypercube sampling (LHS) achieves nearly comparable calibration accuracy while delivering markedly better out-of-sample stability across held-out surfaces. These results suggest that apparent improvements from co-evolutionary data generation largely reflect target-specific specialization rather than a more reliable global inverse mapping, and that maintaining dataset diversity is critical for robust amortized calibration.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文评估了一种用于Heston模型校准的协同进化框架，其中参数遗传算法（GA）与从期权曲面到参数的进化神经逆映射相结合。研究发现，虽然GA历史采样能快速降低训练损失并在样本内对目标曲面实现强拟合，但学习曲线诊断显示代际间训练-验证差距扩大，表明由集中且多样性不足的数据集引发了显著过拟合。相比之下，通过拉丁超立方采样（LHS）生成的广泛、空间填充数据集实现了几乎相当的校准精度，同时在保留曲面上表现出明显更好的样本外稳定性。这些结果表明，协同进化数据生成带来的明显改进主要反映了针对特定目标的专门化，而非更可靠的全局逆映射，且保持数据集多样性对于稳健的摊销校准至关重要。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8a431d9a"
  },
  {
    "title": "The Effect of High-Speed Rail Connectivity on Capital Market Earnings Forecast Error: Evidence from the Chinese Stock Market",
    "url": "https://arxiv.org/pdf/2512.03709v1",
    "source": "ArXiv",
    "date": "2025-12-03",
    "abstract": "This study examines how China's high-speed rail (HSR) expansion affects analyst earnings forecast errors from an economic information friction perspective. Using firm-year panel data from 2008-2019, a period that covers HSR's early introduction and rapid nationwide rollout, the findings show that analysts' relative earnings forecast errors (RFE) decline significantly only after firms' cities become connected by high-speed rail. The placebo test, which artificially shifts HSR connectivity 3 years earlier than the actual opening year, yields an insignificant DID coefficient, rejecting the possibility that forecast errors were improving before the infrastructure shock. This supports the conclusion that forecast error reduction is linked to real geographic accessibility improvements rather than coincidence, pre-existing trends, or analyst anticipation. Economically, the study highlights that HSR reduces analysts' costs of gathering private, incremental information, particularly soft information obtained via plant or management visits. The rail network does not directly alter firms' internal capital allocation or earnings generation paths, but it lowers spatial barriers to information collection, enabling analysts to update EPS expectations under reduced travel friction. This work provides intuitive evidence that geography and mobility improvements contribute to forecasting accuracy in China's emerging, decentralized capital market corridors, and it encourages future research to consider transport accessibility as an exogenous information cost shock rather than an internal firm-capital shock.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究从经济信息摩擦视角，探讨中国高速铁路（HSR）扩张如何影响分析师盈利预测误差。利用2008-2019年企业年度面板数据（覆盖HSR早期引入和全国快速推广期），研究发现仅在企业所在城市通过高铁连接后，分析师的相对盈利预测误差（RFE）显著下降。安慰剂测试（人为将HSR连通时间提前3年）显示DID系数不显著，排除了预测误差在基础设施冲击前已改善的可能性，支持预测误差减少与真实地理可达性改善相关，而非巧合、既有趋势或分析师预期。经济意义上，研究强调HSR降低了分析师收集私有增量信息（特别是通过工厂或管理层访问获取的软信息）的成本。铁路网络并未直接改变企业内部资本配置或盈利生成路径，但降低了信息获取的空间壁垒。",
    "fetch_date": "2025-12-30",
    "id": "20251230_8749e3da"
  },
  {
    "title": "A Stochastic Thermodynamics Approach to Price Impact and Round-Trip Arbitrage: Theory and Empirical Implications",
    "url": "https://arxiv.org/pdf/2512.03123v1",
    "source": "ArXiv",
    "date": "2025-12-02",
    "abstract": "This paper develops a comprehensive theoretical framework that imports concepts from stochastic thermodynamics to model price impact and characterize the feasibility of round-trip arbitrage in financial markets. A trading cycle is treated as a non-equilibrium thermodynamic process, where price impact represents dissipative work and market noise plays the role of thermal fluctuations. The paper proves a Financial Second Law: under general convex impact functionals, any round-trip trading strategy yields non-positive expected profit. This structural constraint is complemented by a fluctuation theorem that bounds the probability of profitable cycles in terms of dissipated work and market volatility. The framework introduces a statistical ensemble of trading strategies governed by a Gibbs measure, leading to a free energy decomposition that connects expected cost, strategy entropy, and a market temperature parameter. The framework provides rigorous, testable inequalities linking microstructural impact to macroscopic no-arbitrage conditions, offering a novel physics-inspired perspective on market efficiency. The paper derives explicit analytical results for prototypical trading strategies and discusses empirical validation protocols.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一个将随机热力学概念引入金融市场的理论框架，用于建模价格影响并分析往返套利的可行性。交易周期被视为非平衡热力学过程，其中价格影响代表耗散功，市场噪声类比热涨落。论文证明了“金融第二定律”：在一般凸影响泛函下，任何往返交易策略的期望利润均为非正。该结构约束辅以一个涨落定理，将盈利周期的概率与耗散功和市场波动率联系起来。框架引入了由吉布斯测度支配的交易策略统计系综，导出了连接期望成本、策略熵和市场温度参数的自由能分解。该框架提供了连接微观结构影响与宏观无套利条件的严格可检验不等式，为市场效率提供了新颖的物理学视角。论文针对典型交易策略推导了显式解析结果。",
    "fetch_date": "2025-12-30",
    "id": "20251230_a03b3233"
  },
  {
    "title": "The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading",
    "url": "https://arxiv.org/pdf/2512.15732v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the \"Holy Grail\" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of \"Galaxy Empire,\" a hybrid framework coupling LSTM/Transformer-based perception with a genetic \"Time-is-Life\" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\\%$) and live performance (Capital Decay $>70\\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \\textit{Aleatoric Uncertainty} in low-entropy time-series, the \\textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "《红皇后的陷阱：高频交易中深度进化的局限》对实战交易具有重要警示价值。该论文对名为“银河帝国”的混合框架（结合LSTM/Transformer感知与遗传“时间即生命”生存机制）进行了严谨的事后分析。在加密货币高频环境中部署500个自主代理后，观察到训练指标（验证年化收益率>300%）与实际表现（资本衰减>70%）之间的灾难性背离。研究通过多学科视角解构了失败原因，识别出三个关键失效模式：低熵时间序列中偶然不确定性的过拟合、高方差下进化选择固有的幸存者偏差，以及缺乏订单流数据时无法克服微观结构摩擦的数学不可能性。研究结果为模型复杂性增加在缺乏信息不对称时加剧系统脆弱性提供了实证证据。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f1a0b406"
  },
  {
    "title": "Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks",
    "url": "https://arxiv.org/pdf/2512.05868v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文《利用脉冲神经网络预测高频金融数据中的价格变动》针对高频交易环境中传统模型难以捕捉的毫秒级价格尖峰问题，提出了一种生物启发的解决方案。研究将高频股票数据转换为脉冲序列，并评估了三种架构：基于无监督STDP训练的经典脉冲神经网络、具有显式抑制竞争的新型脉冲神经网络，以及监督式反向传播网络。通过贝叶斯优化结合新颖的惩罚性脉冲准确率目标函数进行超参数调优，确保模型预测的价格尖峰率与实证事件率一致。模拟交易显示，经PSA优化的模型在性能上持续优于仅使用脉冲准确率调优的对比模型及基线方法。",
    "fetch_date": "2025-12-29",
    "id": "20251229_8824f09c"
  },
  {
    "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
    "url": "https://arxiv.org/pdf/2512.05753v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "针对现代战争中认知雷达快速部署以对抗干扰的关键挑战，本文提出了一种基于深度强化学习的快速抗干扰雷达部署算法（FARDA）。现有方法主要基于进化算法，耗时且易陷入局部最优。FARDA通过神经网络高效推理，将雷达部署建模为端到端任务，设计了集成神经模块感知热图信息和新奖励格式的深度强化学习算法。实证结果表明，该方法在达到与进化算法相当覆盖范围的同时，部署速度提升约7000倍。消融实验验证了FARDA各组件的必要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5a33edc9"
  },
  {
    "title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming",
    "url": "https://arxiv.org/pdf/2512.15735v3",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种将强化学习（RL）控制器与抗扰扩展状态观测器（ESO）相结合的统一控制架构，并辅以事件触发机制（ETM）以减少不必要的计算。ESO用于实时估计系统状态和集总扰动，为有效扰动补偿奠定基础。为在缺乏精确系统描述的情况下获得近似最优行为，采用基于值迭代的自适应动态规划（ADP）方法进行策略逼近。ETM的引入确保学习模块的参数更新仅在状态偏差超过预设界限时执行，从而避免过度学习活动并显著降低计算负荷。通过李雅普诺夫导向分析表征了闭环系统的稳定性。数值实验进一步证实，与标准时间触发ADP方案相比，该方法在保持强大控制性能和扰动容忍度的同时，显著减少了采样和处理工作量。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f5507cc4"
  },
  {
    "title": "A Unified AI System For Data Quality Control and DataOps Management in Regulated Environments",
    "url": "https://arxiv.org/pdf/2512.05559v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "In regulated domains such as finance, the integrity and governance of data pipelines are critical - yet existing systems treat data quality control (QC) as an isolated preprocessing step rather than a first-class system component. We present a unified AI-driven Data QC and DataOps Management framework that embeds rule-based, statistical, and AI-based QC methods into a continuous, governed layer spanning ingestion, model pipelines, and downstream applications. Our architecture integrates open-source tools with custom modules for profiling, audit logging, breach handling, configuration-driven policies, and dynamic remediation. We demonstrate deployment in a production-grade financial setup: handling streaming and tabular data across multiple asset classes and transaction streams, with configurable thresholds, cloud-native storage interfaces, and automated alerts. We show empirical gains in anomaly detection recall, reduction of manual remediation effort, and improved auditability and traceability in high-throughput data workflows. By treating QC as a system concern rather than an afterthought, our framework provides a foundation for trustworthy, scalable, and compliant AI pipelines in regulated environments.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在金融等受监管领域，数据管道的完整性和治理至关重要。本文提出了一种统一的AI驱动数据质量控制和DataOps管理框架，将基于规则、统计和AI的质量控制方法嵌入到一个跨越数据摄取、模型管道和下游应用的连续治理层中。该架构整合了开源工具与定制模块，用于数据剖析、审计日志记录、违规处理、配置驱动策略和动态修复。在金融生产环境中部署的演示表明，该框架能够处理多资产类别和交易流中的流数据和表格数据，具有可配置阈值、云原生存储接口和自动警报功能。实证结果显示，在高吞吐量数据工作流中，异常检测召回率得到提升，手动修复工作量减少，审计性和可追溯性得到改善。通过将质量控制视为系统核心而非事后补救，该框架为可信、可扩展且合规的AI管道提供了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_144f113c"
  },
  {
    "title": "Differential ML with a Difference",
    "url": "https://arxiv.org/pdf/2512.05301v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Differential ML (Huge and Savine 2020) is a technique for training neural networks to provide fast approximations to complex simulation-based models for derivatives pricing and risk management. It uses price sensitivities calculated through pathwise adjoint differentiation to reduce pricing and hedging errors. However, for options with discontinuous payoffs, such as digital or barrier options, the pathwise sensitivities are biased, and incorporating them into the loss function can magnify errors. We consider alternative methods for estimating sensitivities and find that they can substantially reduce test errors in prices and in their sensitivities. Using differential labels calculated through the likelihood ratio method expands the scope of Differential ML to discontinuous payoffs. A hybrid method incorporates gamma estimates as well as delta estimates, providing further regularization.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "差分机器学习（Differential ML）是一种训练神经网络快速近似复杂模拟模型的技术，用于衍生品定价和风险管理。该方法利用通过路径伴随微分计算的价格敏感性来减少定价和对冲误差。然而，对于具有不连续收益的期权（如数字或障碍期权），路径敏感性存在偏差，将其纳入损失函数可能放大误差。研究探讨了替代敏感性估计方法，发现这些方法能显著降低价格及其敏感性的测试误差。使用似然比法计算的差分标签将差分机器学习扩展到不连续收益领域。混合方法结合了伽马估计和德尔塔估计，提供了进一步的规范化。",
    "fetch_date": "2025-12-29",
    "id": "20251229_c043a012"
  },
  {
    "title": "Continuous-time reinforcement learning for optimal switching over multiple regimes",
    "url": "https://arxiv.org/pdf/2512.04697v2",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了多体制下最优切换问题的连续时间强化学习。在熵正则化的探索性框架下，代理通过关联连续时间有限状态马尔可夫链的生成矩阵，随机化切换时机和体制选择。我们建立了相关哈密顿-雅可比-贝尔曼方程组的适定性，并给出了最优策略的表征。通过分析方程组，严格证明了策略改进和策略迭代的收敛性。当温度参数趋近于零时，我们还展示了探索性框架下的价值函数向经典框架价值函数的收敛。最后，基于鞅表征的策略评估，设计并实现了一种强化学习算法。借助神经网络的数值算例说明了所提RL算法的有效性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_126a24ad"
  },
  {
    "title": "Convolution-FFT for option pricing in the Heston model",
    "url": "https://arxiv.org/pdf/2512.05326v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "We propose a convolution-FFT method for pricing European options under the Heston model that leverages a continuously differentiable representation of the joint characteristic function. Unlike existing Fourier-based methods that rely on branch-cut adjustments or empirically tuned damping parameters, our approach yields a stable integrand even under large frequency oscillations. Crucially, we derive fully analytical error bounds that quantify both truncation error and discretization error in terms of model parameters and grid settings. To the best of our knowledge, this is the first work to provide such explicit, closed-form error estimates for an FFT-based convolution method specialized to the Heston model. Numerical experiments confirm the theoretical rates and illustrate robust, high-accuracy option pricing at modest computational cost.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文提出了一种用于Heston模型下欧式期权定价的卷积-FFT方法，该方法利用联合特征函数的连续可微表示。与现有依赖分支切割调整或经验调优阻尼参数的傅里叶方法不同，本方法即使在大频率振荡下也能产生稳定的被积函数。关键贡献在于推导了完全解析的误差界，以模型参数和网格设置量化截断误差和离散化误差。据我们所知，这是首个为专门针对Heston模型的基于FFT的卷积方法提供此类显式闭式误差估计的工作。数值实验验证了理论收敛速度，并展示了以适中计算成本实现稳健、高精度的期权定价。",
    "fetch_date": "2025-12-29",
    "id": "20251229_111d7440"
  },
  {
    "title": "Market Reactions to Material Cybersecurity Incident Disclosures",
    "url": "https://arxiv.org/pdf/2512.06144v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study examines short-term market responses to material cybersecurity incidents disclosed under Item 1.05 of Form 8-K. Drawing on a sample of disclosures made between 2023 and 2025, daily stock price movements were evaluated over a standardized event window surrounding each filing. On average, companies experienced negative price reactions following the disclosure of a material cybersecurity incident. Comparisons across company characteristics indicate that smaller companies tended to incur more pronounced declines, while differences by sector and beta were not evident. These findings offer empirical insight into how public markets interpret cybersecurity risks when they are formally reported and suggest that firm size may influence the degree of sensitivity to such events.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本研究考察了根据8-K表格第1.05项披露的重大网络安全事件引发的短期市场反应。基于2023年至2025年间的披露样本，评估了每份申报文件周围标准化事件窗口内的每日股价变动。平均而言，公司在披露重大网络安全事件后经历了负面价格反应。跨公司特征的比较表明，规模较小的公司往往遭受更明显的股价下跌，而按行业和贝塔值的差异并不明显。这些发现为公开市场如何解释正式报告的网络安全风险提供了实证见解，并表明公司规模可能影响对此类事件的敏感程度。",
    "fetch_date": "2025-12-29",
    "id": "20251229_da2b1b43"
  },
  {
    "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem",
    "url": "https://arxiv.org/pdf/2512.05946v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "资源分配问题因其组合复杂性而保持NP难。虽然深度强化学习方法（如Rainbow DQN）通过优先回放和分布头提高了可扩展性，但经典函数逼近器限制了其表示能力。本文提出变分量子Rainbow DQN（VQR-DQN），将环形拓扑变分量子电路与Rainbow DQN集成，以利用量子叠加和纠缠。我们将人力资源分配问题（HRAP）建模为基于官员能力、事件时间表和转移时间的组合动作空间的马尔可夫决策过程（MDP）。在四个HRAP基准测试中，VQR-DQN相比随机基线实现了26.8%的归一化完工时间减少，并优于Double DQN和经典Rainbow DQN 4.9-13.4%。这些收益与电路表达能力、纠缠和策略质量之间的理论联系一致，展示了量子增强DRL在大规模资源分配中的潜力。",
    "fetch_date": "2025-12-29",
    "id": "20251229_4b5ad711"
  },
  {
    "title": "FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction",
    "url": "https://arxiv.org/pdf/2512.15728v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \\emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\\% and stability of 93.33\\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文介绍了FedSight AI，这是一个利用大型语言模型（LLMs）模拟联邦公开市场委员会（FOMC）审议过程以预测联邦基金目标利率的多智能体系统架构。该系统通过智能体分析结构化指标（如经济数据）和非结构化输入（如褐皮书），进行辩论和投票，模拟委员会决策逻辑。其Chain-of-Draft（CoD）扩展通过强制简洁的多阶段推理，进一步提升了效率和准确性。在2023-2024年会议评估中，FedSight CoD实现了93.75%的准确率和93.33%的稳定性，优于包括MiniFed和Ordinal Random Forest（RF）在内的基线模型，同时提供了与真实FOMC沟通一致的透明推理。",
    "fetch_date": "2025-12-29",
    "id": "20251229_dc3f93f2"
  },
  {
    "title": "Standard and stressed value at risk forecasting using dynamic Bayesian networks",
    "url": "https://arxiv.org/pdf/2512.05661v1",
    "source": "ArXiv",
    "date": "2025-12-05",
    "abstract": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究引入动态贝叶斯网络（DBN）框架来预测风险价值（VaR）和压力风险价值（SVaR），并与几种常用模型进行性能比较。使用1991年至2020年标普500指数的日收益率数据，通过滚动周期和历史收益率生成10天99%的VaR和SVaR预测，而三个DBN模型同时使用历史收益率和预测收益率。通过标准回测和预测误差指标评估模型预测准确性。结果显示，自回归模型提供最准确的VaR预测，而DBN模型尽管包含前瞻性收益率预测，其表现与历史模拟模型相当。对于SVaR，所有模型均产生高度保守的预测，违约次数极少且准确性差异有限。虽然DBN未超越传统模型，但其作为前瞻性方法的可行性为未来将因果推断整合到金融风险预测的研究奠定了基础。",
    "fetch_date": "2025-12-29",
    "id": "20251229_f41d7e01"
  },
  {
    "title": "Risk aversion of insider and dynamic asymmetric information",
    "url": "https://arxiv.org/pdf/2512.05011v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "This paper studies a Kyle-Back model with a risk-averse insider possessing exponential utility and a dynamic stochastic signal about the asset's terminal fundamental value. While the existing literature considers either risk-neutral insiders with dynamic signals or risk-averse insiders with static signals, we establish equilibrium when both features are present. Our approach imposes no restrictions on the magnitude of the risk aversion parameter, extending beyond previous work that requires sufficiently small risk aversion. We employ a weak conditioning methodology to construct a Schrödinger bridge between the insider's signal and the asset price process, an approach that naturally accommodates stochastic signal evolution and removes risk aversion constraints.\n  We derive necessary conditions for equilibrium, showing that the optimal insider strategy must be continuous with bounded variation. Under these conditions, we characterize the market-maker pricing rule and insider strategy that achieve equilibrium. We obtain explicit closed-form solutions for important cases including deterministic and quadratic signal volatilities, demonstrating the tractability of our framework.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了一个Kyle-Back模型，其中内幕交易者具有风险厌恶特征（采用指数效用函数）并拥有关于资产最终基础价值的动态随机信号。现有文献通常只考虑风险中性内幕交易者配动态信号，或风险厌恶内幕交易者配静态信号，而本文在两者同时存在的情况下建立了均衡。我们的方法不对风险厌恶参数的大小施加限制，超越了先前要求风险厌恶足够小的工作。我们采用弱条件方法在内幕交易者信号与资产价格过程之间构建了一个薛定谔桥，这种方法自然地适应了随机信号的演化并消除了风险厌恶约束。我们推导了均衡的必要条件，表明最优内幕交易策略必须是连续且有界变差的。在这些条件下，我们描述了实现均衡的做市商定价规则和内幕交易策略。我们针对包括确定性和二次信号波动率在内的重要案例获得了显式闭式解，证明了我们框架的可处理性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_5800e438"
  },
  {
    "title": "Coordinated Mean-Field Control for Systemic Risk",
    "url": "https://arxiv.org/pdf/2512.04704v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "We develop a robust linear-quadratic mean-field control framework for systemic risk under model uncertainty, in which a central bank jointly optimizes interest rate policy and supervisory monitoring intensity against adversarial distortions. Our model features multiple policy instruments with interactive dynamics, implemented via a variance weight that depends on the policy rate, generating coupling effects absent in single-instrument models. We establish viscosity solutions for the associated HJB--Isaacs equation, prove uniqueness via comparison principles, and provide verification theorems. The linear-quadratic structure yields explicit feedback controls derived from a coupled Riccati system, preserving analytical tractability despite adversarial uncertainty. Simulations reveal distinct loss-of-control regimes driven by robustness-breakdown and control saturation, alongside a pronounced asymmetry in sensitivity between the mean and variance channels. These findings demonstrate the importance of instrument complementarity in systemic risk modeling and control.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文开发了一个稳健的线性二次平均场控制框架，用于在模型不确定性下处理系统性风险。中央银行通过联合优化利率政策和监管监控强度来对抗对抗性扭曲。模型具有多个政策工具的动态交互，通过依赖于政策利率的方差权重实现，产生了单工具模型中不存在的耦合效应。作者建立了相关HJB-Isaacs方程的粘性解，通过比较原理证明了唯一性，并提供了验证定理。线性二次结构产生了从耦合Riccati系统导出的显式反馈控制，尽管存在对抗性不确定性，但仍保持了分析的可处理性。模拟揭示了由稳健性崩溃和控制饱和驱动的不同失控机制，以及均值通道和方差通道之间显著的敏感性不对称。这些发现证明了在系统性风险建模和控制中工具互补性的重要性。",
    "fetch_date": "2025-12-29",
    "id": "20251229_05510e0b"
  },
  {
    "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control",
    "url": "https://arxiv.org/pdf/2512.04653v1",
    "source": "ArXiv",
    "date": "2025-12-04",
    "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种用于多交叉口自适应交通信号控制的半集中训练去中心化执行架构，通过区域划分、参数共享和复合状态奖励设计来解决完全集中式方法的维度灾难和完全分布式方法的局部可观测性问题。虽然架构具有可迁移性并实现了两个具体模型，但其针对交通信号控制这一特定领域，与量化交易的实战应用关联较弱。",
    "fetch_date": "2025-12-29",
    "id": "20251229_6d6beb51"
  },
  {
    "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks",
    "url": "https://arxiv.org/pdf/2512.07162v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "DeepSVM：一种基于物理信息深度算子网络学习随机波动率模型的方法。实时校准随机波动率模型（SVMs）的计算瓶颈在于需要反复求解耦合偏微分方程（PDEs）。本文提出的DeepSVM无需标记训练数据，通过硬约束假设强制满足终端收益和静态无套利条件，并采用残差自适应细化（RAR）稳定高梯度区域的训练。该模型在典型市场动态范围内实现了高度准确的期权定价，但发现其衍生品（Greeks）在平价（ATM）区域存在噪声，凸显了物理信息算子学习中高阶正则化的需求。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b22309f0"
  },
  {
    "title": "Learning to Hedge Swaptions",
    "url": "https://arxiv.org/pdf/2512.06639v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文研究了基于强化学习的深度对冲框架在掉期期权动态对冲中的应用，并与传统的基于敏感性的Rho对冲方法进行了性能对比。研究设计了三种不同目标函数（均方误差、下行风险、条件风险价值）的智能体以捕捉不同的风险偏好，并评估这些目标如何塑造对冲风格。基于三因子无套利动态Nelson-Siegel模型进行模拟实验，结果表明使用两种互换作为对冲工具时能实现接近最优的对冲效果。深度对冲策略能根据市场状态动态调整对冲组合对风险因子的敞口。实验显示，即使在模型存在一定误设的情况下，其表现仍持续优于Rho对冲策略。这些结果突显了强化学习在提供更高效、更具韧性的掉期期权对冲策略方面的潜力。",
    "fetch_date": "2025-12-28",
    "id": "20251228_80dea847"
  },
  {
    "title": "Unveiling Hedge Funds: Topic Modeling and Sentiment Correlation with Fund Performance",
    "url": "https://arxiv.org/pdf/2512.06620v1",
    "source": "ArXiv",
    "date": "2025-12-07",
    "abstract": "The hedge fund industry presents significant challenges for investors due to its opacity and limited disclosure requirements. This pioneering study introduces two major innovations in financial text analysis. First, we apply topic modeling to hedge fund documents-an unexplored domain for automated text analysis-using a unique dataset of over 35,000 documents from 1,125 hedge fund managers. We compared three state-of-the-art methods: Latent Dirichlet Allocation (LDA), Top2Vec, and BERTopic. Our findings reveal that LDA with 20 topics produces the most interpretable results for human users and demonstrates higher robustness in topic assignments when the number of topics varies, while Top2Vec shows superior classification performance. Second, we establish a novel quantitative framework linking document sentiment to fund performance, transforming qualitative information traditionally requiring expert interpretation into systematic investment signals. In sentiment analysis, contrary to expectations, the general-purpose DistilBERT outperforms the finance-specific FinBERT in generating sentiment scores, demonstrating superior adaptability to diverse linguistic patterns found in hedge fund documents that extend beyond specialized financial news text. Furthermore, sentiment scores derived using DistilBERT in combination with Top2Vec show stronger correlations with subsequent fund performance compared to other model combinations. These results demonstrate that automated topic modeling and sentiment analysis can effectively process hedge fund documents, providing investors with new data-driven decision support tools.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本研究在金融文本分析领域引入两大创新：首先，将主题建模应用于对冲基金文档这一自动化文本分析未涉足领域，使用来自1,125家对冲基金管理人的超过35,000份文档数据集，比较了潜在狄利克雷分配（LDA）、Top2Vec和BERTopic三种前沿方法。研究发现，20个主题的LDA对人类用户最具可解释性，且在主题数量变化时表现出更高鲁棒性，而Top2Vec则展现更优的分类性能。其次，建立了一个将文档情感与基金表现相关联的量化框架，将传统需要专家解读的定性信息转化为系统性投资信号。在情感分析中，通用模型DistilBERT意外地优于金融专用模型FinBERT，显示出对对冲基金多样化语言模式的更强适应性。",
    "fetch_date": "2025-12-28",
    "id": "20251228_3d108839"
  },
  {
    "title": "Hybrid Quantum-Classical Ensemble Learning for S\\&P 500 Directional Prediction",
    "url": "https://arxiv.org/pdf/2512.15738v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\\% directional accuracy on S\\&P 500 prediction, a 3.10\\% improvement over individual models.\n  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\\% vs.\\ 52.80\\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\\% to +1.5\\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\\%$), improving ensemble performance (Top-7 models: 60.14\\% vs.\\ all 35 models: 51.2\\%).\n  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文提出了一种混合量子-经典集成学习框架，用于标普500指数的方向性预测。核心创新包括：1）架构多样性优于数据集多样性——在相同数据上结合不同算法（LSTM、决策Transformer、XGBoost等）比单一架构多数据集训练效果更佳（60.14% vs. 52.80%）；2）4量子比特变分量子电路增强情感分析，为各模型带来0.8%-1.5%的准确率提升；3）智能筛选排除弱预测器（准确率<52%），优化集成性能。最终实现60.14%的方向预测准确率，较单一模型提升3.10%，对量化交易实战具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_fab16cfe"
  },
  {
    "title": "Deep learning approaches in Finance: Forecasting volatility and enhancing Quantitative trading strategies",
    "url": "https://etheses.whiterose.ac.uk/id/eprint/27202/",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… services industry using Deep Learning architectures. The main focus is on advancing current approaches in the areas of Risk Management and Quantitative Trading. The former is …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文聚焦于深度学习在金融领域的应用，主要致力于推动风险管理与量化交易领域的现有方法。核心内容包括利用深度学习架构预测波动率，并以此增强量化交易策略。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6403d992"
  },
  {
    "title": "Intraday and Post-Market Investor Sentiment for Stock Price Prediction: A Deep Learning Framework with Explainability and Quantitative Trading Strategy",
    "url": "https://www.mdpi.com/2079-8954/13/5/390",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… In contrast to traditional deep learning models, which are often … Quantitative trading backtesting under the T+1 trading … Most academic studies neglect real-world trading constraints…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种用于股价预测的深度学习框架，结合了盘中与盘后投资者情绪分析，并具备模型可解释性。研究特别设计了考虑T+1交易制度的量化交易策略进行回测，强调了传统学术研究常忽略的实际交易约束，对实战交易具有直接应用价值。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ce0d31fc"
  },
  {
    "title": "Research on quantitative investment strategies based on deep learning",
    "url": "https://www.mdpi.com/1999-4893/12/2/35",
    "source": "Scholar",
    "date": "2025-12-28",
    "abstract": "… with four trading strategies (Long Call, Short Call, Long Put, Short Put) where deep learning … mirror the impact of its accuracy on quantitative trading strategies in a straightforward way. …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "基于深度学习的量化投资策略研究，涉及四种交易策略（买入看涨期权、卖出看涨期权、买入看跌期权、卖出看跌期权），通过深度学习模型直接反映其预测准确性对量化交易策略的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d4332f98"
  },
  {
    "title": "Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing",
    "url": "https://arxiv.org/pdf/2512.17923v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "我们引入了一种新颖的混淆测试方法，用于验证大型语言模型是否通过因果推理而非时间关联来检测结构性市场模式。在标普500期权数据的242个交易日（覆盖95.6%）上测试三种做市商对冲约束模式（伽马持仓、股票钉住、0DTE对冲），我们发现LLM使用无偏提示（仅提供原始伽马暴露值，无制度标签或时间背景）实现了71.5%的检测率。WHO-WHOM-WHAT因果框架迫使模型识别观察到的市场动态背后的经济参与者（做市商）、受影响方（方向性交易者）和结构性机制（强制对冲）。关键的是，即使季度经济盈利能力变化，检测准确率（91.2%）保持稳定，表明模型识别的是结构性约束而非盈利模式。当提供制度标签时，检测率提升至100%，但71.5%的无偏率验证了真正的模式识别。我们的发现表明，LLM通过纯结构性推理检测复杂金融机制具有新兴能力，对系统交易有潜在影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_19f9e1af"
  },
  {
    "title": "Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance",
    "url": "https://arxiv.org/pdf/2512.15739v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种贝叶斯分析框架，用于金融风险预测与合规中的不确定性管理。该方法在波动率预测、欺诈检测和合规监控方面提升了风险处理能力。通过评估标普500指数日收益率的单日95%风险价值预测（训练期2000-2019，测试期2020-2024），发现LSTM基线接近名义校准，而带学生t分布的GARCH(1,1)模型低估尾部风险。提出的折扣因子DLM模型产生略宽松的VaR估计，存在聚集性违规证据。贝叶斯逻辑回归提高了欺诈检测的召回率和AUC-ROC，分层Beta状态空间模型提供了透明自适应的合规风险评估。该框架以精确的不确定性量化、可解释性和GPU加速分析为特点。",
    "fetch_date": "2025-12-28",
    "id": "20251228_29b631c7"
  },
  {
    "title": "Analysis of Contagion in China's Stock Market: A Hawkes Process Perspective",
    "url": "https://arxiv.org/pdf/2512.08000v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "This study explores contagion in the Chinese stock market using Hawkes processes to analyze autocorrelation and cross-correlation in multivariate time series data. We examine whether market indices exhibit trending behavior and whether sector indices influence one another. By fitting self-exciting and inhibitory Hawkes processes to daily returns of indices like the Shanghai Composite, Shenzhen Component, and ChiNext, as well as sector indices (CSI Consumer, Healthcare, and Financial), we identify long-term dependencies and trending patterns, including upward, downward, and oversold rebound trends. Results show that during high trading activity, sector indices tend to sustain their trends, while low activity periods exhibit strong sector rotation. This research models stock price movements using spatiotemporal Hawkes processes, leveraging conditional intensity functions to explain sector rotation, advancing the understanding of financial contagion.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究使用Hawkes过程分析中国股市的传染效应，通过拟合自激励和抑制型Hawkes过程到上证综指、深证成指、创业板指及消费、医疗、金融等行业指数的日收益率数据，识别长期依赖性和趋势模式（包括上升、下降和超跌反弹趋势）。研究发现高交易活跃度期间行业指数倾向于维持趋势，低活跃度期间则呈现显著的行业轮动现象。该研究通过时空Hawkes过程建模股价变动，利用条件强度函数解释行业轮动，深化了对金融传染机制的理解。",
    "fetch_date": "2025-12-28",
    "id": "20251228_d0ecab01"
  },
  {
    "title": "Asian option valuation under price impact",
    "url": "https://arxiv.org/pdf/2512.07154v1",
    "source": "ArXiv",
    "date": "2025-12-08",
    "abstract": "We study the valuation of Asian options in a binomial market with permanent price impact, extending the Cox-Ross-Rubinstein framework under a modified risk-neutral probability. We obtain an exact pathwise representation for geometric Asian options and derive two-sided bounds for arithmetic Asian options. Our analysis identifies the no-arbitrage region in terms of hedging volumes and shows that permanent price impact systematically raises Asian option prices. Numerical examples illustrate the effect of the impact parameter and hedging volumes on the resulting prices.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在带有永久价格影响的二项式市场中研究亚式期权的定价，扩展了修正风险中性概率下的Cox-Ross-Rubinstein框架。研究获得了几何亚式期权的精确路径表示，并推导了算术亚式期权的双边边界。分析从对冲头寸角度识别了无套利区间，表明永久价格影响会系统性提高亚式期权价格。数值示例展示了影响参数和对冲头寸对最终价格的影响。",
    "fetch_date": "2025-12-28",
    "id": "20251228_a5b016a2"
  },
  {
    "title": "Market Reactions and Information Spillovers in Bank Mergers: A Multi-Method Analysis of the Japanese Banking Sector",
    "url": "https://arxiv.org/pdf/2512.06550v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Major bank mergers and acquisitions (M&A) transform the financial market structure, but their valuation and spillover effects remain open to question. This study examines the market reaction to two M&A events: the 2005 creation of Mitsubishi UFJ Financial Group following the Financial Big Bang in Japan, and the 2018 merger involving Resona Holdings after the global financial crisis. The multi-method analysis in this research combines several distinct methods to explore these M&A events. An event study using the market model, the capital asset pricing model (CAPM), and the Fama-French three-factor model is implemented to estimate cumulative abnormal returns (CAR) for valuation purposes. Vector autoregression (VAR) models are used to test for Granger causality and map dynamic effects using impulse response functions (IRFs) to investigate spillovers. Propensity score matching (PSM) helps provide a causal estimate of the average treatment effect on the treated (ATT). The analysis detected a significant positive market reaction to the mergers. The findings also suggest the presence of prolonged positive spillovers to other banks, which may indicate a synergistic effect among Japanese banks. Combining these methods provides a unique perspective on M&A events in the Japanese banking sector, offering valuable insights for investors, managers, and regulators concerned with market efficiency and systemic stability",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究通过多方法分析评估日本银行业两次重大并购事件（2005年三菱UFJ金融集团成立与2018年Resona控股合并）的市场反应与信息溢出效应。采用事件研究法（市场模型、CAPM、Fama-French三因子模型）计算累积异常收益（CAR），运用向量自回归（VAR）模型检验格兰杰因果关系并通过脉冲响应函数（IRFs）分析动态溢出效应，辅以倾向得分匹配（PSM）估计平均处理效应（ATT）。研究发现并购产生显著正向市场反应，并存在对其他银行的持续正向溢出效应，暗示日本银行业可能存在协同效应。",
    "fetch_date": "2025-12-28",
    "id": "20251228_b46875b4"
  },
  {
    "title": "Amortizing Perpetual Options",
    "url": "https://arxiv.org/pdf/2512.06505v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "In this work, we introduce amortizing perpetual options (AmPOs), a fungible variant of continuous-installment options suitable for exchange-based trading. Traditional installment options lapse when holders cease their payments, destroying fungibility across units of notional. AmPOs replace explicit installment payments and the need for lapsing logic with an implicit payment scheme via a deterministic decay in the claimable notional. This amortization ensures all units evolve identically, preserving fungibility. Under the Black-Scholes framework, AmPO valuation can be reduced to an equivalent vanilla perpetual American option on a dividend-paying asset. In this way, analytical expressions are possible for the exercise boundaries and risk-neutral valuations for calls and puts. These formulas and relations allow us to derive the Greeks and study comparative statics with respect to the amortization rate. Illustrative numerical case studies demonstrate how the amortization rate shapes option behavior and reveal the resulting tradeoffs in the effective volatility sensitivity.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文介绍了摊销型永续期权（AmPOs），这是一种适用于交易所交易的可替代连续分期期权变体。传统分期期权在持有人停止支付时会失效，破坏了名义金额单位间的可替代性。AmPOs通过可索赔名义金额的确定性衰减隐含支付方案，取代了显性分期付款和失效逻辑。这种摊销确保所有单位以相同方式演变，保持了可替代性。在Black-Scholes框架下，AmPO估值可简化为等价于带股息资产上的普通永续美式期权。通过这种方式，可以得出看涨和看跌期权的行权边界和风险中性估值的解析表达式。这些公式和关系使我们能够推导希腊字母，并研究相对于摊销率的比较静态。示例数值案例研究展示了摊销率如何塑造期权行为，并揭示了有效波动率敏感性中的权衡取舍。",
    "fetch_date": "2025-12-28",
    "id": "20251228_7b969bb3"
  },
  {
    "title": "Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market",
    "url": "https://arxiv.org/pdf/2512.06473v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $ρ_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "复杂系统中的相关性常被非平稳性、长程记忆和重尾波动所掩盖，限制了传统协方差分析的有效性。为应对这些挑战，本文使用多分形去趋势互相关系数ρ_r构建了尺度与波动依赖的相关矩阵，该系数能选择性地强调不同幅度的波动。作者检验了这些去趋势相关矩阵的谱特性，并与从合成高斯及q高斯信号以相同方式计算得到的矩阵谱特性进行比较。结果表明，即使在时间序列不存在互相关的情况下，去趋势处理、重尾分布和波动阶参数r共同产生的谱也与随机情况显著偏离。将该框架应用于2021-2024年间140种主要加密货币的一分钟收益率数据，揭示了稳健的集体模式，包括一个主导的市场因子和几个行业成分，其强度取决于分析的尺度和波动阶。在滤除市场模式后，经验特征值的主体部分与随机矩阵理论预测的极限分布紧密对齐。",
    "fetch_date": "2025-12-28",
    "id": "20251228_ae9ae498"
  },
  {
    "title": "Wealth or Stealth? The Camouflage Effect in Insider Trading",
    "url": "https://arxiv.org/pdf/2512.06309v1",
    "source": "ArXiv",
    "date": "2025-12-06",
    "abstract": "We consider a Kyle-type model where insider trading takes place among a potentially large population of liquidity traders and is subject to legal penalties. Insiders exploit the liquidity provided by the trading masses to \"camouflage\" their actions and balance expected wealth with the necessary stealth to avoid detection. Under a diverse spectrum of prosecution schemes, we establish the existence of equilibria for arbitrary population sizes and a unique limiting equilibrium. A convergence analysis determines the scale of insider trading by a stealth index $γ$, revealing that the equilibrium can be closely approximated by a simple limit due to diminished price informativeness. Empirical aspects are derived from two calibration experiments using non-overlapping data sets spanning from 1980 to 2018, which underline the indispensable role of a large population in insider trading models with legal risk, along with important implications for the incidence of stealth trading and the deterrent effect of legal enforcement.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个Kyle型模型，研究内幕交易者在面临法律处罚风险时，如何利用大量流动性交易者提供的流动性来“伪装”其交易行为，在预期财富与避免被发现的必要隐蔽性之间取得平衡。通过分析多种起诉方案，证明了任意市场规模下均衡的存在性及唯一的极限均衡。收敛分析通过隐蔽指数γ确定内幕交易规模，并揭示由于价格信息性减弱，均衡可被一个简单极限近似逼近。实证部分基于1980年至2018年的两个非重叠数据集进行校准实验，强调了在考虑法律风险的内幕交易模型中大规模交易群体的不可或缺作用，并对隐蔽交易的发生频率及法律执行的威慑效应提出了重要启示。",
    "fetch_date": "2025-12-28",
    "id": "20251228_6e05cc4e"
  },
  {
    "title": "Not All Factors Crowd Equally: Modeling, Measuring, and Trading on Alpha Decay",
    "url": "https://arxiv.org/pdf/2512.11913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We derive a specific functional form for factor alpha decay -- hyperbolic decay alpha(t) = K/(1+lambda*t) -- from a game-theoretic equilibrium model, and test it against linear and exponential alternatives. Using eight Fama-French factors (1963--2024), we find: (1) Hyperbolic decay fits mechanical factors. Momentum exhibits clear hyperbolic decay (R^2 = 0.65), outperforming linear (0.51) and exponential (0.61) baselines -- validating the equilibrium foundation. (2) Not all factors crowd equally. Mechanical factors (momentum, reversal) fit the model; judgment-based factors (value, quality) do not -- consistent with a signal-ambiguity taxonomy paralleling Hua and Sun's \"barriers to entry.\" (3) Crowding accelerated post-2015. Out-of-sample, the model over-predicts remaining alpha (0.30 vs. 0.15), correlating with factor ETF growth (rho = -0.63). (4) Average returns are efficiently priced. Crowding-based factor selection fails to generate alpha (Sharpe: 0.22 vs. 0.39 factor momentum benchmark). (5) Crowding predicts tail risk. Out-of-sample (2001--2024), crowded reversal factors show 1.7--1.8x higher crash probability (bottom decile returns), while crowded momentum shows lower crash risk (0.38x, p = 0.006). Our findings extend equilibrium crowding models (DeMiguel et al.) to temporal dynamics and show that crowding predicts crashes, not means -- useful for risk management, not alpha generation.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该论文从博弈论均衡模型推导出因子阿尔法衰减的特定函数形式——双曲衰减alpha(t)=K/(1+lambda*t)，并在线性和指数衰减基准上进行检验。基于八个Fama-French因子（1963-2024）的研究发现：（1）机械因子（如动量）呈现清晰的双曲衰减（R²=0.65），验证了均衡基础；（2）因子拥挤存在异质性——机械因子（动量、反转）符合模型，而基于判断的因子（价值、质量）则不符合，这与Hua和Sun的“进入壁垒”信号模糊度分类一致；（3）2015年后拥挤加速，样本外模型高估剩余阿尔法（0.30 vs. 0.15），且与因子ETF增长负相关（ρ=-0.63）；（4）平均收益已被有效定价，基于拥挤的因子选择未能产生阿尔法（夏普比率：0.22 vs. 因子动量基准0.39）；（5）拥挤可预测尾部风险——样本外（2001-2024）拥挤的反转因子崩盘概率高1.7-1.8倍，而拥挤的动量因子崩盘风险较低。",
    "fetch_date": "2025-12-27",
    "id": "20251227_0101e588"
  },
  {
    "title": "Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making",
    "url": "https://arxiv.org/pdf/2512.17936v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in Türkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "面对日益增长的金融不确定性和市场复杂性，本研究提出了一种新颖的风险感知金融预测框架，该框架将先进的机器学习技术与直觉模糊多准则决策（MCDM）相结合。该框架专为BIST 100指数设计，并通过土耳其一家主要国防公司的案例研究进行验证，融合了结构化金融数据、非结构化文本数据和宏观经济指标，以提高预测准确性和鲁棒性。它包含一套混合模型，包括极端梯度提升（XGBoost）、长短期记忆（LSTM）网络和图神经网络（GNN），以提供具有量化不确定性的概率预测。实证结果显示高预测准确性，净利润平均绝对百分比误差（MAPE）为3.03%，关键财务指标的95%置信区间较窄。风险感知分析显示有利的风险回报特征，夏普比率为1.25，索提诺比率更高为1.80，表明在市场波动下相对较低的下行波动性和稳健表现。敏感性分析表明关键财务指标具有稳定性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_96d3f766"
  },
  {
    "title": "Exploratory Mean-Variance with Jumps: An Equilibrium Approach",
    "url": "https://arxiv.org/pdf/2512.09224v1",
    "source": "ArXiv",
    "date": "2025-12-10",
    "abstract": "Revisiting the continuous-time Mean-Variance (MV) Portfolio Optimization problem, we model the market dynamics with a jump-diffusion process and apply Reinforcement Learning (RL) techniques to facilitate informed exploration within the control space. We recognize the time-inconsistency of the MV problem and adopt the time-inconsistent control (TIC) approach to analytically solve for an exploratory equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem. Our approach accounts for time-inconsistent preferences and actions, and our equilibrium policy is the best option an investor can take at any given time during the investment period. Moreover, we leverage the martingale properties of the equilibrium policy, design a RL model, and propose an Actor-Critic RL algorithm. All of our RL model parameters converge to the corresponding true values in a simulation study. Our numerical study on 24 years of real market data shows that the proposed RL model is profitable in 13 out of 14 tests, demonstrating its practical applicability in real world investment.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文重新审视连续时间均值-方差（MV）投资组合优化问题，采用跳跃扩散过程建模市场动态，并应用强化学习（RL）技术在控制空间内进行知情探索。针对MV问题的时间不一致性，采用时间不一致控制（TIC）方法解析求解探索性均衡投资策略——一个以经典MV问题均衡控制为中心的高斯分布。该策略考虑了时间不一致的偏好与行动，是投资期内任一时刻的最佳选择。此外，利用均衡策略的鞅性质设计RL模型，提出Actor-Critic RL算法。模拟研究中所有RL模型参数均收敛至真实值，基于24年真实市场数据的数值研究表明，所提RL模型在14次测试中13次盈利，证明了其在实际投资中的适用性。",
    "fetch_date": "2025-12-27",
    "id": "20251227_b4ce1604"
  },
  {
    "title": "Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market",
    "url": "https://arxiv.org/abs/2506.06356",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… -day turnover quantitative trading algorithm that integrates advanced deep learning techniques … Index Terms—quantitative trading, deep learning, crosssectional prediction, multi-day …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度学习增强的多日换手率量化交易算法应用于中国A股市场。该算法整合了先进的深度学习技术，专注于横截面预测和多日交易策略。",
    "fetch_date": "2025-12-27",
    "id": "20251227_c3728bb7"
  },
  {
    "title": "Intelligent optimization based multi-factor deep learning stock selection model and quantitative trading strategy",
    "url": "https://www.mdpi.com/2227-7390/10/4/566",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… Thirdly, this paper designs and implements a quantitative trading strategy. Based on the CS-GRU stock selection model, this paper designs and implements a quantitative trading …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了一种基于智能优化的多因子深度学习选股模型与量化交易策略。首先，构建了CS-GRU选股模型（结合了Cuckoo Search优化算法与门控循环单元网络），用于预测股票收益。其次，基于该模型设计并实施了量化交易策略，包括信号生成、仓位管理和风险控制等实战环节。最后，通过回测验证了策略的有效性，表明该模型在实战交易中具有潜在应用价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_887f4f01"
  },
  {
    "title": "Research on Deep Learning-Based Quantitative Trading Models",
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-99477-7_12",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… This paper examines the role of quantitative trading in finance and the potential applications of deep learning. Quantitative trading automates investment strategies using mathematical …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文探讨了量化交易在金融领域的作用及深度学习的潜在应用。量化交易利用数学模型自动化投资策略...",
    "fetch_date": "2025-12-27",
    "id": "20251227_ed097bf0"
  },
  {
    "title": "Sustainability, accuracy, fairness, and explainability (safe) machine learning in quantitative trading",
    "url": "https://www.mdpi.com/2227-7390/13/3/442",
    "source": "Scholar",
    "date": "2025-12-27",
    "abstract": "… based signal strategies: those based on deep learning models and those grounded in classical machine learning techniques. The deep learning models employed in this research were …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了可持续性、准确性、公平性和可解释性（SAFE）机器学习在量化交易中的应用，比较了基于深度学习模型和经典机器学习技术的信号策略。研究采用的深度学习模型包括...，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-27",
    "id": "20251227_020ded1a"
  },
  {
    "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
    "url": "https://arxiv.org/pdf/2512.10913v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "强化学习（RL）作为一种金融决策的创新方法，为传统方法难以解决的复杂投资问题提供了专门解决方案。该综述分析了2017-2025年的167篇文章，重点关注做市、投资组合优化和算法交易。研究发现RL在金融应用中存在关键性能问题和挑战，但在做市等领域相比传统方法具有优势。研究提出了一个统一框架来解决可解释性、鲁棒性和部署可行性等常见问题。基于合成数据的实证证据表明，实施质量和领域知识通常比算法复杂性更重要。研究强调需要可解释的RL架构以满足监管要求、增强非平稳环境下的鲁棒性，并建立标准化基准测试协议。建议机构应减少对算法复杂性的关注，更多关注市场微观结构、监管约束和风险管理。",
    "fetch_date": "2025-12-27",
    "id": "20251227_801c29bb"
  },
  {
    "title": "Local and Global Balance in Financial Correlation Networks: an Application to Investment Decisions",
    "url": "https://arxiv.org/pdf/2512.10606v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "The global balance is a well-known indicator of the behavior of a signed network. Recent literature has introduced the concept of local balance as a measure of the contribution of a single node to the overall balance of the network. In the present research, we investigate the potential of using deviations of local balance from global balance as a criterion for selecting outperforming assets. The underlying idea is that, during financial crises, most assets in the investment universe behave similarly: losses are severe and widespread, and the global balance of the correlation-based signed network reaches its maximum value. Under such circumstances, standard diversification (mainly related to portfolio size) is unable to reduce risk or limit losses. Therefore, it may be useful to concentrate portfolio exposures on the few assets - if such assets exist-that behave differently from the rest of the market. We argue that these assets are those for which the local balance strongly departs from the global balance of the underlying signed network. The paper supports this hypothesis through an application using real financial data. The results, in both descriptive and predictive contexts, confirm the proposed intuition.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种基于金融相关网络局部平衡与全局平衡偏离度的资产选择方法。核心观点是：在金融危机期间，大多数资产表现趋同，传统分散化策略失效；此时，应集中投资于那些局部平衡与网络全局平衡显著偏离的资产，因为这些资产的市场行为与整体市场不同。研究通过真实金融数据验证了这一假设，表明该方法在描述性和预测性场景下均能识别出表现优异的资产。",
    "fetch_date": "2025-12-27",
    "id": "20251227_9fed57fe"
  },
  {
    "title": "A New Application of Hoeffding's Inequality Can Give Traders Early Warning of Financial Regime Change",
    "url": "https://arxiv.org/pdf/2512.08851v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "Hoeffding's Inequality provides the maximum probability that a series of n draws from a bounded random variable differ from the variable's true expectation u by more than given tolerance t. The random variable is typically the error rate of a classifier in machine learning applications. Here, a trading strategy is premised on the assumption of an underlying distribution of causal factors, in other words, a market regime, and the random variable is the performance of that trading strategy. A larger deviation of observed performance from the trader's expectation u can be characterized as a lower probability that the financial regime supporting that strategy remains in force, and a higher probability of financial regime change. The changing Hoeffding probabilities can be used as an early warning indicator of this change.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种将霍夫丁不等式应用于量化交易的新方法。该方法将交易策略的绩效视为有界随机变量，通过计算观察到的绩效与预期绩效之间的偏差概率，来评估当前市场状态（即金融制度）是否发生变化。当偏差增大时，霍夫丁概率降低，表明支持该策略的市场制度可能不再有效，从而为金融制度变更提供早期预警。该方法为基于市场制度假设的交易策略提供了一种理论上的风险监控工具。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d6fbced2"
  },
  {
    "title": "Pareto-optimal reinsurance under dependence uncertainty",
    "url": "https://arxiv.org/pdf/2512.11430v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper studies Pareto-optimal reinsurance design in a monopolistic market with multiple primary insurers and a single reinsurer, all with heterogeneous risk preferences. The risk preferences are characterized by a family of risk measures, called Range Value-at-Risk (RVaR), which includes both Value-at-Risk (VaR) and Expected Shortfall (ES) as special cases. Recognizing the practical difficulty of accurately estimating the dependence structure among the insurers' losses, we adopt a robust optimization approach that assumes the marginal distributions are known while leaving the dependence structure unspecified. We provide a complete characterization of optimal indemnity schedules under the worst-case scenario, showing that the infinite-dimensional optimization problem can be reduced to a tractable finite-dimensional problem involving only two or three parameters for each indemnity function. Additionally, for independent and identically distributed risks, we exploit the argument of asymptotic normality to derive optimal two-parameter layer contracts. Finally, numerical applications are considered in a two-insurer setting to illustrate the influence of the dependence structures and heterogeneous risk tolerances on optimal strategies and the corresponding risk evaluation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究垄断市场中多个原保险公司与单一再保险公司之间的帕累托最优再保险设计，各方具有异质性风险偏好。风险偏好由一系列风险度量（称为范围风险价值，RVaR）表征，该度量包含风险价值（VaR）和预期损失（ES）作为特例。考虑到准确估计保险公司损失间依赖结构的实际困难，作者采用鲁棒优化方法，假设边际分布已知而依赖结构未指定。在最坏情况下，作者完整刻画了最优赔偿方案，表明无限维优化问题可简化为仅涉及每个赔偿函数两到三个参数的可处理有限维问题。此外，对于独立同分布风险，作者利用渐近正态性论证推导出最优两参数分层合约。最后，通过双保险公司设置的数值应用说明了依赖结构的影响。",
    "fetch_date": "2025-12-27",
    "id": "20251227_80cceaa5"
  },
  {
    "title": "Generative AI for Analysts",
    "url": "https://arxiv.org/pdf/2512.19705v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究探讨生成式人工智能（AI）如何改变金融分析师的工作。以2023年FactSet AI平台上线为自然实验，发现采用AI后报告内容显著丰富全面——信息源增加40%，主题覆盖扩大34%，高级分析方法使用提升25%，同时时效性改善。然而，预测误差上升59%，因为AI辅助报告呈现更平衡的正负面信息，尤其对认知负荷较重的分析师更难整合。使用其他数据供应商的安慰剂测试证实这些效应是FactSet AI集成独有的。总体而言，研究揭示了生成式AI在金融信息生产中既带来生产力提升，也存在认知局限。",
    "fetch_date": "2025-12-27",
    "id": "20251227_778da003"
  },
  {
    "title": "Option-Implied Zero-Coupon Yields: Unifying Bond and Equity Markets",
    "url": "https://arxiv.org/pdf/2512.10823v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "This paper addresses a critical inconsistency in models of the term structure of interest rates (TSIR), where zero-coupon bonds are priced under risk-neutral measures distinct from those used in equity markets. We propose a unified TSIR framework that treats zero-coupon bonds as European options with deterministic payoffs ensuring that they are priced under the same risk-neutral measure that governs equity derivatives. Using put-call parity, we extract zero-coupon bond implied yield curves from S&P 500 index options and compare them with the US daily treasury par yield curves. As the implied yield curves contain maturity time T and strike price K as independent variables, we investigate the K-dependence of the implied yield curve. Our findings, that at-the-money, option-implied yield curves provide the closest match to treasury par yield curves, support the view that the equity options market contains information that is highly relevant for the TSIR. By insisting that the risk-neutral measure used for bond valuation is the same as that revealed by equity derivatives, we offer a new organizing principle for future TSIR research.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对利率期限结构模型中的关键不一致性提出解决方案，即零息债券与权益衍生品使用不同的风险中性测度定价。作者提出统一框架，将零息债券视为具有确定性收益的欧式期权，确保其与权益衍生品在同一风险中性测度下定价。利用看跌-看涨平价关系，从标普500指数期权中提取零息债券隐含收益率曲线，并与美国国债平价收益率曲线比较。研究发现，平值期权隐含收益率曲线与国债曲线最接近，表明权益期权市场包含对利率期限结构高度相关的信息。该框架为未来TSIR研究提供了新的组织原则。",
    "fetch_date": "2025-12-27",
    "id": "20251227_d45f5bf0"
  },
  {
    "title": "Volatility time series modeling by single-qubit quantum circuit learning",
    "url": "https://arxiv.org/pdf/2512.10584v1",
    "source": "ArXiv",
    "date": "2025-12-11",
    "abstract": "We employ single-qubit quantum circuit learning (QCL) to model the dynamics of volatility time series. To assess its effectiveness, we generate synthetic data using the Rational GARCH model, which is specifically designed to capture volatility asymmetry. Our results show that QCL-based volatility predictions preserve the negative return-volatility correlation, a hallmark of asymmetric volatility dynamics. Moreover, analysis of the Hurst exponent and multifractal characteristics indicates that the predicted series, like the original synthetic data, exhibits anti-persistent behavior and retains its multifractal structure.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该研究采用单量子比特量子电路学习（QCL）对波动率时间序列进行建模。通过使用专门捕捉波动率不对称性的Rational GARCH模型生成合成数据进行评估，结果表明基于QCL的波动率预测保留了负收益-波动率相关性（这是不对称波动率动态的标志特征）。此外，对赫斯特指数和多重分形特征的分析表明，预测序列与原始合成数据类似，表现出反持续性行为并保留了其多重分形结构。",
    "fetch_date": "2025-12-27",
    "id": "20251227_568b9ce8"
  },
  {
    "title": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
    "url": "https://arxiv.org/pdf/2512.17929v1",
    "source": "ArXiv",
    "date": "2025-12-09",
    "abstract": "We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "我们研究了在宏观经济关系不确定且随时间变化的情况下，中央银行应如何动态设定短期名义利率以稳定通胀和失业率。我们将货币政策建模为一个序贯决策问题，中央银行每季度观察宏观经济状况并选择利率调整。使用公开可用的历史联邦储备经济数据（FRED），我们构建了一个线性高斯转移模型，并实现了一个具有二次损失奖励函数的离散动作马尔可夫决策过程。我们选择了九种不同的强化学习方法与泰勒规则和朴素基线进行比较，包括表格Q学习变体、SARSA、Actor-Critic、深度Q网络、具有不确定性量化的贝叶斯Q学习以及具有部分可观测性的POMDP公式。令人惊讶的是，标准的表格Q学习取得了最佳性能（-615.13 ± 309.58 平均回报），优于增强的RL方法和传统政策规则。我们的结果表明，虽然复杂的RL技术在货币政策应用中显示出前景，但更简单的方法在应对不确定性时可能更稳健。",
    "fetch_date": "2025-12-27",
    "id": "20251227_ebb24070"
  },
  {
    "title": "EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction",
    "url": "https://arxiv.org/pdf/2512.12727v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "EXFormer：一种用于外汇收益率预测的多尺度趋势感知Transformer，具有动态变量选择功能。该论文提出了一种新颖的基于Transformer的架构，专门用于预测每日汇率收益率。它引入了多尺度趋势感知自注意力机制，采用具有不同感受野的并行卷积分支，以基于局部斜率对齐观测值，在保持长程依赖性的同时对制度转换保持敏感。动态变量选择器为28个与汇率收益率相关的外生协变量分配时变重要性权重，提供先验可解释性。嵌入的挤压-激励块重新校准通道响应，以强调信息特征并抑制预测中的噪声。使用欧元/美元、美元/日元和英镑/美元的每日数据，在五个不同的滑动窗口上进行了样本外评估。EXFormer始终优于随机游走和其他基线模型，提高了方向准确性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_95743fd6"
  },
  {
    "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting",
    "url": "https://arxiv.org/pdf/2512.12250v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "该研究提出了一种混合建模框架，将随机波动率模型与长短期记忆神经网络相结合，用于标普500指数的波动率预测。SV模型提升了统计精度并捕捉了潜在的波动动态，特别是在应对突发事件时；LSTM网络则增强了模型检测金融时间序列中复杂非线性模式的能力。研究采用滚动窗口方法训练模型并生成一步超前波动率预测，通过统计测试和投资模拟评估了混合SV-LSTM模型的性能。结果表明，该混合方法优于单独的SV和LSTM模型，为改进风险评估和战略投资规划提供了基础。",
    "fetch_date": "2025-12-26",
    "id": "20251226_36f39515"
  },
  {
    "title": "Deep Hedging with Reinforcement Learning: A Practical Framework for Option Risk Management",
    "url": "https://arxiv.org/pdf/2512.12420v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "We present a reinforcement-learning (RL) framework for dynamic hedging of equity index option exposures under realistic transaction costs and position limits. We hedge a normalized option-implied equity exposure (one unit of underlying delta, offset via SPY) by trading the underlying index ETF, using the option surface and macro variables only as state information and not as a direct pricing engine. Building on the \"deep hedging\" paradigm of Buehler et al. (2019), we design a leak-free environment, a cost-aware reward function, and a lightweight stochastic actor-critic agent trained on daily end-of-day panel data constructed from SPX/SPY implied volatility term structure, skew, realized volatility, and macro rate context. On a fixed train/validation/test split, the learned policy improves risk-adjusted performance versus no-hedge, momentum, and volatility-targeting baselines (higher point-estimate Sharpe); only the GAE policy's test-sample Sharpe is statistically distinguishable from zero, although confidence intervals overlap with a long-SPY benchmark so we stop short of claiming formal dominance. Turnover remains controlled and the policy is robust to doubled transaction costs. The modular codebase, comprising a data pipeline, simulator, and training scripts, is engineered for extensibility to multi-asset overlays, alternative objectives (e.g., drawdown or CVaR), and intraday data. From a portfolio management perspective, the learned overlay is designed to sit on top of an existing SPX or SPY allocation, improving the portfolio's mean-variance trade-off with controlled turnover and drawdowns. We discuss practical implications for portfolio overlays and outline avenues for future work.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种基于强化学习的深度对冲框架，用于在现实交易成本和头寸限制下动态管理股指期权风险敞口。该方法采用无泄漏环境设计、成本感知的奖励函数和轻量级随机演员-评论家智能体，使用SPX/SPY隐含波动率期限结构、偏度、已实现波动率和宏观利率等日频面板数据进行训练。在固定训练/验证/测试集划分下，学习到的策略相比无对冲、动量策略和波动率目标基准展现出更高的风险调整后绩效（夏普比率点估计值更高），且交易周转率受控，策略对加倍交易成本具有稳健性。",
    "fetch_date": "2025-12-26",
    "id": "20251226_dfaa45fb"
  },
  {
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "url": "https://arxiv.org/pdf/2512.14744v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《VERAFI：通过神经符号策略生成实现可验证的代理金融智能》针对金融AI系统在推理中产生计算错误和违反监管规定（即使检索完美）的盲点，提出了一个结合神经符号策略生成的代理框架。VERAFI融合了最先进的密集检索与交叉编码器重排序、支持金融工具的代理，以及覆盖GAAP合规性、SEC要求和数学验证的自动化推理策略。在FinanceBench上的综合评估显示显著改进：传统密集检索加重排序仅实现52.4%的事实正确率，而VERAFI的综合方法达到94.7%，相对提升81%。神经符号策略层本身比纯代理处理贡献了4.3个百分点的增益，专门针对持续的数学和逻辑错误。通过将金融领域专业知识直接整合到推理过程中，VERAFI为在实战交易中实现可信赖的金融智能提供了一条实用路径。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b47b6951"
  },
  {
    "title": "Transfer Learning (Il)liquidity",
    "url": "https://arxiv.org/pdf/2512.11731v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "The estimation of the Risk Neutral Density (RND) implicit in option prices is challenging, especially in illiquid markets. We introduce the Deep Log-Sum-Exp Neural Network, an architecture that leverages Deep and Transfer learning to address RND estimation in the presence of irregular and illiquid strikes. We prove key statistical properties of the model and the consistency of the estimator. We illustrate the benefits of transfer learning to improve the estimation of the RND in severe illiquidity conditions through Monte Carlo simulations, and we test it empirically on SPX data, comparing it with popular estimation methods. Overall, our framework shows recovery of the RND in conditions of extreme illiquidity with as few as three option quotes.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "论文《迁移学习（非）流动性》提出了一种用于估计期权价格中隐含风险中性密度（RND）的深度对数求和指数神经网络架构，特别针对非流动性和不规则行权价市场。该模型利用深度学习和迁移学习技术，在极端流动性不足条件下（仅需三个期权报价）仍能有效恢复RND，并通过蒙特卡洛模拟和SPX数据实证验证了其优于传统方法的性能。",
    "fetch_date": "2025-12-26",
    "id": "20251226_f3dbe600"
  },
  {
    "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
    "url": "https://arxiv.org/pdf/2512.17945v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "金融机构在部署机器学习模型进行信用风险评估时，面临预测准确性与可解释性之间的权衡。单调性约束使模型行为与领域知识保持一致，但其性能成本——即“单调性的代价”——尚未得到充分量化。本文在五个公共数据集和三个库上，对信用违约概率的单调约束与无约束梯度提升模型进行了基准测试。我们将单调性代价定义为从无约束模型转向约束模型时标准性能指标的相对变化，通过配对比较和自助法不确定性进行估计。实验结果表明，AUC的单调性代价范围从基本为零到约2.9%：在大型数据集上约束几乎无成本（通常低于0.2%，常与零无异），而在约束覆盖广泛的小型数据集上成本最高（约2-3%）。因此，适当指定的单调性约束通常能以较小的准确性损失提供可解释性，特别是在大规模信用组合中。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b992ef93"
  },
  {
    "title": "The Impact of Bitcoin ETF Approval on Bitcoin's Hedging Properties Against Traditional Assets",
    "url": "https://arxiv.org/pdf/2512.12815v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "The approval of the Bitcoin Spot ETF in January 2024 marked a transformative event in cryptocurrency markets, signaling increased institutional adoption and integration into traditional finance. This study examines Bitcoin's changing relationships with traditional assets, including equities, gold, and fiat currencies, following this milestone. Using rolling correlation analysis, Chow tests, and DCC-GARCH models, we found that Bitcoin's correlation with the S\\&P 500 increased significantly post-ETF approval, indicating stronger alignment with equities. Its relationship with gold stabilized near zero, while its correlation with the U.S. Dollar Index remained consistently negative, reflecting its continued independence from fiat currencies. These findings offer insights into Bitcoin's evolving role in portfolios, implications for market stability, and future research opportunities on cryptocurrency integration into traditional financial systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "2024年1月比特币现货ETF获批标志着加密货币市场进入机构化与主流金融整合的新阶段。本研究通过滚动相关性分析、邹检验和DCC-GARCH模型，实证分析了该事件后比特币与传统资产（股票、黄金、法币）的动态关系。核心发现：比特币与标普500指数的相关性显著增强，表明其与股票市场联动性提升；与黄金的相关性稳定在零值附近；与美元指数的负相关性持续，保持对法币的独立性。这些结果为投资组合配置、市场稳定性评估及加密货币与传统金融体系融合研究提供了实证依据。",
    "fetch_date": "2025-12-26",
    "id": "20251226_aa031b92"
  },
  {
    "title": "Institutionalizing risk curation in decentralized credit",
    "url": "https://arxiv.org/pdf/2512.11976v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This paper maps the emerging market for decentralized credit in which ERC 4626 vaults and third-party curators, rather than monolithic lending protocols alone, increasingly determine underwriting and leverage decisions. We show that modular vaults differ in capital utilization, cross-chain and cross asset concentration, and liquidity risk structure. Further, we show that a small set of curators intermediates a disproportionate share of system TVL, exhibits clustered tail co movement, and captures markedly different fee margins despite broadly similar collateral composition. These findings indicate that the main locus of risk in DeFi lending has migrated upward from base protocols, where underwriting is effectively centralized in a single DAO governed parameter set, to a permissionless curator layer in which competing vault managers decide which assets and loans are originated. We argue that this shift requires a corresponding upgrade in transparency standards and outline a simple set of onchain disclosures that would allow users and DAOs to evaluate curator strategies on a comparable, money market style basis.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了去中心化信贷市场中ERC 4626金库和第三方策展人（而非单一借贷协议）日益主导承销和杠杆决策的新兴格局。研究发现模块化金库在资本利用率、跨链跨资产集中度及流动性风险结构方面存在差异，少数策展人中介了不成比例的系统总锁定价值（TVL），表现出聚集性尾部联动，且尽管抵押品构成相似却获得显著不同的费用边际。这表明DeFi借贷的主要风险点已从基础协议（承销权集中于单一DAO治理参数集）上移至无需许可的策展层，由竞争性金库管理者决定资产和贷款的发起。作者主张需相应提升透明度标准，并提出一套简单的链上披露方案，使用户和DAO能以可比的货币市场风格评估策展策略。",
    "fetch_date": "2025-12-26",
    "id": "20251226_470ff790"
  },
  {
    "title": "Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling",
    "url": "https://arxiv.org/pdf/2512.12526v1",
    "source": "ArXiv",
    "date": "2025-12-14",
    "abstract": "This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究对MSCI世界指数应用经验模态分解（EMD），将得到的本征模态函数（IMFs）转化为图表示，以便用图神经网络（GNNs）建模。使用CEEMDAN提取了九个IMF，涵盖高频波动到长期趋势。每个IMF通过四种时间序列到图的方法（自然可见性、水平可见性、递归图和转移图）转化为图。拓扑分析显示明显的尺度依赖结构：高频IMF产生密集、高度连接的小世界图，而低频IMF产生更稀疏、特征路径长度更长的网络。基于可见性的方法对振幅变化更敏感，通常产生更高的聚类，而递归图更好地保留了时间依赖性。这些结果为设计针对分解成分结构特性的GNN架构提供了指导，支持更有效的金融时间序列预测建模。",
    "fetch_date": "2025-12-26",
    "id": "20251226_17fa87b6"
  },
  {
    "title": "Explainable Prediction of Economic Time Series Using IMFs and Neural Networks",
    "url": "https://arxiv.org/pdf/2512.12499v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "This study investigates the contribution of Intrinsic Mode Functions (IMFs) derived from economic time series to the predictive performance of neural network models, specifically Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. To enhance interpretability, DeepSHAP is applied, which estimates the marginal contribution of each IMF while keeping the rest of the series intact. Results show that the last IMFs, representing long-term trends, are generally the most influential according to DeepSHAP, whereas high-frequency IMFs contribute less and may even introduce noise, as evidenced by improved metrics upon their removal. Differences between MLP and LSTM highlight the effect of model architecture on feature relevance distribution, with LSTM allocating importance more evenly across IMFs.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本研究探讨了从经济时间序列中提取的本征模态函数（IMFs）对神经网络模型（特别是多层感知机MLP和长短期记忆网络LSTM）预测性能的贡献。为增强可解释性，应用DeepSHAP方法评估每个IMF的边际贡献，同时保持序列其余部分不变。结果表明，根据DeepSHAP分析，代表长期趋势的最后几个IMF通常最具影响力，而高频IMF贡献较小甚至可能引入噪声——移除这些高频分量后模型指标得到改善。MLP与LSTM之间的差异凸显了模型架构对特征相关性分布的影响，其中LSTM在IMF间的权重分配更为均匀。",
    "fetch_date": "2025-12-26",
    "id": "20251226_b8bacb63"
  },
  {
    "title": "Unified Approach to Portfolio Optimization using the `Gain Probability Density Function' and Applications",
    "url": "https://arxiv.org/pdf/2512.11649v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "This article proposes a unified framework for portfolio optimization (PO), recognizing an object called the `gain probability density function (PDF)' as the fundamental object of the problem from which any objective function could be derived. The gain PDF has the advantage of being 1-dimensional for any given portfolio and thus is easy to visualize and interpret. The framework allows us to naturally incorporate all existing approaches (Markowitz, CVaR-deviation, higher moments...) and represents an interesting basis to develop new approaches. It leads us to propose a method to directly match a target PDF defined by the portfolio manager, giving them maximal control on the PO problem and moving beyond approaches that focus only on expected return and risk. As an example, we develop an application involving a new objective function to control high profits, to be applied after a conventional PO (including expected return and risk criteria) and thus leading to sub-optimality w.r.t. the conventional objective function. We then propose a methodology to quantify a cost associated with this optimality deviation in a common budget unit, providing a meaningful information to portfolio managers. Numerical experiments considering portfolios with energy-producing assets illustrate our approach. The framework is flexible and can be applied to other sectors (financial assets, etc).",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了一种基于“收益概率密度函数（PDF）”的统一投资组合优化框架，该框架将收益PDF视为问题的基本对象，任何目标函数均可从中推导。该PDF具有一维特性，易于可视化和解释，能够自然整合现有方法（如马科维茨、CVaR-偏差、高阶矩等），并为开发新方法提供了基础。文章提出了一种直接匹配投资组合经理定义的目标PDF的方法，使其能超越仅关注预期收益和风险的传统方法，对优化问题实现最大控制。作为示例，文章开发了一种在传统优化后控制高利润的新目标函数应用，这会导致相对于传统目标函数的次优性，并提出了一种以通用预算单位量化这种最优性偏差成本的方法，为投资组合经理提供有意义的信息。",
    "fetch_date": "2025-12-26",
    "id": "20251226_32dbca85"
  },
  {
    "title": "Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall",
    "url": "https://arxiv.org/pdf/2512.12334v1",
    "source": "ArXiv",
    "date": "2025-12-13",
    "abstract": "In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文扩展了动态贝叶斯网络（DBN）在计算市场风险中的应用，用于估计10天97.5%的预期损失（ES）和压力预期损失（SES）。研究以标普500指数为代理，比较了三种DBN结构学习算法与多种传统市场风险模型（使用正态或偏斜t分布）的表现。回测显示，所有模型在2.5%水平下均未能产生统计上准确的ES和SES预测，反映了建模极端尾部行为的困难。对于ES，EGARCH(1,1)模型（正态）预测最准确；对于SES，GARCH(1,1)模型（正态）表现最佳。所有依赖分布的模型在使用偏斜t分布时性能显著下降。DBN的表现与历史模拟法相当。",
    "fetch_date": "2025-12-26",
    "id": "20251226_5b6423f9"
  },
  {
    "title": "Universal Dynamics of Financial Bubbles in Isolated Markets: Evidence from the Iranian Stock Market",
    "url": "https://arxiv.org/pdf/2512.12054v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "Speculative bubbles exhibit common statistical signatures across many financial markets, suggesting the presence of universal underlying mechanisms. We test this hypothesis in the Iranian stock market, an economy that is highly isolated, subject to capital controls, and largely inaccessible to foreign investors. Using the Log-Periodic Power Law Singularity (LPPLS) model, we analyze two major bubble episodes in 2020 and 2023. The estimated critical exponents beta around 0.46 and 0.20 fall within the empirical ranges documented for canonical historical bubbles such as the 1929 DJIA crash and the 2000 Nasdaq episode. The Tehran Stock Exchange displays clear LPPLS hallmarks, including faster-than-exponential price acceleration, log-periodic corrections, and stable estimates of the critical time horizon. These results indicate that endogenous herding, imitation, and positive-feedback dynamics, rather than exogenous shocks, play a dominant role even in politically and economically isolated markets. By showing that an emerging and semi-closed financial system conforms to the same dynamical patterns observed in global markets, this paper provides new empirical support for the universality of bubble dynamics. To the best of our knowledge, it also presents the first systematic LPPLS analysis of bubbles in the Tehran Stock Exchange. The findings highlight the usefulness of LPPLS-based diagnostic tools for monitoring systemic risk in emerging or restricted economies.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文通过应用对数周期幂律奇异性（LPPLS）模型，分析了伊朗股市（一个高度孤立、受资本管制且外国投资者难以进入的市场）在2020年和2023年的两次主要泡沫事件。研究发现，其估算的关键指数β值（约0.46和0.20）与历史上经典泡沫（如1929年道琼斯工业平均指数崩盘和2000年纳斯达克泡沫）的经验范围一致。德黑兰证券交易所显示出清晰的LPPLS特征，包括快于指数的价格加速、对数周期性修正以及关键时间范围的稳定估计。结果表明，即使是在政治和经济上孤立的市场中，内生的羊群效应、模仿行为和正反馈动态，而非外生冲击，也起着主导作用。通过展示一个新兴的半封闭金融体系遵循与全球市场相同的动态模式，该论文为泡沫动力学的普适性提供了新的实证支持。",
    "fetch_date": "2025-12-26",
    "id": "20251226_16dc055e"
  },
  {
    "title": "High-Frequency Analysis of a Trading Game with Transient Price Impact",
    "url": "https://arxiv.org/pdf/2512.11765v1",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "We study the high-frequency limit of an $n$-trader optimal execution game in discrete time. Traders face transient price impact of Obizhaeva--Wang type in addition to quadratic instantaneous trading costs $θ(ΔX_t)^2$ on each transaction $ΔX_t$. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval $[0,T]$, the discrete equilibrium inventories converge at rate $1/N$ to the continuous-time equilibrium of an Obizhaeva--Wang model with additional quadratic costs $\\vartheta_0(ΔX_0)^2$ and $\\vartheta_T(ΔX_T)^2$ on initial and terminal block trades, where $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case $n=2$ where $\\vartheta_0=\\vartheta_T=1/2$. In particular, we show how the coefficients $\\vartheta_0=(n-1)/2$ and $\\vartheta_T=1/2$ arise endogenously in the high-frequency limit: the initial and terminal block costs of the continuous-time model are identified as the limits of the cumulative discrete instantaneous costs incurred over small neighborhoods of $0$ and $T$, respectively, and these limits are independent of $θ>0$. By contrast, when $θ=0$ the discrete-time equilibrium strategies and costs exhibit persistent oscillations and admit no high-frequency limit, mirroring the non-existence of continuous-time equilibria without boundary block costs. Our results show that two different types of trading frictions -- a fine time discretization and small instantaneous costs in continuous time -- have similar regularizing effects and select a canonical model in the limit.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文研究了具有瞬态价格影响的n交易者最优执行博弈的高频极限。交易者面临Obizhaeva-Wang型瞬态价格影响以及每笔交易ΔX_t的二次瞬时交易成本θ(ΔX_t)^2。存在唯一的纳什均衡，交易者选择最小化预期执行成本的清算策略。在高频极限下，离散均衡库存以1/N的速率收敛于具有额外二次成本ϑ_0(ΔX_0)^2和ϑ_T(ΔX_T)^2的Obizhaeva-Wang模型的连续时间均衡，其中ϑ_0=(n-1)/2，ϑ_T=1/2。该结果扩展并改进了先前针对n=2情况的研究。",
    "fetch_date": "2025-12-26",
    "id": "20251226_54f3bc58"
  },
  {
    "title": "Risk Limited Asset Allocation with a Budget Threshold Utility Function and Leptokurtotic Distributions of Returns",
    "url": "https://arxiv.org/pdf/2512.11666v2",
    "source": "ArXiv",
    "date": "2025-12-12",
    "abstract": "An analytical solution to single-horizon asset allocation for an investor with a piecewise-linear utility function, called herein the \"budget threshold utility,\" and exogenous position limits is presented. The resulting functional form has a surprisingly simple structure and can be readily interpreted as representing the addition of a simple \"risk cost\" to otherwise frictionless trading.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对具有预算阈值效用函数和外部头寸限制的投资者，提出了单期资产配置的解析解。该解具有简洁的结构，可解释为在无摩擦交易基础上增加了简单的“风险成本”。",
    "fetch_date": "2025-12-26",
    "id": "20251226_2daddc47"
  },
  {
    "title": "Sources and Nonlinearity of High Volume Return Premium: An Empirical Study on the Differential Effects of Investor Identity versus Trading Intensity (2020-2024)",
    "url": "https://arxiv.org/pdf/2512.14134v2",
    "source": "ArXiv",
    "date": "2025-12-16",
    "abstract": "Chae and Kang (2019, \\textit{Pacific-Basin Finance Journal}) documented a puzzling Low Volume Return Premium (LVRP) in Korea -- contradicting global High Volume Return Premium (HVRP) evidence. We resolve this puzzle. Using Korean market data (2020-2024), we demonstrate that HVRP exists in Korea but is masked by (1) pooling heterogeneous investor types and (2) using inappropriate intensity normalization. When institutional buying intensity is normalized by market capitalization rather than trading value, a perfect monotonic relationship emerges: highest-conviction institutional buying (Q4) generates +\\institutionLedQFourDayPlusFiftyCAR\\ cumulative abnormal returns over 50 days, while lowest-intensity trades (Q1) yield modest returns (+\\institutionLedQOneDayPlusFiftyCAR). Retail investors exhibit a flat pattern -- their trading generates near-zero returns regardless of conviction level -- confirming the pure noise trader hypothesis. During the Donghak Ant Movement (2020-2021), however, coordinated retail investors temporarily transformed from noise traders to liquidity providers, generating returns comparable to institutional trading. Our findings reconcile conflicting international evidence and demonstrate that detecting informed trading signals requires investor-type decomposition, nonlinear quartile analysis, and conviction-based (market cap) rather than participation-based (trading value) measurement.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "该研究（2020-2024）通过分析韩国市场数据，解决了韩国市场低成交量回报溢价（LVRP）与全球高成交量回报溢价（HVRP）证据相矛盾的谜题。研究发现，当按市值（而非交易额）对机构买入强度进行标准化时，HVRP在韩国市场显现：机构最高确信度买入（Q4）在50天内产生显著累积异常收益（+\\institutionLedQFourDayPlusFiftyCAR），而最低强度交易（Q1）收益较低（+\\institutionLedQOneDayPlusFiftyCAR）。散户交易则呈现平坦模式，收益接近零，符合纯噪声交易者假说；但在东学蚂蚁运动（2020-2021）期间，协调行动的散户暂时转变为流动性提供者，产生与机构相当的收益。研究通过区分投资者身份（机构vs散户）和交易强度标准化方法，调和了国际上的矛盾发现，对实战交易中识别机构驱动信号、优化因子构建具有直接价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_63fdd5c4"
  },
  {
    "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
    "url": "https://arxiv.org/pdf/2512.12924v1",
    "source": "ArXiv",
    "date": "2025-12-15",
    "abstract": "We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
    "broker": "Cornell Univ",
    "score": 8,
    "summary": "本文提出了一种可解释的假设驱动交易框架，采用严格的向前滚动验证方法，旨在减少过拟合和前瞻性偏差。该框架结合了可解释的假设驱动信号生成与强化学习，并执行严格的样本外测试。方法包括：严格执行信息集纪律、在34个独立测试期进行滚动窗口验证、通过自然语言假设解释保持完全可解释性、纳入实际交易成本和头寸约束。在2015-2024年间对100只美国股票验证五种市场微观结构模式，系统产生适中的年化收益（0.55%，夏普比率0.33），具有出色的下行保护（最大回撤-2.76%）和市场中性特征（beta=0.058）。表现呈现强烈的制度依赖性，在高波动期（2020-2024年，季度收益0.60%）产生正收益，而在稳定市场（2015-2019年，-0.16%）表现不佳。报告统计上不显著的总体结果（p值0.34），以展示一个可重复、诚实的验证协议，优先考虑可解释性。",
    "fetch_date": "2025-12-25",
    "id": "20251225_8db06af9"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading: Challenges and opportunities",
    "url": "https://ieeexplore.ieee.org/abstract/document/9779600/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… rise of quantitative trading (QT), which refers to automatically generating trading signals with … for more than 70% and 40% of the trading volume in developed markets (eg, USA) and …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了深度强化学习在量化交易中的应用，量化交易指通过算法自动生成交易信号，在发达市场（如美国）已占据超过70%和40%的交易量。论文分析了深度强化学习在量化交易领域面临的挑战与机遇，对实战交易具有较高参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_e3153d4f"
  },
  {
    "title": "Trademaster: A holistic quantitative trading platform empowered by reinforcement learning",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b8f6f7f2ba4137124ac976286eacb611-Abstract-Datasets_and_Benchmarks.html",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… As shown in Figure 2, we formulate quantitative trading tasks as a Markov Decision Process (MDP) following a standard RL scenario, where an agent (investor) interacts with an …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "Trademaster是一个基于强化学习的综合性量化交易平台。该研究将量化交易任务建模为马尔可夫决策过程，遵循标准强化学习框架，其中智能体（投资者）与环境交互进行决策。",
    "fetch_date": "2025-12-25",
    "id": "20251225_fe85737d"
  },
  {
    "title": "FinRL: Deep reinforcement learning framework to automate trading in quantitative finance",
    "url": "https://dl.acm.org/doi/abs/10.1145/3490354.3494366",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this paper, we present the first open-source framework FinRL as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文提出了首个开源框架FinRL，作为一个完整的流程，旨在帮助量化交易员克服陡峭的学习曲线。FinRL以简洁性为特点，利用深度强化学习自动化量化金融中的交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_f95bf03c"
  },
  {
    "title": "Deep reinforcement learning for quantitative trading",
    "url": "https://ieeexplore.ieee.org/abstract/document/10626209/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… In this section, we explore the distinctive attributes of Quantitative Trading (QT) and elaborate on the rationale behind framing the entire QT process as a Partially Observable Markov …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了将深度强化学习应用于量化交易，将整个量化交易过程建模为部分可观测马尔可夫决策过程，具有实战价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_d085e88f"
  },
  {
    "title": "Reinforcement Learning Framework for Quantitative Trading",
    "url": "https://arxiv.org/abs/2411.07585",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… We investigate how a reinforcement learning agent can utilize financial indicators in specific market conditions and trends to enhance overall trading accuracy. By understanding the …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "本文研究强化学习智能体如何利用特定市场条件和趋势下的金融指标来提升整体交易准确性。通过理解市场动态，该框架旨在优化量化交易策略。",
    "fetch_date": "2025-12-25",
    "id": "20251225_c5574310"
  },
  {
    "title": "Deep reinforcement learning in quantitative algorithmic trading: A review",
    "url": "https://arxiv.org/abs/2106.00123",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… Deep Reinforcement Learning (DRL) agents proved to be to … reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading…",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "深度强化学习在量化算法交易中的应用综述：该论文聚焦于深度强化学习（DRL）在金融人工智能子领域——特别是自动化低频量化股票交易中的实际应用。研究表明，DRL智能体在该领域展现出潜力，通过结合强化学习与深度学习技术，探索在实战交易中生成阿尔法（Alpha）的策略，具有较高的实践参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_307e048c"
  },
  {
    "title": "Quantitative trading on stock market based on deep reinforcement learning",
    "url": "https://ieeexplore.ieee.org/abstract/document/8851831/",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… intelligence, quantitative trading attracts … reinforcement learning in quantitative trading. A LSTM-based agent is proposed to learn the temporal pattern in data and automatically trades …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文提出了一种基于深度强化学习的股票市场量化交易方法，采用LSTM智能体学习数据中的时序模式，实现自动化交易。",
    "fetch_date": "2025-12-25",
    "id": "20251225_92f14b99"
  },
  {
    "title": "Deep Learning in Quantitative Trading",
    "url": "https://www.cambridge.org/core/elements/deep-learning-in-quantitative-trading/C39DE06D255470F6232BC97E2E5474E7",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… for developing deep learning algorithms for quantitative trading. This … deep learning algorithms to various financial problems. One of the most fundamental tasks in quantitative trading …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨深度学习在量化交易中的应用，重点研究开发深度学习算法以解决金融问题，特别是量化交易中的核心任务。",
    "fetch_date": "2025-12-25",
    "id": "20251225_410d6f5e"
  },
  {
    "title": "Portfolio Optimization for Index Tracking with Constraints on Downside Risk and Carbon Footprint",
    "url": "https://arxiv.org/pdf/2512.21092v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "Historically, financial risk management has mostly addressed risk factors that arise from the financial environment. Climate risks present a novel and significant challenge for companies and financial markets. Investors aiming for avoidance of firms with high carbon footprints require suitable risk measures and portfolio management strategies. This paper presents the construction of decarbonized indices for tracking the S \\& P-500 index of the U.S. stock market, as well as the Indian index NIFTY-50, employing two distinct methodologies and study their performances. These decarbonized indices optimize the portfolio weights by minimizing the mean-VaR and mean-ES and seek to reduce the risk of significant financial losses while still pursuing decarbonization goals. Investors can thereby find a balance between financial performance and environmental responsibilities. Ensuring transparency in the development of these indices will encourage the excluded and under-weighted asset companies to lower their carbon footprints through appropriate action plans. For long-term passive investors, these indices may present a more favourable option than green stocks.",
    "broker": "Cornell Univ",
    "score": 5,
    "summary": "本文提出了两种构建去碳化指数的方法，用于跟踪美国标普500指数和印度NIFTY-50指数。通过最小化均值-风险价值（mean-VaR）和均值-期望损失（mean-ES）来优化投资组合权重，旨在降低重大财务损失风险的同时实现去碳化目标。投资者可借此在财务绩效与环境责任间取得平衡，这些指数为长期被动投资者提供了比绿色股票更优的选择，并可能激励高碳排企业降低碳足迹。",
    "fetch_date": "2025-12-25",
    "id": "20251225_9ee0c108"
  },
  {
    "title": "Reinforcement learning in quantitative trading: A survey",
    "url": "https://www.techrxiv.org/doi/full/10.36227/techrxiv.19303853",
    "source": "Scholar",
    "date": "2025-12-25",
    "abstract": "… two concepts on quantitative trading evolved with time along with the emergence of RL. To this end, we devote one section to discuss the literature of quantitative trading with the tools of …",
    "broker": "Google Scholar",
    "score": 5,
    "summary": "该论文是一篇关于强化学习在量化交易中应用的综述性研究，主要梳理了随着强化学习发展而演变的两个量化交易概念，并专门用一节讨论了使用相关工具的量化交易文献。作为综述，它提供了理论框架和文献梳理，但缺乏具体的实战策略、代码实现或可验证的Alpha生成方法，因此对直接实战交易的价值有限，更适合作为理论参考。",
    "fetch_date": "2025-12-25",
    "id": "20251225_c48f4542"
  },
  {
    "title": "Discrete-time asset price bubbles with short sales prohibitions under model uncertainty",
    "url": "https://arxiv.org/pdf/2512.21115v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "In this study, we investigate asset price bubbles in a discrete-time, discrete-state market under model uncertainty and short sales prohibitions. Building on a new fundamental theorem of asset pricing and a superhedging duality in this setting, we introduce a notion of bubble based on a novel definition of the fundamental price, and analyze their types and characterization. We show that two distinct types of bubbles arise, depending on the maturity structure of the asset. For assets with bounded maturity and no dividend payments, the $G$-supermartingale property of prices provides a necessary and sufficient condition for the existence of bubbles. In contrast, when maturity is unbounded, the infi-supermartingale property yields a necessary condition, while the $G$-supermartingale property remains sufficient. Moreover, there is no bubble under a strengthened no dominance condition. As applications, we examine price bubbles for several standard contingent claims. We show that put-call parity generally fails for fundamental prices, whereas it holds for market prices under no dominance assumption. Furthermore, we establish bounds for the fundamental and market prices of American call options in terms of the corresponding European call prices, adjusted by the associated bubble components.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本研究在离散时间、离散状态市场下，结合模型不确定性和卖空禁令，探讨资产价格泡沫。基于该设定下的新基本资产定价定理和超对冲对偶性，我们引入了一种基于新定义的基本价格的泡沫概念，并分析了其类型和特征。研究表明，根据资产的期限结构，会出现两种不同类型的泡沫。对于有界期限且无股息支付的资产，价格的G-上鞅性质为泡沫存在提供了必要且充分条件。相比之下，当期限无界时，infi-上鞅性质产生必要条件，而G-上鞅性质仍为充分条件。此外，在强化的无支配条件下不存在泡沫。作为应用，我们检验了几种标准或有债权的价格泡沫。研究表明，基本价格下的看跌-看涨平价通常不成立，而在无支配假设下市场价格则成立。此外，我们建立了美式看涨期权的基本价格和市场价格的界限。",
    "fetch_date": "2025-12-25",
    "id": "20251225_d9f8f8a4"
  },
  {
    "title": "Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal",
    "url": "https://arxiv.org/pdf/2512.20850v1",
    "source": "ArXiv",
    "date": "2025-12-24",
    "abstract": "We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文针对限价订单簿中的做市商，研究了结合随机控制和脉冲控制的最优做市问题。该问题被表述为Hamilton-Jacobi-Bellman拟变分不等式（HJBQVI）。作者提出了一种隐式时间离散化方案，并结合策略迭代算法。该方法消除了显式方法典型的时间步长限制，确保了无条件稳定性。通过验证单调性、稳定性和一致性条件，并应用比较原理，建立了收敛于唯一黏性解的理论基础。",
    "fetch_date": "2025-12-25",
    "id": "20251225_aea7403d"
  },
  {
    "title": "Fixed-Income Pricing and the Replication of Liabilities",
    "url": "https://arxiv.org/pdf/2512.14662v2",
    "source": "ArXiv",
    "date": "2025-12-16",
    "abstract": "This paper develops a model-free framework for static fixed-income pricing and the replication of liability cash flows. We show that the absence of static arbitrage across a universe of fixed-income instruments is equivalent to the existence of a strictly positive discount curve that reproduces all observed market prices. We then study the replication and super-replication of liabilities and establish conditions ensuring the existence of least-cost super-replicating portfolios, including a rigorous interpretation of swap--repo replication within this static framework. The results provide a unified foundation for discount-curve construction and liability-driven investment, with direct relevance for economic capital assessment and regulatory practice.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文提出了一种无模型的静态固定收益定价框架及负债现金流复制方法。研究表明，固定收益工具间不存在静态套利等价于存在严格为正的贴现曲线能重现所有市场价格。进一步探讨了负债的复制与超复制，建立了确保最低成本超复制组合存在的条件，并对互换-回购复制进行了严谨解释。该结果为贴现曲线构建和负债驱动投资提供了统一理论基础，对经济资本评估和监管实践具有直接参考价值。",
    "fetch_date": "2025-12-25",
    "id": "20251225_63fda1dc"
  },
  {
    "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
    "url": "https://arxiv.org/pdf/2512.16251v2",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "我们提出了共识瓶颈资产定价模型（CB-APM），这是一种部分可解释的神经网络，通过捕捉分散的投资者信念如何通过共识形成过程压缩为资产价格，来复制卖方分析师推理过程。该模型通过建模这一“瓶颈”来汇总公司和宏观层面的信息，不仅预测美国股票的未来风险溢价，还在结构上以可解释的方式将信念聚合与预期回报联系起来。CB-APM改进了长期回报预测，在预测准确性和解释力方面均优于标准深度学习方法。全面的投资组合分析表明，CB-APM的样本外预测转化为具有经济意义的回报，具有单调的回报差异和跨正则化设置的稳定多空表现。实证上，CB-APM利用共识作为正则化器来增强长期可预测性，并产生基于共识的可解释组件，阐明信息如何在回报中定价。此外，回归和基于Gibbons-Ross-Shanken（GRS）的定价诊断揭示了所学内容。",
    "fetch_date": "2025-12-24",
    "id": "20251224_da05e7f2"
  },
  {
    "title": "Reinforcement learning for quantitative trading",
    "url": "https://dl.acm.org/doi/abs/10.1145/3582560",
    "source": "Scholar",
    "date": "2025-12-24",
    "abstract": "… we used are reinforcement learning, quantitative finance, algorithmic trading, portfolio … a brief overview of financial markets and quantitative trading. Then, we introduce the preliminaries …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "该论文探讨了强化学习在量化交易中的应用，涵盖量化金融、算法交易和投资组合管理等领域。文章首先概述了金融市场和量化交易的基本概念，随后介绍了强化学习的理论基础及其在交易策略优化中的实际应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_3a476d40"
  },
  {
    "title": "Adaptive quantitative trading: An imitative deep reinforcement learning approach",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5587",
    "source": "Scholar",
    "date": "2025-12-24",
    "abstract": "… mathematical symbols then formally introduce the quantitative trading problem in detail. … always the goal of quantitative traders. This further proves that our trading agent can benefit from …",
    "broker": "Google Scholar",
    "score": 8,
    "summary": "《自适应量化交易：一种模仿深度强化学习方法》提出了一种基于深度强化学习的交易代理，能够从市场数据中学习并自适应调整交易策略。该方法通过模仿学习结合强化学习框架，旨在实现持续盈利这一量化交易的核心目标。论文表明该交易代理能够从历史数据中受益并优化交易决策，具有较高的实战应用潜力。",
    "fetch_date": "2025-12-24",
    "id": "20251224_766e09bf"
  },
  {
    "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
    "url": "https://arxiv.org/pdf/2512.18317v1",
    "source": "ArXiv",
    "date": "2025-12-20",
    "abstract": "This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\\,\\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种可信赖的强化学习方法，用于工业压缩空气系统的控制。该框架能够在实际边界条件下实现安全、节能的运行，并引入了结合输入扰动测试、基于梯度的敏感性分析和SHAP（Shapley Additive exPlanations）特征归因的多层次可解释性管道。对多种压缩机配置的实证评估表明，学习到的策略在物理上是合理的，能够预测未来需求，并始终尊重系统边界。与已安装的工业控制器相比，所提出的方法减少了不必要的过压，在不依赖显式物理模型的情况下实现了约4%的节能。结果进一步表明，系统压力和预测信息主导了策略决策，而压缩机级别的输入则起次要作用。总体而言，效率提升、预测行为和透明验证的结合支持了强化学习在工业能源系统中的可信赖部署。",
    "fetch_date": "2025-12-24",
    "id": "20251224_4eec8973"
  },
  {
    "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection",
    "url": "https://arxiv.org/pdf/2512.16411v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文研究了经验相对熵的分布，推导了有限样本的集中不等式、渐近分布以及前渐近状态下的Berry-Esseen界，并提出了一种基于相对熵的变点检测方法。通过模拟和真实数据（包括股票指数波动率）验证了该方法相对于基于矩或信息准则的传统方法的实用性。",
    "fetch_date": "2025-12-24",
    "id": "20251224_c7720f56"
  },
  {
    "title": "Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities",
    "url": "https://arxiv.org/pdf/2512.16080v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "在去中心化金融（DeFi）中，由于时间相关的复杂性，设计固定收益借贷自动做市商（AMM）极具挑战性。现有协议仅支持单一期限借贷。本文基于BondMM协议，论证其数学不变量足够优雅，可推广至任意期限。因此，本文提出改进设计BondMM-A，支持任何期限的借贷活动。通过将不同期限的固定收益工具整合到单一智能合约中，BondMM-A为用户和流动性提供者（LPs）提供更大的操作自由和资本效率。实验结果表明，BondMM-A在利率稳定性和金融稳健性方面表现优异。",
    "fetch_date": "2025-12-24",
    "id": "20251224_c04304c9"
  },
  {
    "title": "Multi-Objective Bayesian Optimization of Deep Reinforcement Learning for Environmental, Social, and Governance (ESG) Financial Portfolio Management",
    "url": "https://arxiv.org/pdf/2512.14992v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "DRL agents circumvent the issue of classic models in the sense that they do not make assumptions like the financial returns being normally distributed and are able to deal with any information like the ESG score if they are configured to gain a reward that makes an objective better. However, the performance of DRL agents has high variability and it is very sensible to the value of their hyperparameters. Bayesian optimization is a class of methods that are suited to the optimization of black-box functions, that is, functions whose analytical expression is unknown, are noisy and expensive to evaluate. The hyperparameter tuning problem of DRL algorithms perfectly suits this scenario. As training an agent just for one objective is a very expensive period, requiring millions of timesteps, instead of optimizing an objective being a mixture of a risk-performance metric and an ESG metric, we choose to separate the objective and solve the multi-objective scenario to obtain an optimal Pareto set of portfolios representing the best tradeoff between the Sharpe ratio and the ESG mean score of the portfolio and leaving to the investor the choice of the final portfolio. We conducted our experiments using environments encoded within the OpenAI Gym, adapted from the FinRL platform. The experiments are carried out in the Dow Jones Industrial Average (DJIA) and the NASDAQ markets in terms of the Sharpe ratio achieved by the agent and the mean ESG score of the portfolio. We compare the performance of the obtained Pareto sets in hypervolume terms illustrating how portfolios are the best trade-off between the Sharpe ratio and mean ESG score. Also, we show the usefulness of our proposed methodology by comparing the obtained hypervolume with one achieved by a Random Search methodology on the DRL hyperparameter space.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种用于环境、社会和治理（ESG）金融投资组合管理的多目标贝叶斯优化深度强化学习方法。深度强化学习（DRL）代理无需假设金融收益服从正态分布，并能处理ESG评分等信息，但其性能对超参数高度敏感。贝叶斯优化适用于优化黑盒函数，DRL的超参数调优正符合此场景。由于单目标训练成本高昂（需数百万时间步），作者将目标分离，通过多目标优化获得代表夏普比率与投资组合ESG平均评分之间最佳权衡的帕累托最优投资组合集，最终由投资者选择具体组合。",
    "fetch_date": "2025-12-24",
    "id": "20251224_e237fd82"
  },
  {
    "title": "Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning",
    "url": "https://arxiv.org/pdf/2512.17185v1",
    "source": "ArXiv",
    "date": "2025-12-19",
    "abstract": "Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.\n  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.\n  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "《系统性风险雷达：用于早期市场崩盘预警的多层图框架》提出了一种将金融市场建模为多层图以检测系统性脆弱性和崩盘状态转换早期迹象的框架。该研究评估了SRR在互联网泡沫、全球金融危机和COVID-19冲击三个重大危机中的表现，实验表明结构网络信息相比纯特征模型能提供更有用的早期预警信号。虽然该框架展示了图衍生特征在压力事件期间捕捉市场结构有意义变化的能力，但当前实现主要基于相关性分析，属于理论验证阶段。论文建议通过添加更多图层（行业/因子暴露、情绪）和更强大的时序架构（LSTM/GRU或Transformer编码器）来扩展SRR，这为实战交易系统开发提供了有价值的理论框架和方向指引。",
    "fetch_date": "2025-12-24",
    "id": "20251224_7b111b14"
  },
  {
    "title": "An Efficient Machine Learning Framework for Option Pricing via Fourier Transform",
    "url": "https://arxiv.org/pdf/2512.16115v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "The increasing need for rapid recalibration of option pricing models in dynamic markets places stringent computational demands on data generation and valuation algorithms. In this work, we propose a hybrid algorithmic framework that integrates the smooth offset algorithm (SOA) with supervised machine learning models for the fast pricing of multiple path-independent options under exponential Lévy dynamics. Building upon the SOA-generated dataset, we train neural networks, random forests, and gradient boosted decision trees to construct surrogate pricing operators. Extensive numerical experiments demonstrate that, once trained, these surrogates achieve order-of-magnitude acceleration over direct SOA evaluation. Importantly, the proposed framework overcomes key numerical limitations inherent to fast Fourier transform-based methods, including the consistency of input data and the instability in deep out-of-the-money option pricing.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "动态市场中期权定价模型快速重新校准的需求日益增长，这对数据生成和估值算法提出了严格的计算要求。本文提出了一种混合算法框架，将平滑偏移算法（SOA）与监督机器学习模型相结合，用于在指数Lévy动态下快速定价多种路径无关期权。基于SOA生成的数据集，我们训练神经网络、随机森林和梯度提升决策树来构建替代定价算子。大量数值实验表明，一旦训练完成，这些替代模型相比直接SOA评估实现了数量级的加速。重要的是，所提出的框架克服了基于快速傅里叶变换方法固有的关键数值限制，包括输入数据的一致性和深度虚值期权定价的不稳定性。",
    "fetch_date": "2025-12-24",
    "id": "20251224_21c43b5a"
  },
  {
    "title": "SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs",
    "url": "https://arxiv.org/pdf/2512.15088v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文提出SigMA（Signature Multi-head Attention）神经网络架构，将路径签名与多头自注意力机制结合，用于学习分数布朗运动驱动的随机微分方程参数。该方法针对具有粗糙动态和长程依赖的系统建模，如量化金融中的分数布朗运动、分数Ornstein-Uhlenbeck和粗糙Heston模型。研究聚焦于通过合成生成路径进行参数估计，探讨路径签名在深度学习架构中如何平衡估计精度与模型复杂度。",
    "fetch_date": "2025-12-24",
    "id": "20251224_a46aba2c"
  },
  {
    "title": "Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes",
    "url": "https://arxiv.org/pdf/2512.14991v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "本文研究针对具有无界连续状态空间、有界连续动作和多项式增长奖励的受控扩散过程的强化学习——这些设定自然出现在金融、经济和运筹学中。为克服连续高维领域的挑战，我们提出一种基于模型的算法，自适应地划分联合状态-动作空间。该算法在每个分区内维护漂移、波动率和奖励的估计器，并在估计偏差超过统计置信度时细化离散化。这种自适应方案平衡了探索与近似，使得在无界域中高效学习成为可能。我们的分析建立了遗憾界，其依赖于问题时域、状态维度、奖励增长阶数以及为无界扩散过程定制的新定义的“缩放维度”概念。这些界限将现有有界设定结果作为特例恢复，同时将理论保证扩展到更广泛的扩散类问题。最后，我们通过数值实验验证了方法的有效性，包括在高维问题中的应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_140ee511"
  },
  {
    "title": "Modelling financial time series with $φ^{4}$ quantum field theory",
    "url": "https://arxiv.org/pdf/2512.17225v1",
    "source": "ArXiv",
    "date": "2025-12-19",
    "abstract": "We use a $φ^{4}$ quantum field theory with inhomogeneous couplings and explicit symmetry-breaking to model an ensemble of financial time series from the S$\\&$P 500 index. The continuum nature of the $φ^4$ theory avoids the inaccuracies that occur in Ising-based models which require a discretization of the time series. We demonstrate this using the example of the 2008 global financial crisis. The $φ^{4}$ quantum field theory is expressive enough to reproduce the higher-order statistics such as the market kurtosis, which can serve as an indicator of possible market shocks. Accurate reproduction of high kurtosis is absent in binarized models. Therefore Ising models, despite being widely employed in econophysics, are incapable of fully representing empirical financial data, a limitation not present in the generalization of the $φ^{4}$ scalar field theory. We then investigate the scaling properties of the $φ^{4}$ machine learning algorithm and extract exponents which govern the behavior of the learned couplings (or weights and biases in ML language) in relation to the number of stocks in the model. Finally, we use our model to forecast the price changes of the AAPL, MSFT, and NVDA stocks. We conclude by discussing how the $φ^{4}$ scalar field theory could be used to build investment strategies and the possible intuitions that the QFT operations of dimensional compactification and renormalization can provide for financial modelling.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文使用具有非均匀耦合和显式对称性破缺的φ⁴量子场论来建模标普500指数中的金融时间序列集合。与需要离散化时间序列的伊辛模型相比，φ⁴理论的连续性质避免了不准确性，并能重现高阶统计量（如市场峰度），可作为市场冲击的指标。论文还研究了φ⁴机器学习算法的标度特性，并提取了控制学习耦合（或ML中的权重和偏置）与模型中股票数量关系的指数。最后，使用该模型预测了AAPL、MSFT和NVDA股票的价格变化。",
    "fetch_date": "2025-12-24",
    "id": "20251224_1cb49c57"
  },
  {
    "title": "Global universal approximation with Brownian signatures",
    "url": "https://arxiv.org/pdf/2512.16396v1",
    "source": "ArXiv",
    "date": "2025-12-18",
    "abstract": "We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文在加权粗糙路径空间上建立了L^p型全局通用逼近定理，证明了对时间扩展粗糙路径的签名进行线性泛函作用在L^p距离意义下是稠密的。特别地，这些定理适用于布朗运动，因此布朗运动时间扩展签名的线性泛函可以逼近任何适应于布朗滤波的p可积随机过程，包括随机微分方程的解。",
    "fetch_date": "2025-12-24",
    "id": "20251224_94b3ca35"
  },
  {
    "title": "Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions",
    "url": "https://arxiv.org/pdf/2512.15113v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Long-term price forecasting remains a formidable challenge due to the inherent uncertainty over the long term, despite some success in short-term predictions. Nonetheless, accurate long-term forecasts are essential for high-net-worth individuals, institutional investors, and traders. The proposed improved genetic algorithm-optimized support vector regression (IGA-SVR) model is specifically designed for long-term price prediction of global indices. The performance of the IGA-SVR model is rigorously evaluated and compared against the state-of-the-art baseline models, the Long Short-Term Memory (LSTM), and the forward-validating genetic algorithm optimized support vector regression (OGA-SVR). Extensive testing was conducted on the five global indices, namely Nifty, Dow Jones Industrial Average (DJI), DAX Performance Index (DAX), Nikkei 225 (N225), and Shanghai Stock Exchange Composite Index (SSE) from 2021 to 2024 of daily price prediction up to a year. Overall, the proposed IGA-SVR model achieved a reduction in MAPE by 19.87% compared to LSTM and 50.03% compared to OGA-SVR, demonstrating its superior performance in long-term daily price forecasting of global indices. Further, the execution time for LSTM was approximately 20 times higher than that of IGA-SVR, highlighting the high accuracy and computational efficiency of the proposed model. The genetic algorithm selects the optimal hyperparameters of SVR by minimizing the arithmetic mean of the Mean Absolute Percentage Error (MAPE) calculated over the full training dataset and the most recent five years of training data. This purposefully designed training methodology adjusts for recent trends while retaining long-term trend information, thereby offering enhanced generalization compared to the LSTM and rolling-forward validation approach employed by OGA-SVR, which forgets long-term trends and suffers from recency bias.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文提出了一种改进的遗传算法优化支持向量回归（IGA-SVR）模型，专门用于全球股票指数的长期价格预测。在2021年至2024年期间，对Nifty、道琼斯工业平均指数（DJI）、DAX绩效指数（DAX）、日经225指数（N225）和上证综合指数（SSE）等五个全球指数进行了长达一年的每日价格预测测试。结果表明，与LSTM和OGA-SVR相比，IGA-SVR模型在平均绝对百分比误差（MAPE）上分别降低了19.87%和50.03%，显示出其在长期预测中的优越性能。",
    "fetch_date": "2025-12-24",
    "id": "20251224_b3187dce"
  },
  {
    "title": "Arbitrage-Free Pricing with Diffusion-Dependent Jumps",
    "url": "https://arxiv.org/pdf/2512.15071v1",
    "source": "ArXiv",
    "date": "2025-12-17",
    "abstract": "Standard jump-diffusion models assume independence between jumps and diffusion components. We develop a multi-type jump-diffusion model where jump occurrence and magnitude depend on contemporaneous diffusion movements. Unlike previous one-sided models that create arbitrage opportunities, our framework includes upward and downward jumps triggered by both large upward and large downward diffusion increments. We derive the explicit no-arbitrage condition linking the physical drift to model parameters and market risk premia by constructing an Equivalent Martingale Measure using Girsanov's theorem and a normalized Esscher transform. This condition provides a rigorous foundation for arbitrage-free pricing in models with diffusion-dependent jumps.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "标准跳跃扩散模型假设跳跃与扩散成分相互独立。本文开发了一种多类型跳跃扩散模型，其中跳跃的发生和幅度取决于同期的扩散运动。与以往可能产生套利机会的单边模型不同，我们的框架包含由大幅向上和向下扩散增量触发的向上和向下跳跃。通过使用Girsanov定理和归一化Esscher变换构建等价鞅测度，我们推导出将物理漂移与模型参数及市场风险溢价联系起来的明确无套利条件。该条件为具有扩散依赖跳跃的模型中的无套利定价提供了严格基础。",
    "fetch_date": "2025-12-24",
    "id": "20251224_33dbfdbc"
  },
  {
    "title": "Optimal Signal Extraction from Order Flow: A Matched Filter Perspective on Normalization and Market Microstructure",
    "url": "https://arxiv.org/pdf/2512.18648v1",
    "source": "ArXiv",
    "date": "2025-12-21",
    "abstract": "We demonstrate that the choice of normalization for order flow intensity is fundamental to signal extraction in finance, not merely a technical detail. Through theoretical modeling, Monte Carlo simulation, and empirical validation using Korean market data, we prove that market capitalization normalization acts as a ``matched filter'' for informed trading signals, achieving 1.32--1.97$\\times$ higher correlation with future returns compared to traditional trading value normalization. The key insight is that informed traders scale positions by firm value (market capitalization), while noise traders respond to daily liquidity (trading volume), creating heteroskedastic corruption when normalizing by trading volume. By reframing the normalization problem using signal processing theory, we show that dividing order flow by market capitalization preserves the information signal while traditional volume normalization multiplies the signal by inverse turnover -- a highly volatile quantity. Our theoretical predictions are robust across parameter specifications and validated by empirical evidence showing 482\\% improvement in explanatory power. These findings have immediate implications for high-frequency trading algorithms, risk factor construction, and information-based trading strategies.",
    "broker": "Cornell Univ",
    "score": 9,
    "summary": "本文论证了订单流强度归一化方法的选择对信号提取至关重要，而不仅仅是技术细节。通过理论建模、蒙特卡洛模拟和韩国市场数据的实证验证，证明市值归一化可作为知情交易信号的“匹配滤波器”，相比传统的交易额归一化，与未来收益的相关性提高1.32-1.97倍。核心观点是：知情交易者按公司价值（市值）调整头寸，而噪声交易者则响应日流动性（交易量），使用交易量归一化会导致异方差干扰。通过信号处理理论重新构建归一化问题，发现除以市值能保留信息信号，而传统的交易量归一化会将信号乘以逆换手率——一个高度波动的变量。理论预测在不同参数设定下均稳健，实证证据显示解释力提升482%。这些发现对高频交易、算法交易和风险模型具有直接应用价值。",
    "fetch_date": "2025-12-24",
    "id": "20251224_eb1783a9"
  },
  {
    "title": "Switching between states and the COVID-19 turbulence",
    "url": "https://arxiv.org/pdf/2512.20477v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "In Aarab (2020), I examine U.S. stock return predictability across economic regimes and document evidence of time-varying expected returns across market states in the long run. The analysis introduces a state-switching specification in which the market state is proxied by the slope of the yield curve, and proposes an Aligned Economic Index built from the popular predictors of Welch and Goyal (2008) (augmented with bond and equity premium measures). The Aligned Economic Index under the state-switching model exhibits statistically and economically meaningful in-sample ($R^2 = 5.9\\%$) and out-of-sample ($R^2_{\\text{oos}} = 4.12\\%$) predictive power across both recessions and expansions, while outperforming a range of widely used predictors. In this work, I examine the added value for professional practitioners by computing the economic gains for a mean-variance investor and find substantial added benefit of using the new index under the state switching model across all market states. The Aligned Economic Index can thus be implemented on a consistent real-time basis. These findings are crucial for both academics and practitioners as expansions are much longer-lived than recessions. Finally, I extend the empirical exercises by incorporating data through September 2020 and document sizable gains from using the Aligned Economic Index, relative to more traditional approaches, during the COVID-19 market turbulence.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文通过引入状态切换模型（以收益率曲线斜率作为市场状态代理变量）和构建对齐经济指数（基于Welch和Goyal（2008）的经典预测因子，并加入债券和股权溢价指标），实证显示该指数在样本内（R²=5.9%）和样本外（R²oos=4.12%）均对美股收益具有统计及经济意义的预测能力，且优于多种常用预测因子。研究进一步通过均值-方差投资者模型验证了该模型在所有市场状态下均能带来显著经济收益，并强调该指数可实时应用于实践。这些发现对从业者尤为重要，因为经济扩张期远长于衰退期。",
    "fetch_date": "2025-12-24",
    "id": "20251224_a39154aa"
  },
  {
    "title": "The Aligned Economic Index & The State Switching Model",
    "url": "https://arxiv.org/pdf/2512.20460v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \\parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \\textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文研究了美国股票收益在不同经济状态下的可预测性，提出了两个对实战交易有价值的贡献：一是引入基于收益率曲线斜率实时定义市场状态的状态切换预测回归模型，相比传统单状态模型显著提高了样本内和样本外预测性能；二是通过偏最小二乘法构建了新的聚合预测指标——对齐经济指数，在状态切换模型下展现出统计和经济意义上显著的样本内外预测能力。论文聚焦于经济状态依赖的股票溢价预测，其状态切换模型和对齐经济指数可直接应用于实战中的择时和风险管理策略。",
    "fetch_date": "2025-12-24",
    "id": "20251224_23ae3263"
  },
  {
    "title": "GIFfluence: A Visual Approach to Investor Sentiment and the Stock Market",
    "url": "https://arxiv.org/pdf/2512.20027v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "We study dynamic visual representations as a proxy for investor sentiment about the stock market. Our sentiment index, GIFsentiment, is constructed from millions of posts in the Graphics Interchange Format (GIF) on a leading investment social media platform. GIFsentiment correlates with seasonal mood variations and the severity of COVID lockdowns. It is positively associated with contemporaneous market returns and negatively predicts returns for up to four weeks, even after controlling for other sentiment and attention measures. These effects are stronger among portfolios that are more susceptible to mispricing. GIFsentiment positively predicts trading volume, market volatility, and flows toward equity funds and away from debt funds. Our evidence suggests that GIFsentiment is a proxy for misperceptions that are later corrected.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种基于投资社交媒体平台GIF帖子的投资者情绪指标GIFsentiment。研究发现：1）该指标与季节性情绪波动和疫情封锁严重程度相关；2）与同期市场收益正相关，并能负向预测未来四周收益（控制其他情绪指标后仍显著）；3）对易受错误定价影响的投资组合预测效果更强；4）能正向预测交易量、市场波动率以及资金从债券基金流向股票基金。证据表明GIFsentiment可作为市场错误认知的代理指标，这些认知后续会被修正。",
    "fetch_date": "2025-12-24",
    "id": "20251224_78339dde"
  },
  {
    "title": "Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization",
    "url": "https://arxiv.org/pdf/2512.19986v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "本文提出了一种用于基数约束投资组合优化的协方差感知单纯形投影方法。针对元启发式算法中修复算子将不可行候选解映射到可行区域时忽略协方差结构的问题，该方法采用两阶段修复：首先基于波动率归一化评分选择目标资产数量，然后使用与跟踪误差风险对齐的协方差感知几何进行权重投影。在S&P 500数据上的实证表明，该方法在不依赖收益预测的情况下显著降低了投资组合方差，且改进具有统计显著性。消融实验显示波动率归一化选择贡献了主要方差降低，而协方差感知投影提供了额外的稳定改进。",
    "fetch_date": "2025-12-24",
    "id": "20251224_432db6dc"
  },
  {
    "title": "Needles in a haystack: using forensic network science to uncover insider trading",
    "url": "https://arxiv.org/pdf/2512.18918v1",
    "source": "ArXiv",
    "date": "2025-12-21",
    "abstract": "Although the automation and digitisation of anti-financial crime investigation has made significant progress in recent years, detecting insider trading remains a unique challenge, partly due to the limited availability of labelled data. To address this challenge, we propose using a data-driven networks approach that flags groups of corporate insiders who report coordinated transactions that are indicative of insider trading. Specifically, we leverage data on 2.9 million trades reported to the U.S. Securities and Exchange Commission (SEC) by company insiders (C-suite executives, board members and major shareholders) between 2014 and 2024. Our proposed algorithm constructs weighted edges between insiders based on the temporal similarity of their trades over the 10-year timeframe. Within this network we then uncover trends that indicate insider trading by focusing on central nodes and anomalous subgraphs. To highlight the validity of our approach we evaluate our findings with reference to two null models, generated by running our algorithm on synthetic empirically calibrated and shuffled datasets. The results indicate that our approach can be used to detect pairs or clusters of insiders whose behaviour suggests insider trading and/or market manipulation.",
    "broker": "Cornell Univ",
    "score": 7,
    "summary": "该论文提出了一种利用法证网络科学检测内幕交易的数据驱动网络方法。通过分析2014-2024年间美国证券交易委员会（SEC）收到的290万笔公司内部人（高管、董事会成员和大股东）交易报告，该算法基于交易时间相似性构建内部人之间的加权边网络，并通过识别中心节点和异常子图来揭示内幕交易模式。研究使用合成经验校准和随机数据集生成的零模型验证了方法的有效性，表明该方法能有效检测协同交易的内幕交易对或集群。",
    "fetch_date": "2025-12-24",
    "id": "20251224_944d270d"
  },
  {
    "title": "Quantitative Financial Modeling for Sri Lankan Markets: Approach Combining NLP, Clustering and Time-Series Forecasting",
    "url": "https://arxiv.org/pdf/2512.20216v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "This research introduces a novel quantitative methodology tailored for quantitative finance applications, enabling banks, stockbrokers, and investors to predict economic regimes and market signals in emerging markets, specifically Sri Lankan stock indices (S&P SL20 and ASPI) by integrating Environmental, Social, and Governance (ESG) sentiment analysis with macroeconomic indicators and advanced time-series forecasting. Designed to leverage quantitative techniques for enhanced risk assessment, portfolio optimization, and trading strategies in volatile environments, the architecture employs FinBERT, a transformer-based NLP model, to extract sentiment from ESG texts, followed by unsupervised clustering (UMAP/HDBSCAN) to identify 5 latent ESG regimes, validated via PCA. These regimes are mapped to economic conditions using a dense neural network and gradient boosting classifier, achieving 84.04% training and 82.0% validation accuracy. Concurrently, time-series models (SRNN, MLP, LSTM, GRU) forecast daily closing prices, with GRU attaining an R-squared of 0.801 and LSTM delivering 52.78% directional accuracy on intraday data. A strong correlation between S&P SL20 and S&P 500, observed through moving average and volatility trend plots, further bolsters forecasting precision. A rule-based fusion logic merges ESG and time-series outputs for final market signals. By addressing literature gaps that overlook emerging markets and holistic integration, this quant-driven framework combines global correlations and local sentiment analysis to offer scalable, accurate tools for quantitative finance professionals navigating complex markets like Sri Lanka.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该研究为斯里兰卡市场（S&P SL20和ASPI指数）提出了一种新颖的量化金融建模方法，通过整合ESG情感分析（使用FinBERT模型）、宏观经济指标和先进的时间序列预测技术（包括SRNN、MLP、LSTM、GRU），旨在预测经济状态和市场信号。该方法采用无监督聚类（UMAP/HDBSCAN）识别出5个潜在的ESG状态，并通过密集神经网络和梯度提升分类器将其映射到经济条件，训练和验证准确率分别达到84.04%和82.0%。在时间序列预测方面，GRU模型的R平方为0.801，LSTM在日内数据上的方向准确性为52.78%。研究还观察到S&P SL20与S&P 500之间存在强相关性。该框架旨在增强风险评估、投资组合优化和交易策略，适用于波动环境中的新兴市场。",
    "fetch_date": "2025-12-24",
    "id": "20251224_6cec85b9"
  },
  {
    "title": "Pricing of wrapped Bitcoin and Ethereum on-chain options",
    "url": "https://arxiv.org/pdf/2512.20190v1",
    "source": "ArXiv",
    "date": "2025-12-23",
    "abstract": "This paper measures price differences between Hegic option quotes on Arbitrum and a model-based benchmark built on Black--Scholes model with regime-sensitive volatility estimated via a two-regime MS-AR-(GJR)-GARCH model. Using option-level feasible GLS, we find benchmark prices exceed Hegic quotes on average, especially for call options. The price spread rises with order size, strike, maturity, and estimated volatility, and falls with trading volume. By underlying, wrapped Bitcoin options show larger and more persistent spreads, while Ethereum options are closer to the benchmark. The framework offers a data-driven analysis for monitoring and calibrating on-chain option pricing logic.",
    "broker": "Cornell Univ",
    "score": 6,
    "summary": "该论文通过对比Hegic在Arbitrum上的期权报价与基于两区制MS-AR-(GJR)-GARCH模型估计波动率的Black-Scholes基准模型，发现基准价格普遍高于Hegic报价（尤其是看涨期权）。价差随订单规模、行权价、期限和估计波动率增加而扩大，随交易量增加而缩小。其中，包装比特币期权价差更大且更持久，以太坊期权则更接近基准。该框架为监控和校准链上期权定价逻辑提供了数据驱动的分析方法。",
    "fetch_date": "2025-12-24",
    "id": "20251224_6243b8b8"
  },
  {
    "title": "Equilibrium Liquidity and Risk Offsetting in Decentralised Markets",
    "url": "https://arxiv.org/pdf/2512.19838v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "We develop an economic model of decentralised exchanges (DEXs) in which risk-averse liquidity providers (LPs) manage risk in a centralised exchange (CEX) based on preferences, information, and trading costs. Rational, risk-averse LPs anticipate the frictions associated with replication and manage risk primarily by reducing the reserves supplied to the DEX. Greater aversion reduces the equilibrium viability of liquidity provision, resulting in thinner markets and lower trading volumes. Greater uninformed demand supports deeper liquidity, whereas higher fundamental price volatility erodes it. Finally, while moderate anticipated price changes can improve LP performance, larger changes require more intensive trading in the CEX, generate higher replication costs, and induce LPs to reduce liquidity supply.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文构建了一个去中心化交易所（DEX）的经济模型，研究风险厌恶的流动性提供者（LPs）如何基于偏好、信息与交易成本，在中心化交易所（CEX）管理风险。理性且风险厌恶的LPs会预判与复制相关的摩擦，主要通过减少向DEX提供的储备来管理风险。更高的风险厌恶会降低流动性提供的均衡可行性，导致市场更薄、交易量更低。更大的非知情需求支持更深的流动性，而更高的基础价格波动则会削弱流动性。最后，虽然适度的预期价格变动可能改善LP表现，但更大的变动需要在CEX进行更密集的交易，产生更高的复制成本，并促使LPs减少流动性供给。",
    "fetch_date": "2025-12-24",
    "id": "20251224_27c66c3f"
  },
  {
    "title": "How to choose my stochastic volatility parameters? A review",
    "url": "https://arxiv.org/pdf/2512.19821v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Based on the existing literature, this article presents the different ways of choosing the parameters of stochastic volatility models in general, in the context of pricing financial derivative contracts. This includes the use of stochastic volatility inside stochastic local volatility models.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "基于现有文献，本文综述了在金融衍生品定价背景下，选择随机波动率模型参数的各种方法，包括在随机局部波动率模型中使用随机波动率。",
    "fetch_date": "2025-12-24",
    "id": "20251224_faddc8fe"
  },
  {
    "title": "Counterexamples for FX Options Interpolations -- Part II",
    "url": "https://arxiv.org/pdf/2512.19625v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "This follow-up article analyzes the impact of foreign exchange option interpolation on the vanilla option implied volatilities. In particular different exact interpolations of broker quotes may lead to different implied volatilities at the 10$Δ$ and 25$Δ$ Puts and Calls.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "这篇后续文章分析了外汇期权插值对普通期权隐含波动率的影响。具体而言，经纪商报价的不同精确插值方法可能导致10Δ和25Δ看跌期权与看涨期权的隐含波动率出现差异。",
    "fetch_date": "2025-12-24",
    "id": "20251224_bee73e28"
  },
  {
    "title": "Counterexamples for FX Options Interpolations -- Part I",
    "url": "https://arxiv.org/pdf/2512.19621v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "This article provides a list of counterexamples, where some of the popular fx option interpolations break down. Interpolation of FX option prices (or equivalently volatilities), is key to risk-manage not only vanilla FX option books, but also more exotic derivatives which are typically valued with local volatility or local stochastic volatilility models.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文列举了外汇期权插值方法失效的反例，指出插值对于风险管理至关重要，不仅涉及普通外汇期权，也影响使用局部波动率或局部随机波动率模型定价的奇异衍生品。",
    "fetch_date": "2025-12-24",
    "id": "20251224_43d5d092"
  },
  {
    "title": "Heston vol-of-vol and the VVIX",
    "url": "https://arxiv.org/pdf/2512.19611v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "The Heston stochastic volatility model is arguably, the most popular stochastic volatility model used to price and risk manage exotic derivatives. In spite of this, it is not necessarily easy to calibrate to the market and obtain stable exotic option prices with this model. This paper focuses on the vol-of-vol parameter and its relation with the volatility of volatility index (VVIX) level. Four different approaches to estimate the VVIX in the Heston model are presented: two based on the known transition density of the variance, one analytical approximation, and one based on the Heston PDE which computes the value directly out of the underlying SPX500. Finally we explore their use to improve calibration stability.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "本文聚焦于Heston随机波动率模型中的波动率波动率参数及其与波动率波动率指数（VVIX）水平的关系。提出了四种在Heston模型中估计VVIX的方法：两种基于已知的方差转移密度，一种解析近似方法，以及一种基于Heston偏微分方程直接从标普500指数计算的方法。最后探讨了这些方法在提高模型校准稳定性方面的应用。",
    "fetch_date": "2025-12-24",
    "id": "20251224_dad4d8e3"
  },
  {
    "title": "Learning General Policies with Policy Gradient Methods",
    "url": "https://arxiv.org/pdf/2512.19366v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了强化学习方法（特别是策略梯度方法）在何种条件下能学习到具有泛化能力的策略，类似于组合方法在经典规划中所实现的。作者结合了组合方法与深度学习的思路，将策略建模为状态转移分类器（因为具体动作会随问题实例变化），并使用图神经网络（GNNs）来处理关系结构以表示规划状态的价值函数和策略。研究旨在弥合强化学习与可证明泛化的组合方法之间的差距，但重点在于理论框架与条件分析，而非直接应用于实战交易场景。",
    "fetch_date": "2025-12-24",
    "id": "20251224_5412cd38"
  },
  {
    "title": "Institutional Backing and Crypto Volatility: A Hybrid Framework for DeFi Stabilization",
    "url": "https://arxiv.org/pdf/2512.19251v1",
    "source": "ArXiv",
    "date": "2025-12-22",
    "abstract": "Decentralized finance (DeFi) lacks centralized oversight, often resulting in heightened volatility. In contrast, centralized finance (CeFi) offers a more stable environment with institutional safeguards. Institutional backing can play a stabilizing role in a hybrid structure (HyFi), enhancing transparency, governance, and market discipline. This study investigates whether HyFi-like cryptocurrencies, those backed by institutions, exhibit lower price risk than fully decentralized counterparts. Using daily data for 18 major cryptocurrencies from January 2020 to November 2024, we estimate panel EGLS models with fixed, random, and dynamic specifications. Results show that HyFi-like assets consistently experience lower price risk, with this effect intensifying during periods of elevated market volatility. The negative interaction between HyFi status and market-wide volatility confirms their stabilizing role. Conversely, greater decentralization is strongly associated with increased volatility, particularly during periods of market stress. Robustness checks using quantile regressions and pre-/post-Terra Luna subsamples reinforce these findings, with stronger effects observed in high-volatility quantiles and post-crisis conditions. These results highlight the importance of institutional architecture in enhancing the resilience of digital asset markets.",
    "broker": "Cornell Univ",
    "score": 4,
    "summary": "该论文探讨了去中心化金融（DeFi）因缺乏中心化监管而波动性较高，而中心化金融（CeFi）通过机构保障提供更稳定环境。研究发现，在混合结构（HyFi）中，机构支持能通过增强透明度、治理和市场纪律发挥稳定作用。基于2020年1月至2024年11月18种主要加密货币的日度数据，面板EGLS模型显示，具有机构支持的HyFi类资产价格风险较低，且在市场波动加剧时期稳定作用更明显。相反，去中心化程度越高，波动性越强，尤其在市场压力时期。分位数回归及Terra Luna事件前后子样本的稳健性检验支持了这些结论。",
    "fetch_date": "2025-12-24",
    "id": "20251224_36795ecf"
  }
]